<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acs.SD%26id_list%3D%26start%3D0%26max_results%3D1100" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:cs.SD&amp;id_list=&amp;start=0&amp;max_results=1100</title>
  <id>http://arxiv.org/api/j9kJs4tEyLZcm3EZs/JPiY5NGUk</id>
  <updated>2025-05-27T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">17053</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1100</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/0902.2783v2</id>
    <updated>2010-01-08T10:28:32Z</updated>
    <published>2009-02-16T21:03:30Z</published>
    <title>New Ica-Beamforming Method to Under-Determined BSS</title>
    <summary>  This paper has been withdrawn by the author ali pourmohammad.
</summary>
    <author>
      <name>Ali Pourmohammad</name>
    </author>
    <author>
      <name>Seyed Mohammad Ahadi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/0902.2783v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.2783v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.08982v1</id>
    <updated>2015-12-30T15:46:30Z</updated>
    <published>2015-12-30T15:46:30Z</published>
    <title>Technical Report: a tool for measuring Prosodic Accommodation</title>
    <summary>  This article has been withdrawn by arXiv administrators because the submitter
did not have the legal authority to grant the license applied to the work.
</summary>
    <author>
      <name>Sucheta Ghosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Withdrawn by arXiv administrators</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.08982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.08982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.3198v1</id>
    <updated>2009-03-18T16:51:25Z</updated>
    <published>2009-03-18T16:51:25Z</published>
    <title>TR02: State dependent oracle masks for improved dynamical features</title>
    <summary>  Using the AURORA-2 digit recognition task, we show that recognition
accuracies obtained with classical, SNR based oracle masks can be substantially
improved by using a state-dependent mask estimation technique.
</summary>
    <author>
      <name>J. F. Gemmeke</name>
    </author>
    <author>
      <name>B. Cranen</name>
    </author>
    <link href="http://arxiv.org/abs/0903.3198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.3198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.08526v1</id>
    <updated>2022-01-21T03:24:27Z</updated>
    <published>2022-01-21T03:24:27Z</published>
    <title>Can Machines Generate Personalized Music? A Hybrid Favorite-aware Method
  for User Preference Music Transfer</title>
    <summary>  User preference music transfer (UPMT) is a new problem in music style
transfer that can be applied to many scenarios but remains understudied.
</summary>
    <author>
      <name>Zhejing Hu</name>
    </author>
    <author>
      <name>Yan Liu</name>
    </author>
    <author>
      <name>Gong Chen</name>
    </author>
    <author>
      <name>Yongxu Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2201.08526v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.08526v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.12642v1</id>
    <updated>2023-05-22T02:32:34Z</updated>
    <published>2023-05-22T02:32:34Z</published>
    <title>The HCCL system for VoxCeleb Speaker Recognition Challenge 2022</title>
    <summary>  This report describes our submission to track1 and track3 for VoxCeleb
Speaker Recognition Challenge 2022(VoxSRC2022). Our best system achieves minDCF
0.1397 and EER 2.414 in track1, minDCF 0.388 and EER 7.030 in track3.
</summary>
    <author>
      <name>Zhenduo Zhao</name>
    </author>
    <author>
      <name>Zhuo Li</name>
    </author>
    <author>
      <name>Wenchao Wang</name>
    </author>
    <author>
      <name>Pengyuan Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2305.12642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.12642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.10578v1</id>
    <updated>2024-04-16T14:02:17Z</updated>
    <published>2024-04-16T14:02:17Z</published>
    <title>Vivo : une approche multimodale de la synthese concatenative par corpus
  dans le cadre d'une oeuvre audiovisuelle immersive</title>
    <summary>  Which visual descriptors are suitable for multi-modal interaction and how to
integrate them via real-time video data analysis into a corpus-based
concatenative synthesis sound system.
</summary>
    <author>
      <name>Mateo Fayet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French language</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.10578v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.10578v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703049v1</id>
    <updated>2007-03-10T23:59:55Z</updated>
    <published>2007-03-10T23:59:55Z</published>
    <title>Algorithm of Segment-Syllabic Synthesis in Speech Recognition Problem</title>
    <summary>  Speech recognition based on the syllable segment is discussed in this paper.
The principal search methods in space of states for the speech recognition
problem by segment-syllabic parameters trajectory synthesis are investigated.
Recognition as comparison the parameters trajectories in chosen speech units on
the sections of the segmented speech is realized. Some experimental results are
given and discussed.
</summary>
    <author>
      <name>Oleg N. Karpov</name>
    </author>
    <author>
      <name>Olga A. Savenkova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0703049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.3538v1</id>
    <updated>2009-11-18T13:36:20Z</updated>
    <published>2009-11-18T13:36:20Z</published>
    <title>Noise Speech wavelet analyzing in special time ranges</title>
    <summary>  Speech analyzing in special periods of time has been presented in this paper.
One of the most important periods in signal processing is near to Zero. By this
paper, we analyze noise speech signals when these signals are near to Zero. Our
strategy is defining some subfunctions and compress histograms when a noise
speech signal is in a special period. It can be so useful for wavelet signal
processing and spoken systems analyzing.
</summary>
    <author>
      <name>Amin Daneshmand Malayeri</name>
    </author>
    <link href="http://arxiv.org/abs/0911.3538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.3538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.6031v1</id>
    <updated>2013-02-25T10:13:09Z</updated>
    <published>2013-02-25T10:13:09Z</published>
    <title>Phoneme discrimination using KS algebra I</title>
    <summary>  In our work we define a new algebra of operators as a substitute for fuzzy
logic. Its primary purpose is for construction of binary discriminators for
phonemes based on spectral content. It is optimized for design of
non-parametric computational circuits, and makes uses of 4 operations: $\min$,
$\max$, the difference and generalized additively homogenuous means.
</summary>
    <author>
      <name>Ondrej Such</name>
    </author>
    <link href="http://arxiv.org/abs/1302.6031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.6031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.5.2; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.6194v1</id>
    <updated>2013-02-25T18:56:49Z</updated>
    <published>2013-02-25T18:56:49Z</published>
    <title>Phoneme discrimination using $KS$-algebra II</title>
    <summary>  $KS$-algebra consists of expressions constructed with four kinds operations,
the minimum, maximum, difference and additively homogeneous generalized means.
Five families of $Z$-classifiers are investigated on binary classification
tasks between English phonemes. It is shown that the classifiers are able to
reflect well known formant characteristics of vowels, while having very small
Kolmogoroff's complexity.
</summary>
    <author>
      <name>Ondrej Such</name>
    </author>
    <author>
      <name>Lenka Mackovicova</name>
    </author>
    <link href="http://arxiv.org/abs/1302.6194v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.6194v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.7866v1</id>
    <updated>2014-05-30T13:58:36Z</updated>
    <published>2014-05-30T13:58:36Z</published>
    <title>Vocal signal digital processing. Instrument for analog to digital
  conversion study</title>
    <summary>  The goal of this article is to present interactive didactic software for
analog to digital conversion using PCM method. After a short introduction
regarding vocal signal processing we present some method for analog to digital
conversion. The didactic software is an applet that can be direct accessed by
any interested person.
</summary>
    <author>
      <name>Ovidiu-Andrei Schipor</name>
    </author>
    <author>
      <name>Felicia-Florentina Giza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures, in Romanian</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.7866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.7866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0117v1</id>
    <updated>2014-08-30T14:51:33Z</updated>
    <published>2014-08-30T14:51:33Z</published>
    <title>Computerized Multi Microphone Test System</title>
    <summary>  An acoustic testing approach based on the concept of a microphone sensor
surrounding the product under test is proposed. Microphone signals are
processed simultaneously by a test system computer, according to the objective
of the test. The spatial and frequency domain selectivity features of this
method are examined. Sound-spatial visualization algorithm is observed. A test
system design based on the concept of a microphone surrounding the tested
product has the potential to improve distortion measurement accuracy.
</summary>
    <author>
      <name>A. M. Dorman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.0117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.07411v1</id>
    <updated>2015-09-24T15:46:55Z</updated>
    <published>2015-09-24T15:46:55Z</published>
    <title>Speech Dereverberation in the STFT Domain</title>
    <summary>  Reverberation is damaging to both the quality and the intelligibility of a
speech signal. We propose a novel single-channel method of dereverberation
based on a linear filter in the Short Time Fourier Transform domain. Each
enhanced frame is constructed from a linear sum of nearby frames based on the
channel impulse response. The results show that the method can resolve any
reverberant signal with knowledge of the impulse response to a non-reverberant
signal.
</summary>
    <author>
      <name>Richard Stanton</name>
    </author>
    <author>
      <name>Mike Brookes</name>
    </author>
    <link href="http://arxiv.org/abs/1509.07411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.07411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07767v1</id>
    <updated>2016-02-25T01:41:34Z</updated>
    <published>2016-02-25T01:41:34Z</published>
    <title>Breath Activity Detection Algorithm</title>
    <summary>  This report describes the use of a support vector machines with a novel
kernel, to determine the breathing rate and inhalation duration of a fire
fighter wearing a Self-Contained Breathing Apparatus. With this information, an
incident commander can monitor the firemen in his command for exhaustion and
ensure timely rotation of personnel to ensure overall fire fighter safety
</summary>
    <author>
      <name>Eric E. Hamke</name>
    </author>
    <author>
      <name>Ramiro Jordan</name>
    </author>
    <author>
      <name>Manel Ramon-Martinez</name>
    </author>
    <link href="http://arxiv.org/abs/1602.07767v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07767v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03296v1</id>
    <updated>2016-09-12T07:50:41Z</updated>
    <published>2016-09-12T07:50:41Z</published>
    <title>A Neural Network Alternative to Non-Negative Audio Models</title>
    <summary>  We present a neural network that can act as an equivalent to a Non-Negative
Matrix Factorization (NMF), and further show how it can be used to perform
supervised source separation. Due to the extensibility of this approach we show
how we can achieve better source separation performance as compared to
NMF-based methods, and propose a variety of derivative architectures that can
be used for further improvements.
</summary>
    <author>
      <name>Paris Smaragdis</name>
    </author>
    <author>
      <name>Shrikant Venkataramani</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01651v1</id>
    <updated>2017-05-03T23:19:56Z</updated>
    <published>2017-05-03T23:19:56Z</published>
    <title>Modeling temporal constraints for a system of interactive scores</title>
    <summary>  In this chapter we explain briefly the fundamentals of the interactive scores
formalism. Then we develop a solution for implementing the ECO machine by
mixing petri nets and constraints propagation. We also present another solution
for implementing the ECO machine using concurrent constraint programming.
Finally, we present an extension of interactive score with conditional
branching.
</summary>
    <author>
      <name>Mauricio Toro</name>
    </author>
    <author>
      <name>Myriam Desainte-Catherine</name>
    </author>
    <author>
      <name>Antoine Allombert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of Book Chapter published in Constraint Programming
  in Music, 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.01651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05322v1</id>
    <updated>2017-05-15T16:34:20Z</updated>
    <published>2017-05-15T16:34:20Z</published>
    <title>Understanding MIDI: A Painless Tutorial on Midi Format</title>
    <summary>  A short overview demystifying the midi audio format is presented. The goal is
to explain the file structure and how the instructions are used to produce a
music signal, both in the case of monophonic signals as for polyphonic signals.
</summary>
    <author>
      <name>H. M. de Oliveira</name>
    </author>
    <author>
      <name>R. C. de Oliveira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.05322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.04486v2</id>
    <updated>2017-06-15T20:09:52Z</updated>
    <published>2017-06-14T13:35:46Z</published>
    <title>Learning and Evaluating Musical Features with Deep Autoencoders</title>
    <summary>  In this work we describe and evaluate methods to learn musical embeddings.
Each embedding is a vector that represents four contiguous beats of music and
is derived from a symbolic representation. We consider autoencoding-based
methods including denoising autoencoders, and context reconstruction, and
evaluate the resulting embeddings on a forward prediction and a classification
task.
</summary>
    <author>
      <name>Mason Bretan</name>
    </author>
    <author>
      <name>Sageev Oore</name>
    </author>
    <author>
      <name>Doug Eck</name>
    </author>
    <author>
      <name>Larry Heck</name>
    </author>
    <link href="http://arxiv.org/abs/1706.04486v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04486v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09557v1</id>
    <updated>2017-06-29T02:52:25Z</updated>
    <published>2017-06-29T02:52:25Z</published>
    <title>Machine listening intelligence</title>
    <summary>  This manifesto paper will introduce machine listening intelligence, an
integrated research framework for acoustic and musical signals modelling, based
on signal processing, deep learning and computational musicology.
</summary>
    <author>
      <name>C. E. Cella</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Conference on Deep Learning
  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Workshop on Deep Learning
  and Music joint with IJCNN. Anchorage, US. 1(1). pp 50-55 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.09557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09758v1</id>
    <updated>2017-06-29T13:51:31Z</updated>
    <published>2017-06-29T13:51:31Z</published>
    <title>Using Second-Order Hidden Markov Model to Improve Speaker Identification
  Recognition Performance under Neutral Condition</title>
    <summary>  In this paper, second-order hidden Markov model (HMM2) has been used and
implemented to improve the recognition performance of text-dependent speaker
identification systems under neutral talking condition. Our results show that
HMM2 improves the recognition performance under neutral talking condition
compared to the first-order hidden Markov model (HMM1). The recognition
performance has been improved by 9%.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <link href="http://arxiv.org/abs/1706.09758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00149v1</id>
    <updated>2017-07-01T12:23:37Z</updated>
    <published>2017-07-01T12:23:37Z</published>
    <title>Modeling and Analyzing the Vocal Tract under Normal and Stressful
  Talking Conditions</title>
    <summary>  In this research, we model and analyze the vocal tract under normal and
stressful talking conditions. This research answers the question of the
degradation in the recognition performance of text-dependent speaker
identification under stressful talking conditions. This research can be used
(for future research) to improve the recognition performance under stressful
talking conditions.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <author>
      <name>Nazeih Botros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE SOUTHEASTCON 2001, Clemson, South Carolina, USA, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.00149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.02322v1</id>
    <updated>2017-08-07T22:18:55Z</updated>
    <published>2017-08-07T22:18:55Z</published>
    <title>Automatic Raga Recognition in Hindustani Classical Music</title>
    <summary>  Raga is the central melodic concept in Hindustani Classical Music. It has a
complex structure, often characterized by pathos. In this paper, we describe a
technique for Automatic Raga Recognition, based on pitch distributions. We are
able to successfully classify ragas with a commendable accuracy on our test
dataset.
</summary>
    <author>
      <name>Sanchit Alekh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Seminar on Computer Music, RWTH Aachen,
  http://hpac.rwth-aachen.de/teaching/sem-mus-17/Reports/Alekh.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.02322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.02322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03603v1</id>
    <updated>2017-12-10T22:47:29Z</updated>
    <published>2017-12-10T22:47:29Z</published>
    <title>A Cascade Architecture for Keyword Spotting on Mobile Devices</title>
    <summary>  We present a cascade architecture for keyword spotting with speaker
verification on mobile devices. By pairing a small computational footprint with
specialized digital signal processing (DSP) chips, we are able to achieve low
power consumption while continuously listening for a keyword.
</summary>
    <author>
      <name>Alexander Gruenstein</name>
    </author>
    <author>
      <name>Raziel Alvarez</name>
    </author>
    <author>
      <name>Chris Thornton</name>
    </author>
    <author>
      <name>Mohammadali Ghodrat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31st Conference on Neural Information Processing Systems (NIPS 2017),
  Long Beach, CA, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.03603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.05337v1</id>
    <updated>2018-03-13T15:58:58Z</updated>
    <published>2018-03-13T15:58:58Z</published>
    <title>Learning to Recognize Musical Genre from Audio</title>
    <summary>  We here summarize our experience running a challenge with open data for
musical genre recognition. Those notes motivate the task and the challenge
design, show some statistics about the submissions, and present the results.
</summary>
    <author>
      <name>Michaël Defferrard</name>
    </author>
    <author>
      <name>Sharada P. Mohanty</name>
    </author>
    <author>
      <name>Sean F. Carroll</name>
    </author>
    <author>
      <name>Marcel Salathé</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to WWW'18 after challenge round-1</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.05337v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.05337v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02939v1</id>
    <updated>2018-08-08T20:59:26Z</updated>
    <published>2018-08-08T20:59:26Z</published>
    <title>Towards Learning Fine-Grained Disentangled Representations from Speech</title>
    <summary>  Learning disentangled representations of high-dimensional data is currently
an active research area. However, compared to the field of computer vision,
less work has been done for speech processing. In this paper, we provide a
review of two representative efforts on this topic and propose the novel
concept of fine-grained disentangled speech representation learning.
</summary>
    <author>
      <name>Yuan Gong</name>
    </author>
    <author>
      <name>Christian Poellabauer</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.04390v1</id>
    <updated>2019-02-12T14:00:59Z</updated>
    <published>2019-02-12T14:00:59Z</published>
    <title>Multitask Learning for Polyphonic Piano Transcription, a Case Study</title>
    <summary>  Viewing polyphonic piano transcription as a multitask learning problem, where
we need to simultaneously predict onsets, intermediate frames and offsets of
notes, we investigate the performance impact of additional prediction targets,
using a variety of suitable convolutional neural network architectures. We
quantify performance differences of additional objectives on the large MAESTRO
dataset.
</summary>
    <author>
      <name>Rainer Kelz</name>
    </author>
    <author>
      <name>Sebastian Böck</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <link href="http://arxiv.org/abs/1902.04390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.04390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.07171v1</id>
    <updated>2020-04-15T15:56:45Z</updated>
    <published>2020-04-15T15:56:45Z</published>
    <title>Musical Features for Automatic Music Transcription Evaluation</title>
    <summary>  This technical report gives a detailed, formal description of the features
introduced in the paper: Adrien Ycart, Lele Liu, Emmanouil Benetos and Marcus
T. Pearce. "Investigating the Perceptual Validity of Evaluation Metrics for
Automatic Piano Music Transcription", Transactions of the International Society
for Music Information Retrieval (TISMIR), Accepted, 2020.
</summary>
    <author>
      <name>Adrien Ycart</name>
    </author>
    <author>
      <name>Lele Liu</name>
    </author>
    <author>
      <name>Emmanouil Benetos</name>
    </author>
    <author>
      <name>Marcus T. Pearce</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.07171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.07171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.05007v1</id>
    <updated>2020-06-09T01:40:57Z</updated>
    <published>2020-06-09T01:40:57Z</published>
    <title>The Hitchhiker's Guide to the All-Interval 12-Tone Rows</title>
    <summary>  This article revisits the generation, classification and categorization of
all-intervals 12-tone series (AIS). Inspired by the seminal work of Morris and
Starr in 1974 (Morris and Starr, The Structure of All-Interval Series 1974), it
expands their analysis using complex network theory and provides composers and
theorists with the re-ordering scheme that links all AISs together by chains of
relations.
</summary>
    <author>
      <name>Marco Buongiorno Nardelli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.05007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.05007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.07904v1</id>
    <updated>2021-03-14T12:11:11Z</updated>
    <published>2021-03-14T12:11:11Z</published>
    <title>Blind Estimation of Room Acoustic Parameters and Speech Transmission
  Index using MTF-based CNNs</title>
    <summary>  This paper proposes a blind estimation method based on the modulation
transfer function and Schroeder model for estimating reverberation time in
seven-octave bands. Therefore, the speech transmission index and five
room-acoustic parameters can be estimated.
</summary>
    <author>
      <name>Suradej Duangpummet</name>
    </author>
    <author>
      <name>Jessada Karnjana</name>
    </author>
    <author>
      <name>Waree Kongprawechnon</name>
    </author>
    <author>
      <name>Masashi Unoki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 10 figures, IEEEtran class</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.07904v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.07904v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.0197v1</id>
    <updated>2008-03-03T09:04:46Z</updated>
    <published>2008-03-03T09:04:46Z</published>
    <title>DSP Based System for Real time Voice Synthesis Applications Development</title>
    <summary>  This paper describes an experimental system designed for development of real
time voice synthesis applications. The system is composed from a DSP
coprocessor card, equipped with an TMS320C25 or TMS320C50 chip, voice
acquisition module (ADDA2),host computer (IBM-PC compatible), software specific
tools.
</summary>
    <author>
      <name>Radu Arsinte</name>
    </author>
    <author>
      <name>Attila Ferencz</name>
    </author>
    <author>
      <name>Costin Miron</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, SPECOM' 96 Conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of SPECOM' 96 Conference, 1996, St. Petersburg, Russia</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.0197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.0197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.1368v1</id>
    <updated>2011-12-06T18:30:13Z</updated>
    <published>2011-12-06T18:30:13Z</published>
    <title>Discovering novel computer music techniques by exploring the space of
  short computer programs</title>
    <summary>  Very short computer programs, sometimes consisting of as few as three
arithmetic operations in an infinite loop, can generate data that sounds like
music when output as raw PCM audio. The space of such programs was recently
explored by dozens of individuals within various on-line communities. This
paper discusses the programs resulting from this exploratory work and
highlights some rather unusual methods they use for synthesizing sound and
generating musical structure.
</summary>
    <author>
      <name>Ville-Matias Heikkilä</name>
    </author>
    <link href="http://arxiv.org/abs/1112.1368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.1368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.10555v1</id>
    <updated>2019-06-25T14:11:14Z</updated>
    <published>2019-06-25T14:11:14Z</published>
    <title>Naver at ActivityNet Challenge 2019 -- Task B Active Speaker Detection
  (AVA)</title>
    <summary>  This report describes our submission to the ActivityNet Challenge at CVPR
2019. We use a 3D convolutional neural network (CNN) based front-end and an
ensemble of temporal convolution and LSTM classifiers to predict whether a
visible person is speaking or not. Our results show significant improvements
over the baseline on the AVA-ActiveSpeaker dataset.
</summary>
    <author>
      <name>Joon Son Chung</name>
    </author>
    <link href="http://arxiv.org/abs/1906.10555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.10555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.10413v2</id>
    <updated>2020-09-30T14:53:06Z</updated>
    <published>2020-08-24T13:16:47Z</published>
    <title>CRNNs for Urban Sound Tagging with spatiotemporal context</title>
    <summary>  This paper describes CRNNs we used to participate in Task 5 of the DCASE 2020
challenge. This task focuses on hierarchical multilabel urban sound tagging
with spatiotemporal context. The code is available on our GitHub repository at
https://github.com/multitel-ai/urban-sound-tagging.
</summary>
    <author>
      <name>Augustin Arnault</name>
    </author>
    <author>
      <name>Nicolas Riche</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures, DCASE2020 Challenge - DCASE</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.10413v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.10413v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.06524v1</id>
    <updated>2020-10-06T17:10:13Z</updated>
    <published>2020-10-06T17:10:13Z</published>
    <title>Principles for Designing Computer Music Controllers</title>
    <summary>  This paper will present observations on the design, artistic, and human
factors of creating digital music controllers. Specific projects will be
presented, and a set of design principles will be supported from those
examples.
</summary>
    <author>
      <name>Perry R. Cook</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1176358</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1176358" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.06524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.06524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.15989v2</id>
    <updated>2020-11-20T00:11:27Z</updated>
    <published>2020-10-29T23:19:58Z</published>
    <title>Latent Space Oddity: Exploring Latent Spaces to Design Guitar Timbres</title>
    <summary>  We introduce a novel convolutional network architecture with an interpretable
latent space for modeling guitar amplifiers. Leveraging domain knowledge of
popular amplifiers spanning a range of styles, the proposed system intuitively
combines or subtracts characteristics of different amplifiers, allowing
musicians to design entirely new guitar timbres.
</summary>
    <author>
      <name>Jason Taylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 figure. To appear in the 2020 NeurIps Workshop on Machine
  Learning for Creativity and Design</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.15989v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.15989v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.08727v1</id>
    <updated>2021-07-19T09:52:57Z</updated>
    <published>2021-07-19T09:52:57Z</published>
    <title>Measuring a Six-hole Recorder Flute's Response to Breath Pressure
  Variations and Fitting a Model</title>
    <summary>  We propose the Siamese-flute method that measures the breath pressure and the
acoustic sound in parallel. We fit a 6-DoF model to describe how the breath
pressure affects the octave and the microtonal pitch bend, revealing the octave
hysteresis. We release both our model parameters and our data analysis tools.
</summary>
    <author>
      <name>Daniel Chin</name>
    </author>
    <author>
      <name>Gus Xia</name>
    </author>
    <link href="http://arxiv.org/abs/2107.08727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.08727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.02099v1</id>
    <updated>2022-01-06T15:30:33Z</updated>
    <published>2022-01-06T15:30:33Z</published>
    <title>Implementing simple spectral denoising for environmental audio
  recordings</title>
    <summary>  This technical report details changes applied to a noise filter to facilitate
its application and improve its results. The filter is applied to denoise
natural sounds recorded in the wild and to generate an acoustic index used in
soundscape analysis.
</summary>
    <author>
      <name>Fábio Felix Dias</name>
    </author>
    <author>
      <name>Moacir Antonelli Ponti</name>
    </author>
    <author>
      <name>Rosane Minghim</name>
    </author>
    <link href="http://arxiv.org/abs/2201.02099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.02099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.02785v1</id>
    <updated>2022-09-06T19:13:09Z</updated>
    <published>2022-09-06T19:13:09Z</published>
    <title>Read it to me: An emotionally aware Speech Narration Application</title>
    <summary>  In this work we try to perform emotional style transfer on audios. In
particular, MelGAN-VC architecture is explored for various emotion-pair
transfers. The generated audio is then classified using an LSTM-based emotion
classifier for audio. We find that "sad" audio is generated well as compared to
"happy" or "anger" as people have similar expressions of sadness.
</summary>
    <author>
      <name>Rishibha Bansal</name>
    </author>
    <link href="http://arxiv.org/abs/2209.02785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.02785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.11225v1</id>
    <updated>2022-11-21T07:40:01Z</updated>
    <published>2022-11-21T07:40:01Z</published>
    <title>TimbreCLIP: Connecting Timbre to Text and Images</title>
    <summary>  We present work in progress on TimbreCLIP, an audio-text cross modal
embedding trained on single instrument notes. We evaluate the models with a
cross-modal retrieval task on synth patches. Finally, we demonstrate the
application of TimbreCLIP on two tasks: text-driven audio equalization and
timbre to image generation.
</summary>
    <author>
      <name>Nicolas Jonason</name>
    </author>
    <author>
      <name>Bob L. T. Sturm</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to AAAI workshop on creative AI across modalities</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.11225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.11225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.02610v2</id>
    <updated>2022-12-07T09:46:01Z</updated>
    <published>2022-12-05T21:51:33Z</published>
    <title>Audio Latent Space Cartography</title>
    <summary>  We explore the generation of visualisations of audio latent spaces using an
audio-to-image generation pipeline. We believe this can help with the
interpretability of audio latent spaces. We demonstrate a variety of results on
the NSynth dataset. A web demo is available.
</summary>
    <author>
      <name>Nicolas Jonason</name>
    </author>
    <author>
      <name>Bob L. T. Sturm</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Late Breaking / Demo, ISMIR 2022
  (https://ismir2022program.ismir.net/lbd_413.html)</arxiv:comment>
    <link href="http://arxiv.org/abs/2212.02610v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.02610v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.15601v2</id>
    <updated>2025-05-08T14:53:57Z</updated>
    <published>2023-05-24T22:21:13Z</published>
    <title>Metamathematics of Algorithmic Composition</title>
    <summary>  This essay recounts my personal journey towards a deeper understanding of the
mathematical foundations of algorithmic music composition. I do not spend much
time on specific mathematical algorithms used by composers; rather, I focus on
general issues such as fundamental limits and possibilities, by analogy with
metalogic, metamathematics, and computability theory. I discuss implications
from these foundations for the future of algorithmic composition.
</summary>
    <author>
      <name>Michael Gogins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 0 figures. Comments are very welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.15601v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.15601v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.19304v1</id>
    <updated>2023-05-30T15:42:13Z</updated>
    <published>2023-05-30T15:42:13Z</published>
    <title>Audio classification using ML methods</title>
    <summary>  Machine Learning systems have achieved outstanding performance in different
domains. In this paper machine learning methods have been applied to
classification task to classify music genre. The code shows how to extract
features from audio files and classify them using supervised learning into 2
genres namely classical and metal. Algorithms used are LogisticRegression, SVC
using different kernals (linear, sigmoid, rbf and poly), KNeighborsClassifier ,
RandomForestClassifier, DecisionTreeClassifier and GaussianNB.
</summary>
    <author>
      <name>Krishna Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.19304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.19304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.13691v1</id>
    <updated>2023-06-23T00:36:53Z</updated>
    <published>2023-06-23T00:36:53Z</published>
    <title>Modulation Graphs in Popular Music</title>
    <summary>  In this paper, graph theory is used to explore the musical notion of tonal
modulation, in theory and application. We define (pivot) modulation graphs
based on the common scales used in popular music. Properties and parameters of
these graphs are discussed. We also investigate modulation graphs for the canon
of Lennon-McCartney songs in the works of The Beatles. Our approach may provide
composers with mathematical insights into pivot modulation.
</summary>
    <author>
      <name>Jason I. Brown</name>
    </author>
    <author>
      <name>Ian George</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 6 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.13691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.13691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="05C90" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.10113v1</id>
    <updated>2023-11-16T02:55:13Z</updated>
    <published>2023-11-16T02:55:13Z</published>
    <title>AQUATK: An Audio Quality Assessment Toolkit</title>
    <summary>  Recent advancements in Neural Audio Synthesis (NAS) have outpaced the
development of standardized evaluation methodologies and tools. To bridge this
gap, we introduce AquaTk, an open-source Python library specifically designed
to simplify and standardize the evaluation of NAS systems. AquaTk offers a
range of audio quality metrics, including a unique Python implementation of the
basic PEAQ algorithm, and operates in multiple modes to accommodate various
user needs.
</summary>
    <author>
      <name>Ashvala Vinay</name>
    </author>
    <author>
      <name>Alexander Lerch</name>
    </author>
    <link href="http://arxiv.org/abs/2311.10113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.10113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.14239v1</id>
    <updated>2023-11-24T00:48:11Z</updated>
    <published>2023-11-24T00:48:11Z</published>
    <title>Allpass impulse response modelling</title>
    <summary>  This document defines a method for FIR system modelling which is very trivial
as it only depends on phase introduction and removal (allpass filters). As
magnitude is not altered, the processing is numerically stable. It is limited
to phase alteration which maintains the time domain magnitude to force a system
within its linear limits.
</summary>
    <author>
      <name>Matt R. Flax</name>
    </author>
    <link href="http://arxiv.org/abs/2311.14239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.14239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.09321v1</id>
    <updated>2024-03-14T12:10:47Z</updated>
    <published>2024-03-14T12:10:47Z</published>
    <title>A Practical Guide to Spectrogram Analysis for Audio Signal Processing</title>
    <summary>  The paper summarizes spectrogram and gives practical application of
spectrogram in signal processing. For analysis, finger-snapping is recorded
with a sampling rate of 441000 Hz and 96000 Hz. The effects of the number of
segments on the Power Spectral Density (PSD) and spectrogram are analyzed and
visualized.
</summary>
    <author>
      <name>Zulfidin Khodzhaev</name>
    </author>
    <link href="http://arxiv.org/abs/2403.09321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.09321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.09426v1</id>
    <updated>2024-04-15T03:16:20Z</updated>
    <published>2024-04-15T03:16:20Z</published>
    <title>Analyzing phonetic structure of Mandarin using Audacity</title>
    <summary>  Mandarin Chinese is the official language in China, Taiwan, and Singapore. It
is also the main non-official language spoken predominantly at home in Toronto
and Vancouver. This article employs the audio software Audacity and leverages
theoretical knowledge to conduct a comprehensive analysis of Mandarin Chinese.
The study initiates with an overview of the fundamental principles underlying
Mandarin pronunciation, aiming to provide insights into its phonetic structure.
</summary>
    <author>
      <name>Shizheng Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">audio source: https://leetcafe.com/language-analysis/</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.09426v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.09426v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0005022v2</id>
    <updated>2000-06-12T06:21:40Z</updated>
    <published>2000-05-17T14:30:01Z</published>
    <title>Fractionally-addressed delay lines</title>
    <summary>  While traditional implementations of variable-length digital delay lines are
based on a circular buffer accessed by two pointers, we propose an
implementation where a single fractional pointer is used both for read and
write operations. On modern general-purpose architectures, the proposed method
is nearly as efficient as the popularinterpolated circular buffer, and it
behaves well for delay-length modulations commonly found in digital audio
effects. The physical interpretation of the new implementation shows that it is
suitable for simulating tension or density modulations in wave-propagating
media.
</summary>
    <author>
      <name>Davide Rocchesso</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/89.876310</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/89.876310" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 19 figures, to be published in IEEE Transactions on Speech
  and Audio Processing Corrected ACM-class</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Speech and Audio Processing, vol. 8, no. 6,
  november 2000, pp. 717-727</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0005022v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0005022v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0303025v1</id>
    <updated>2003-03-24T16:01:46Z</updated>
    <published>2003-03-24T16:01:46Z</published>
    <title>Algorithmic Clustering of Music</title>
    <summary>  We present a fully automatic method for music classification, based only on
compression of strings that represent the music pieces. The method uses no
background knowledge about music whatsoever: it is completely general and can,
without change, be used in different areas like linguistic classification and
genomics. It is based on an ideal theory of the information content in
individual objects (Kolmogorov complexity), information distance, and a
universal similarity metric. Experiments show that the method distinguishes
reasonably well between various musical genres and can even cluster pieces by
composer.
</summary>
    <author>
      <name>Rudi Cilibrasi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CWI</arxiv:affiliation>
    </author>
    <author>
      <name>Paul Vitanyi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CWI and University of Amsterdam</arxiv:affiliation>
    </author>
    <author>
      <name>Ronald de Wolf</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CWI</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0303025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0303025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4, H.3.1, I.5.3, F.1.3, J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612139v1</id>
    <updated>2006-12-28T06:45:43Z</updated>
    <published>2006-12-28T06:45:43Z</published>
    <title>Alignment of Speech to Highly Imperfect Text Transcriptions</title>
    <summary>  We introduce a novel and inexpensive approach for the temporal alignment of
speech to highly imperfect transcripts from automatic speech recognition (ASR).
Transcripts are generated for extended lecture and presentation videos, which
in some cases feature more than 30 speakers with different accents, resulting
in highly varying transcription qualities. In our approach we detect a subset
of phonemes in the speech track, and align them to the sequence of phonemes
extracted from the transcript. We report on the results for 4 speech-transcript
sets ranging from 22 to 108 minutes. The alignment performance is promising,
showing a correct matching of phonemes within 10, 20, 30 second error margins
for more than 60%, 75%, 90% of text, respectively, on average.
</summary>
    <author>
      <name>Alexander Haubold</name>
    </author>
    <author>
      <name>John R. Kender</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0612139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; H.5.1; H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701177v4</id>
    <updated>2008-08-17T19:17:05Z</updated>
    <published>2007-01-26T20:07:43Z</published>
    <title>Pitch Tracking of Acoustic Signals based on Average Squared Mean
  Difference Function</title>
    <summary>  In this paper, a method of pitch tracking based on variance minimization of
locally periodic subsamples of an acoustic signal is presented. Replicates
along the length of the periodically sampled data of the signal vector are
taken and locally averaged sample variances are minimized to estimate the
fundamental frequency. Using this method, pitch tracking of any text
independent voiced signal is possible for different speakers.
</summary>
    <author>
      <name>Roudra Chakraborty</name>
    </author>
    <author>
      <name>Debapriya Sengupta</name>
    </author>
    <author>
      <name>Sagnik Sinha</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0701177v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701177v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.2416v1</id>
    <updated>2009-01-16T08:24:14Z</updated>
    <published>2009-01-16T08:24:14Z</published>
    <title>TR01: Time-continuous Sparse Imputation</title>
    <summary>  An effective way to increase the noise robustness of automatic speech
recognition is to label noisy speech features as either reliable or unreliable
(missing) prior to decoding, and to replace the missing ones by clean speech
estimates. We present a novel method to obtain such clean speech estimates.
Unlike previous imputation frameworks which work on a frame-by-frame basis, our
method focuses on exploiting information from a large time-context. Using a
sliding window approach, denoised speech representations are constructed using
a sparse representation of the reliable features in an overcomplete basis of
fixed-length exemplar fragments. We demonstrate the potential of our approach
with experiments on the AURORA-2 connected digit database.
</summary>
    <author>
      <name>J. F. Gemmeke</name>
    </author>
    <author>
      <name>B. Cranen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures, Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/0901.2416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.2416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.3902v1</id>
    <updated>2009-01-25T14:57:55Z</updated>
    <published>2009-01-25T14:57:55Z</published>
    <title>iKlax: a New Musical Audio Format for Active Listening</title>
    <summary>  In this paper, we are presenting a new model for interactive music. Unlike
most interactive systems, our model is based on file organization, but does not
require digital audio treatments. This model includes a definition of a
constraints system and its solver. The products of this project are intended
for the general public, inexperienced users, as well as professional musicians,
and will be distributed commercially. We are here presenting three products of
this project. The difficulty of this project is to design a technology and
software products for interactive music which must be easy to use by the
general public and by professional composers.
</summary>
    <author>
      <name>Fabien Gallot</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Owen Lagadec</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Myriam Desainte-Catherine</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Sylvain Marchand</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Computer Music Conference (ICMC), Belfast : Irlande
  (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0901.3902v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.3902v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.3976v2</id>
    <updated>2011-09-10T12:02:21Z</updated>
    <published>2009-09-22T12:24:19Z</published>
    <title>The Information Theory of Emotions of Musical Chords</title>
    <summary>  The paper offers a solution to the centuries-old puzzle - why the major
chords are perceived as happy and the minor chords as sad - based on the
information theory of emotions. A theory and a formula of musical emotions were
created. They define the sign and the amplitude of the utilitarian emotional
coloration of separate major and minor chords through relative pitches of
constituent sounds. Keywords: chord, major, minor, the formula of musical
emotions, the information theory of emotions.
</summary>
    <author>
      <name>Vadim R. Madgazin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 2 figures, in English, and copy in Russian</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.3976v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.3976v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.4642v1</id>
    <updated>2009-11-24T15:07:37Z</updated>
    <published>2009-11-24T15:07:37Z</published>
    <title>G3 : GENESIS software envrionment update</title>
    <summary>  GENESIS3 is the new version of the GENESIS software environment for musical
creation by means of mass-interaction physics network modeling. It was
designed, and developed from scratch, in hindsight of more than 10 years
working on and using the previous version. We take the opportunity of this
birth to provide in this article (1) an analysis of the peculiarities in
GENESIS, aiming at highlighting its core ?software paradigm?; and (2) an update
on the features of the new version as compared to the last.
</summary>
    <author>
      <name>Nicolas Castagné</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACROE</arxiv:affiliation>
    </author>
    <author>
      <name>Claude Cadoz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACROE, ICA</arxiv:affiliation>
    </author>
    <author>
      <name>Ali Allaoui</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACROE, ICA</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Michel Tache</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACROE</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Computer Music Conference (ICMC), Montr\'eal :
  Canada (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.4642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.4642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.0745v1</id>
    <updated>2009-12-04T20:27:14Z</updated>
    <published>2009-12-04T20:27:14Z</published>
    <title>A Digital Guitar Tuner</title>
    <summary>  The objective of this paper is to understand the critical parameters that
need to be addressed while designing a guitar tuner. The focus of the design
lies in developing a suitable algorithm to accurately detect the fundamental
frequency of a plucked guitar string from its frequency spectrum. A
userfriendly graphical interface is developed using Matlab to allow any user to
easily tune his guitar using the developed program.
</summary>
    <author>
      <name>Mary Lourde R.</name>
    </author>
    <author>
      <name>Anjali Kuppayil Saji</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS November 2009, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 6, No. 2, pp. 082-088, November 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.0745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.0745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.4190v1</id>
    <updated>2010-01-23T19:04:50Z</updated>
    <published>2010-01-23T19:04:50Z</published>
    <title>Speech Recognition of the letter 'zha' in Tamil Language using HMM</title>
    <summary>  Speech signals of the letter 'zha' in Tamil language of 3 males and 3 females
were coded using an improved version of Linear Predictive Coding (LPC). The
sampling frequency was at 16 kHz and the bit rate was at 15450 bits per second,
where the original bit rate was at 128000 bits per second with the help of wave
surfer audio tool. The output LPC cepstrum is implemented in first order three
state Hidden Markov Model(HMM) chain.
</summary>
    <author>
      <name>A. Srinivasan</name>
    </author>
    <author>
      <name>K. Srinivasa Rao</name>
    </author>
    <author>
      <name>K. Kannan</name>
    </author>
    <author>
      <name>D. Narasimhan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJEST Volume 1 Issue 2 2009 67-72</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.4190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.4190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.2465v3</id>
    <updated>2024-05-08T12:02:54Z</updated>
    <published>2010-05-14T07:26:49Z</published>
    <title>Dichotic harmony for the musical practice</title>
    <summary>  The dichotic method of hearing sound adapts in the region of musical harmony.
The algorithm of the separation of the being dissonant voices into several
separate groups is proposed. For an increase in the pleasantness of chords the
different groups of voices are heard out through the different channels of
headphones. Is created two demonstration program for PC. Keywords: music,
harmony, chord, dichotic listening, dissonance, consonance, headphones,
pleasantness, midi.
</summary>
    <author>
      <name>Vadim R. Madgazin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, in Russian, links added</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.2465v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.2465v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.2796v1</id>
    <updated>2010-09-14T21:36:25Z</updated>
    <published>2010-09-14T21:36:25Z</published>
    <title>Estimation of Infants' Cry Fundamental Frequency using a Modified SIFT
  algorithm</title>
    <summary>  This paper addresses the problem of infants' cry fundamental frequency
estimation. The fundamental frequency is estimated using a modified simple
inverse filtering tracking (SIFT) algorithm. The performance of the modified
SIFT is studied using a real database of infants' cry. It is shown that the
algorithm is capable of overcoming the problem of under-estimation and
over-estimation of the cry fundamental frequency, with an estimation accuracy
of 6.15% and 3.75%, for hyperphonated and phonated cry segments, respectively.
Some typical examples of the fundamental frequency contour in typical cases of
pathological and healthy cry signals are presented and discussed.
</summary>
    <author>
      <name>Dror Lederman</name>
    </author>
    <link href="http://arxiv.org/abs/1009.2796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.2796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4719v1</id>
    <updated>2010-09-23T20:38:06Z</updated>
    <published>2010-09-23T20:38:06Z</published>
    <title>A Fast Audio Clustering Using Vector Quantization and Second Order
  Statistics</title>
    <summary>  This paper describes an effective unsupervised speaker indexing approach. We
suggest a two stage algorithm to speed-up the state-of-the-art algorithm based
on the Bayesian Information Criterion (BIC). In the first stage of the merging
process a computationally cheap method based on the vector quantization (VQ) is
used. Then in the second stage a more computational expensive technique based
on the BIC is applied. In the speaker indexing task a turning parameter or a
threshold is used. We suggest an on-line procedure to define the value of a
turning parameter without using development data. The results are evaluated
using 10 hours of audio data.
</summary>
    <author>
      <name>Konstantin Biatov</name>
    </author>
    <link href="http://arxiv.org/abs/1009.4719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.2797v1</id>
    <updated>2010-12-13T16:44:46Z</updated>
    <published>2010-12-13T16:44:46Z</published>
    <title>Should Corpora be Big, Rich, or Dense?</title>
    <summary>  In this paper, we ask what properties makes a large corpus more or less
useful. We suggest that size, by itself, should not be the ultimate goal of
building a corpus. Large-scale corpora are considered desirable because they
offer statistical stability and rich variation. But this rich variation means
more factors to control and evaluate, which can limit the advantages of size.
We discuss the use of multi-channel data to complement large-scale speech
corpora. Even though multi-channel data may limit the scale of a corpus (due to
the complex and labor-intensive nature of data collection) they can offer
information that allows us to tease apart various factors related to speech
production.
</summary>
    <author>
      <name>Greg P. Kochanski</name>
    </author>
    <author>
      <name>Chilin Shih</name>
    </author>
    <author>
      <name>Ryan Shosted</name>
    </author>
    <link href="http://arxiv.org/abs/1012.2797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.2797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.4118v1</id>
    <updated>2011-03-21T19:32:00Z</updated>
    <published>2011-03-21T19:32:00Z</published>
    <title>Sampling-rate-aware noise generation</title>
    <summary>  In this paper we consider the generation of discrete white noise. Despite
this seems to be a simple problem, common noise generator implementations do
not deliver comparable results at different sampling rates. First we define
what we mean with "comparable results". From this we conclude, that the
variance of the random variables shall grow proportionally to the sampling
rate. Eventually we consider how noise behaves under common signal
transformations, such as frequency filters, quantisation and impulse generation
and we explore how these signal transformations must be designed in order
generate sampling-rate-aware results when applied to white noise.
</summary>
    <author>
      <name>Henning Thielemann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.4118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.4118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.4969v1</id>
    <updated>2011-07-25T15:09:37Z</updated>
    <published>2011-07-25T15:09:37Z</published>
    <title>An end-to-end machine learning system for harmonic analysis of music</title>
    <summary>  We present a new system for simultaneous estimation of keys, chords, and bass
notes from music audio. It makes use of a novel chromagram representation of
audio that takes perception of loudness into account. Furthermore, it is fully
based on machine learning (instead of expert knowledge), such that it is
potentially applicable to a wider range of genres as long as training data is
available. As compared to other models, the proposed system is fast and memory
efficient, while achieving state-of-the-art performance.
</summary>
    <author>
      <name>Yizhao Ni</name>
    </author>
    <author>
      <name>Matt Mcvicar</name>
    </author>
    <author>
      <name>Raul Santos-Rodriguez</name>
    </author>
    <author>
      <name>Tijl De Bie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MIREX report and preparation of Journal submission</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.4969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.4969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.3236v1</id>
    <updated>2012-04-15T04:49:08Z</updated>
    <published>2012-04-15T04:49:08Z</published>
    <title>Using Mimicry to Learn about Mental Representations</title>
    <summary>  Phonology typically describes speech in terms of discrete signs like
features. The field of intonational phonology uses discrete accents to describe
intonation and prosody. But, are such representations useful? The results of
mimicry experiments indicate that discrete signs are not a useful
representation of the shape of intonation contours. Human behaviour seems to be
better represented by a attractors where memory retains substantial fine detail
about an utterance. There is no evidence that discrete abstract representations
that might be formed that have an effect on the speech that is subsequently
produced. This paper also discusses conditions under which a discrete phonology
can arise from an attractor model and why - for intonation - attractors can be
inferred without the implying a discrete phonology.
</summary>
    <author>
      <name>Greg Kochanski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages, plus extra figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.3236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.3236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.5827v1</id>
    <updated>2012-07-24T21:08:35Z</updated>
    <published>2012-07-24T21:08:35Z</published>
    <title>Algorithm to suppress scanner noise in recorded speech during functional
  magnetic resonance imaging</title>
    <summary>  The high-intensity, repetitive noise associated with functional magnetic
resonance imaging hinders on-line monitoring of subjects' speech and/or
recording speech signals suitable for off-line analysis. The proposed algorithm
enhances the speech signal by suppressing the scanner noise in the signal
recorded by a single-channel microphone. Significant increases in
signal-to-noise ratio are achieved using an adaptive filter that combines time
and frequency domain elements. In addition to providing a recording suitable
for speech analysis, such a real-time system provides an alternative means (to,
e.g., the "panic ball") for communication between the patient and the operator
during image acquisition.
</summary>
    <author>
      <name>Satrajit S. Ghosh</name>
    </author>
    <link href="http://arxiv.org/abs/1207.5827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.5827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.3119v1</id>
    <updated>2012-12-13T10:39:26Z</updated>
    <published>2012-12-13T10:39:26Z</published>
    <title>A nuclear-norm based convex formulation for informed source separation</title>
    <summary>  We study the problem of separating audio sources from a single linear
mixture. The goal is to find a decomposition of the single channel spectrogram
into a sum of individual contributions associated to a certain number of
sources. In this paper, we consider an informed source separation problem in
which the input spectrogram is partly annotated. We propose a convex
formulation that relies on a nuclear norm penalty to induce low rank for the
contributions. We show experimentally that solving this model with a simple
subgradient method outperforms a previously introduced nonnegative matrix
factorization (NMF) technique, both in terms of source separation quality and
computation time.
</summary>
    <author>
      <name>Augustin Lefèvre</name>
    </author>
    <author>
      <name>François Glineur</name>
    </author>
    <author>
      <name>P. -A. Absil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ESANN 2013 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.3119v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.3119v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.0278v1</id>
    <updated>2013-01-02T17:50:00Z</updated>
    <published>2013-01-02T17:50:00Z</published>
    <title>Evaluation of a Multi-Resolution Dyadic Wavelet Transform Method for
  usable Speech Detection</title>
    <summary>  Many applications of speech communication and speaker identification suffer
from the problem of co-channel speech. This paper deals with a multi-resolution
dyadic wavelet transform method for usable segments of co-channel speech
detection that could be processed by a speaker identification system.
Evaluation of this method is performed on TIMIT database referring to the
Target to Interferer Ratio measure. Co-channel speech is constructed by mixing
all possible gender speakers. Results do not show much difference for different
mixtures. For the overall mixtures 95.76% of usable speech is correctly
detected with false alarms of 29.65%.
</summary>
    <author>
      <name>Wajdi Ghezaiel</name>
    </author>
    <author>
      <name>Amel Ben Slimane Rahmouni</name>
    </author>
    <author>
      <name>Ezzedine Ben Braiek</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">WASET Journal, 2011 Vol.79 P. 829-833</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1301.0278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.0136v1</id>
    <updated>2013-02-01T10:51:20Z</updated>
    <published>2013-02-01T10:51:20Z</published>
    <title>Maximum a posteriori estimation of piecewise arcs in tempo time-series</title>
    <summary>  In musical performances with expressive tempo modulation, the tempo variation
can be modelled as a sequence of tempo arcs. Previous authors have used this
idea to estimate series of piecewise arc segments from data. In this paper we
describe a probabilistic model for a time-series process of this nature, and
use this to perform inference of single- and multi-level arc processes from
data. We describe an efficient Viterbi-like process for MAP inference of arcs.
Our approach is score-agnostic, and together with efficient inference allows
for online analysis of performances including improvisations, and can predict
immediate future tempo trajectories.
</summary>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Elaine Chew</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to postprint volume for Computer Music Modeling and
  Retrieval (CMMR) 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.0136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.0136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.3462v2</id>
    <updated>2013-02-15T21:18:13Z</updated>
    <published>2013-02-14T16:44:10Z</published>
    <title>Improved multiple birdsong tracking with distribution derivative method
  and Markov renewal process clustering</title>
    <summary>  Segregating an audio mixture containing multiple simultaneous bird sounds is
a challenging task. However, birdsong often contains rapid pitch modulations,
and these modulations carry information which may be of use in automatic
recognition. In this paper we demonstrate that an improved spectrogram
representation, based on the distribution derivative method, leads to improved
performance of a segregation algorithm which uses a Markov renewal process
model to track vocalisation patterns consisting of singing and silences.
</summary>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Sašo Muševič</name>
    </author>
    <author>
      <name>Jordi Bonada</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2013.6637691</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2013.6637691" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICASSP 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.3462v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.3462v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.7070v1</id>
    <updated>2013-02-28T03:43:08Z</updated>
    <published>2013-02-28T03:43:08Z</published>
    <title>Sound localization using compressive sensing</title>
    <summary>  In a sensor network with remote sensor devices, it is important to have a
method that can accurately localize a sound event with a small amount of data
transmitted from the sensors. In this paper, we propose a novel method for
localization of a sound source using compressive sensing. Instead of sampling a
large amount of data at the Nyquist sampling rate in time domain, the acoustic
sensors take compressive measurements integrated in time. The compressive
measurements can be used to accurately compute the location of a sound source.
</summary>
    <author>
      <name>Hong Jiang</name>
    </author>
    <author>
      <name>Boyd Mathews</name>
    </author>
    <author>
      <name>Paul Wilford</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. SENSORNETS, 2012, pp.159-166</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1302.7070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.7070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.6763v2</id>
    <updated>2014-01-10T20:35:56Z</updated>
    <published>2013-04-24T21:50:03Z</published>
    <title>Deep Scattering Spectrum</title>
    <summary>  A scattering transform defines a locally translation invariant representation
which is stable to time-warping deformations. It extends MFCC representations
by computing modulation spectrum coefficients of multiple orders, through
cascades of wavelet convolutions and modulus operators. Second-order scattering
coefficients characterize transient phenomena such as attacks and amplitude
modulation. A frequency transposition invariant representation is obtained by
applying a scattering transform along log-frequency. State-the-of-art
classification results are obtained for musical genre and phone classification
on GTZAN and TIMIT databases, respectively.
</summary>
    <author>
      <name>Joakim Andén</name>
    </author>
    <author>
      <name>Stéphane Mallat</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2014.2326991</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2014.2326991" rel="related"/>
    <link href="http://arxiv.org/abs/1304.6763v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.6763v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.2959v1</id>
    <updated>2013-05-09T08:47:47Z</updated>
    <published>2013-05-09T08:47:47Z</published>
    <title>Automatic Speech Recognition Using Template Model for Man-Machine
  Interface</title>
    <summary>  Speech is a natural form of communication for human beings, and computers
with the ability to understand speech and speak with a human voice are expected
to contribute to the development of more natural man-machine interfaces.
Computers with this kind of ability are gradually becoming a reality, through
the evolution of speech recognition technologies. Speech is being an important
mode of interaction with computers. In this paper Feature extraction is
implemented using well-known Mel-Frequency Cepstral Coefficients (MFCC).Pattern
matching is done using Dynamic time warping (DTW) algorithm.
</summary>
    <author>
      <name>Neema Mishra</name>
    </author>
    <author>
      <name>Urmila Shrawankar</name>
    </author>
    <author>
      <name>V M Thakare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pages: 05 Figures : 01 Tables : 03 Proceedings of the International
  Conference ICAET 2010, Chennai, India. arXiv admin note: text overlap with
  arXiv:1305.2847</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.2959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.2959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.5275v2</id>
    <updated>2013-10-01T21:29:13Z</updated>
    <published>2013-09-20T14:12:04Z</published>
    <title>An open dataset for research on audio field recording archives:
  freefield1010</title>
    <summary>  We introduce a free and open dataset of 7690 audio clips sampled from the
field-recording tag in the Freesound audio archive. The dataset is designed for
use in research related to data mining in audio archives of field recordings /
soundscapes. Audio is standardised, and audio and metadata are Creative Commons
licensed. We describe the data preparation process, characterise the dataset
descriptively, and illustrate its use through an auto-tagging experiment.
</summary>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <link href="http://arxiv.org/abs/1309.5275v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.5275v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.6047v1</id>
    <updated>2013-09-24T05:14:53Z</updated>
    <published>2013-09-24T05:14:53Z</published>
    <title>Non-negative Matrix Factorization with Linear Constraints for
  Single-Channel Speech Enhancement</title>
    <summary>  This paper investigates a non-negative matrix factorization (NMF)-based
approach to the semi-supervised single-channel speech enhancement problem where
only non-stationary additive noise signals are given. The proposed method
relies on sinusoidal model of speech production which is integrated inside NMF
framework using linear constraints on dictionary atoms. This method is further
developed to regularize harmonic amplitudes. Simple multiplicative algorithms
are presented. The experimental evaluation was made on TIMIT corpus mixed with
various types of noise. It has been shown that the proposed method outperforms
some of the state-of-the-art noise suppression techniques in terms of
signal-to-noise ratio.
</summary>
    <author>
      <name>Nikolay Lyubimov</name>
    </author>
    <author>
      <name>Mikhail Kotov</name>
    </author>
    <link href="http://arxiv.org/abs/1309.6047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.6047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4014v1</id>
    <updated>2013-12-14T07:55:23Z</updated>
    <published>2013-12-14T07:55:23Z</published>
    <title>A Simple Method to Produce Algorithmic MIDI Music based on Randomness,
  Simple Probabilities and Multi-Threading</title>
    <summary>  This paper introduces a simple method for producing multichannel MIDI music
that is based on randomness and simple probabilities. One distinctive feature
of the method is that it produces and sends in parallel to the sound card more
than one unsynchronized channels by exploiting the multi-threading capabilities
of general purpose programming languages. As consequence the derived sound
offers a quite ``full" and ``unpredictable" acoustic experience to the
listener. Subsequently the paper reports the results of an evaluation with
users. The results were very surprising: the majority of users responded that
they could tolerate this music in various occasions.
</summary>
    <author>
      <name>Yannis Tzitzikas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.4014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.3689v1</id>
    <updated>2014-02-15T13:27:01Z</updated>
    <published>2014-02-15T13:27:01Z</published>
    <title>Sound Representation and Classification Benchmark for Domestic Robots</title>
    <summary>  We address the problem of sound representation and classification and present
results of a comparative study in the context of a domestic robotic scenario. A
dataset of sounds was recorded in realistic conditions (background noise,
presence of several sound sources, reverberations, etc.) using the humanoid
robot NAO. An extended benchmark is carried out to test a variety of
representations combined with several classifiers. We provide results obtained
with the annotated dataset and we assess the methods quantitatively on the
basis of their classification scores, computation times and memory
requirements. The annotated dataset is publicly available at
https://team.inria.fr/perception/nard/.
</summary>
    <author>
      <name>Maxime Janvier</name>
    </author>
    <author>
      <name>Xavier Alameda-Pineda</name>
    </author>
    <author>
      <name>Laurent Girin</name>
    </author>
    <author>
      <name>Radu Horaud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.3689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.3689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.1501v1</id>
    <updated>2014-03-06T17:29:39Z</updated>
    <published>2014-03-06T17:29:39Z</published>
    <title>Sparse DOA Estimation of Wideband Sound Sources Using Circular Harmonics</title>
    <summary>  Sparse signal models are in the focus of recent developments in narrowband
DOA estimation. Applying these methods to localizing audio sources, however, is
challenging due to the wideband nature of the signals. The common approach of
processing all frequency bands separately and fusing the results is costly and
can introduce errors in the solution. We show how these problems can be
overcome by decomposing the wavefield of a circular microphone array and using
circular harmonic coefficients instead of time-frequency data for sparse DOA
estimation. As a result, we present the super-resolution localization method
WASCHL (Wideband Audio Sparse Circular Harmonics Localizer) that is inherently
frequency-coherent and highly efficient from a computational point of view.
</summary>
    <author>
      <name>Clemens Hage</name>
    </author>
    <author>
      <name>Tim Habigt</name>
    </author>
    <author>
      <name>Martin Kleinsteuber</name>
    </author>
    <link href="http://arxiv.org/abs/1403.1501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.1501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.2180v2</id>
    <updated>2014-12-18T15:59:17Z</updated>
    <published>2014-03-10T09:18:54Z</published>
    <title>Optimal Window and Lattice in Gabor Transform Application to Audio
  Analysis</title>
    <summary>  This article deals with the use of optimal lattice and optimal window in
Discrete Gabor Transform computation. In the case of a generalized Gaussian
window, extending earlier contributions, we introduce an additional local
window adaptation technique for non-stationary signals. We illustrate our
approach and the earlier one by addressing three time-frequency analysis
problems to show the improvements achieved by the use of optimal lattice and
window: close frequencies distinction, frequency estimation and SNR estimation.
The results are presented, when possible, with real world audio signals.
</summary>
    <author>
      <name>Helene Lachambre</name>
    </author>
    <author>
      <name>Benjamin Ricaud</name>
    </author>
    <author>
      <name>Guillaume Stempfel</name>
    </author>
    <author>
      <name>Bruno Torresani</name>
    </author>
    <author>
      <name>Christoph Wiesmeyr</name>
    </author>
    <author>
      <name>Darian M. Onchis</name>
    </author>
    <link href="http://arxiv.org/abs/1403.2180v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.2180v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.6881v1</id>
    <updated>2014-04-28T07:17:18Z</updated>
    <published>2014-04-28T07:17:18Z</published>
    <title>Improving Blind Source Separation Performance By Adaptive Array
  Geometries For Humanoid Robots</title>
    <summary>  In this paper, the concept of an adaptation algorithm is proposed, which can
be used to blindly adapt the microphone array geometry of a humanoid robot such
that the performance of the underlying signal separation algorithm is improved.
As a decisive feature, an online performance measure for blind source
separation is introduced which allows a robust and reliable estimation of the
instantaneous separation performance based on currently observable data.
Experimental results from a simulated environment confirm the efficacy of the
concept.
</summary>
    <author>
      <name>Hendrik Barfuss</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <link href="http://arxiv.org/abs/1404.6881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.6881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.6945v1</id>
    <updated>2014-05-25T22:22:04Z</updated>
    <published>2014-05-25T22:22:04Z</published>
    <title>Sparsity-Aware Filtered-X Affine Projection Algorithms for Active Noise
  Control</title>
    <summary>  This paper describes a novel technique for promoting sparsity in the modified
filtered-x algorithms required for active noise control. The proposed
algorithms are based on recent techniques incorporating approximations to the
\ell_0-norm in the cost functions that are used to derive adaptive filtering
algorithms. In particular, zero-attracting and reweighted zero-attracting
filtered-x adaptive algorithms are developed and considered for active noise
control problems. The results of simulations indicate that the proposed
techniques improve the convergence of the existing modified algorithm in the
case where the primary and secondary paths exhibit a degree of sparsity.
</summary>
    <author>
      <name>A. Gully</name>
    </author>
    <author>
      <name>R. C. de Lamare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 figures, 5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.6945v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.6945v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.3915v1</id>
    <updated>2014-06-16T06:41:54Z</updated>
    <published>2014-06-16T06:41:54Z</published>
    <title>A Bengali HMM Based Speech Synthesis System</title>
    <summary>  The paper presents the capability of an HMM-based TTS system to produce
Bengali speech. In this synthesis method, trajectories of speech parameters are
generated from the trained Hidden Markov Models. A final speech waveform is
synthesized from those speech parameters. In our experiments, spectral
properties were represented by Mel Cepstrum Coefficients. Both the training and
synthesis issues are investigated in this paper using annotated Bengali speech
database. Experimental evaluation depicts that the developed text-to-speech
system is capable of producing adequately natural speech in terms of
intelligibility and intonation for Bengali.
</summary>
    <author>
      <name>Sankar Mukherjee</name>
    </author>
    <author>
      <name>Shyamal Kumar Das Mandal</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Oriental COCOSDA 2012, pp.225 259</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.3915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.2430v1</id>
    <updated>2014-10-09T11:46:02Z</updated>
    <published>2014-10-09T11:46:02Z</published>
    <title>Phase-Optimized K-SVD for Signal Extraction from Underdetermined
  Multichannel Sparse Mixtures</title>
    <summary>  We propose a novel sparse representation for heavily underdetermined
multichannel sound mixtures, i.e., with much more sources than microphones. The
proposed approach operates in the complex Fourier domain, thus preserving
spatial characteristics carried by phase differences. We derive a
generalization of K-SVD which jointly estimates a dictionary capturing both
spectral and spatial features, a sparse activation matrix, and all
instantaneous source phases from a set of signal examples. The dictionary can
then be used to extract the learned signal from a new input mixture. The method
is applied to the challenging problem of ego-noise reduction for robot
audition. We demonstrate its superiority relative to conventional
dictionary-based techniques using recordings made in a real room.
</summary>
    <author>
      <name>Antoine Deleforge</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <link href="http://arxiv.org/abs/1410.2430v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.2430v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6645v3</id>
    <updated>2015-04-20T12:35:32Z</updated>
    <published>2014-12-20T11:54:41Z</published>
    <title>Weakly Supervised Multi-Embeddings Learning of Acoustic Models</title>
    <summary>  We trained a Siamese network with multi-task same/different information on a
speech dataset, and found that it was possible to share a network for both
tasks without a loss in performance. The first task was to discriminate between
two same or different words, and the second was to discriminate between two
same or different talkers.
</summary>
    <author>
      <name>Gabriel Synnaeve</name>
    </author>
    <author>
      <name>Emmanuel Dupoux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.6645v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6645v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.7; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.07866v1</id>
    <updated>2015-01-28T21:58:51Z</updated>
    <published>2015-01-28T21:58:51Z</published>
    <title>A Comparison of Classifiers in Performing Speaker Accent Recognition
  Using MFCCs</title>
    <summary>  An algorithm involving Mel-Frequency Cepstral Coefficients (MFCCs) is
provided to perform signal feature extraction for the task of speaker accent
recognition. Then different classifiers are compared based on the MFCC feature.
For each signal, the mean vector of MFCC matrix is used as an input vector for
pattern recognition. A sample of 330 signals, containing 165 US voice and 165
non-US voice, is analyzed. By comparison, k-nearest neighbors yield the highest
average test accuracy, after using a cross-validation of size 500, and least
time being used in the computation
</summary>
    <author>
      <name>Zichen Ma</name>
    </author>
    <author>
      <name>Ernest Fokoue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Open Journal of Statistics, 2014, 4, 258-266</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1501.07866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.07866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62H25, 62H30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.05268v1</id>
    <updated>2015-06-17T10:17:59Z</updated>
    <published>2015-06-17T10:17:59Z</published>
    <title>Deep Denoising Auto-encoder for Statistical Speech Synthesis</title>
    <summary>  This paper proposes a deep denoising auto-encoder technique to extract better
acoustic features for speech synthesis. The technique allows us to
automatically extract low-dimensional features from high dimensional spectral
features in a non-linear, data-driven, unsupervised way. We compared the new
stochastic feature extractor with conventional mel-cepstral analysis in
analysis-by-synthesis and text-to-speech experiments. Our results confirm that
the proposed method increases the quality of synthetic speech in both
experiments.
</summary>
    <author>
      <name>Zhenzhou Wu</name>
    </author>
    <author>
      <name>Shinji Takaki</name>
    </author>
    <author>
      <name>Junichi Yamagishi</name>
    </author>
    <link href="http://arxiv.org/abs/1506.05268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.05268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05143v1</id>
    <updated>2015-07-18T03:55:50Z</updated>
    <published>2015-07-18T03:55:50Z</published>
    <title>Cover Song Identification with Timbral Shape Sequences</title>
    <summary>  We introduce a novel low level feature for identifying cover songs which
quantifies the relative changes in the smoothed frequency spectrum of a song.
Our key insight is that a sliding window representation of a chunk of audio can
be viewed as a time-ordered point cloud in high dimensions. For corresponding
chunks of audio between different versions of the same song, these point clouds
are approximately rotated, translated, and scaled copies of each other. If we
treat MFCC embeddings as point clouds and cast the problem as a relative shape
sequence, we are able to correctly identify 42/80 cover songs in the "Covers
80" dataset. By contrast, all other work to date on cover songs exclusively
relies on matching note sequences from Chroma derived features.
</summary>
    <author>
      <name>Christopher J. Tralie</name>
    </author>
    <author>
      <name>Paul Bendich</name>
    </author>
    <link href="http://arxiv.org/abs/1507.05143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.07348v1</id>
    <updated>2015-07-27T10:08:24Z</updated>
    <published>2015-07-27T10:08:24Z</published>
    <title>A model for the temporal evolution of the spatial coherence in decaying
  reverberant sound fields</title>
    <summary>  Reverberant sound fields are often modeled as isotropic. However, it has been
observed that spatial properties change during the decay of the sound field
energy, due to non-isotropic attenuation in non-ideal rooms. In this letter, a
model for the spatial coherence between two sensors in a decaying reverberant
sound field is developed for rectangular rooms. The modeled coherence function
depends on room dimensions, surface reflectivity and orientation of the sensor
pair, but is independent of the position of source and sensors in the room. The
model includes the spherically isotropic (diffuse) and cylindrically isotropic
sound field models as special cases.
</summary>
    <author>
      <name>Sam Nees</name>
    </author>
    <author>
      <name>Andreas Schwarz</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1121/1.4929733</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1121/1.4929733" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for JASA Express Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.07348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.07348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.00334v1</id>
    <updated>2015-09-01T15:04:16Z</updated>
    <published>2015-09-01T15:04:16Z</published>
    <title>Transformée en scattering sur la spirale temps-chroma-octave</title>
    <summary>  We introduce a scattering representation for the analysis and classification
of sounds. It is locally translation-invariant, stable to deformations in time
and frequency, and has the ability to capture harmonic structures. The
scattering representation can be interpreted as a convolutional neural network
which cascades a wavelet transform in time and along a harmonic spiral. We
study its application for the analysis of the deformations of the source-filter
model.
</summary>
    <author>
      <name>Vincent Lostanlen</name>
    </author>
    <author>
      <name>Stéphane Mallat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French, 4 pages, 3 figures, presented at GRETSI 2015 in Lyon,
  France</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.00334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.00334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06279v1</id>
    <updated>2015-09-18T12:47:09Z</updated>
    <published>2015-09-18T12:47:09Z</published>
    <title>Sports highlights generation based on acoustic events detection: A rugby
  case study</title>
    <summary>  We approach the challenging problem of generating highlights from sports
broadcasts utilizing audio information only. A language-independent,
multi-stage classification approach is employed for detection of key acoustic
events which then act as a platform for summarization of highlight scenes.
Objective results and human experience indicate that our system is highly
efficient.
</summary>
    <author>
      <name>Anant Baijal</name>
    </author>
    <author>
      <name>Jaeyoun Cho</name>
    </author>
    <author>
      <name>Woojung Lee</name>
    </author>
    <author>
      <name>Byeong-Seob Ko</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICCE.2015.7066303</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICCE.2015.7066303" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Consumer Electronics (IEEE ICCE
  2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.06279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.07298v4</id>
    <updated>2017-03-13T16:27:03Z</updated>
    <published>2015-09-24T10:16:09Z</published>
    <title>An Investigation of Universal Background Sparse Coding Based Speaker
  Verification on TIMIT</title>
    <summary>  In this paper, we propose a universal background model, named universal
background sparse coding (UBSC), for speaker verification. The proposed method
trains an ensemble of clusterings by data resampling, and produces sparse codes
from the clusterings by one-nearest-neighbor optimization plus binarization.
The main advantage of UBSC is that it does not suffer from local minima and
does not make Gaussian assumptions on data distributions. We evaluated UBSC on
a clean speech corpus---TIMIT. We used the cosine similarity and inner product
similarity as the scoring methods of a trial. Experimental results show that
UBSC is comparable to Gaussian mixture model.
</summary>
    <author>
      <name>Xiao-Lei Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1509.07298v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.07298v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00266v1</id>
    <updated>2015-10-01T14:53:29Z</updated>
    <published>2015-10-01T14:53:29Z</published>
    <title>Noise robust integration for blind and non-blind reverberation time
  estimation</title>
    <summary>  The estimation of the decay rate of a signal section is an integral component
of both blind and non-blind reverberation time estimation methods. Several
decay rate estimators have previously been proposed, based on, e.g., linear
regression and maximum-likelihood estimation. Unfortunately, most approaches
are sensitive to background noise, and/or are fairly demanding in terms of
computational complexity. This paper presents a low complexity decay rate
estimator, robust to stationary noise, for reverberation time estimation.
Simulations using artificial signals, and experiments with speech in
ventilation noise, demonstrate the performance and noise robustness of the
proposed method.
</summary>
    <author>
      <name>Christian Schüldt</name>
    </author>
    <author>
      <name>Peter Händel</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2015.7177931</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2015.7177931" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), Brisbane, Australia, April 19-24, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.00266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.01193v1</id>
    <updated>2015-10-05T15:42:45Z</updated>
    <published>2015-10-05T15:42:45Z</published>
    <title>Reverberation time estimation on the ACE corpus using the SDD method</title>
    <summary>  Reverberation Time (T60) is an important measure for characterizing the
properties of a room. The author's T60 estimation algorithm was previously
tested on simulated data where the noise is artificially added to the speech
after convolution with a impulse responses simulated using the image method. We
test the algorithm on speech convolved with real recorded impulse responses and
noise from the same rooms from the Acoustic Characterization of Environments
(ACE) corpus and achieve results comparable results to those using simulated
data.
</summary>
    <author>
      <name>James Eaton</name>
    </author>
    <author>
      <name>Patrick A. Naylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383)</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.01193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.01193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.08484v1</id>
    <updated>2015-10-28T20:59:04Z</updated>
    <published>2015-10-28T20:59:04Z</published>
    <title>MUSAN: A Music, Speech, and Noise Corpus</title>
    <summary>  This report introduces a new corpus of music, speech, and noise. This dataset
is suitable for training models for voice activity detection (VAD) and
music/speech discrimination. Our corpus is released under a flexible Creative
Commons license. The dataset consists of music from several genres, speech from
twelve languages, and a wide assortment of technical and non-technical noises.
We demonstrate use of this corpus for music/speech discrimination on Broadcast
news and VAD for speaker identification.
</summary>
    <author>
      <name>David Snyder</name>
    </author>
    <author>
      <name>Guoguo Chen</name>
    </author>
    <author>
      <name>Daniel Povey</name>
    </author>
    <link href="http://arxiv.org/abs/1510.08484v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.08484v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.08963v1</id>
    <updated>2015-10-30T03:45:41Z</updated>
    <published>2015-10-30T03:45:41Z</published>
    <title>PSD estimation in Beamspace for Estimating Direct-to-Reverberant Ratio
  from A Reverberant Speech Signal</title>
    <summary>  A method for estimation of direct-to-reverberant ratio (DRR) using a
microphone array is proposed. The proposed method estimates the power spectral
density (PSD) of the direct sound and the reverberation using the algorithm
\textit{PSD estimation in beamspace} with a microphone array and calculates the
DRR of the observed signal. The speech corpus of the ACE (Acoustic
Characterisation of Environments) Challenge was utilised for evaluating the
practical feasibility of the proposed method. The experimental results revealed
that the proposed method was able to effectively estimate the DRR from a
recording of a reverberant speech signal which included various environmental
noise.
</summary>
    <author>
      <name>Yusuke Hioka</name>
    </author>
    <author>
      <name>Kenta Niwa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA2015 (arXiv:1510.00383)</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.08963v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.08963v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.05520v1</id>
    <updated>2015-11-17T19:43:53Z</updated>
    <published>2015-11-17T19:43:53Z</published>
    <title>Automatic Instrument Recognition in Polyphonic Music Using Convolutional
  Neural Networks</title>
    <summary>  Traditional methods to tackle many music information retrieval tasks
typically follow a two-step architecture: feature engineering followed by a
simple learning algorithm. In these "shallow" architectures, feature
engineering and learning are typically disjoint and unrelated. Additionally,
feature engineering is difficult, and typically depends on extensive domain
expertise.
  In this paper, we present an application of convolutional neural networks for
the task of automatic musical instrument identification. In this model, feature
extraction and learning algorithms are trained together in an end-to-end
fashion. We show that a convolutional neural network trained on raw audio can
achieve performance surpassing traditional methods that rely on hand-crafted
features.
</summary>
    <author>
      <name>Peter Li</name>
    </author>
    <author>
      <name>Jiyuan Qian</name>
    </author>
    <author>
      <name>Tian Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1511.05520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.05520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02125v1</id>
    <updated>2015-12-07T17:11:01Z</updated>
    <published>2015-12-07T17:11:01Z</published>
    <title>Joint Time-Frequency Scattering for Audio Classification</title>
    <summary>  We introduce the joint time-frequency scattering transform, a time shift
invariant descriptor of time-frequency structure for audio classification. It
is obtained by applying a two-dimensional wavelet transform in time and
log-frequency to a time-frequency wavelet scalogram. We show that this
descriptor successfully characterizes complex time-frequency phenomena such as
time-varying filters and frequency modulated excitations. State-of-the-art
results are achieved for signal reconstruction and phone segment classification
on the TIMIT dataset.
</summary>
    <author>
      <name>Joakim Andén</name>
    </author>
    <author>
      <name>Vincent Lostanlen</name>
    </author>
    <author>
      <name>Stéphane Mallat</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MLSP.2015.7324385</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MLSP.2015.7324385" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures in IEEE 25th International Workshop on Machine
  Learning for Signal Processing (MLSP), 2015. Sept. 17-20. Boston, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.02125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.04243v1</id>
    <updated>2015-12-14T10:14:53Z</updated>
    <published>2015-12-14T10:14:53Z</published>
    <title>Trigonometric dictionary based codec for music compression with high
  quality recovery</title>
    <summary>  A codec for compression of music signals is proposed. The method belongs to
the class of transform lossy compression. It is conceived to be applied in the
high quality recovery range though. The transformation, endowing the codec with
its distinctive feature, relies on the ability to construct high quality sparse
approximation of music signals. This is achieved by a redundant trigonometric
dictionary and a dedicated pursuit strategy. The potential of the approach is
illustrated by comparison with the OGG Vorbis format, on a sample consisting of
clips of melodic music. The comparison evidences remarkable improvements in
compression performance for the identical quality of the decompressed signal.
</summary>
    <author>
      <name>Laura Rebollo-Neira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Software implementing the codec is available on
  http://www.nonlinear-approx.info/examples/node03.html</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.04243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.04243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.02546v1</id>
    <updated>2016-01-11T18:14:03Z</updated>
    <published>2016-01-11T18:14:03Z</published>
    <title>Automatic Determination of Chord Roots</title>
    <summary>  Even though chord roots constitute a fundamental concept in music theory,
existing models do not explain and determine them to full satisfaction. We
present a new method which takes sequential context into account to resolve
ambiguities and detect nonharmonic tones. We extract features from chord pairs
and use a decision tree to determine chord roots. This leads to a quantitative
improvement in correctness of the predicted roots in comparison to other
models. All this raises the question how much harmonic and nonharmonic tones
actually contribute to the perception of chord roots.
</summary>
    <author>
      <name>Samuel Rupprechter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BSc Thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.02546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.02546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07394v1</id>
    <updated>2016-02-24T04:33:49Z</updated>
    <published>2016-02-24T04:33:49Z</published>
    <title>Improved Accent Classification Combining Phonetic Vowels with Acoustic
  Features</title>
    <summary>  Researches have shown accent classification can be improved by integrating
semantic information into pure acoustic approach. In this work, we combine
phonetic knowledge, such as vowels, with enhanced acoustic features to build an
improved accent classification system. The classifier is based on Gaussian
Mixture Model-Universal Background Model (GMM-UBM), with normalized Perceptual
Linear Predictive (PLP) features. The features are further optimized by
Principle Component Analysis (PCA) and Hetroscedastic Linear Discriminant
Analysis (HLDA). Using 7 major types of accented speech from the Foreign
Accented English (FAE) corpus, the system achieves classification accuracy 54%
with input test data as short as 20 seconds, which is competitive to the state
of the art in this field.
</summary>
    <author>
      <name>Zhenhao Ge</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CISP.2015.7408064</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CISP.2015.7408064" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Congress on Image and Signal Processing (CISP) 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.07394v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07394v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08044v1</id>
    <updated>2016-02-25T19:17:19Z</updated>
    <published>2016-02-25T19:17:19Z</published>
    <title>On Adjusting the Learning Rate in Frequency Domain Echo Cancellation
  With Double-Talk</title>
    <summary>  One of the main difficulties in echo cancellation is the fact that the
learning rate needs to vary according to conditions such as double-talk and
echo path change. In this paper we propose a new method of varying the learning
rate of a frequency-domain echo canceller. This method is based on the
derivation of the optimal learning rate of the NLMS algorithm in the presence
of noise. The method is evaluated in conjunction with the multidelay block
frequency domain (MDF) adaptive filter. We demonstrate that it performs better
than current double-talk detection techniques and is simple to implement.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASL.2006.885935</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASL.2006.885935" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Audio, Speech and Language Processing, Vol.
  15, No. 3, pp. 1030-1034, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.08044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08215v1</id>
    <updated>2016-02-26T06:47:06Z</updated>
    <published>2016-02-26T06:47:06Z</published>
    <title>Bandwidth Extension of Narrowband Speech for Low Bit-Rate Wideband
  Coding</title>
    <summary>  Wireless telephone speech is usually limited to the 300-3400 Hz band, which
reduces its quality. There is thus a growing demand for wideband speech systems
that transmit from 50 Hz to 8000 Hz. This paper presents an algorithm to
generate wideband speech from narrowband speech using as low as 500 bits/s of
side information. The 50-300 Hz band is predicted from the narrowband signal. A
source-excitation model is used for the 3400-8000 Hz band, where the excitation
is extrapolated at the receiver, and the spectral envelope is transmitted.
Though some artifacts are present, the resulting wideband speech has enhanced
quality compared to narrowband speech.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Roch Lefebvre</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SCFT.2000.878425</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SCFT.2000.878425" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. IEEE Speech Coding Workshop (SCW), 2000, pp. 130-132</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.08215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08633v1</id>
    <updated>2016-02-27T19:32:25Z</updated>
    <published>2016-02-27T19:32:25Z</published>
    <title>Perceptually-Motivated Nonlinear Channel Decorrelation For Stereo
  Acoustic Echo Cancellation</title>
    <summary>  Acoustic echo cancellation with stereo signals is generally an
under-determined problem because of the high coherence between the left and
right channels. In this paper, we present a novel method of significantly
reducing inter-channel coherence without affecting the audio quality. Our work
takes into account psychoacoustic masking and binaural auditory cues. The
proposed non-linear processing combines a shaped comb-allpass (SCAL) filter
with the injection of psychoacoustically masked noise. We show that the
proposed method performs significantly better than other known methods for
reducing inter-channel coherence.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HSCMA.2008.4538718</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HSCMA.2008.4538718" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Joint Workshop on Hands-free Speech Communication
  and Microphone Arrays (HSCMA), pp. 188-191, 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.08633v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08633v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08668v1</id>
    <updated>2016-02-28T04:38:33Z</updated>
    <published>2016-02-28T04:38:33Z</published>
    <title>Speex: A Free Codec For Free Speech</title>
    <summary>  The Speex project has been started in 2002 to address the need for a free,
open-source speech codec. Speex is based on the Code Excited Linear Prediction
(CELP) algorithm and, unlike the previously existing Vorbis codec, is optimised
for transmitting speech for low latency communication over an unreliable packet
network. This paper presents an overview of Speex, the technology involved in
it and how it can be used in applications. The most recent developments in
Speex, such as the fixed-point port, acoustic echo cancellation and noise
suppression are also addressed.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at linux.conf.au 2006, Dunedin. 8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.08668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03364v1</id>
    <updated>2016-03-10T18:51:24Z</updated>
    <published>2016-03-10T18:51:24Z</published>
    <title>Channel Decorrelation For Stereo Acoustic Echo Cancellation In
  High-Quality Audio Communication</title>
    <summary>  In this paper, we address an important problem in high-quality audio
communication systems. Acoustic echo cancellation with stereo signals is
generally an under-determined problem because of the generally important
correlation that exists between the left and right channels. In this paper, we
present a novel method of significantly reducing that correlation without
affecting the audio quality. This method is perceptually motivated and combines
a shaped comb-allpass (SCAL) filter with the injection of psychoacoustically
masked noise. We show that the proposed method performs significantly better
than other known methods for channel decorrelation.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages in Proceedings of Workshop on the Internet,
  Telecommunications and Signal Processing (WITSP), 2006. arXiv admin note:
  substantial text overlap with arXiv:1602.08633</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.03364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04979v1</id>
    <updated>2016-03-16T06:59:11Z</updated>
    <published>2016-03-16T06:59:11Z</published>
    <title>Guitar Solos as Networks</title>
    <summary>  This paper presents an approach to model melodies (and music pieces in
general) as networks. Notes of a melody can be seen as nodes of a network that
are connected whenever these are played in sequence. This creates a directed
graph. By using complex network theory, it is possible to extract some main
metrics, typical of networks, that characterize the piece. Using this
framework, we provide an analysis on a set of guitar solos performed by main
musicians. The results of this study indicate that this model can have an
impact on multimedia applications such as music classification, identification,
and automatic music generation.
</summary>
    <author>
      <name>Stefano Ferretti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICME.2016.7553001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICME.2016.7553001" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2016 IEEE International Conference on
  Multimedia and Expo (ICME 2016), IEEE, Seattle, 2016, pp. 1-6</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.04979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07173v1</id>
    <updated>2016-03-23T13:21:12Z</updated>
    <published>2016-03-23T13:21:12Z</published>
    <title>Deductive Refinement of Species Labelling in Weakly Labelled Birdsong
  Recordings</title>
    <summary>  Many approaches have been used in bird species classification from their
sound in order to provide labels for the whole of a recording. However, a more
precise classification of each bird vocalization would be of great importance
to the use and management of sound archives and bird monitoring. In this work,
we introduce a technique that using a two step process can first automatically
detect all bird vocalizations and then, with the use of 'weakly' labelled
recordings, classify them. Evaluations of our proposed method show that it
achieves a correct classification of 61% when used in a synthetic dataset, and
up to 89% when the synthetic dataset only consists of vocalizations larger than
1000 pixels.
</summary>
    <author>
      <name>Veronica Morfi</name>
    </author>
    <author>
      <name>Dan Stowell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.07173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08740v1</id>
    <updated>2016-03-29T12:21:36Z</updated>
    <published>2016-03-29T12:21:36Z</published>
    <title>On the Impact of Localization Errors on HRTF-based Robust Least-Squares
  Beamforming</title>
    <summary>  In this work, a recently proposed Head-Related Transfer Function (HRTF)-based
Robust Least-Squares Frequency-Invariant (RLSFI) beamformer design is analyzed
with respect to its robustness against localization errors, which lead to a
mismatch between the HRTFs corresponding to the actual target source position
and the HRTFs which have been used for the beamformer design. The impact of
this mismatch on the performance of the HRTF-based RLSFI beamformer is
evaluated, including a comparison to the free-field-based beamformer design,
using signal-based measures and word error rates for an off-the-shelf speech
recognizer.
</summary>
    <author>
      <name>Hendrik Barfuss</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <link href="http://arxiv.org/abs/1603.08740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.04669v1</id>
    <updated>2016-04-16T00:26:59Z</updated>
    <published>2016-04-16T00:26:59Z</published>
    <title>Two Pairwise Iterative Schemes For High Dimensional Blind Source
  Separation</title>
    <summary>  This paper addresses the high dimensionality problem in blind source
separation (BSS), where the number of sources is greater than two. Two pairwise
iterative schemes are proposed to tackle this high dimensionality problem. The
two pairwise schemes realize nonparametric independent component analysis (ICA)
algorithms based on a new high-performance Convex CauchySchwarz Divergence
(CCSDIV). These two schemes enable fast and efficient demixing of sources in
real-world high dimensional source applications. Finally, the performance
superiority of the proposed schemes is demonstrated in metric-comparison with
FastICA, RobustICA, convex ICA (CICA), and other leading existing algorithms.
</summary>
    <author>
      <name>Zaid Albataineh</name>
    </author>
    <author>
      <name>Fathi M. Salem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figures, 6 tables. arXiv admin note: substantial text
  overlap with arXiv:1408.0192</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.04669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.04669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08095v1</id>
    <updated>2016-02-24T02:50:44Z</updated>
    <published>2016-02-24T02:50:44Z</published>
    <title>Accent Classification with Phonetic Vowel Representation</title>
    <summary>  Previous accent classification research focused mainly on detecting accents
with pure acoustic information without recognizing accented speech. This work
combines phonetic knowledge such as vowels with acoustic information to build
Guassian Mixture Model (GMM) classifier with Perceptual Linear Predictive (PLP)
features, optimized by Hetroscedastic Linear Discriminant Analysis (HLDA). With
input about 20-second accented speech, this system achieves classification rate
of 51% on a 7-way classification system focusing on the major types of accents
in English, which is competitive to the state-of-the-art results in this field.
</summary>
    <author>
      <name>Zhenhao Ge</name>
    </author>
    <author>
      <name>Yingyi Tan</name>
    </author>
    <author>
      <name>Aravind Ganapathiraju</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Asian Conference on Pattern Recognition (ACPR) 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08852v1</id>
    <updated>2016-04-29T14:32:03Z</updated>
    <published>2016-04-29T14:32:03Z</published>
    <title>Joint Sound Source Separation and Speaker Recognition</title>
    <summary>  Non-negative Matrix Factorization (NMF) has already been applied to learn
speaker characterizations from single or non-simultaneous speech for speaker
recognition applications. It is also known for its good performance in (blind)
source separation for simultaneous speech. This paper explains how NMF can be
used to jointly solve the two problems in a multichannel speaker recognizer for
simultaneous speech. It is shown how state-of-the-art multichannel NMF for
blind source separation can be easily extended to incorporate speaker
recognition. Experiments on the CHiME corpus show that this method outperforms
the sequential approach of first applying source separation, followed by
speaker recognition that uses state-of-the-art i-vector techniques.
</summary>
    <author>
      <name>Jeroen Zegers</name>
    </author>
    <author>
      <name>Hugo Van hamme</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to INTERSPEECH2016. 4 pages, 1 extra page for references</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08852v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08852v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01755v1</id>
    <updated>2016-04-28T22:21:01Z</updated>
    <published>2016-04-28T22:21:01Z</published>
    <title>DCTNet and PCANet for acoustic signal feature extraction</title>
    <summary>  We introduce the use of DCTNet, an efficient approximation and alternative to
PCANet, for acoustic signal classification. In PCANet, the eigenfunctions of
the local sample covariance matrix (PCA) are used as filterbanks for
convolution and feature extraction. When the eigenfunctions are well
approximated by the Discrete Cosine Transform (DCT) functions, each layer of of
PCANet and DCTNet is essentially a time-frequency representation. We relate
DCTNet to spectral feature representation methods, such as the the short time
Fourier transform (STFT), spectrogram and linear frequency spectral
coefficients (LFSC). Experimental results on whale vocalization data show that
DCTNet improves classification rate, demonstrating DCTNet's applicability to
signal processing problems such as underwater acoustics.
</summary>
    <author>
      <name>Yin Xian</name>
    </author>
    <author>
      <name>Andrew Thompson</name>
    </author>
    <author>
      <name>Xiaobai Sun</name>
    </author>
    <author>
      <name>Douglas Nowacek</name>
    </author>
    <author>
      <name>Loren Nolte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01755v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01755v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02427v1</id>
    <updated>2016-05-09T06:13:37Z</updated>
    <published>2016-05-09T06:13:37Z</published>
    <title>Speech Enhancement In Multiple-Noise Conditions using Deep Neural
  Networks</title>
    <summary>  In this paper we consider the problem of speech enhancement in real-world
like conditions where multiple noises can simultaneously corrupt speech. Most
of the current literature on speech enhancement focus primarily on presence of
single noise in corrupted speech which is far from real-world environments.
Specifically, we deal with improving speech quality in office environment where
multiple stationary as well as non-stationary noises can be simultaneously
present in speech. We propose several strategies based on Deep Neural Networks
(DNN) for speech enhancement in these scenarios. We also investigate a DNN
training strategy based on psychoacoustic models from speech coding for
enhancement of noisy speech
</summary>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Dinei Florencio</name>
    </author>
    <link href="http://arxiv.org/abs/1605.02427v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02427v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02542v1</id>
    <updated>2016-06-08T13:19:01Z</updated>
    <published>2016-06-08T13:19:01Z</published>
    <title>Symbolic Music Data Version 1.0</title>
    <summary>  In this document, we introduce a new dataset designed for training machine
learning models of symbolic music data. Five datasets are provided, one of
which is from a newly collected corpus of 20K midi files. We describe our
preprocessing and cleaning pipeline, which includes the exclusion of a number
of files based on scores from a previously developed probabilistic machine
learning model. We also define training, testing and validation splits for the
new dataset, based on a clustering scheme which we also describe. Some simple
histograms are included.
</summary>
    <author>
      <name>Christian Walder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1606.01368</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02542v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02542v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02816v2</id>
    <updated>2016-11-11T02:45:44Z</updated>
    <published>2016-06-09T04:01:36Z</published>
    <title>Audio Content based Geotagging in Multimedia</title>
    <summary>  In this paper we propose methods to extract geographically relevant
information in a multimedia recording using its audio. Our method primarily is
based on the fact that urban acoustic environment consists of a variety of
sounds. Hence, location information can be inferred from the composition of
sound events/classes present in the audio. More specifically, we adopt matrix
factorization techniques to obtain semantic content of recording in terms of
different sound classes. These semantic information are then combined to
identify the location of recording.
</summary>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Benjamin Elizalde</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02816v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02816v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03365v4</id>
    <updated>2017-06-27T09:34:28Z</updated>
    <published>2015-12-17T10:40:22Z</published>
    <title>Acoustic Characterization of Environments (ACE) Challenge Results
  Technical Report</title>
    <summary>  This document provides the results of the tests of acoustic parameter
estimation algorithms on the Acoustic Characterization of Environments (ACE)
Challenge Evaluation dataset which were subsequently submitted and written up
into papers for the Proceedings of the ACE Challenge. This document is
supporting material for a forthcoming journal paper on the ACE Challenge which
will provide further analysis of the results.
</summary>
    <author>
      <name>James Eaton</name>
    </author>
    <author>
      <name>Nikolay D. Gaubitch</name>
    </author>
    <author>
      <name>Alastair H. Moore</name>
    </author>
    <author>
      <name>Patrick A. Naylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Supporting material for Proceedings of the ACE Challenge Workshop - a
  satellite event of IEEE-WASPAA 2015 (arXiv:1510.00383)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03365v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03365v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03664v1</id>
    <updated>2016-06-12T04:07:45Z</updated>
    <published>2016-06-12T04:07:45Z</published>
    <title>Weakly Supervised Scalable Audio Content Analysis</title>
    <summary>  Audio Event Detection is an important task for content analysis of multimedia
data. Most of the current works on detection of audio events is driven through
supervised learning approaches. We propose a weakly supervised learning
framework which can make use of the tremendous amount of web multimedia data
with significantly reduced annotation effort and expense. Specifically, we use
several multiple instance learning algorithms to show that audio event
detection through weak labels is feasible. We also propose a novel scalable
multiple instance learning algorithm and show that its competitive with other
multiple instance learning algorithms for audio event detection tasks.
</summary>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICME 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03664v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03664v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05765v1</id>
    <updated>2016-07-19T21:29:03Z</updated>
    <published>2016-07-19T21:29:03Z</published>
    <title>Features and Kernels for Audio Event Recognition</title>
    <summary>  One of the most important problems in audio event detection research is
absence of benchmark results for comparison with any proposed method. Different
works consider different sets of events and datasets which makes it difficult
to comprehensively analyze any novel method with an existing one. In this paper
we propose to establish results for audio event recognition on two recent
publicly-available datasets. In particular we use Gaussian Mixture model based
feature representation and combine them with linear as well as non-linear
kernel Support Vector Machines.
</summary>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.05765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06642v2</id>
    <updated>2016-09-08T09:32:17Z</updated>
    <published>2016-07-22T11:56:52Z</published>
    <title>HRTF-based Robust Least-Squares Frequency-Invariant Polynomial
  Beamforming</title>
    <summary>  In this work, we propose a robust Head-Related Transfer Function (HRTF)-based
polynomial beamformer design which accounts for the influence of a humanoid
robot's head on the sound field. In addition, it allows for a flexible steering
of our previously proposed robust HRTF-based beamformer design. We evaluate the
HRTF-based polynomial beamformer design and compare it to the original
HRTF-based beamformer design by means of signal-independent measures as well as
word error rates of an off-the-shelf speech recognition system. Our results
confirm the effectiveness of the polynomial beamformer design, which makes it a
promising approach to robust beamforming for robot audition.
</summary>
    <author>
      <name>Hendrik Barfuss</name>
    </author>
    <author>
      <name>Marcel Mueglich</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, accepted for IWAENC 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.06642v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06642v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06706v2</id>
    <updated>2016-08-25T15:12:31Z</updated>
    <published>2016-07-22T15:15:53Z</published>
    <title>Experiments on the DCASE Challenge 2016: Acoustic Scene Classification
  and Sound Event Detection in Real Life Recording</title>
    <summary>  In this paper we present our work on Task 1 Acoustic Scene Classi- fication
and Task 3 Sound Event Detection in Real Life Recordings. Among our experiments
we have low-level and high-level features, classifier optimization and other
heuristics specific to each task. Our performance for both tasks improved the
baseline from DCASE: for Task 1 we achieved an overall accuracy of 78.9%
compared to the baseline of 72.6% and for Task 3 we achieved a Segment-Based
Error Rate of 0.76 compared to the baseline of 0.91.
</summary>
    <author>
      <name>Benjamin Elizalde</name>
    </author>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Ankit Shah</name>
    </author>
    <author>
      <name>Rohan Badlani</name>
    </author>
    <author>
      <name>Emmanuel Vincent</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <author>
      <name>Ian Lane</name>
    </author>
    <link href="http://arxiv.org/abs/1607.06706v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06706v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07801v1</id>
    <updated>2016-06-22T05:51:40Z</updated>
    <published>2016-06-22T05:51:40Z</published>
    <title>ABROA : Audio-Based Room-Occupancy Analysis using Gaussian Mixtures and
  Hidden Markov Models</title>
    <summary>  This paper outlines preliminary steps towards the development of an audio-
based room-occupancy analysis model. Our approach borrows from speech
recognition tradition and is based on Gaussian Mixtures and Hidden Markov
Models. We analyze possible challenges encountered in the development of such a
model, and offer several solutions including feature design and prediction
strategies. We provide results obtained from experiments with audio data from a
retail store in Palo Alto, California. Model assessment is done via
leave-two-out Bootstrap and model convergence achieves good accuracy, thus
representing a contribution to multimodal people counting algorithms.
</summary>
    <author>
      <name>Rafael Valle</name>
    </author>
    <link href="http://arxiv.org/abs/1607.07801v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07801v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03417v1</id>
    <updated>2016-08-11T11:01:21Z</updated>
    <published>2016-08-11T11:01:21Z</published>
    <title>Bird detection in audio: a survey and a challenge</title>
    <summary>  Many biological monitoring projects rely on acoustic detection of birds.
Despite increasingly large datasets, this detection is often manual or
semi-automatic, requiring manual tuning/postprocessing. We review the state of
the art in automatic bird sound detection, and identify a widespread need for
tuning-free and species-agnostic approaches. We introduce new datasets and an
IEEE research challenge to address this need, to make possible the development
of fully automatic algorithms for bird sound detection.
</summary>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Mike Wood</name>
    </author>
    <author>
      <name>Yannis Stylianou</name>
    </author>
    <author>
      <name>Hervé Glotin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MLSP.2016.7738875</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MLSP.2016.7738875" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Slightly extended preprint of paper accepted for MLSP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.07713v1</id>
    <updated>2016-08-27T14:23:51Z</updated>
    <published>2016-08-27T14:23:51Z</published>
    <title>Diffuse-field coherence of sensors with arbitrary directional responses</title>
    <summary>  Knowledge of the diffuse-field coherence between array sensors is a basic
assumption for a wide range of array processing applications. Explicit
relations previously existed only for omnidirectional and first-order
directional sensors, or a restricted arrangement of differential patterns. We
present a closed-form formulation of the theoretical coherence function between
arbitrary directionally band-limited sensors for the general cases that a) the
responses of the individual sensors are known or estimated, and the coherence
needs to be known for an arbitrary arrangement, and b) that no information on
the sensor directionality or on array geometry exists, but calibration
measurements around the array are available.
</summary>
    <author>
      <name>Archontis Politis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.07713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.07713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03409v2</id>
    <updated>2016-09-13T13:09:54Z</updated>
    <published>2016-09-12T14:07:05Z</published>
    <title>Acoustic intensity, energy-density and diffuseness estimation in a
  directionally-constrained region</title>
    <summary>  This work presents a method for estimation of the acoustic intensity, the
energy density and the associated sound field diffuseness around the origin,
when the sound field is weighted with a spatial filter. The method permits
energetic DOA estimation and sound field characterization focused in a specific
angular region determined by the beam pattern of the spatial filter. The
formulation of the estimators is presented and their behavior is analyzed for
the fundamental cases useful in parametric sound field models of a single plane
wave, a uniform diffuse field and a mixture of the two.
</summary>
    <author>
      <name>Archontis Politis</name>
    </author>
    <author>
      <name>Ville Pulkki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.03409v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03409v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04301v3</id>
    <updated>2017-04-11T15:15:20Z</updated>
    <published>2016-09-14T14:39:36Z</published>
    <title>TristouNet: Triplet Loss for Speaker Turn Embedding</title>
    <summary>  TristouNet is a neural network architecture based on Long Short-Term Memory
recurrent networks, meant to project speech sequences into a fixed-dimensional
euclidean space. Thanks to the triplet loss paradigm used for training, the
resulting sequence embeddings can be compared directly with the euclidean
distance, for speaker comparison purposes. Experiments on short (between 500ms
and 5s) speech turn comparison and speaker change detection show that
TristouNet brings significant improvements over the current state-of-the-art
techniques for both tasks.
</summary>
    <author>
      <name>Hervé Bredin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICASSP 2017 (42nd IEEE International Conference on Acoustics, Speech
  and Signal Processing). Code available at
  http://github.com/hbredin/TristouNet</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04301v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04301v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08211v1</id>
    <updated>2016-09-26T22:18:49Z</updated>
    <published>2016-09-26T22:18:49Z</published>
    <title>A Robust Diarization System for Measuring Dominance in Peer-Led Team
  Learning Groups</title>
    <summary>  Peer-Led Team Learning (PLTL) is a structured learning model where a team
leader is appointed to facilitate collaborative problem solving among students
for Science, Technology, Engineering and Mathematics (STEM) courses. This paper
presents an informed HMM-based speaker diarization system. The minimum duration
of short conversationalturns and number of participating students were fed as
side information to the HMM system. A modified form of Bayesian Information
Criterion (BIC) was used for iterative merging and re-segmentation. Finally, we
used the diarization output to compute a novel dominance score based on
unsupervised acoustic analysis.
</summary>
    <author>
      <name>Harishchandra Dubey</name>
    </author>
    <author>
      <name>Abhijeet Sangwan</name>
    </author>
    <author>
      <name>John H. L. Hansen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Workshop on Spoken Language Technology 2016, December, 2016,
  San Diego, California, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.08211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08442v2</id>
    <updated>2017-05-23T09:56:54Z</updated>
    <published>2016-09-27T13:48:01Z</published>
    <title>Collaborative Learning for Language and Speaker Recognition</title>
    <summary>  This paper presents a unified model to perform language and speaker
recognition simultaneously and altogether. The model is based on a multi-task
recurrent neural network where the output of one task is fed as the input of
the other, leading to a collaborative learning framework that can improve both
language and speaker recognition by borrowing information from each other. Our
experiments demonstrated that the multi-task model outperforms the
task-specific models on both tasks.
</summary>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Zhiyuan Tang</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Andrew Abel</name>
    </author>
    <author>
      <name>Yang Feng</name>
    </author>
    <author>
      <name>Shiyue Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1609.08442v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08442v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.06214v1</id>
    <updated>2016-10-19T20:56:05Z</updated>
    <published>2016-10-19T20:56:05Z</published>
    <title>A model of infant speech perception and learning</title>
    <summary>  Infant speech perception and learning is modeled using Echo State Network
classification and Reinforcement Learning. Ambient speech for the modeled
infant learner is created using the speech synthesizer Vocaltractlab. An
auditory system is trained to recognize vowel sounds from a series of speakers
of different anatomies in Vocaltractlab. Having formed perceptual targets, the
infant uses Reinforcement Learning to imitate his ambient speech. A possible
way of bridging the problem of speaker normalisation is proposed, using direct
imitation but also including a caregiver who listens to the infants sounds and
imitates those that sound vowel-like.
</summary>
    <author>
      <name>Philip Zurbuchen</name>
    </author>
    <link href="http://arxiv.org/abs/1610.06214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.06214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00514v1</id>
    <updated>2016-11-02T09:24:10Z</updated>
    <published>2016-11-02T09:24:10Z</published>
    <title>The Intelligent Voice 2016 Speaker Recognition System</title>
    <summary>  This paper presents the Intelligent Voice (IV) system submitted to the NIST
2016 Speaker Recognition Evaluation (SRE). The primary emphasis of SRE this
year was on developing speaker recognition technology which is robust for novel
languages that are much more heterogeneous than those used in the current
state-of-the-art, using significantly less training data, that does not contain
meta-data from those languages. The system is based on the state-of-the-art
i-vector/PLDA which is developed on the fixed training condition, and the
results are reported on the protocol defined on the development set of the
challenge.
</summary>
    <author>
      <name>Abbas Khosravani</name>
    </author>
    <author>
      <name>Cornelius Glackin</name>
    </author>
    <author>
      <name>Nazim Dugan</name>
    </author>
    <author>
      <name>Gérard Chollet</name>
    </author>
    <author>
      <name>Nigel Cannings</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, NIST SRE 2016 Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.00514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.03477v1</id>
    <updated>2016-11-10T20:35:47Z</updated>
    <published>2016-11-10T20:35:47Z</published>
    <title>Song From PI: A Musically Plausible Network for Pop Music Generation</title>
    <summary>  We present a novel framework for generating pop music. Our model is a
hierarchical Recurrent Neural Network, where the layers and the structure of
the hierarchy encode our prior knowledge about how pop music is composed. In
particular, the bottom layers generate the melody, while the higher levels
produce the drums and chords. We conduct several human studies that show strong
preference of our generated music over that produced by the recent method by
Google. We additionally show two applications of our framework: neural dancing
and karaoke, as well as neural story singing.
</summary>
    <author>
      <name>Hang Chu</name>
    </author>
    <author>
      <name>Raquel Urtasun</name>
    </author>
    <author>
      <name>Sanja Fidler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">under review at ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.03477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.03477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09524v1</id>
    <updated>2016-11-29T08:33:48Z</updated>
    <published>2016-11-29T08:33:48Z</published>
    <title>Understanding Audio Pattern Using Convolutional Neural Network From Raw
  Waveforms</title>
    <summary>  One key step in audio signal processing is to transform the raw signal into
representations that are efficient for encoding the original information.
Traditionally, people transform the audio into spectral representations, as a
function of frequency, amplitude and phase transformation. In this work, we
take a purely data-driven approach to understand the temporal dynamics of audio
at the raw signal level. We maximize the information extracted from the raw
signal through a deep convolutional neural network (CNN) model. Our CNN model
is trained on the urbansound8k dataset. We discover that salient audio patterns
embedded in the raw waveforms can be efficiently extracted through a
combination of nonlinear filters learned by the CNN model.
</summary>
    <author>
      <name>Shuhui Qu</name>
    </author>
    <author>
      <name>Juncheng Li</name>
    </author>
    <author>
      <name>Wei Dai</name>
    </author>
    <author>
      <name>Samarjit Das</name>
    </author>
    <link href="http://arxiv.org/abs/1611.09524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09733v1</id>
    <updated>2016-11-29T17:19:45Z</updated>
    <published>2016-11-29T17:19:45Z</published>
    <title>Getting Closer to the Essence of Music: The Con Espressione Manifesto</title>
    <summary>  This text offers a personal and very subjective view on the current situation
of Music Information Research (MIR). Motivated by the desire to build systems
with a somewhat deeper understanding of music than the ones we currently have,
I try to sketch a number of challenges for the next decade of MIR research,
grouped around six simple truths about music that are probably generally agreed
on, but often ignored in everyday research.
</summary>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2899004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2899004" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint (author's accepted manuscript) of a journal article (see
  "Journal Reference")</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Intelligent Systems and Technology (TIST)
  8(2), Article No. 19, November 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.09733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.00876v1</id>
    <updated>2016-12-02T22:02:04Z</updated>
    <published>2016-12-02T22:02:04Z</published>
    <title>FRIDA: FRI-Based DOA Estimation for Arbitrary Array Layouts</title>
    <summary>  In this paper we present FRIDA---an algorithm for estimating directions of
arrival of multiple wideband sound sources. FRIDA combines multi-band
information coherently and achieves state-of-the-art resolution at extremely
low signal-to-noise ratios. It works for arbitrary array layouts, but unlike
the various steered response power and subspace methods, it does not require a
grid search. FRIDA leverages recent advances in sampling signals with a finite
rate of innovation. It is based on the insight that for any array layout, the
entries of the spatial covariance matrix can be linearly transformed into a
uniformly sampled sum of sinusoids.
</summary>
    <author>
      <name>Hanjie Pan</name>
    </author>
    <author>
      <name>Robin Scheibler</name>
    </author>
    <author>
      <name>Eric Bezzam</name>
    </author>
    <author>
      <name>Ivan Dokmanic</name>
    </author>
    <author>
      <name>Martin Vetterli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2017.7952744</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2017.7952744" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICASSP2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.00876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.00876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04919v1</id>
    <updated>2016-12-15T04:22:39Z</updated>
    <published>2016-12-15T04:22:39Z</published>
    <title>Combination of Linear Prediction and Phase Decomposition for Glottal
  Source Analysis on Voiced Speech</title>
    <summary>  Some glottal analysis approaches based upon linear prediction or complex
cepstrum approaches have been proved to be effective to estimate glottal source
from real speech utterances. We propose a new approach employing both an
all-pole odd-order linear prediction to provide a coarse estimation and phase
decomposition based causality/anti-causality separation to generate further
refinements. The obtained measures show that this method improved performance
in terms of reducing source-filter separation in estimation of glottal flow
pulses (GFP). No glottal model fitting is required by this method, thus it has
wide and flexible adaptation to retain fidelity of speakers's vocal features
with computationally affordable resource. The method is evaluated on real
speech utterances to validate it.
</summary>
    <author>
      <name>Yiqiao Chen</name>
    </author>
    <author>
      <name>John N. Gowdy</name>
    </author>
    <link href="http://arxiv.org/abs/1612.04919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05070v1</id>
    <updated>2016-12-15T14:07:51Z</updated>
    <published>2016-12-15T14:07:51Z</published>
    <title>Towards End-to-End Audio-Sheet-Music Retrieval</title>
    <summary>  This paper demonstrates the feasibility of learning to retrieve short
snippets of sheet music (images) when given a short query excerpt of music
(audio) -- and vice versa --, without any symbolic representation of music or
scores. This would be highly useful in many content-based musical retrieval
scenarios. Our approach is based on Deep Canonical Correlation Analysis (DCCA)
and learns correlated latent spaces allowing for cross-modality retrieval in
both directions. Initial experiments with relatively simple monophonic music
show promising results.
</summary>
    <author>
      <name>Matthias Dorfer</name>
    </author>
    <author>
      <name>Andreas Arzt</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In NIPS 2016 End-to-end Learning for Speech and Audio Processing
  Workshop, Barcelona, Spain</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05076v1</id>
    <updated>2016-12-15T14:16:56Z</updated>
    <published>2016-12-15T14:16:56Z</published>
    <title>Live Score Following on Sheet Music Images</title>
    <summary>  In this demo we show a novel approach to score following. Instead of relying
on some symbolic representation, we are using a multi-modal convolutional
neural network to match the incoming audio stream directly to sheet music
images. This approach is in an early stage and should be seen as proof of
concept. Nonetheless, the audience will have the opportunity to test our
implementation themselves via 3 simple piano pieces.
</summary>
    <author>
      <name>Matthias Dorfer</name>
    </author>
    <author>
      <name>Andreas Arzt</name>
    </author>
    <author>
      <name>Sebastian Böck</name>
    </author>
    <author>
      <name>Amaury Durand</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17th International Society for Music Information Retrieval Conference
  (ISMIR 2016), Late Breaking/Demo Papers, New York, NY</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05168v1</id>
    <updated>2016-12-15T17:59:05Z</updated>
    <published>2016-12-15T17:59:05Z</published>
    <title>LIA system description for NIST SRE 2016</title>
    <summary>  This paper describes the LIA speaker recognition system developed for the
Speaker Recognition Evaluation (SRE) campaign. Eight sub-systems are developed,
all based on a state-of-the-art approach: i-vector/PLDA which represents the
mainstream technique in text-independent speaker recognition. These sub-systems
differ: on the acoustic feature extraction front-end (MFCC, PLP), at the
i-vector extraction stage (UBM, DNN or two-feats posteriors) and finally on the
data-shifting (IDVC, mean-shifting). The submitted system is a fusion at the
score-level of these eight sub-systems.
</summary>
    <author>
      <name>Mickael Rouvier</name>
    </author>
    <author>
      <name>Pierre-Michel Bousquet</name>
    </author>
    <author>
      <name>Moez Ajili</name>
    </author>
    <author>
      <name>Waad Ben Kheder</name>
    </author>
    <author>
      <name>Driss Matrouf</name>
    </author>
    <author>
      <name>Jean-François Bonastre</name>
    </author>
    <link href="http://arxiv.org/abs/1612.05168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03834v1</id>
    <updated>2017-01-09T15:10:38Z</updated>
    <published>2017-01-09T15:10:38Z</published>
    <title>On Higher Order Positive Differential Energy Operator</title>
    <summary>  The higher order differential energy operator (DEO), denoted via
$\Upsilon_k(x)$, is an extension to the second order famous Teager-Kaiser
operator. The DEO helps measuring the higher order gauge of energy of a signal
which is useful for AM-FM demodulation. However, the energy criterion defined
by the DEO is not compliant with the presumption of positivity of energy. In
this paper we introduce a higher order operator called Positive Differential
Energy Operator (PDEO). This operator which can be obtained using alternative
recursive relations, resolves the energy sign problem. The simulations
demonstrate that the proposed operator can outperform DEOs in terms of Average
Signal to Error Ratio (ASER) in AM/FM demodulation.
</summary>
    <author>
      <name>Amirhossein Javaheri</name>
    </author>
    <author>
      <name>Mohammad Bagher Shamsollahi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.03834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.08156v2</id>
    <updated>2018-04-26T18:17:00Z</updated>
    <published>2017-01-27T12:38:47Z</published>
    <title>A Comprehensive Survey on Bengali Phoneme Recognition</title>
    <summary>  Hidden Markov model based various phoneme recognition methods for Bengali
language is reviewed. Automatic phoneme recognition for Bengali language using
multilayer neural network is reviewed. Usefulness of multilayer neural network
over single layer neural network is discussed. Bangla phonetic feature table
construction and enhancement for Bengali speech recognition is also discussed.
Comparison among these methods is discussed.
</summary>
    <author>
      <name>Sadia Tasnim Swarna</name>
    </author>
    <author>
      <name>Shamim Ehsan</name>
    </author>
    <author>
      <name>Md. Saiful Islam</name>
    </author>
    <author>
      <name>Marium E Jannat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, reference added in phoneme recognition methods</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.08156v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.08156v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.07713v1</id>
    <updated>2017-02-24T17:23:01Z</updated>
    <published>2017-02-24T17:23:01Z</published>
    <title>Multichannel Linear Prediction for Blind Reverberant Audio Source
  Separation</title>
    <summary>  A class of methods based on multichannel linear prediction (MCLP) can achieve
effective blind dereverberation of a source, when the source is observed with a
microphone array. We propose an inventive use of MCLP as a pre-processing step
for blind source separation with a microphone array. We show theoretically
that, under certain assumptions, such pre-processing reduces the original blind
reverberant source separation problem to a non-reverberant one, which in turn
can be effectively tackled using existing methods. We demonstrate our claims
using real recordings obtained with an eight-microphone circular array in
reverberant environments.
</summary>
    <author>
      <name>İlker Bayram</name>
    </author>
    <author>
      <name>Savaşkan Bulek</name>
    </author>
    <link href="http://arxiv.org/abs/1702.07713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00009v1</id>
    <updated>2017-02-28T18:26:44Z</updated>
    <published>2017-02-28T18:26:44Z</published>
    <title>Nonlinear Model and its Inverse of an Audio System</title>
    <summary>  This computer science master thesis aims at modelling the nonlinearities of a
loudspeaker. A piecewise linear approximation is initially explored and then we
present a nonlinear Volterra model to simulate the behavior of the system. The
general theory of continuous and discrete Volterra series is summarised. A
Normalized Least Mean Square algorithm is used to determine the Volterra series
to third order. We also present as inverted system which is trained with the
same algorithm. Training data for the models were collected measuring a
physical speaker using a laser interferometer. Results indicate a decrease in
Mean Squared Error compared to the linear model with a dependency on the
particular test signal, the order and the parameters of the model.
</summary>
    <author>
      <name>Alessandro Loriga</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">113 pages, master thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.00009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.02317v1</id>
    <updated>2017-03-07T10:36:30Z</updated>
    <published>2017-03-07T10:36:30Z</published>
    <title>Convolutional Recurrent Neural Networks for Bird Audio Detection</title>
    <summary>  Bird sounds possess distinctive spectral structure which may exhibit small
shifts in spectrum depending on the bird species and environmental conditions.
In this paper, we propose using convolutional recurrent neural networks on the
task of automated bird audio detection in real-life environments. In the
proposed method, convolutional layers extract high dimensional, local frequency
shift invariant features, while recurrent layers capture longer term
dependencies between the features extracted from short time frames. This method
achieves 88.5% Area Under ROC Curve (AUC) score on the unseen evaluation data
and obtains the second place in the Bird Audio Detection challenge.
</summary>
    <author>
      <name> EmreÇakır</name>
    </author>
    <author>
      <name>Sharath Adavanne</name>
    </author>
    <author>
      <name>Giambattista Parascandolo</name>
    </author>
    <author>
      <name>Konstantinos Drossos</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to EUSIPCO 2017 Special Session on Bird Audio Signal
  Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.02317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.02317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07588v2</id>
    <updated>2017-08-31T12:01:36Z</updated>
    <published>2017-03-22T10:08:51Z</published>
    <title>Gate Activation Signal Analysis for Gated Recurrent Neural Networks and
  Its Correlation with Phoneme Boundaries</title>
    <summary>  In this paper we analyze the gate activation signals inside the gated
recurrent neural networks, and find the temporal structure of such signals is
highly correlated with the phoneme boundaries. This correlation is further
verified by a set of experiments for phoneme segmentation, in which better
results compared to standard approaches were obtained.
</summary>
    <author>
      <name>Yu-Hsuan Wang</name>
    </author>
    <author>
      <name>Cheng-Tao Chung</name>
    </author>
    <author>
      <name>Hung-yi Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, The code is available at
  https://github.com/allyoushawn/timit_gas.git</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.07588v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07588v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04971v1</id>
    <updated>2017-05-14T14:43:10Z</updated>
    <published>2017-05-14T14:43:10Z</published>
    <title>Musical Instrument Recognition Using Their Distinctive Characteristics
  in Artificial Neural Networks</title>
    <summary>  In this study an Artificial Neural Network was trained to classify musical
instruments, using audio samples transformed to the frequency domain. Different
features of the sound, in both time and frequency domain, were analyzed and
compared in relation to how much information that could be derived from that
limited data. The study concluded that in comparison with the base experiment,
that had an accuracy of 93.5%, using the attack only resulted in 80.2% and the
initial 100 Hz in 64.2%.
</summary>
    <author>
      <name>Babak Toghiani-Rizi</name>
    </author>
    <author>
      <name>Marcus Windmark</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Results based on a study conducted during the course Machine Learning
  at Uppsala University</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.04971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05458v4</id>
    <updated>2018-11-12T22:27:43Z</updated>
    <published>2017-05-15T21:21:06Z</published>
    <title>Music generation with variational recurrent autoencoder supported by
  history</title>
    <summary>  A new architecture of an artificial neural network that helps to generate
longer melodic patterns is introduced alongside with methods for
post-generation filtering. The proposed approach called variational autoencoder
supported by history is based on a recurrent highway gated network combined
with a variational autoencoder. Combination of this architecture with filtering
heuristics allows generating pseudo-live acoustically pleasing and melodically
diverse music.
</summary>
    <author>
      <name>Ivan P. Yamshchikov</name>
    </author>
    <author>
      <name>Alexey Tikhonov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s42452-020-03715-w</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s42452-020-03715-w" rel="related"/>
    <link href="http://arxiv.org/abs/1705.05458v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05458v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.10134v2</id>
    <updated>2017-05-30T13:17:47Z</updated>
    <published>2017-05-29T11:50:57Z</published>
    <title>On Residual CNN in text-dependent speaker verification task</title>
    <summary>  Deep learning approaches are still not very common in the speaker
verification field. We investigate the possibility of using deep residual
convolutional neural network with spectrograms as an input features in the
text-dependent speaker verification task. Despite the fact that we were not
able to surpass the baseline system in quality, we achieved a quite good
results for such a new approach getting an 5.23% ERR on the RSR2015 evaluation
part. Fusion of the baseline and proposed systems outperformed the best
individual system by 18% relatively.
</summary>
    <author>
      <name>Egor Malykh</name>
    </author>
    <author>
      <name>Sergey Novoselov</name>
    </author>
    <author>
      <name>Oleg Kudashev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for Specom 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.10134v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.10134v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02047v1</id>
    <updated>2017-06-07T05:09:41Z</updated>
    <published>2017-06-07T05:09:41Z</published>
    <title>Stacked Convolutional and Recurrent Neural Networks for Bird Audio
  Detection</title>
    <summary>  This paper studies the detection of bird calls in audio segments using
stacked convolutional and recurrent neural networks. Data augmentation by
blocks mixing and domain adaptation using a novel method of test mixing are
proposed and evaluated in regard to making the method robust to unseen data.
The contributions of two kinds of acoustic features (dominant frequency and log
mel-band energy) and their combinations are studied in the context of bird
audio detection. Our best achieved AUC measure on five cross-validations of the
development data is 95.5% and 88.1% on the unseen evaluation data.
</summary>
    <author>
      <name>Sharath Adavanne</name>
    </author>
    <author>
      <name>Konstantinos Drossos</name>
    </author>
    <author>
      <name>Emre Çakır</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for European Signal Processing Conference 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.02047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02292v1</id>
    <updated>2017-06-07T06:06:14Z</updated>
    <published>2017-06-07T06:06:14Z</published>
    <title>Stacked Convolutional and Recurrent Neural Networks for Music Emotion
  Recognition</title>
    <summary>  This paper studies the emotion recognition from musical tracks in the
2-dimensional valence-arousal (V-A) emotional space. We propose a method based
on convolutional (CNN) and recurrent neural networks (RNN), having
significantly fewer parameters compared with the state-of-the-art method for
the same task. We utilize one CNN layer followed by two branches of RNNs
trained separately for arousal and valence. The method was evaluated using the
'MediaEval2015 emotion in music' dataset. We achieved an RMSE of 0.202 for
arousal and 0.268 for valence, which is the best result reported on this
dataset.
</summary>
    <author>
      <name>Miroslav Malik</name>
    </author>
    <author>
      <name>Sharath Adavanne</name>
    </author>
    <author>
      <name>Konstantinos Drossos</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <author>
      <name>Dasa Ticha</name>
    </author>
    <author>
      <name>Roman Jarina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for Sound and Music Computing (SMC 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.02292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.05077v1</id>
    <updated>2017-06-08T11:13:32Z</updated>
    <published>2017-06-08T11:13:32Z</published>
    <title>SUT System Description for NIST SRE 2016</title>
    <summary>  This paper describes the submission to fixed condition of NIST SRE 2016 by
Sharif University of Technology (SUT) team. We provide a full description of
the systems that were included in our submission. We start with an overview of
the datasets that were used for training and development. It is followed by
describing front-ends which contain different VAD and feature types. UBM and
i-vector extractor training are the next details in this paper. As one of the
important steps in system preparation, preconditioning the i-vectors are
explained in more details. Then, we describe the classifier and score
normalization methods. And finally, some results on SRE16 evaluation dataset
are reported and analyzed.
</summary>
    <author>
      <name>Hossein Zeinali</name>
    </author>
    <author>
      <name>Hossein Sameti</name>
    </author>
    <author>
      <name>Nooshin Maghsoodi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in NIST SRE 2016 Evaluation Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.05077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.05077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.05781v1</id>
    <updated>2017-06-19T04:42:14Z</updated>
    <published>2017-06-19T04:42:14Z</published>
    <title>Kapre: On-GPU Audio Preprocessing Layers for a Quick Implementation of
  Deep Neural Network Models with Keras</title>
    <summary>  We introduce Kapre, Keras layers for audio and music signal preprocessing.
Music research using deep neural networks requires a heavy and tedious
preprocessing stage, for which audio processing parameters are often ignored in
parameter optimisation. To solve this problem, Kapre implements time-frequency
conversions, normalisation, and data augmentation as Keras layers. We report
simple benchmark results, showing real-time on-GPU preprocessing adds a
reasonable amount of computation.
</summary>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>Deokjin Joo</name>
    </author>
    <author>
      <name>Juho Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML 2017 machine learning for music discovery</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.05781v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.05781v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06810v1</id>
    <updated>2017-06-21T09:57:24Z</updated>
    <published>2017-06-21T09:57:24Z</published>
    <title>Multi-Level and Multi-Scale Feature Aggregation Using Sample-level Deep
  Convolutional Neural Networks for Music Classification</title>
    <summary>  Music tag words that describe music audio by text have different levels of
abstraction. Taking this issue into account, we propose a music classification
approach that aggregates multi-level and multi-scale features using pre-trained
feature extractors. In particular, the feature extractors are trained in
sample-level deep convolutional neural networks using raw waveforms. We show
that this approach achieves state-of-the-art results on several music
classification datasets.
</summary>
    <author>
      <name>Jongpil Lee</name>
    </author>
    <author>
      <name>Juhan Nam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML Music Discovery Workshop 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.06810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09691v1</id>
    <updated>2017-06-29T11:43:44Z</updated>
    <published>2017-06-29T11:43:44Z</published>
    <title>Speaker Identification in the Shouted Environment Using Suprasegmental
  Hidden Markov Models</title>
    <summary>  In this paper, Suprasegmental Hidden Markov Models (SPHMMs) have been used to
enhance the recognition performance of text-dependent speaker identification in
the shouted environment. Our speech database consists of two databases: our
collected database and the Speech Under Simulated and Actual Stress (SUSAS)
database. Our results show that SPHMMs significantly enhance speaker
identification performance compared to Second-Order Circular Hidden Markov
Models (CHMM2s) in the shouted environment. Using our collected database,
speaker identification performance in this environment is 68% and 75% based on
CHMM2s and SPHMMs respectively. Using the SUSAS database, speaker
identification performance in the same environment is 71% and 79% based on
CHMM2s and SPHMMs respectively.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.sigpro.2008.05.012</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.sigpro.2008.05.012" rel="related"/>
    <link href="http://arxiv.org/abs/1706.09691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09736v1</id>
    <updated>2017-06-29T13:04:50Z</updated>
    <published>2017-06-29T13:04:50Z</published>
    <title>Speaking Style Authentication Using Suprasegmental Hidden Markov Models</title>
    <summary>  The importance of speaking style authentication from human speech is gaining
an increasing attention and concern from the engineering community. The
importance comes from the demand to enhance both the naturalness and efficiency
of spoken language human-machine interface. Our work in this research focuses
on proposing, implementing, and testing speaker-dependent and text-dependent
speaking style authentication (verification) systems that accept or reject the
identity claim of a speaking style based on suprasegmental hidden Markov models
(SPHMMs). Based on using SPHMMs, our results show that the average speaking
style authentication performance is: 99%, 37%, 85%, 60%, 61%, 59%, 41%, 61%,
and 57% belonging respectively to the speaking styles: neutral, shouted, slow,
loud, soft, fast, angry, happy, and fearful.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <link href="http://arxiv.org/abs/1706.09736v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09736v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09754v1</id>
    <updated>2017-06-29T13:45:09Z</updated>
    <published>2017-06-29T13:45:09Z</published>
    <title>Speaker Identification Investigation and Analysis in Unbiased and Biased
  Emotional Talking Environments</title>
    <summary>  This work aims at investigating and analyzing speaker identification in each
unbiased and biased emotional talking environments based on a classifier called
Suprasegmental Hidden Markov Models (SPHMMs). The first talking environment is
unbiased towards any emotion, while the second talking environment is biased
towards different emotions. Each of these talking environments is made up of
six distinct emotions. These emotions are neutral, angry, sad, happy, disgust
and fear. The investigation and analysis of this work show that speaker
identification performance in the biased talking environment is superior to
that in the unbiased talking environment. The obtained results in this work are
close to those achieved in subjective assessment by human judges.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10772-012-9156-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10772-012-9156-2" rel="related"/>
    <link href="http://arxiv.org/abs/1706.09754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00138v1</id>
    <updated>2017-07-01T11:15:21Z</updated>
    <published>2017-07-01T11:15:21Z</published>
    <title>Speaker Identification in Shouted Talking Environments Based on Novel
  Third-Order Hidden Markov Models</title>
    <summary>  In this work we propose, implement, and evaluate novel models called
Third-Order Hidden Markov Models (HMM3s) to enhance low performance of
text-independent speaker identification in shouted talking environments. The
proposed models have been tested on our collected speech database using
Mel-Frequency Cepstral Coefficients (MFCCs). Our results demonstrate that HMM3s
significantly improve speaker identification performance in such talking
environments by 11.3% and 166.7% compared to second-order hidden Markov models
(HMM2s) and first-order hidden Markov models (HMM1s), respectively. The
achieved results based on the proposed models are close to those obtained in
subjective assessment by human listeners.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 4th International Conference on Audio, Language and Image
  Processing (ICALIP2014), Shanghai, China, 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.00138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00276v1</id>
    <updated>2017-07-02T10:31:10Z</updated>
    <published>2017-07-02T10:31:10Z</published>
    <title>Emirati Speaker Verification Based on HMM1s, HMM2s, and HMM3s</title>
    <summary>  This work focuses on Emirati speaker verification systems in neutral talking
environments based on each of First-Order Hidden Markov Models (HMM1s),
Second-Order Hidden Markov Models (HMM2s), and Third-Order Hidden Markov Models
(HMM3s) as classifiers. These systems have been evaluated on our collected
Emirati speech database which is comprised of 25 male and 25 female Emirati
speakers using Mel-Frequency Cepstral Coefficients (MFCCs) as extracted
features. Our results show that HMM3s outperform each of HMM1s and HMM2s for a
text-independent Emirati speaker verification. The obtained results based on
HMM3s are close to those achieved in subjective assessment by human listeners.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13th International Conference on Signal Processing, Chengdu, China,
  2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.00276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00679v1</id>
    <updated>2017-07-01T10:25:21Z</updated>
    <published>2017-07-01T10:25:21Z</published>
    <title>Talking Condition Identification Using Second-Order Hidden Markov Models</title>
    <summary>  This work focuses on enhancing the performance of text-dependent and
speaker-dependent talking condition identification systems using second-order
hidden Markov models (HMM2s). Our results show that the talking condition
identification performance based on HMM2s has been improved significantly
compared to first-order hidden Markov models (HMM1s). Our talking conditions in
this work are neutral, shouted, loud, angry, happy, and fear.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3rd International Conference on Information &amp; Communication
  Technologies: from Theory to Applications, Damascus, Syria, 2008. arXiv admin
  note: text overlap with arXiv:1706.09691, arXiv:1706.09716</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.00679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01653v1</id>
    <updated>2017-07-06T06:40:19Z</updated>
    <published>2017-07-06T06:40:19Z</published>
    <title>pch2csd: an application for converting Nord Modular G2 patches into
  Csound code</title>
    <summary>  The paper presents the pch2csd project, focused on converting patches of
popular Clavia Nord Modular G2 synthesizer into code of Csound language. Now
discontinued, Nord Modular G2 left a lot of interesting patches for sound
synthesis and algorithmic composition. To give this heritage a new life, we
created our project with the hope for being able to simulate the original sound
and behavior of Nord Modular.
</summary>
    <author>
      <name>Gleb Rogozinsky</name>
    </author>
    <author>
      <name>Mihail Chesnokov</name>
    </author>
    <author>
      <name>Eugene Cherny</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures, for associated source code, see
  https://github.com/gleb812/pch2csd/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 14th Sound and Music Computing Conf. (SMC 2017) (2017)
  415-421</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1707.01653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03465v1</id>
    <updated>2017-08-11T08:14:19Z</updated>
    <published>2017-08-11T08:14:19Z</published>
    <title>DNN Transfer Learning based Non-linear Feature Extraction for Acoustic
  Event Classification</title>
    <summary>  Recent acoustic event classification research has focused on training
suitable filters to represent acoustic events. However, due to limited
availability of target event databases and linearity of conventional filters,
there is still room for improving performance. By exploiting the non-linear
modeling of deep neural networks (DNNs) and their ability to learn beyond
pre-trained environments, this letter proposes a DNN-based feature extraction
scheme for the classification of acoustic events. The effectiveness and
robustness to noise of the proposed method are demonstrated using a database of
indoor surveillance environments.
</summary>
    <author>
      <name>Seongkyu Mun</name>
    </author>
    <author>
      <name>Minkyu Shin</name>
    </author>
    <author>
      <name>Suwon Shon</name>
    </author>
    <author>
      <name>Wooil Kim</name>
    </author>
    <author>
      <name>David K. Han</name>
    </author>
    <author>
      <name>Hanseok Ko</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1587/transinf.2017EDL8048</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1587/transinf.2017EDL8048" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEICE TRANSACTIONS on Information and Systems, Vol.E100-D, No.9
  (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1708.03465v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03465v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03535v1</id>
    <updated>2017-08-11T13:24:32Z</updated>
    <published>2017-08-11T13:24:32Z</published>
    <title>Neural Translation of Musical Style</title>
    <summary>  Music is an expressive form of communication often used to convey emotion in
scenarios where "words are not enough". Part of this information lies in the
musical composition where well-defined language exists. However, a significant
amount of information is added during a performance as the musician interprets
the composition. The performer injects expressiveness into the written score
through variations of different musical properties such as dynamics and tempo.
In this paper, we describe a model that can learn to perform sheet music. Our
research concludes that the generated performances are indistinguishable from a
human performance, thereby passing a test in the spirit of a "musical Turing
test".
</summary>
    <author>
      <name>Iman Malik</name>
    </author>
    <author>
      <name>Carl Henrik Ek</name>
    </author>
    <link href="http://arxiv.org/abs/1708.03535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04821v1</id>
    <updated>2017-08-16T09:23:28Z</updated>
    <published>2017-08-16T09:23:28Z</published>
    <title>Underdetermined source separation using a sparse STFT framework and
  weighted laplacian directional modelling</title>
    <summary>  The instantaneous underdetermined audio source separation problem of
K-sensors, L-sources mixing scenario (where K &lt; L) has been addressed by many
different approaches, provided the sources remain quite distinct in the virtual
positioning space spanned by the sensors. This problem can be tackled as a
directional clustering problem along the source position angles in the mixture.
The use of Generalised Directional Laplacian Densities (DLD) in the MDCT domain
for underdetermined source separation has been proposed before. Here, we derive
weighted mixtures of DLDs in a sparser representation of the data in the STFT
domain to perform separation. The proposed approach yields improved results
compared to our previous offering and compares favourably with the
state-of-the-art.
</summary>
    <author>
      <name>Thomas Sgouros</name>
    </author>
    <author>
      <name>Nikolaos Mitianoudis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/EUSIPCO.2016.7760549</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/EUSIPCO.2016.7760549" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EUSIPCO 2016, Budapest, Hungary</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.04821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05132v2</id>
    <updated>2018-01-14T21:05:59Z</updated>
    <published>2017-08-17T04:35:00Z</published>
    <title>An instrumental intelligibility metric based on information theory</title>
    <summary>  We propose a monaural intrusive instrumental intelligibility metric called
speech intelligibility in bits (SIIB). SIIB is an estimate of the amount of
information shared between a talker and a listener in bits per second. Unlike
existing information theoretic intelligibility metrics, SIIB accounts for
talker variability and statistical dependencies between time-frequency units.
Our evaluation shows that relative to state-of-the-art intelligibility metrics,
SIIB is highly correlated with the intelligibility of speech that has been
degraded by noise and processed by speech enhancement algorithms.
</summary>
    <author>
      <name>Steven Van Kuyk</name>
    </author>
    <author>
      <name>W. Bastiaan Kleijn</name>
    </author>
    <author>
      <name>Richard C. Hendriks</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2017.2774250</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2017.2774250" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in IEEE Signal Processing Letters</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Signal Processing Letters, vol. 25, no. 1, pp. 115-119, Jan.
  2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1708.05132v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05132v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05826v2</id>
    <updated>2017-10-03T11:14:13Z</updated>
    <published>2017-08-19T09:18:35Z</published>
    <title>Ensemble Of Deep Neural Networks For Acoustic Scene Classification</title>
    <summary>  Deep neural networks (DNNs) have recently achieved great success in a
multitude of classification tasks. Ensembles of DNNs have been shown to improve
the performance. In this paper, we explore the recent state-of-the-art DNNs
used for image classification. We modified these DNNs and applied them to the
task of acoustic scene classification. We conducted a number of experiments on
the TUT Acoustic Scenes 2017 dataset to empirically compare these methods.
Finally, we show that the best model improves the baseline score for DCASE-2017
Task 1 by 3.1% in the test set and by 10% in the development set.
</summary>
    <author>
      <name>Venkatesh Duppada</name>
    </author>
    <author>
      <name>Sushant Hiray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Detection and Classification of Acoustic Scenes and Events 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.05826v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05826v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05987v1</id>
    <updated>2017-08-20T16:18:20Z</updated>
    <published>2017-08-20T16:18:20Z</published>
    <title>Perceptual audio loss function for deep learning</title>
    <summary>  PESQ and POLQA , are standards are standards for automated assessment of
voice quality of speech as experienced by human beings. The predictions of
those objective measures should come as close as possible to subjective quality
scores as obtained in subjective listening tests. Wavenet is a deep neural
network originally developed as a deep generative model of raw audio
wave-forms. Wavenet architecture is based on dilated causal convolutions, which
exhibit very large receptive fields. In this short paper we suggest using the
Wavenet architecture, in particular its large receptive filed in order to learn
PESQ algorithm. By doing so we can use it as a differentiable loss function for
speech enhancement.
</summary>
    <author>
      <name>Dan Elbaz</name>
    </author>
    <author>
      <name>Michael Zibulevsky</name>
    </author>
    <link href="http://arxiv.org/abs/1708.05987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01922v3</id>
    <updated>2021-02-22T13:21:38Z</updated>
    <published>2017-09-06T12:44:01Z</published>
    <title>A Comparison of Audio Signal Preprocessing Methods for Deep Neural
  Networks on Music Tagging</title>
    <summary>  In this paper, we empirically investigate the effect of audio preprocessing
on music tagging with deep neural networks. We perform comprehensive
experiments involving audio preprocessing using different time-frequency
representations, logarithmic magnitude compression, frequency weighting, and
scaling. We show that many commonly used input preprocessing techniques are
redundant except magnitude compression.
</summary>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>György Fazekas</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages. EUSIPCO 2018 camera-ready. arXiv:1706.02361 does not have
  the overlapped part with this submission anymore</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.01922v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01922v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02076v1</id>
    <updated>2017-09-07T05:39:00Z</updated>
    <published>2017-09-07T05:39:00Z</published>
    <title>Composition by Conversation</title>
    <summary>  Most musical programming languages are developed purely for coding virtual
instruments or algorithmic compositions. Although there has been some work in
the domain of musical query languages for music information retrieval, there
has been little attempt to unify the principles of musical programming and
query languages with cognitive and natural language processing models that
would facilitate the activity of composition by conversation. We present a
prototype framework, called MusECI, that merges these domains, permitting
score-level algorithmic composition in a text editor while also supporting
connectivity to existing natural language processing frameworks.
</summary>
    <author>
      <name>Donya Quick</name>
    </author>
    <author>
      <name>Clayton T. Morrison</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures, accepted to ICMC 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1; H.5.5; I.2.4; I.2.5; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07541v1</id>
    <updated>2017-09-21T23:23:49Z</updated>
    <published>2017-09-21T23:23:49Z</published>
    <title>A fundamental frequency estimation method for tonal sounds inspired on
  bird song studies</title>
    <summary>  A fast implementation of fundamental frequency estimation is presented in
this work. The algorithm is based on a frequency-domain approach. It was mainly
develop for tonal sounds and used in Canary bird song analysis. The method was
implemented but not restricted for this kind of data. It could be easily
adapted for other proposes. Python libraries were used to develop a code with a
simple algorithm to obtain fundamental frequency. A simple open source code is
provided in the local university repository.
</summary>
    <author>
      <name>C. Jarne</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.mex.2018.12.011</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.mex.2018.12.011" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">pre-print version of fundamental frequency estimation method for
  tonal sounds</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.07541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07908v1</id>
    <updated>2017-09-20T20:45:53Z</updated>
    <published>2017-09-20T20:45:53Z</published>
    <title>Neural Network Alternatives to Convolutive Audio Models for Source
  Separation</title>
    <summary>  Convolutive Non-Negative Matrix Factorization model factorizes a given audio
spectrogram using frequency templates with a temporal dimension. In this paper,
we present a convolutional auto-encoder model that acts as a neural network
alternative to convolutive NMF. Using the modeling flexibility granted by
neural networks, we also explore the idea of using a Recurrent Neural Network
in the encoder. Experimental results on speech mixtures from TIMIT dataset
indicate that the convolutive architecture provides a significant improvement
in separation performance in terms of BSSeval metrics.
</summary>
    <author>
      <name>Shrikant Venkataramani</name>
    </author>
    <author>
      <name>Y. Cem Subakan</name>
    </author>
    <author>
      <name>Paris Smaragdis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in MLSP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.07908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.08243v3</id>
    <updated>2018-05-31T21:39:13Z</updated>
    <published>2017-09-24T19:23:22Z</published>
    <title>A Hybrid DSP/Deep Learning Approach to Real-Time Full-Band Speech
  Enhancement</title>
    <summary>  Despite noise suppression being a mature area in signal processing, it
remains highly dependent on fine tuning of estimator algorithms and parameters.
In this paper, we demonstrate a hybrid DSP/deep learning approach to noise
suppression. A deep neural network with four hidden layers is used to estimate
ideal critical band gains, while a more traditional pitch filter attenuates
noise between pitch harmonics. The approach achieves significantly higher
quality than a traditional minimum mean squared error spectral estimator, while
keeping the complexity low enough for real-time operation at 48 kHz on a
low-power processor.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, MMSP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.08243v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.08243v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10779v1</id>
    <updated>2017-10-30T05:42:25Z</updated>
    <published>2017-10-30T05:42:25Z</published>
    <title>Generative Adversarial Source Separation</title>
    <summary>  Generative source separation methods such as non-negative matrix
factorization (NMF) or auto-encoders, rely on the assumption of an output
probability density. Generative Adversarial Networks (GANs) can learn data
distributions without needing a parametric assumption on the output density. We
show on a speech source separation experiment that, a multi-layer perceptron
trained with a Wasserstein-GAN formulation outperforms NMF, auto-encoders
trained with maximum likelihood, and variational auto-encoders in terms of
source to distortion ratio.
</summary>
    <author>
      <name>Cem Subakan</name>
    </author>
    <author>
      <name>Paris Smaragdis</name>
    </author>
    <link href="http://arxiv.org/abs/1710.10779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03228v1</id>
    <updated>2017-12-08T11:18:22Z</updated>
    <published>2017-12-08T11:18:22Z</published>
    <title>Music Transcription by Deep Learning with Data and "Artificial Semantic"
  Augmentation</title>
    <summary>  In this progress paper the previous results of the single note recognition by
deep learning are presented. The several ways for data augmentation and
"artificial semantic" augmentation are proposed to enhance efficiency of deep
learning approaches for monophonic and polyphonic note recognition by increase
of dimensions of training data, their lossless and lossy transformations.
</summary>
    <author>
      <name>Vladyslav Sarnatskyi</name>
    </author>
    <author>
      <name>Vadym Ovcharenko</name>
    </author>
    <author>
      <name>Mariia Tkachenko</name>
    </author>
    <author>
      <name>Sergii Stirenko</name>
    </author>
    <author>
      <name>Yuri Gordienko</name>
    </author>
    <author>
      <name>Anis Rojbi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Systems Applications Engineering and
  Development, 11, 212-215 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.03228v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03228v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.04276v1</id>
    <updated>2017-12-12T13:17:30Z</updated>
    <published>2017-12-12T13:17:30Z</published>
    <title>Multi-Speaker Localization Using Convolutional Neural Network Trained
  with Noise</title>
    <summary>  The problem of multi-speaker localization is formulated as a multi-class
multi-label classification problem, which is solved using a convolutional
neural network (CNN) based source localization method. Utilizing the common
assumption of disjoint speaker activities, we propose a novel method to train
the CNN using synthesized noise signals. The proposed localization method is
evaluated for two speakers and compared to a well-known steered response power
method.
</summary>
    <author>
      <name>Soumitro Chakrabarty</name>
    </author>
    <author>
      <name>Emanuël A. P. Habets</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Machine Learning for Audio Processing (ML4Audio)
  Workshop at NIPS 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.04276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.04382v2</id>
    <updated>2017-12-22T17:33:47Z</updated>
    <published>2017-12-12T16:43:33Z</published>
    <title>auDeep: Unsupervised Learning of Representations from Audio with Deep
  Recurrent Neural Networks</title>
    <summary>  auDeep is a Python toolkit for deep unsupervised representation learning from
acoustic data. It is based on a recurrent sequence to sequence autoencoder
approach which can learn representations of time series data by taking into
account their temporal dynamics. We provide an extensive command line interface
in addition to a Python API for users and developers, both of which are
comprehensively documented and publicly available at
https://github.com/auDeep/auDeep. Experimental results indicate that auDeep
features are competitive with state-of-the art audio classification.
</summary>
    <author>
      <name>Michael Freitag</name>
    </author>
    <author>
      <name>Shahin Amiriparian</name>
    </author>
    <author>
      <name>Sergey Pugachevskiy</name>
    </author>
    <author>
      <name>Nicholas Cummins</name>
    </author>
    <author>
      <name>Björn Schuller</name>
    </author>
    <link href="http://arxiv.org/abs/1712.04382v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.04382v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05119v1</id>
    <updated>2017-12-14T08:13:02Z</updated>
    <published>2017-12-14T08:13:02Z</published>
    <title>DLR : Toward a deep learned rhythmic representation for music content
  analysis</title>
    <summary>  In the use of deep neural networks, it is crucial to provide appropriate
input representations for the network to learn from. In this paper, we propose
an approach to learn a representation that focus on rhythmic representation
which is named as DLR (Deep Learning Rhythmic representation). The proposed
approach aims to learn DLR from the raw audio signal and use it for other music
informatics tasks. A 1-dimensional convolutional network is utilised in the
learning of DLR. In the experiment, we present the results from the source task
and the target task as well as visualisations of DLRs. The results reveals that
DLR provides compact rhythmic information which can be used on multi-tagging
task.
</summary>
    <author>
      <name>Yeonwoo Jeong</name>
    </author>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>Hosan Jeong</name>
    </author>
    <link href="http://arxiv.org/abs/1712.05119v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05119v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08370v1</id>
    <updated>2017-12-22T09:49:26Z</updated>
    <published>2017-12-22T09:49:26Z</published>
    <title>Music Genre Classification with Paralleling Recurrent Convolutional
  Neural Network</title>
    <summary>  Deep learning has been demonstrated its effectiveness and efficiency in music
genre classification. However, the existing achievements still have several
shortcomings which impair the performance of this classification task. In this
paper, we propose a hybrid architecture which consists of the paralleling CNN
and Bi-RNN blocks. They focus on spatial features and temporal frame orders
extraction respectively. Then the two outputs are fused into one powerful
representation of musical signals and fed into softmax function for
classification. The paralleling network guarantees the extracting features
robust enough to represent music. Moreover, the experiments prove our proposed
architecture improve the music genre classification performance and the
additional Bi-RNN block is a supplement for CNNs.
</summary>
    <author>
      <name>Lin Feng</name>
    </author>
    <author>
      <name>Shenlan Liu</name>
    </author>
    <author>
      <name>Jianing Yao</name>
    </author>
    <link href="http://arxiv.org/abs/1712.08370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.02384v1</id>
    <updated>2018-01-08T11:17:56Z</updated>
    <published>2018-01-08T11:17:56Z</published>
    <title>Attacking Speaker Recognition With Deep Generative Models</title>
    <summary>  In this paper we investigate the ability of generative adversarial networks
(GANs) to synthesize spoofing attacks on modern speaker recognition systems. We
first show that samples generated with SampleRNN and WaveNet are unable to fool
a CNN-based speaker recognition system. We propose a modification of the
Wasserstein GAN objective function to make use of data that is real but not
from the class being learned. Our semi-supervised learning method is able to
perform both targeted and untargeted attacks, raising questions related to
security in speaker authentication systems.
</summary>
    <author>
      <name>Wilson Cai</name>
    </author>
    <author>
      <name>Anish Doshi</name>
    </author>
    <author>
      <name>Rafael Valle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 Figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.02384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.02384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04081v1</id>
    <updated>2018-01-12T08:10:53Z</updated>
    <published>2018-01-12T08:10:53Z</published>
    <title>Separation of Instrument Sounds using Non-negative Matrix Factorization
  with Spectral Envelope Constraints</title>
    <summary>  Spectral envelope is one of the most important features that characterize the
timbre of an instrument sound. However, it is difficult to use spectral
information in the framework of conventional spectrogram decomposition methods.
We overcome this problem by suggesting a simple way to provide a constraint on
the spectral envelope calculated by linear prediction. In the first part of
this study, we use a pre-trained spectral envelope of known instruments as the
constraint. Then we apply the same idea to a blind scenario in which the
instruments are unknown. The experimental results reveal that the proposed
method outperforms the conventional methods.
</summary>
    <author>
      <name>Jeongsoo Park</name>
    </author>
    <author>
      <name>Jaeyoung Shin</name>
    </author>
    <author>
      <name>Kyogu Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1801.04081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.01405v1</id>
    <updated>2018-01-31T22:18:56Z</updated>
    <published>2018-01-31T22:18:56Z</published>
    <title>Comparing approaches for mitigating intergroup variability in
  personality recognition</title>
    <summary>  Personality have been found to predict many life outcomes, and there have
been huge interests on automatic personality recognition from a speaker's
utterance. Previously, we achieved accuracies between 37%-44% for three-way
classification of high, medium or low for each of the Big Five personality
traits (Openness to Experience, Conscientiousness, Extraversion, Agreeableness,
Neuroticism). We show here that we can improve performance on this task by
accounting for heterogeneity of gender and L1 in our data, which has English
speech from female and male native speakers of Chinese and Standard American
English (SAE). We experiment with personalizing models by L1 and gender and
normalizing features by speaker, L1 group, and/or gender.
</summary>
    <author>
      <name>Guozhen An</name>
    </author>
    <author>
      <name>Rivka Levitan</name>
    </author>
    <link href="http://arxiv.org/abs/1802.01405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.01405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08008v1</id>
    <updated>2018-02-22T12:24:24Z</updated>
    <published>2018-02-22T12:24:24Z</published>
    <title>Sounderfeit: Cloning a Physical Model with Conditional Adversarial
  Autoencoders</title>
    <summary>  An adversarial autoencoder conditioned on known parameters of a physical
modeling bowed string synthesizer is evaluated for use in parameter estimation
and resynthesis tasks. Latent dimensions are provided to capture variance not
explained by the conditional parameters. Results are compared with and without
the adversarial training, and a system capable of "copying" a given
parameter-signal bidirectional relationship is examined. A real-time synthesis
system built on a generative, conditioned and regularized neural network is
presented, allowing to construct engaging sound synthesizers based purely on
recorded data.
</summary>
    <author>
      <name>Stephen Sinclair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in the Brazilian Symposium on Computer Music (SBCM 2017)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. Brazilian Symp. on Comp. Music., 2017. p. 67--74</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.08008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.01094v3</id>
    <updated>2018-05-25T21:22:19Z</updated>
    <published>2018-03-03T02:30:55Z</published>
    <title>SpeechPy - A Library for Speech Processing and Recognition</title>
    <summary>  SpeechPy is an open source Python package that contains speech preprocessing
techniques, speech features, and important post-processing operations. It
provides most frequent used speech features including MFCCs and filterbank
energies alongside with the log-energy of filter-banks. The aim of the package
is to provide researchers with a simple tool for speech feature extraction and
processing purposes in applications such as Automatic Speech Recognition and
Speaker Verification.
</summary>
    <author>
      <name>Amirsina Torfi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.21105/joss.00749</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.21105/joss.00749" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Open Source Software, 3(27), 749, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.01094v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01094v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.08276v1</id>
    <updated>2018-03-22T09:21:56Z</updated>
    <published>2018-03-22T09:21:56Z</published>
    <title>Speaker Clustering With Neural Networks And Audio Processing</title>
    <summary>  Speaker clustering is the task of differentiating speakers in a recording. In
a way, the aim is to answer "who spoke when" in audio recordings. A common
method used in industry is feature extraction directly from the recording
thanks to MFCC features, and by using well-known techniques such as Gaussian
Mixture Models (GMM) and Hidden Markov Models (HMM). In this paper, we studied
neural networks (especially CNN) followed by clustering and audio processing in
the quest to reach similar accuracy to state-of-the-art methods.
</summary>
    <author>
      <name>Maxime Jumelle</name>
    </author>
    <author>
      <name>Taqiyeddine Sakmeche</name>
    </author>
    <link href="http://arxiv.org/abs/1803.08276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.08276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.09033v1</id>
    <updated>2018-03-24T02:06:11Z</updated>
    <published>2018-03-24T02:06:11Z</published>
    <title>Automatic Music Accompanist</title>
    <summary>  Automatic musical accompaniment is where a human musician is accompanied by a
computer musician. The computer musician is able to produce musical
accompaniment that relates musically to the human performance. The
accompaniment should follow the performance using observations of the notes
they are playing. This paper describes a complete and detailed construction of
a score following and accompanying system using Hidden Markov Models (HMMs). It
details how to train a score HMM, how to deal with polyphonic input, how this
HMM work when following score, how to build up a musical accompanist. It
proposes a new parallel hidden Markov model for score following and a fast
decoding algorithm to deal with performance errors.
</summary>
    <author>
      <name>Anyi Rao</name>
    </author>
    <author>
      <name>Francis Lau</name>
    </author>
    <link href="http://arxiv.org/abs/1803.09033v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09033v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06779v1</id>
    <updated>2018-04-18T15:11:19Z</updated>
    <published>2018-04-18T15:11:19Z</published>
    <title>Shaking Acoustic Spectral Sub-bands Can Better Regularize Learning in
  Affective Computing</title>
    <summary>  In this work, we investigate a recently proposed regularization technique
based on multi-branch architectures, called Shake-Shake regularization, for the
task of speech emotion recognition. In addition, we also propose variants to
incorporate domain knowledge into model configurations. The experimental
results demonstrate: $1)$ independently shaking sub-bands delivers favorable
models compared to shaking the entire spectral-temporal feature maps. $2)$ with
proper patience in early stopping, the proposed models can simultaneously
outperform the baseline and maintain a smaller performance gap between training
and validation.
</summary>
    <author>
      <name>Che-Wei Huang</name>
    </author>
    <author>
      <name>Shrikanth Narayanan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICASSP paper with follow-up exps</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.06779v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06779v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.07300v1</id>
    <updated>2018-04-18T21:14:00Z</updated>
    <published>2018-04-18T21:14:00Z</published>
    <title>Generating Music using an LSTM Network</title>
    <summary>  A model of music needs to have the ability to recall past details and have a
clear, coherent understanding of musical structure. Detailed in the paper is a
neural network architecture that predicts and generates polyphonic music
aligned with musical rules. The probabilistic model presented is a Bi-axial
LSTM trained with a kernel reminiscent of a convolutional kernel. When analyzed
quantitatively and qualitatively, this approach performs well in composing
polyphonic music. Link to the code is provided.
</summary>
    <author>
      <name>Nikhil Kotecha</name>
    </author>
    <author>
      <name>Paul Young</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.07300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.07300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.09202v1</id>
    <updated>2018-04-24T18:28:48Z</updated>
    <published>2018-04-24T18:28:48Z</published>
    <title>Vocal melody extraction using patch-based CNN</title>
    <summary>  A patch-based convolutional neural network (CNN) model presented in this
paper for vocal melody extraction in polyphonic music is inspired from object
detection in image processing. The input of the model is a novel time-frequency
representation which enhances the pitch contours and suppresses the harmonic
components of a signal. This succinct data representation and the patch-based
CNN model enable an efficient training process with limited labeled data.
Experiments on various datasets show excellent speed and competitive accuracy
comparing to other deep learning approaches.
</summary>
    <author>
      <name>Li Su</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. Int. Conf. Acoustic, Speech and Signal Processing (ICASSP),
  2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1804.09202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.09202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.09808v2</id>
    <updated>2018-05-02T16:56:08Z</updated>
    <published>2018-04-25T21:39:39Z</published>
    <title>Off the Beaten Track: Using Deep Learning to Interpolate Between Music
  Genres</title>
    <summary>  We describe a system based on deep learning that generates drum patterns in
the electronic dance music domain. Experimental results reveal that generated
patterns can be employed to produce musically sound and creative transitions
between different genres, and that the process of generation is of interest to
practitioners in the field.
</summary>
    <author>
      <name>Tijn Borghuis</name>
    </author>
    <author>
      <name>Alessandro Tibo</name>
    </author>
    <author>
      <name>Simone Conforti</name>
    </author>
    <author>
      <name>Luca Canciello</name>
    </author>
    <author>
      <name>Lorenzo Brusci</name>
    </author>
    <author>
      <name>Paolo Frasconi</name>
    </author>
    <link href="http://arxiv.org/abs/1804.09808v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.09808v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.11300v1</id>
    <updated>2018-04-30T16:26:45Z</updated>
    <published>2018-04-30T16:26:45Z</published>
    <title>A toolbox for rendering virtual acoustic environments in the context of
  audiology</title>
    <summary>  A toolbox for creation and rendering of dynamic virtual acoustic environments
(TASCAR) that allows direct user interaction was developed for application in
hearing aid research and audiology. This technical paper describes the general
software structure and the time-domain simulation methods, i.e., transmission
model, image source model, and render formats, used to produce virtual acoustic
environments with moving objects. Implementation-specific properties are
described, and the computational performance of the system was measured as a
function of simulation complexity. Results show that on commercially available
commonly used hardware the simulation of several hundred virtual sound sources
is possible in the time domain.
</summary>
    <author>
      <name>Giso Grimm</name>
    </author>
    <author>
      <name>Joanna Luberadzka</name>
    </author>
    <author>
      <name>Volker Hohmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3813/AAA.919337</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3813/AAA.919337" rel="related"/>
    <link href="http://arxiv.org/abs/1804.11300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.11300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09010v1</id>
    <updated>2018-06-23T17:42:32Z</updated>
    <published>2018-06-23T17:42:32Z</published>
    <title>Evaluating Gammatone Frequency Cepstral Coefficients with Neural
  Networks for Emotion Recognition from Speech</title>
    <summary>  Current approaches to speech emotion recognition focus on speech features
that can capture the emotional content of a speech signal. Mel Frequency
Cepstral Coefficients (MFCCs) are one of the most commonly used representations
for audio speech recognition and classification. This paper proposes Gammatone
Frequency Cepstral Coefficients (GFCCs) as a potentially better representation
of speech signals for emotion recognition. The effectiveness of MFCC and GFCC
representations are compared and evaluated over emotion and intensity
classification tasks with fully connected and recurrent neural network
architectures. The results provide evidence that GFCCs outperform MFCCs in
speech emotion recognition.
</summary>
    <author>
      <name>Gabrielle K. Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10570v1</id>
    <updated>2018-06-27T17:05:48Z</updated>
    <published>2018-06-27T17:05:48Z</published>
    <title>Modeling Majorness as a Perceptual Property in Music from Listener
  Ratings</title>
    <summary>  For the tasks of automatic music emotion recognition, genre recognition,
music recommendation it is helpful to be able to extract mode from any section
of a musical piece as a perceived amount of major or minor mode (majorness)
inside that section, perceived as a whole (one or several melodies and any
harmony present). In this paper we take a data-driven approach (modeling
directly from data without giving an explicit definition or explicitly
programming an algorithm) towards modeling this property. We collect
annotations from musicians and show that majorness can be understood by
musicians in an intuitive way. We model this property from the data using deep
learning.
</summary>
    <author>
      <name>Anna Aljanaki</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">short paper for ICMPC proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.10570v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10570v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02254v1</id>
    <updated>2018-07-06T04:32:18Z</updated>
    <published>2018-07-06T04:32:18Z</published>
    <title>Singing Style Transfer Using Cycle-Consistent Boundary Equilibrium
  Generative Adversarial Networks</title>
    <summary>  Can we make a famous rap singer like Eminem sing whatever our favorite song?
Singing style transfer attempts to make this possible, by replacing the vocal
of a song from the source singer to the target singer. This paper presents a
method that learns from unpaired data for singing style transfer using
generative adversarial networks.
</summary>
    <author>
      <name>Cheng-Wei Wu</name>
    </author>
    <author>
      <name>Jen-Yu Liu</name>
    </author>
    <author>
      <name>Yi-Hsuan Yang</name>
    </author>
    <author>
      <name>Jyh-Shing R. Jang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 3 figures, demo website:
  http://mirlab.org/users/haley.wu/cybegan</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICML Workshop 2018 (Joint Music Workshop)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.02254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07278v1</id>
    <updated>2018-07-19T08:13:34Z</updated>
    <published>2018-07-19T08:13:34Z</published>
    <title>Audio-to-Score Alignment using Transposition-invariant Features</title>
    <summary>  Audio-to-score alignment is an important pre-processing step for in-depth
analysis of classical music. In this paper, we apply novel
transposition-invariant audio features to this task. These low-dimensional
features represent local pitch intervals and are learned in an unsupervised
fashion by a gated autoencoder. Our results show that the proposed features are
indeed fully transposition-invariant and enable accurate alignments between
transposed scores and performances. Furthermore, they can even outperform
widely used features for audio-to-score alignment on `untransposed data', and
thus are a viable and more flexible alternative to well-established features
for music alignment and matching.
</summary>
    <author>
      <name>Andreas Arzt</name>
    </author>
    <author>
      <name>Stefan Lattner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19th International Society for Music Information Retrieval
  Conference, Paris, France, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.07278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09208v1</id>
    <updated>2018-07-24T16:14:09Z</updated>
    <published>2018-07-24T16:14:09Z</published>
    <title>A Hybrid of Deep Audio Feature and i-vector for Artist Recognition</title>
    <summary>  Artist recognition is a task of modeling the artist's musical style. This
problem is challenging because there is no clear standard. We propose a hybrid
method of the generative model i-vector and the discriminative model deep
convolutional neural network. We show that this approach achieves
state-of-the-art performance by complementing each other. In addition, we
briefly explain the advantages and disadvantages of each approach.
</summary>
    <author>
      <name>Jiyoung Park</name>
    </author>
    <author>
      <name>Donghyun Kim</name>
    </author>
    <author>
      <name>Jongpil Lee</name>
    </author>
    <author>
      <name>Sangeun Kum</name>
    </author>
    <author>
      <name>Juhan Nam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Joint Workshop on Machine Learning for Music, the 34th International
  Conference on Machine Learning (ICML), 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00960v2</id>
    <updated>2020-02-02T06:16:57Z</updated>
    <published>2018-08-02T13:47:27Z</published>
    <title>Statistical Speech Model Description with VMF Mixture Model</title>
    <summary>  In this paper, we present the LSF parameters by a unit vector form, which has
directional characteristics. The underlying distribution of this unit vector
variable is modeled by a von Mises-Fisher mixture model (VMM). With the high
rate theory, the optimal inter-component bit allocation strategy is proposed
and the distortion-rate (D-R) relation is derived for the VMM based-VQ (VVQ).
Experimental results show that the VVQ outperforms our recently introduced DVQ
and the conventional GVQ.
</summary>
    <author>
      <name>Zhanyu Ma</name>
    </author>
    <author>
      <name>Arne Leijon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00960v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00960v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06676v1</id>
    <updated>2018-08-20T19:59:09Z</updated>
    <published>2018-08-20T19:59:09Z</published>
    <title>A simple model for detection of rare sound events</title>
    <summary>  We propose a simple recurrent model for detecting rare sound events, when the
time boundaries of events are available for training. Our model optimizes the
combination of an utterance-level loss, which classifies whether an event
occurs in an utterance, and a frame-level loss, which classifies whether each
frame corresponds to the event when it does occur. The two losses make use of a
shared vectorial representation the event, and are connected by an attention
mechanism. We demonstrate our model on Task 2 of the DCASE 2017 challenge, and
achieve competitive performance.
</summary>
    <author>
      <name>Weiran Wang</name>
    </author>
    <author>
      <name>Chieh-chi Kao</name>
    </author>
    <author>
      <name>Chao Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by Interspeech 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06676v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06676v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08344v2</id>
    <updated>2018-11-11T14:41:45Z</updated>
    <published>2018-08-25T01:48:05Z</published>
    <title>Multiobjective Optimization Training of PLDA for Speaker Verification</title>
    <summary>  Most current state-of-the-art text-independent speaker verification systems
take probabilistic linear discriminant analysis (PLDA) as their backend
classifiers. The parameters of PLDA are often estimated by maximizing the
objective function, which focuses on increasing the value of log-likelihood
function, but ignoring the distinction between speakers. In order to better
distinguish speakers, we propose a multi-objective optimization training for
PLDA. Experiment results show that the proposed method has more than 10%
relative performance improvement in both EER and MinDCF on the NIST SRE14
i-vector challenge dataset, and about 20% relative performance improvement in
EER on the MCE18 dataset.
</summary>
    <author>
      <name>Liang He</name>
    </author>
    <author>
      <name>Xianhong Chen</name>
    </author>
    <author>
      <name>Can Xu</name>
    </author>
    <author>
      <name>Jia Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08344v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08344v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.00790v1</id>
    <updated>2018-10-01T16:21:55Z</updated>
    <published>2018-10-01T16:21:55Z</published>
    <title>Eigentriads and Eigenprogressions on the Tonnetz</title>
    <summary>  We introduce a new multidimensional representation, named eigenprogression
transform, that characterizes some essential patterns of Western tonal harmony
while being equivariant to time shifts and pitch transpositions. This
representation is deep, multiscale, and convolutional in the piano-roll domain,
yet incurs no prior training, and is thus suited to both supervised and
unsupervised MIR tasks. The eigenprogression transform combines ideas from the
spiral scattering transform, spectral graph theory, and wavelet shrinkage
denoising. We report state-of-the-art results on a task of supervised composer
recognition (Haydn vs. Mozart) from polyphonic music pieces in MIDI format.
</summary>
    <author>
      <name>Vincent Lostanlen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Late-Breaking / Demo session (LBD) of the
  International Society of Music Information Retrieval (ISMIR). September 2018,
  Paris, France. Source code at github.com/lostanlen/ismir2018-lbd</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.00790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.00790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.04276v1</id>
    <updated>2018-10-05T02:53:26Z</updated>
    <published>2018-10-05T02:53:26Z</published>
    <title>Current Trends and Future Research Directions for Interactive Music</title>
    <summary>  In this review, it is explained and compared different software and
formalisms used in music interaction: sequencers, computer-assisted
improvisation, meta- instruments, score-following, asynchronous dataflow
languages, synchronous dataflow languages, process calculi, temporal
constraints and interactive scores. Formal approaches have the advantage of
providing rigorous semantics of the behavior of the model and proving
correctness during execution. The main disadvantage of formal approaches is
lack of commercial tools.
</summary>
    <author>
      <name>Mauricio Toro</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Theoretical &amp; Applied Information Technologies 96(16),
  2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1810.04276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.04276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.04506v3</id>
    <updated>2019-05-20T18:41:52Z</updated>
    <published>2018-10-10T13:19:27Z</published>
    <title>On Time-frequency Scattering and Computer Music</title>
    <summary>  Time-frequency scattering is a mathematical transformation of sound waves.
Its core purpose is to mimick the way the human auditory system extracts
information from its environment. In the context of improving the artificial
intelligence of sounds, it has found succesful applications in automatic speech
transcription as well as the recognition of urban sounds and musical sounds. In
this article, we show that time-frequency scattering can also be useful for
applications in contemporary music creations.
</summary>
    <author>
      <name>Vincent Lostanlen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages. Published as a chapter in the book: "Florian Hecker:
  Halluzination, Perspektive, Synthese", pp. 97--102. Nicolaus Schafhausen,
  Vanessa Joan M\"uller, editors. Sternberg Press, Berlin, 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.04506v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.04506v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.08611v1</id>
    <updated>2018-10-19T12:50:20Z</updated>
    <published>2018-10-19T12:50:20Z</published>
    <title>A database linking piano and orchestral MIDI scores with application to
  automatic projective orchestration</title>
    <summary>  This article introduces the Projective Orchestral Database (POD), a
collection of MIDI scores composed of pairs linking piano scores to their
corresponding orchestrations. To the best of our knowledge, this is the first
database of its kind, which performs piano or orchestral prediction, but more
importantly which tries to learn the correlations between piano and orchestral
scores. Hence, we also introduce the projective orchestration task, which
consists in learning how to perform the automatic orchestration of a piano
score. We show how this task can be addressed using learning methods and also
provide methodological guidelines in order to properly use this database.
</summary>
    <author>
      <name>Léopold Crestel</name>
    </author>
    <author>
      <name>Philippe Esling</name>
    </author>
    <author>
      <name>Lena Heng</name>
    </author>
    <author>
      <name>Stephen McAdams</name>
    </author>
    <link href="http://arxiv.org/abs/1810.08611v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.08611v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.12722v1</id>
    <updated>2018-10-30T13:30:19Z</updated>
    <published>2018-10-30T13:30:19Z</published>
    <title>Feature Trajectory Dynamic Time Warping for Clustering of Speech
  Segments</title>
    <summary>  Dynamic time warping (DTW) can be used to compute the similarity between two
sequences of generally differing length. We propose a modification to DTW that
performs individual and independent pairwise alignment of feature trajectories.
The modified technique, termed feature trajectory dynamic time warping (FTDTW),
is applied as a similarity measure in the agglomerative hierarchical clustering
of speech segments. Experiments using MFCC and PLP parametrisations extracted
from TIMIT and from the Spoken Arabic Digit Dataset (SADD) show consistent and
statistically significant improvements in the quality of the resulting clusters
in terms of F-measure and normalised mutual information (NMI).
</summary>
    <author>
      <name>Lerato Lerato</name>
    </author>
    <author>
      <name>Thomas Niesler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.12722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.12722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.00003v2</id>
    <updated>2018-11-02T04:46:09Z</updated>
    <published>2018-10-31T04:52:18Z</published>
    <title>Deep Net Features for Complex Emotion Recognition</title>
    <summary>  This paper investigates the influence of different acoustic features,
audio-events based features and automatic speech translation based lexical
features in complex emotion recognition such as curiosity. Pretrained networks,
namely, AudioSet Net, VoxCeleb Net and Deep Speech Net trained extensively for
different speech based applications are studied for this objective. Information
from deep layers of these networks are considered as descriptors and encoded
into feature vectors. Experimental results on the EmoReact dataset consisting
of 8 complex emotions show the effectiveness, yielding highest F1 score of 0.85
as against the baseline of 0.69 in the literature.
</summary>
    <author>
      <name>Bhalaji Nagarajan</name>
    </author>
    <author>
      <name>V Ramana Murthy Oruganti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conflict of interest</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.00003v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.00003v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.00348v1</id>
    <updated>2018-11-01T12:53:53Z</updated>
    <published>2018-11-01T12:53:53Z</published>
    <title>Sequence-to-sequence Models for Small-Footprint Keyword Spotting</title>
    <summary>  In this paper, we propose a sequence-to-sequence model for keyword spotting
(KWS). Compared with other end-to-end architectures for KWS, our model
simplifies the pipelines of production-quality KWS system and satisfies the
requirement of high accuracy, low-latency, and small-footprint. We also
evaluate the performances of different encoder architectures, which include
LSTM and GRU. Experiments on the real-world wake-up data show that our approach
outperforms the recently proposed attention-based end-to-end model.
Specifically speaking, with 73K parameters, our sequence-to-sequence model
achieves $\sim$3.05\% false rejection rate (FRR) at 0.1 false alarm (FA) per
hour.
</summary>
    <author>
      <name>Haitong Zhang</name>
    </author>
    <author>
      <name>Junbo Zhang</name>
    </author>
    <author>
      <name>Yujun Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICASSP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.00348v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.00348v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.01850v2</id>
    <updated>2019-05-09T16:55:38Z</updated>
    <published>2018-11-05T17:12:54Z</published>
    <title>End-to-End Sound Source Separation Conditioned On Instrument Labels</title>
    <summary>  Can we perform an end-to-end music source separation with a variable number
of sources using a deep learning model? We present an extension of the
Wave-U-Net model which allows end-to-end monaural source separation with a
non-fixed number of sources. Furthermore, we propose multiplicative
conditioning with instrument labels at the bottleneck of the Wave-U-Net and
show its effect on the separation results. This approach leads to other types
of conditioning such as audio-visual source separation and score-informed
source separation.
</summary>
    <author>
      <name>Olga Slizovskaia</name>
    </author>
    <author>
      <name>Leo Kim</name>
    </author>
    <author>
      <name>Gloria Haro</name>
    </author>
    <author>
      <name>Emilia Gomez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, 2 tables, ICASSP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.01850v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.01850v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.04139v1</id>
    <updated>2018-11-09T21:24:47Z</updated>
    <published>2018-11-09T21:24:47Z</published>
    <title>Audio Spectrogram Factorization for Classification of Telephony Signals
  below the Auditory Threshold</title>
    <summary>  Traffic Pumping attacks are a form of high-volume SPAM that target telephone
networks, defraud customers and squander telephony resources. One type of call
in these attacks is characterized by very low-amplitude signal levels, notably
below the auditory threshold. We propose a technique to classify so-called
"dead air" or "silent" SPAM calls based on features derived from factorizing
the caller audio spectrogram. We describe the algorithms for feature extraction
and classification as well as our data collection methods and production
performance on millions of calls per week.
</summary>
    <author>
      <name>Iroro Orife</name>
    </author>
    <author>
      <name>Shane Walker</name>
    </author>
    <author>
      <name>Jason Flaks</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures. Marchex Technical Report on VoIP SPAM
  classification</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.04139v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.04139v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.04419v1</id>
    <updated>2018-11-11T14:05:52Z</updated>
    <published>2018-11-11T14:05:52Z</published>
    <title>Multi-Temporal Resolution Convolutional Neural Networks for Acoustic
  Scene Classification</title>
    <summary>  In this paper we present a Deep Neural Network architecture for the task of
acoustic scene classification which harnesses information from increasing
temporal resolutions of Mel-Spectrogram segments. This architecture is composed
of separated parallel Convolutional Neural Networks which learn spectral and
temporal representations for each input resolution. The resolutions are chosen
to cover fine-grained characteristics of a scene's spectral texture as well as
its distribution of acoustic events. The proposed model shows a 3.56% absolute
improvement of the best performing single resolution model and 12.49% of the
DCASE 2017 Acoustic Scenes Classification task baseline.
</summary>
    <author>
      <name>Alexander Schindler</name>
    </author>
    <author>
      <name>Thomas Lidy</name>
    </author>
    <author>
      <name>Andreas Rauber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the Detection and Classification of Acoustic Scenes
  and Events 2017 Workshop (DCASE2017), November 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.04419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.04419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.04448v1</id>
    <updated>2018-11-11T18:58:30Z</updated>
    <published>2018-11-11T18:58:30Z</published>
    <title>A Multi-modal Deep Neural Network approach to Bird-song identification</title>
    <summary>  We present a multi-modal Deep Neural Network (DNN) approach for bird song
identification. The presented approach takes both audio samples and metadata as
input. The audio is fed into a Convolutional Neural Network (CNN) using four
convolutional layers. The additionally provided metadata is processed using
fully connected layers. The flattened convolutional layers and the fully
connected layer of the metadata are joined and fed into a fully connected
layer. The resulting architecture achieved 2., 3. and 4. rank in the
BirdCLEF2017 task in various training configurations.
</summary>
    <author>
      <name>Botond Fazeka</name>
    </author>
    <author>
      <name>Alexander Schindler</name>
    </author>
    <author>
      <name>Thomas Lidy</name>
    </author>
    <author>
      <name>Andreas Rauber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LifeCLEF 2017 working notes, Dublin, Ireland</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.04448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.04448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.05550v2</id>
    <updated>2018-11-16T19:54:20Z</updated>
    <published>2018-11-13T22:27:17Z</published>
    <title>Neural Wavetable: a playable wavetable synthesizer using neural networks</title>
    <summary>  We present Neural Wavetable, a proof-of-concept wavetable synthesizer that
uses neural networks to generate playable wavetables. The system can produce
new, distinct waveforms through the interpolation of traditional wavetables in
an autoencoder's latent space. It is available as a VST/AU plugin for use in a
Digital Audio Workstation.
</summary>
    <author>
      <name>Lamtharn Hantrakul</name>
    </author>
    <author>
      <name>Li-Chia Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, Accepted by Conference on Neural Information Processing
  Systems (NIPS), Workshop on Machine Learning for Creativity and Design</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.05550v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.05550v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.06016v2</id>
    <updated>2021-12-02T13:46:46Z</updated>
    <published>2018-11-14T19:16:27Z</published>
    <title>To bee or not to bee: Investigating machine learning approaches for
  beehive sound recognition</title>
    <summary>  In this work, we aim to explore the potential of machine learning methods to
the problem of beehive sound recognition. A major contribution of this work is
the creation and release of annotations for a selection of beehive recordings.
By experimenting with both support vector machines and convolutional neural
networks, we explore important aspects to be considered in the development of
beehive sound recognition systems using machine learning approaches.
</summary>
    <author>
      <name>Inês Nolasco</name>
    </author>
    <author>
      <name>Emmanouil Benetos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at Detection and Classification of Acoustic Scenes and
  Events (DCASE) workshop 2018</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Detection and Classification of Acoustic Scenes
  and Events 2018 Workshop (DCASE2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.06016v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.06016v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.06639v1</id>
    <updated>2018-11-16T00:54:05Z</updated>
    <published>2018-11-16T00:54:05Z</published>
    <title>Generating Black Metal and Math Rock: Beyond Bach, Beethoven, and
  Beatles</title>
    <summary>  We use a modified SampleRNN architecture to generate music in modern genres
such as black metal and math rock. Unlike MIDI and symbolic models, SampleRNN
generates raw audio in the time domain. This requirement becomes increasingly
important in modern music styles where timbre and space are used
compositionally. Long developmental compositions with rapid transitions between
sections are possible by increasing the depth of the network beyond the number
used for speech datasets. We are delighted by the unique characteristic
artifacts of neural synthesis.
</summary>
    <author>
      <name>Zack Zukowski</name>
    </author>
    <author>
      <name>CJ Carr</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS Workshop on Machine Learning for Creativity and Design (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1811.06639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.06639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.09607v1</id>
    <updated>2018-11-21T19:20:43Z</updated>
    <published>2018-11-21T19:20:43Z</published>
    <title>Towards Emotion Recognition: A Persistent Entropy Application</title>
    <summary>  Emotion recognition and classification is a very active area of research. In
this paper, we present a first approach to emotion classification using
persistent entropy and support vector machines. A topology-based model is
applied to obtain a single real number from each raw signal. These data are
used as input of a support vector machine to classify signals into 8 different
emotions (calm, happy, sad, angry, fearful, disgust and surprised).
</summary>
    <author>
      <name>R. Gonzalez-Diaz</name>
    </author>
    <author>
      <name>E. Paluzo-Hidalgo</name>
    </author>
    <author>
      <name>J. F. Quesada</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-10828-1_8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-10828-1_8" rel="related"/>
    <link href="http://arxiv.org/abs/1811.09607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.09607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.11663v1</id>
    <updated>2018-11-28T16:43:56Z</updated>
    <published>2018-11-28T16:43:56Z</published>
    <title>Multiple source direction of arrival estimation using subspace
  pseudointensity vectors</title>
    <summary>  The recently proposed subspace pseudointensity method for direction of
arrival estimation is applied in the context of Tasks 1 and 2 of the LOCATA
Challenge using the Eigenmike recordings. Specific implementation details are
described and results reported for the development dataset, for which the
ground truth source directions are available. For both single and multiple
source scenarios, the average absolute error angle is about 9 degrees.
</summary>
    <author>
      <name>Alastair H. Moore</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the LOCATA Challenge Workshop - a satellite event
  of IWAENC 2018 (arXiv:1811.08482 )</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.11663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.11663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.08608v1</id>
    <updated>2019-01-24T19:02:17Z</updated>
    <published>2019-01-24T19:02:17Z</published>
    <title>Multi-stream Network With Temporal Attention For Environmental Sound
  Classification</title>
    <summary>  Environmental sound classification systems often do not perform robustly
across different sound classification tasks and audio signals of varying
temporal structures. We introduce a multi-stream convolutional neural network
with temporal attention that addresses these problems. The network relies on
three input streams consisting of raw audio and spectral features and utilizes
a temporal attention function computed from energy changes over time. Training
and classification utilizes decision fusion and data augmentation techniques
that incorporate uncertainty. We evaluate this network on three commonly used
data sets for environmental sound and audio scene classification and achieve
new state-of-the-art performance without any changes in network architecture or
front-end preprocessing, thus demonstrating better generalizability.
</summary>
    <author>
      <name>Xinyu Li</name>
    </author>
    <author>
      <name>Venkata Chebiyyam</name>
    </author>
    <author>
      <name>Katrin Kirchhoff</name>
    </author>
    <link href="http://arxiv.org/abs/1901.08608v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.08608v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.07292v1</id>
    <updated>2019-02-19T21:31:50Z</updated>
    <published>2019-02-19T21:31:50Z</published>
    <title>Data Efficient Voice Cloning for Neural Singing Synthesis</title>
    <summary>  There are many use cases in singing synthesis where creating voices from
small amounts of data is desirable. In text-to-speech there have been several
promising results that apply voice cloning techniques to modern deep learning
based models. In this work, we adapt one such technique to the case of singing
synthesis. By leveraging data from many speakers to first create a multispeaker
model, small amounts of target data can then efficiently adapt the model to new
unseen voices. We evaluate the system using listening tests across a number of
different use cases, languages and kinds of data.
</summary>
    <author>
      <name>Merlijn Blaauw</name>
    </author>
    <author>
      <name>Jordi Bonada</name>
    </author>
    <author>
      <name>Ryunosuke Daido</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICASSP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.07292v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.07292v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.10839v1</id>
    <updated>2019-03-26T12:43:09Z</updated>
    <published>2019-03-26T12:43:09Z</published>
    <title>Musical Tempo and Key Estimation using Convolutional Neural Networks
  with Directional Filters</title>
    <summary>  In this article we explore how the different semantics of spectrograms' time
and frequency axes can be exploited for musical tempo and key estimation using
Convolutional Neural Networks (CNN). By addressing both tasks with the same
network architectures ranging from shallow, domain-specific approaches to deep
variants with directional filters, we show that axis-aligned architectures
perform similarly well as common VGG-style networks developed for computer
vision, while being less vulnerable to confounding factors and requiring fewer
model parameters.
</summary>
    <author>
      <name>Hendrik Schreiber</name>
    </author>
    <author>
      <name>Meinard Müller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Sound &amp; Music Computing Conference (SMC), M\'alaga, Spain, May 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.10839v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.10839v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.03479v1</id>
    <updated>2019-04-06T15:53:43Z</updated>
    <published>2019-04-06T15:53:43Z</published>
    <title>Large Margin Softmax Loss for Speaker Verification</title>
    <summary>  In neural network based speaker verification, speaker embedding is expected
to be discriminative between speakers while the intra-speaker distance should
remain small. A variety of loss functions have been proposed to achieve this
goal. In this paper, we investigate the large margin softmax loss with
different configurations in speaker verification. Ring loss and minimum
hyperspherical energy criterion are introduced to further improve the
performance. Results on VoxCeleb show that our best system outperforms the
baseline approach by 15\% in EER, and by 13\%, 33\% in minDCF08 and minDCF10,
respectively.
</summary>
    <author>
      <name>Yi Liu</name>
    </author>
    <author>
      <name>Liang He</name>
    </author>
    <author>
      <name>Jia Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to Interspeech 2019. The code and models have been released</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.03479v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.03479v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.03617v1</id>
    <updated>2019-04-07T10:11:58Z</updated>
    <published>2019-04-07T10:11:58Z</published>
    <title>VAE-based regularization for deep speaker embedding</title>
    <summary>  Deep speaker embedding has achieved state-of-the-art performance in speaker
recognition. A potential problem of these embedded vectors (called `x-vectors')
are not Gaussian, causing performance degradation with the famous PLDA back-end
scoring. In this paper, we propose a regularization approach based on
Variational Auto-Encoder (VAE). This model transforms x-vectors to a latent
space where mapped latent codes are more Gaussian, hence more suitable for PLDA
scoring.
</summary>
    <author>
      <name>Yang Zhang</name>
    </author>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1904.03617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.03617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.03787v1</id>
    <updated>2019-04-08T00:39:30Z</updated>
    <published>2019-04-08T00:39:30Z</published>
    <title>Bayesian Non-Parametric Multi-Source Modelling Based Determined Blind
  Source Separation</title>
    <summary>  This paper proposes a determined blind source separation method using
Bayesian non-parametric modelling of sources. Conventionally source signals are
separated from a given set of mixture signals by modelling them using
non-negative matrix factorization (NMF). However in NMF, a latent variable
signifying model complexity must be appropriately specified to avoid
over-fitting or under-fitting. As real-world sources can be of varying and
unknown complexities, we propose a Bayesian non-parametric framework which is
invariant to such latent variables. We show that our proposed method adapts to
different source complexities, while conventional methods require parameter
tuning for optimal separation.
</summary>
    <author>
      <name>Chaitanya Narisetty</name>
    </author>
    <author>
      <name>Tatsuya Komatsu</name>
    </author>
    <author>
      <name>Reishi Kondo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures. Accepted at ICASSP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.03787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.03787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.08971v1</id>
    <updated>2019-04-18T18:38:28Z</updated>
    <published>2019-04-18T18:38:28Z</published>
    <title>On Acoustic Modeling for Broadband Beamforming</title>
    <summary>  In this work, we describe limitations of the free-field propagation model for
designing broadband beamformers for microphone arrays on a rigid surface.
Towards this goal, we describe a general framework for quantifying the
microphone array performance in a general wave-field by directly solving the
acoustic wave equation. The model utilizes Finite-Element-Method (FEM) for
evaluating the response of the microphone array surface to background 3D planar
and spherical waves. The effectiveness of the framework is established by
designing and evaluating a representative broadband beamformer under realistic
acoustic conditions.
</summary>
    <author>
      <name>Amit Chhetri</name>
    </author>
    <author>
      <name>Mohamed Mansour</name>
    </author>
    <author>
      <name>Wontak Kim</name>
    </author>
    <author>
      <name>Guangdong Pan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">European Signal Processing Conference (EUSIPCO 2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1904.08971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.08971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94A12, 94A40, 94A15" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.2; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.08983v2</id>
    <updated>2020-07-26T11:18:14Z</updated>
    <published>2019-04-18T19:35:24Z</published>
    <title>TTS Skins: Speaker Conversion via ASR</title>
    <summary>  We present a fully convolutional wav-to-wav network for converting between
speakers' voices, without relying on text. Our network is based on an
encoder-decoder architecture, where the encoder is pre-trained for the task of
Automatic Speech Recognition, and a multi-speaker waveform decoder is trained
to reconstruct the original signal in an autoregressive manner. We train the
network on narrated audiobooks, and demonstrate multi-voice TTS in those
voices, by converting the voice of a TTS robot.
</summary>
    <author>
      <name>Adam Polyak</name>
    </author>
    <author>
      <name>Lior Wolf</name>
    </author>
    <author>
      <name>Yaniv Taigman</name>
    </author>
    <link href="http://arxiv.org/abs/1904.08983v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.08983v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.11641v1</id>
    <updated>2019-04-26T01:42:41Z</updated>
    <published>2019-04-26T01:42:41Z</published>
    <title>Speaker Sincerity Detection based on Covariance Feature Vectors and
  Ensemble Methods</title>
    <summary>  Automatic measuring of speaker sincerity degree is a novel research problem
in computational paralinguistics. This paper proposes covariance-based feature
vectors to model speech and ensembles of support vector regressors to estimate
the degree of sincerity of a speaker. The elements of each covariance vector
are pairwise statistics between the short-term feature components. These
features are used alone as well as in combination with the ComParE acoustic
feature set. The experimental results on the development set of the Sincerity
Speech Corpus using a cross-validation procedure have shown an 8.1% relative
improvement in the Spearman's correlation coefficient over the baseline system.
</summary>
    <author>
      <name>Mohammed Senoussaoui</name>
    </author>
    <author>
      <name>Patrick Cardinal</name>
    </author>
    <author>
      <name>Najim Dehak</name>
    </author>
    <author>
      <name>Alessandro Lameiras Koerich</name>
    </author>
    <link href="http://arxiv.org/abs/1904.11641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.11641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.12194v1</id>
    <updated>2019-04-27T18:54:51Z</updated>
    <published>2019-04-27T18:54:51Z</published>
    <title>Towards Automation of Creativity: A Machine Intelligence Approach</title>
    <summary>  This paper demonstrates emergence of computational creativity in the field of
music. Different aspects of creativity such as producer, process, product and
press are studied and formulated. Different notions of computational creativity
such as novelty, quality and typicality of compositions as products are studied
and evaluated. We formulate an algorithmic perception on human creativity and
propose a prototype that is capable of demonstrating human-level creativity. We
then validate the proposed prototype by applying various creativity benchmarks
with the results obtained and compare the proposed prototype with the other
existing computational creative systems.
</summary>
    <author>
      <name>Subodh Deolekar</name>
    </author>
    <author>
      <name>Siby Abraham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 24 figures, 12 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.12194v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.12194v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.13285v1</id>
    <updated>2019-04-30T14:50:12Z</updated>
    <published>2019-04-30T14:50:12Z</published>
    <title>Performing Structured Improvisations with pre-trained Deep Learning
  Models</title>
    <summary>  The quality of outputs produced by deep generative models for music have seen
a dramatic improvement in the last few years. However, most deep learning
models perform in "offline" mode, with few restrictions on the processing time.
Integrating these types of models into a live structured performance poses a
challenge because of the necessity to respect the beat and harmony. Further,
these deep models tend to be agnostic to the style of a performer, which often
renders them impractical for live performance. In this paper we propose a
system which enables the integration of out-of-the-box generative models by
leveraging the musician's creativity and expertise.
</summary>
    <author>
      <name>Pablo Samuel Castro</name>
    </author>
    <link href="http://arxiv.org/abs/1904.13285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.13285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.11689v1</id>
    <updated>2019-05-28T09:00:19Z</updated>
    <published>2019-05-28T09:00:19Z</published>
    <title>Demonstration of PerformanceNet: A Convolutional Neural Network Model
  for Score-to-Audio Music Generation</title>
    <summary>  We present in this paper PerformacnceNet, a neural network model we proposed
recently to achieve score-to-audio music generation. The model learns to
convert a music piece from the symbolic domain to the audio domain, assigning
performance-level attributes such as changes in velocity automatically to the
music and then synthesizing the audio. The model is therefore not just a neural
audio synthesizer, but an AI performer that learns to interpret a musical score
in its own way. The code and sample outputs of the model can be found online at
https://github.com/bwang514/PerformanceNet.
</summary>
    <author>
      <name>Yu-Hua Chen</name>
    </author>
    <author>
      <name>Bryan Wang</name>
    </author>
    <author>
      <name>Yi-Hsuan Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures, IJCAI Demo 2019 camera-ready version</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.11689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.11689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.11760v1</id>
    <updated>2019-05-28T12:08:54Z</updated>
    <published>2019-05-28T12:08:54Z</published>
    <title>Two-level Explanations in Music Emotion Recognition</title>
    <summary>  Current ML models for music emotion recognition, while generally working
quite well, do not give meaningful or intuitive explanations for their
predictions. In this work, we propose a 2-step procedure to arrive at
spectrogram-level explanations that connect certain aspects of the audio to
interpretable mid-level perceptual features, and these to the actual emotion
prediction. That makes it possible to focus on specific musical reasons for a
prediction (in terms of perceptual features), and to trace these back to
patterns in the audio that can be interpreted visually and acoustically.
</summary>
    <author>
      <name>Verena Haunschmid</name>
    </author>
    <author>
      <name>Shreyan Chowdhury</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ML4MD Workshop of the 36th International Conference on Machine
  Learning</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.11760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.11760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.12324v1</id>
    <updated>2019-05-29T11:02:02Z</updated>
    <published>2019-05-29T11:02:02Z</published>
    <title>A new definition of the distortion matrix for an audio-to-score
  alignment system</title>
    <summary>  In this paper we present a new definition of the distortion matrix for a
score following framework based on DTW. The proposal consists of arranging the
score information in a sequence of note combinations and learning a spectral
pattern for each combination using instrument models. Then, the distortion
matrix is computed using these spectral patterns and a novel decomposition of
the input signal.
</summary>
    <author>
      <name>A. J. Muñoz-Montoro</name>
    </author>
    <author>
      <name>P. Vera-Candeas</name>
    </author>
    <author>
      <name>D. Suarez-Dou</name>
    </author>
    <author>
      <name>R. Cortina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CMMSE 2019</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational and Mathematical Methods, Wiley Online Library. 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1905.12324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.12324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.01964v1</id>
    <updated>2019-08-06T05:36:01Z</updated>
    <published>2019-08-06T05:36:01Z</published>
    <title>Acceleration of rank-constrained spatial covariance matrix estimation
  for blind speech extraction</title>
    <summary>  In this paper, we propose new accelerated update rules for rank-constrained
spatial covariance model estimation, which efficiently extracts a directional
target source in diffuse background noise.The naive updat e rule requires heavy
computation such as matrix inversion or matrix multiplication. We resolve this
problem by expanding matrix inversion to reduce computational complexity; in
the parameter update step, we need neither matrix inversion nor multiplication.
In an experiment, we show that the proposed accelerated update rule achieves 87
times faster calculation than the naive one.
</summary>
    <author>
      <name>Yuki Kubo</name>
    </author>
    <author>
      <name>Norihiro Takamune</name>
    </author>
    <author>
      <name>Daichi Kitamura</name>
    </author>
    <author>
      <name>Hiroshi Saruwatari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, To appear in the Proceedings of Asia-Pacific
  Signal and Information Processing Association Annual Summit and Conference
  2019 (APSIPA 2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.01964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.07517v1</id>
    <updated>2019-08-20T03:50:57Z</updated>
    <published>2019-08-20T03:50:57Z</published>
    <title>AI for Earth: Rainforest Conservation by Acoustic Surveillance</title>
    <summary>  Saving rainforests is a key to halting adverse climate changes. In this
paper, we introduce an innovative solution built on acoustic surveillance and
machine learning technologies to help rainforest conservation. In particular,
We propose new convolutional neural network (CNN) models for environmental
sound classification and achieved promising preliminary results on two
datasets, including a public audio dataset and our real rainforest sound
dataset. The proposed audio classification models can be easily extended in an
automated machine learning paradigm and integrated in cloud-based services for
real world deployment.
</summary>
    <author>
      <name>Yuan Liu</name>
    </author>
    <author>
      <name>Zhongwei Cheng</name>
    </author>
    <author>
      <name>Jie Liu</name>
    </author>
    <author>
      <name>Bourhan Yassin</name>
    </author>
    <author>
      <name>Zhe Nan</name>
    </author>
    <author>
      <name>Jiebo Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to KDD2019 Workshop on Data Mining and AI for Conservation</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.07517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.07517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.10133v1</id>
    <updated>2019-08-27T11:20:57Z</updated>
    <published>2019-08-27T11:20:57Z</published>
    <title>A hybrid parametric-deep learning approach for sound event localization
  and detection</title>
    <summary>  This work describes and discusses an algorithm submitted to the Sound Event
Localization and Detection Task of DCASE2019 Challenge. The proposed
methodology relies on parametric spatial audio analysis for source localization
and detection, combined with a deep learning-based monophonic event classifier.
The evaluation of the proposed algorithm yields overall results comparable to
the baseline system. The main highlight is a reduction of the localization
error on the evaluation dataset by a factor of 2.6, compared with the baseline
performance.
</summary>
    <author>
      <name>Andres Perez-Lopez</name>
    </author>
    <author>
      <name>Eduardo Fonseca</name>
    </author>
    <author>
      <name>Xavier Serra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, submitted to DCASE2019 Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.10133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.06375v1</id>
    <updated>2019-10-14T18:41:42Z</updated>
    <published>2019-10-14T18:41:42Z</published>
    <title>The Sounds of Music : Science of Musical Scales III -- Indian Classical</title>
    <summary>  In the previous articles of this series, we have discussed the development of
musical scales particularly that of the heptatonic scale which forms the basis
of Western classical music today. In this last article, we take a look at the
basic structure of scales used in Indian classical music and how different
`raga's are generated through the simple process of scale shifting.
</summary>
    <author>
      <name>Sushan Konar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final part of a 3-article series on Musical Scales, see
  arXiv:1908.07940, arXiv:1909.06259</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Resonance - Journal of Science Education, 24(10), 1125 (2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.06375v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06375v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.06697v1</id>
    <updated>2019-10-15T13:04:45Z</updated>
    <published>2019-10-15T13:04:45Z</published>
    <title>VFNet: A Convolutional Architecture for Accent Classification</title>
    <summary>  Understanding accent is an issue which can derail any human-machine
interaction. Accent classification makes this task easier by identifying the
accent being spoken by a person so that the correct words being spoken can be
identified by further processing, since same noises can mean entirely different
words in different accents of the same language. In this paper, we present
VFNet (Variable Filter Net), a convolutional neural network (CNN) based
architecture which captures a hierarchy of features to beat the previous
benchmarks of accent classification, through a novel and elegant technique of
applying variable filter sizes along the frequency band of the audio
utterances.
</summary>
    <author>
      <name>Asad Ahmed</name>
    </author>
    <author>
      <name>Pratham Tangri</name>
    </author>
    <author>
      <name>Anirban Panda</name>
    </author>
    <author>
      <name>Dhruv Ramani</name>
    </author>
    <author>
      <name>Samarjit Karmakar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at IEEE INDICON 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.06697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.00102v1</id>
    <updated>2019-10-31T20:55:08Z</updated>
    <published>2019-10-31T20:55:08Z</published>
    <title>End-to-end Non-Negative Autoencoders for Sound Source Separation</title>
    <summary>  Discriminative models for source separation have recently been shown to
produce impressive results. However, when operating on sources outside of the
training set, these models can not perform as well and are cumbersome to
update. Classical methods like Non-negative Matrix Factorization (NMF) provide
modular approaches to source separation that can be easily updated to adapt to
new mixture scenarios. In this paper, we generalize NMF to develop end-to-end
non-negative auto-encoders and demonstrate how they can be used for source
separation. Our experiments indicate that these models deliver comparable
separation performance to discriminative approaches, while retaining the
modularity of NMF and the modeling flexibility of neural networks.
</summary>
    <author>
      <name>Shrikant Venkataramani</name>
    </author>
    <author>
      <name>Efthymios Tzinis</name>
    </author>
    <author>
      <name>Paris Smaragdis</name>
    </author>
    <link href="http://arxiv.org/abs/1911.00102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.00102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.04385v1</id>
    <updated>2019-11-11T16:52:11Z</updated>
    <published>2019-11-11T16:52:11Z</published>
    <title>Visualizing and Understanding Self-attention based Music Tagging</title>
    <summary>  Recently, we proposed a self-attention based music tagging model. Different
from most of the conventional deep architectures in music information
retrieval, which use stacked 3x3 filters by treating music spectrograms as
images, the proposed self-attention based model attempted to regard music as a
temporal sequence of individual audio events. Not only the performance, but it
could also facilitate better interpretability. In this paper, we mainly focus
on visualizing and understanding the proposed self-attention based music
tagging model.
</summary>
    <author>
      <name>Minz Won</name>
    </author>
    <author>
      <name>Sanghyuk Chun</name>
    </author>
    <author>
      <name>Xavier Serra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning for Music Discovery Workshop (ML4MD) at ICML 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.04385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.04385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.07041v1</id>
    <updated>2019-11-16T14:53:01Z</updated>
    <published>2019-11-16T14:53:01Z</published>
    <title>Music theme recognition using CNN and self-attention</title>
    <summary>  We present an efficient architecture to detect mood/themes in music tracks on
autotagging-moodtheme subset of the MTG-Jamendo dataset. Our approach consists
of two blocks, a CNN block based on MobileNetV2 architecture and a
self-attention block from Transformer architecture to capture long term
temporal characteristics. We show that our proposed model produces a
significant improvement over the baseline model. Our model (team name: AMLAG)
achieves 4th place on PR-AUC-macro Leaderboard in MediaEval 2019: Emotion and
Theme Recognition in Music Using Jamendo.
</summary>
    <author>
      <name>Manoj Sukhavasi</name>
    </author>
    <author>
      <name>Sainath Adapa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MediaEval 2019, 27-29 October 2019, Sophia Antipolis, France</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.07041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.07041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.02522v1</id>
    <updated>2019-12-05T12:00:45Z</updated>
    <published>2019-12-05T12:00:45Z</published>
    <title>VoxSRC 2019: The first VoxCeleb Speaker Recognition Challenge</title>
    <summary>  The VoxCeleb Speaker Recognition Challenge 2019 aimed to assess how well
current speaker recognition technology is able to identify speakers in
unconstrained or `in the wild' data. It consisted of: (i) a publicly available
speaker recognition dataset from YouTube videos together with ground truth
annotation and standardised evaluation software; and (ii) a public challenge
and workshop held at Interspeech 2019 in Graz, Austria. This paper outlines the
challenge and provides its baselines, results and discussions.
</summary>
    <author>
      <name>Joon Son Chung</name>
    </author>
    <author>
      <name>Arsha Nagrani</name>
    </author>
    <author>
      <name>Ernesto Coto</name>
    </author>
    <author>
      <name>Weidi Xie</name>
    </author>
    <author>
      <name>Mitchell McLaren</name>
    </author>
    <author>
      <name>Douglas A Reynolds</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISCA Archive</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.02522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.02522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.11585v1</id>
    <updated>2019-12-25T03:44:31Z</updated>
    <published>2019-12-25T03:44:31Z</published>
    <title>THUEE system description for NIST 2019 SRE CTS Challenge</title>
    <summary>  This paper describes the systems submitted by the department of electronic
engineering, institute of microelectronics of Tsinghua university and
TsingMicro Co. Ltd. (THUEE) to the NIST 2019 speaker recognition evaluation CTS
challenge. Six subsystems, including etdnn/ams, ftdnn/as, eftdnn/ams, resnet,
multitask and c-vector are developed in this evaluation.
</summary>
    <author>
      <name>Yi Liu</name>
    </author>
    <author>
      <name>Tianyu Liang</name>
    </author>
    <author>
      <name>Can Xu</name>
    </author>
    <author>
      <name>Xianwei Zhang</name>
    </author>
    <author>
      <name>Xianhong Chen</name>
    </author>
    <author>
      <name>Wei-Qiang Zhang</name>
    </author>
    <author>
      <name>Liang He</name>
    </author>
    <author>
      <name>Dandan song</name>
    </author>
    <author>
      <name>Ruyun Li</name>
    </author>
    <author>
      <name>Yangcheng Wu</name>
    </author>
    <author>
      <name>Peng Ouyang</name>
    </author>
    <author>
      <name>Shouyi Yin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is the system description of THUEE submitted to NIST SRE 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.11585v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11585v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.00387v2</id>
    <updated>2020-02-05T03:52:32Z</updated>
    <published>2020-02-02T13:14:37Z</published>
    <title>The FFSVC 2020 Evaluation Plan</title>
    <summary>  The Far-Field Speaker Verification Challenge 2020 (FFSVC20) is designed to
boost the speaker verification research with special focus on far-field
distributed microphone arrays under noisy conditions in real scenarios. The
objectives of this challenge are to: 1) benchmark the current speech
verification technology under this challenging condition, 2) promote the
development of new ideas and technologies in speaker verification, 3) provide
an open, free, and large scale speech database to the community that exhibits
the far-field characteristics in real scenes.
</summary>
    <author>
      <name>Xiaoyi Qin</name>
    </author>
    <author>
      <name>Ming Li</name>
    </author>
    <author>
      <name>Hui Bu</name>
    </author>
    <author>
      <name>Rohan Kumar Das</name>
    </author>
    <author>
      <name>Wei Rao</name>
    </author>
    <author>
      <name>Shrikanth Narayanan</name>
    </author>
    <author>
      <name>Haizhou Li</name>
    </author>
    <link href="http://arxiv.org/abs/2002.00387v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.00387v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.03559v1</id>
    <updated>2020-02-10T05:38:51Z</updated>
    <published>2020-02-10T05:38:51Z</published>
    <title>Modeling Musical Onset Probabilities via Neural Distribution Learning</title>
    <summary>  Musical onset detection can be formulated as a time-to-event (TTE) or
time-since-event (TSE) prediction task by defining music as a sequence of onset
events. Here we propose a novel method to model the probability of onsets by
introducing a sequential density prediction model. The proposed model estimates
TTE &amp; TSE distributions from mel-spectrograms using convolutional neural
networks (CNNs) as a density predictor. We evaluate our model on the Bock
dataset show-ing comparable results to previous deep-learning models.
</summary>
    <author>
      <name>Jaesung Huh</name>
    </author>
    <author>
      <name>Egil Martinsson</name>
    </author>
    <author>
      <name>Adrian Kim</name>
    </author>
    <author>
      <name>Jung-Woo Ha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.03559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.03559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.10266v1</id>
    <updated>2020-02-21T09:36:24Z</updated>
    <published>2020-02-21T09:36:24Z</published>
    <title>Rhythm, Chord and Melody Generation for Lead Sheets using Recurrent
  Neural Networks</title>
    <summary>  Music that is generated by recurrent neural networks often lacks a sense of
direction and coherence. We therefore propose a two-stage LSTM-based model for
lead sheet generation, in which the harmonic and rhythmic templates of the song
are produced first, after which, in a second stage, a sequence of melody notes
is generated conditioned on these templates. A subjective listening test shows
that our approach outperforms the baselines and increases perceived musical
coherence.
</summary>
    <author>
      <name>Cedric De Boom</name>
    </author>
    <author>
      <name>Stephanie Van Laere</name>
    </author>
    <author>
      <name>Tim Verbelen</name>
    </author>
    <author>
      <name>Bart Dhoedt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures, 3 tables, 2 appendices</arxiv:comment>
    <link href="http://arxiv.org/abs/2002.10266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.03160v2</id>
    <updated>2020-03-10T13:00:27Z</updated>
    <published>2020-03-06T12:38:58Z</published>
    <title>A Neural Network Based Framework for Archetypical Sound Synthesis</title>
    <summary>  This paper describes a preliminary approach to algorithmically reproduce the
archetypical structure adopted by humans to classify sounds. In particular, we
propose an approach to predict the human perceived chaos/order level in a sound
and synthesize new timbres that present the desired amount of this feature. We
adopted a Neural Network based method, in order to exploit its inner
predisposition to model perceptive and abstract features. We finally discuss
the obtained accuracy and possible implications in creative contexts.
</summary>
    <author>
      <name>Eric Guizzo</name>
    </author>
    <author>
      <name>Alberto Novello</name>
    </author>
    <link href="http://arxiv.org/abs/2003.03160v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.03160v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.04353v1</id>
    <updated>2020-05-09T02:34:39Z</updated>
    <published>2020-05-09T02:34:39Z</published>
    <title>Dual-track Music Generation using Deep Learning</title>
    <summary>  Music generation is always interesting in a sense that there is no formalized
recipe. In this work, we propose a novel dual-track architecture for generating
classical piano music, which is able to model the inter-dependency of left-hand
and right-hand piano music. Particularly, we experimented with a lot of
different models of neural network as well as different representations of
music, and the results show that our proposed model outperforms all other
tested methods. Besides, we deployed some special policies for model training
and generation, which contributed to the model performance remarkably. Finally,
under two evaluation methods, we compared our models with the MuseGAN project
and true music.
</summary>
    <author>
      <name>Sudi Lyu</name>
    </author>
    <author>
      <name>Anxiang Zhang</name>
    </author>
    <author>
      <name>Rong Song</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.04353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.04353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.04724v1</id>
    <updated>2020-05-10T17:33:48Z</updated>
    <published>2020-05-10T17:33:48Z</published>
    <title>Chirp Complex Cepstrum-based Decomposition for Asynchronous Glottal
  Analysis</title>
    <summary>  It was recently shown that complex cepstrum can be effectively used for
glottal flow estimation by separating the causal and anticausal components of
speech. In order to guarantee a correct estimation, some constraints on the
window have been derived. Among these, the window has to be synchronized on a
Glottal Closure Instant. This paper proposes an extension of the complex
cepstrum-based decomposition by incorporating a chirp analysis. The resulting
method is shown to give a reliable estimation of the glottal flow wherever the
window is located. This technique is then suited for its integration in usual
speech processing systems, which generally operate in an asynchronous way.
Besides its potential for automatic voice quality analysis is highlighted.
</summary>
    <author>
      <name>Thomas Drugman</name>
    </author>
    <author>
      <name>Thierry Dutoit</name>
    </author>
    <link href="http://arxiv.org/abs/2005.04724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.04724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.12230v1</id>
    <updated>2020-05-25T17:00:26Z</updated>
    <published>2020-05-25T17:00:26Z</published>
    <title>Speaker and Posture Classification using Instantaneous Intraspeech
  Breathing Features</title>
    <summary>  Acoustic features extracted from speech are widely used in problems such as
biometric speaker identification and first-person activity detection. However,
the use of speech for such purposes raises privacy issues as the content is
accessible to the processing party. In this work, we propose a method for
speaker and posture classification using intraspeech breathing sounds.
Instantaneous magnitude features are extracted using the Hilbert-Huang
transform (HHT) and fed into a CNN-GRU network for classification of recordings
from the open intraspeech breathing sound dataset, BreathBase, that we
collected for this study. Using intraspeech breathing sounds, 87% speaker
classification, and 98% posture classification accuracy were obtained.
</summary>
    <author>
      <name>Atıl İlerialkan</name>
    </author>
    <author>
      <name>Alptekin Temizel</name>
    </author>
    <author>
      <name>Hüseyin Hacıhabiboğlu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.12230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.12230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.02959v1</id>
    <updated>2020-06-04T15:41:00Z</updated>
    <published>2020-06-04T15:41:00Z</published>
    <title>PJS: phoneme-balanced Japanese singing voice corpus</title>
    <summary>  This paper presents a free Japanese singing voice corpus that can be used for
highly applicable and reproducible singing voice synthesis research. A singing
voice corpus helps develop singing voice synthesis, but existing corpora have
two critical problems: data imbalance (singing voice corpora do not guarantee
phoneme balance, unlike speaking-voice corpora) and copyright issues (cannot
legally share data). As a way to avoid these problems, we constructed a PJS
(phoneme-balanced Japanese singing voice) corpus that guarantees phoneme
balance and is licensed with CC BY-SA 4.0, and we composed melodies using a
phoneme-balanced speaking-voice corpus. This paper describes how we built the
corpus.
</summary>
    <author>
      <name>Junya Koguchi</name>
    </author>
    <author>
      <name>Shinnosuke Takamichi</name>
    </author>
    <link href="http://arxiv.org/abs/2006.02959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.02959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.03471v1</id>
    <updated>2020-06-04T15:29:56Z</updated>
    <published>2020-06-04T15:29:56Z</published>
    <title>Application of Optimization and Simulation to Musical Composition that
  Emerges Dynamically during Ensemble Singing Performance</title>
    <summary>  This paper presents and tests a new approach to composing for ensemble
singing performance: reality opera. In the performance of such a composition,
emotions of the singers are real and emerge as a consequence of their
interactions and reaction and to a dynamic narrative. This paper gives
background and motivation for the form, based on three key concepts,
incorporating the use of technology. Then proposed techniques for creating
reality opera are instantiated in an example, which is performed and a
behavioral analysis done of performer reactions, leading to support for the
feasibility of the reality opera concept.
</summary>
    <author>
      <name>Alexis Kirke</name>
    </author>
    <author>
      <name>Greg B. Davies</name>
    </author>
    <author>
      <name>Joel Eaton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 11 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.03471v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.03471v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="00A65" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.03931v1</id>
    <updated>2020-07-08T07:29:35Z</updated>
    <published>2020-07-08T07:29:35Z</published>
    <title>Training Sound Event Detection On A Heterogeneous Dataset</title>
    <summary>  Training a sound event detection algorithm on a heterogeneous dataset
including both recorded and synthetic soundscapes that can have various
labeling granularity is a non-trivial task that can lead to systems requiring
several technical choices. These technical choices are often passed from one
system to another without being questioned. We propose to perform a detailed
analysis of DCASE 2020 task 4 sound event detection baseline with regards to
several aspects such as the type of data used for training, the parameters of
the mean-teacher or the transformations applied while generating the synthetic
soundscapes. Some of the parameters that are usually used as default are shown
to be sub-optimal.
</summary>
    <author>
      <name>Nicolas Turpault</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MULTISPEECH</arxiv:affiliation>
    </author>
    <author>
      <name>Romain Serizel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MULTISPEECH</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2007.03931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.03931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.00250v1</id>
    <updated>2020-12-01T04:10:24Z</updated>
    <published>2020-12-01T04:10:24Z</published>
    <title>Strike on Stage: a percussion and media performance</title>
    <summary>  This paper describes Strike on Stage, an interface and corresponding
audio-visual performance work developed and performed in 2010 by percussionists
and media artists Chi-Hsia Lai and Charles Martin. The concept of Strike on
Stage is to integrate computer visuals and sound into an improvised percussion
performance. A large projection surface is positioned directly behind the
performers, while a computer vision system tracks their movements. The setup
allows computer visualisation and sonification to be directly responsive and
unified with the performers' gestures.
</summary>
    <author>
      <name>Charles Martin</name>
    </author>
    <author>
      <name>Chi-Hsia Lai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1178103</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1178103" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression (2011) pp. 142-143</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2012.00250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.00250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.03359v1</id>
    <updated>2020-12-06T19:30:26Z</updated>
    <published>2020-12-06T19:30:26Z</published>
    <title>Source Separation and Depthwise Separable Convolutions for Computer
  Audition</title>
    <summary>  Given recent advances in deep music source separation, we propose a feature
representation method that combines source separation with a state-of-the-art
representation learning technique that is suitably repurposed for computer
audition (i.e. machine listening). We train a depthwise separable convolutional
neural network on a challenging electronic dance music (EDM) data set and
compare its performance to convolutional neural networks operating on both
source separated and standard spectrograms. It is shown that source separation
improves classification performance in a limited-data setting compared to the
standard single spectrogram approach.
</summary>
    <author>
      <name>Gabriel Mersy</name>
    </author>
    <author>
      <name>Jin Hong Kuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, to appear in the AAAI-21 student abstract and poster program</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.03359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.03359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.03805v1</id>
    <updated>2020-12-07T15:48:01Z</updated>
    <published>2020-12-07T15:48:01Z</published>
    <title>Diverse Melody Generation from Chinese Lyrics via Mutual Information
  Maximization</title>
    <summary>  In this paper, we propose to adapt the method of mutual information
maximization into the task of Chinese lyrics conditioned melody generation to
improve the generation quality and diversity. We employ scheduled sampling and
force decoding techniques to improve the alignment between lyrics and melodies.
With our method, which we called Diverse Melody Generation (DMG), a
sequence-to-sequence model learns to generate diverse melodies heavily
depending on the input style ids, while keeping the tonality and improving the
alignment. The experimental results of subjective tests show that DMG can
generate more pleasing and coherent tunes than baseline methods.
</summary>
    <author>
      <name>Ruibin Yuan</name>
    </author>
    <author>
      <name>Ge Zhang</name>
    </author>
    <author>
      <name>Anqiao Yang</name>
    </author>
    <author>
      <name>Xinyue Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2012.03805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.03805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.04572v2</id>
    <updated>2020-12-09T20:42:50Z</updated>
    <published>2020-12-08T17:17:28Z</published>
    <title>I'm Sorry for Your Loss: Spectrally-Based Audio Distances Are Bad at
  Pitch</title>
    <summary>  Growing research demonstrates that synthetic failure modes imply poor
generalization. We compare commonly used audio-to-audio losses on a synthetic
benchmark, measuring the pitch distance between two stationary sinusoids. The
results are surprising: many have poor sense of pitch direction. These
shortcomings are exposed using simple rank assumptions. Our task is trivial for
humans but difficult for these audio distances, suggesting significant progress
can be made in self-supervised audio learning by improving current losses.
</summary>
    <author>
      <name>Joseph Turian</name>
    </author>
    <author>
      <name>Max Henry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICBINB@NeurIPS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.04572v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.04572v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.07055v2</id>
    <updated>2021-02-10T11:57:24Z</updated>
    <published>2020-12-13T13:02:55Z</published>
    <title>Improving the Classification of Rare Chords with Unlabeled Data</title>
    <summary>  In this work, we explore techniques to improve performance for rare classes
in the task of Automatic Chord Recognition (ACR). We first explored the use of
the focal loss in the context of ACR, which was originally proposed to improve
the classification of hard samples. In parallel, we adapted a self-learning
technique originally designed for image recognition to the musical domain. Our
experiments show that both approaches individually (and their combination)
improve the recognition of rare chords, but using only self-learning with noise
addition yields the best results.
</summary>
    <author>
      <name>Marcelo Bortolozzo</name>
    </author>
    <author>
      <name>Rodrigo Schramm</name>
    </author>
    <author>
      <name>Claudio R. Jung</name>
    </author>
    <link href="http://arxiv.org/abs/2012.07055v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.07055v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.13699v1</id>
    <updated>2020-12-26T08:25:02Z</updated>
    <published>2020-12-26T08:25:02Z</published>
    <title>Inception-Based Network and Multi-Spectrogram Ensemble Applied For
  Predicting Respiratory Anomalies and Lung Diseases</title>
    <summary>  This paper presents an inception-based deep neural network for detecting lung
diseases using respiratory sound input. Recordings of respiratory sound
collected from patients are firstly transformed into spectrograms where both
spectral and temporal information are well presented, referred to as front-end
feature extraction. These spectrograms are then fed into the proposed network,
referred to as back-end classification, for detecting whether patients suffer
from lung-relevant diseases. Our experiments, conducted over the ICBHI
benchmark meta-dataset of respiratory sound, achieve competitive ICBHI scores
of 0.53/0.45 and 0.87/0.85 regarding respiratory anomaly and disease detection,
respectively.
</summary>
    <author>
      <name>Lam Pham</name>
    </author>
    <author>
      <name>Huy Phan</name>
    </author>
    <author>
      <name>Ross King</name>
    </author>
    <author>
      <name>Alfred Mertins</name>
    </author>
    <author>
      <name>Ian McLoughlin</name>
    </author>
    <link href="http://arxiv.org/abs/2012.13699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.13699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.14761v1</id>
    <updated>2020-12-29T14:21:06Z</updated>
    <published>2020-12-29T14:21:06Z</published>
    <title>Data-driven audio recognition: a supervised dictionary approach</title>
    <summary>  Machine hearing is an emerging area. Motivated by the need of a principled
framework across domain applications for machine listening, we propose a
generic and data-driven representation learning approach. For this sake, a
novel and efficient supervised dictionary learning method is presented.
Experiments are performed on both computational auditory scene (East Anglia and
Rouen) and synthetic music chord recognition datasets. Obtained results show
that our method is capable to reach state-of-the-art hand-crafted features for
both applications
</summary>
    <author>
      <name>Imad Rida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1812.04748</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.14761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.14761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.00169v3</id>
    <updated>2021-11-13T01:26:04Z</updated>
    <published>2021-01-01T05:40:12Z</published>
    <title>Generative Deep Learning for Virtuosic Classical Music: Generative
  Adversarial Networks as Renowned Composers</title>
    <summary>  Current AI-generated music lacks fundamental principles of good compositional
techniques. By narrowing down implementation issues both programmatically and
musically, we can create a better understanding of what parameters are
necessary for a generated composition nearly indistinguishable from that of a
master composer.
</summary>
    <author>
      <name>Daniel Szelogowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures Update: Revised format to align closer to IEEE
  standards</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.00169v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.00169v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2; I.5; I.7; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.06842v2</id>
    <updated>2021-04-26T00:55:26Z</updated>
    <published>2021-01-18T02:17:24Z</published>
    <title>Hierarchical disentangled representation learning for singing voice
  conversion</title>
    <summary>  Conventional singing voice conversion (SVC) methods often suffer from
operating in high-resolution audio owing to a high dimensionality of data. In
this paper, we propose a hierarchical representation learning that enables the
learning of disentangled representations with multiple resolutions
independently. With the learned disentangled representations, the proposed
method progressively performs SVC from low to high resolutions. Experimental
results show that the proposed method outperforms baselines that operate with a
single resolution in terms of mean opinion score (MOS), similarity score, and
pitch accuracy.
</summary>
    <author>
      <name>Naoya Takahashi</name>
    </author>
    <author>
      <name>Mayank Kumar Singh</name>
    </author>
    <author>
      <name>Yuki Mitsufuji</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at IJCNN 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.06842v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.06842v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.03516v1</id>
    <updated>2021-03-05T07:51:21Z</updated>
    <published>2021-03-05T07:51:21Z</published>
    <title>Slow-Fast Auditory Streams For Audio Recognition</title>
    <summary>  We propose a two-stream convolutional network for audio recognition, that
operates on time-frequency spectrogram inputs. Following similar success in
visual recognition, we learn Slow-Fast auditory streams with separable
convolutions and multi-level lateral connections. The Slow pathway has high
channel capacity while the Fast pathway operates at a fine-grained temporal
resolution. We showcase the importance of our two-stream proposal on two
diverse datasets: VGG-Sound and EPIC-KITCHENS-100, and achieve state-of-the-art
results on both.
</summary>
    <author>
      <name>Evangelos Kazakos</name>
    </author>
    <author>
      <name>Arsha Nagrani</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
    <author>
      <name>Dima Damen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for presentation at ICASSP 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.03516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.03516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.14206v1</id>
    <updated>2021-03-26T01:49:08Z</updated>
    <published>2021-03-26T01:49:08Z</published>
    <title>Three dimensional higher-order raypath separation in a shallow-water
  waveguide</title>
    <summary>  Separating raypaths in a multipath shallow-water environment is a challenge
problem due to the interferences between them and colored noise existing in
ocean environment, especially for two raypaths arrive close to each other.
Thus, in this paper, a three dimensional (3D) higher-order raypath separation
in an array to array configuration is proposed. Performance tests using
simulation data in a multipath environment, real data obtained in an ultrasonic
waveguide and ocean shallow-water data, respectively, illustrate that the
proposed algorithm achieves a higher resolution and a stronger robustness
comparing to the existing algorithms.
</summary>
    <author>
      <name>Jiang Longyu</name>
    </author>
    <author>
      <name>Zhang Zhe</name>
    </author>
    <author>
      <name>Roux Philippe</name>
    </author>
    <link href="http://arxiv.org/abs/2103.14206v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.14206v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.14236v1</id>
    <updated>2021-03-26T02:55:28Z</updated>
    <published>2021-03-26T02:55:28Z</published>
    <title>Subspace-based compressive sensing algorithm for raypath separation in a
  shallow-water waveguide</title>
    <summary>  Compressive sensing (CS) has been applied to estimate the direction of
arrival (DOA) in underwater acoustics. However, the key problem needed to be
resolved in a {multipath} propagation environment is to suppress the
interferences between the raypaths. Thus, in this paper, {a subspace-based
compressive sensing algorithm that formulates the statistic information of the
signal subspace in a CS framework is proposed.} The experiment results show
that (1) the proposed algorithm enables the separation of raypaths that arrive
closely at the {receiver} array and (2) the existing algorithms fail,
especially in a low signal-to-noise ratio (SNR) environment.
</summary>
    <author>
      <name>Longyu Jiang</name>
    </author>
    <author>
      <name>Zhe Zhang</name>
    </author>
    <author>
      <name>Rui Jin</name>
    </author>
    <author>
      <name>Xiao Zhou</name>
    </author>
    <author>
      <name>Philippe Roux</name>
    </author>
    <link href="http://arxiv.org/abs/2103.14236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.14236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04325v1</id>
    <updated>2021-04-09T12:13:38Z</updated>
    <published>2021-04-09T12:13:38Z</published>
    <title>Joint Online Multichannel Acoustic Echo Cancellation, Speech
  Dereverberation and Source Separation</title>
    <summary>  This paper presents a joint source separation algorithm that simultaneously
reduces acoustic echo, reverberation and interfering sources. Target speeches
are separated from the mixture by maximizing independence with respect to the
other sources. It is shown that the separation process can be decomposed into
cascading sub-processes that separately relate to acoustic echo cancellation,
speech dereverberation and source separation, all of which are solved using the
auxiliary function based independent component/vector analysis techniques, and
their solving orders are exchangeable. The cascaded solution not only leads to
lower computational complexity but also better separation performance than the
vanilla joint algorithm.
</summary>
    <author>
      <name>Yueyue Na</name>
    </author>
    <author>
      <name>Ziteng Wang</name>
    </author>
    <author>
      <name>Zhang Liu</name>
    </author>
    <author>
      <name>Biao Tian</name>
    </author>
    <author>
      <name>Qiang Fu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to INTERSPEECH 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.04325v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04325v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.11880v1</id>
    <updated>2021-04-24T04:32:45Z</updated>
    <published>2021-04-24T04:32:45Z</published>
    <title>Music Embedding: A Tool for Incorporating Music Theory into
  Computational Music Applications</title>
    <summary>  Advancements in the digital technologies have enabled researchers to develop
a variety of Computational Music applications. Such applications are required
to capture, process, and generate data related to music. Therefore, it is
important to digitally represent music in a music theoretic and concise manner.
Existing approaches for representing music are ineffective in terms of
utilizing music theory. In this paper, we address the disjoint of music theory
and computational music by developing an opensource representation tool based
on music theory. Through the wide range of use cases, we run an analysis on the
classical music pieces to show the usefulness of the developed music embedding.
</summary>
    <author>
      <name>SeyyedPooya HekmatiAthar</name>
    </author>
    <author>
      <name>Mohd Anwar</name>
    </author>
    <link href="http://arxiv.org/abs/2104.11880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.11880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12432v1</id>
    <updated>2021-04-26T09:45:09Z</updated>
    <published>2021-04-26T09:45:09Z</published>
    <title>Generation of musical patterns through operads</title>
    <summary>  We introduce the notion of multi-pattern, a combinatorial abstraction of
polyphonic musical phrases. The interest of this approach lies in the fact that
this offers a way to compose two multi-patterns in order to produce a longer
one. This dives musical phrases into an algebraic context since the set of
multi-patterns has the structure of an operad; operads being structures
offering a formalization of the notion of operators and their compositions.
Seeing musical phrases as operators allows us to perform computations on
phrases and admits applications in generative music: given a set of short
patterns, we propose various algorithms to randomly generate a new and longer
phrase inspired by the inputted patterns.
</summary>
    <author>
      <name>Samuele Giraudo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journ\'ees d'informatique musicale, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.12432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.12432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.00806v1</id>
    <updated>2021-06-01T21:17:53Z</updated>
    <published>2021-06-01T21:17:53Z</published>
    <title>Exploring Exotic Counterpoint Compositions</title>
    <summary>  In this paper, first musical compositions are presented, which are created
using the mathematical counterpoint theory of Guerino Mazzola and his
collaborators. These compositions also use the RUBATO(R) software's components
for counterpoint constructions. The present work aims at opening new "exotic"
directions of contrapuntal composition in non-Fuxian worlds. The authors would
like to receive first impressions about these compositions, which are available
as scores and audio files.
</summary>
    <author>
      <name>Octavio A. Agustín-Aquino</name>
    </author>
    <author>
      <name>Jeffery Liu</name>
    </author>
    <author>
      <name>Guerino Mazzola</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.00806v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.00806v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="00A65, 13P99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.04639v1</id>
    <updated>2021-06-08T18:58:37Z</updated>
    <published>2021-06-08T18:58:37Z</published>
    <title>Optimising Hearing Aid Fittings for Speech in Noise with a
  Differentiable Hearing Loss Model</title>
    <summary>  Current hearing aids normally provide amplification based on a general
prescriptive fitting, and the benefits provided by the hearing aids vary among
different listening environments despite the inclusion of noise suppression
feature. Motivated by this fact, this paper proposes a data-driven machine
learning technique to develop hearing aid fittings that are customised to
speech in different noisy environments. A differentiable hearing loss model is
proposed and used to optimise fittings with back-propagation. The customisation
is reflected on the data of speech in different noise with also the
consideration of noise suppression. The objective evaluation shows the
advantages of optimised custom fittings over general prescriptive fittings.
</summary>
    <author>
      <name>Zehai Tu</name>
    </author>
    <author>
      <name>Ning Ma</name>
    </author>
    <author>
      <name>Jon Barker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Interspeech 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.04639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.04639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.06309v1</id>
    <updated>2021-06-11T10:59:09Z</updated>
    <published>2021-06-11T10:59:09Z</published>
    <title>HUI-Audio-Corpus-German: A high quality TTS dataset</title>
    <summary>  The increasing availability of audio data on the internet lead to a multitude
of datasets for development and training of text to speech applications, based
on neural networks. Highly differing quality of voice, low sampling rates, lack
of text normalization and disadvantageous alignment of audio samples to
corresponding transcript sentences still limit the performance of deep neural
networks trained on this task. Additionally, data resources in languages like
German are still very limited. We introduce the "HUI-Audio-Corpus-German", a
large, open-source dataset for TTS engines, created with a processing pipeline,
which produces high quality audio to transcription alignments and decreases
manual effort needed for creation.
</summary>
    <author>
      <name>Pascal Puchtler</name>
    </author>
    <author>
      <name>Johannes Wirth</name>
    </author>
    <author>
      <name>René Peinl</name>
    </author>
    <link href="http://arxiv.org/abs/2106.06309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.06309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.06840v1</id>
    <updated>2021-06-12T19:37:42Z</updated>
    <published>2021-06-12T19:37:42Z</published>
    <title>Deep Learning Frameworks Applied For Audio-Visual Scene Classification</title>
    <summary>  In this paper, we present deep learning frameworks for audio-visual scene
classification (SC) and indicate how individual visual and audio features as
well as their combination affect SC performance. Our extensive experiments,
which are conducted on DCASE (IEEE AASP Challenge on Detection and
Classification of Acoustic Scenes and Events) Task 1B development dataset,
achieve the best classification accuracy of 82.2%, 91.1%, and 93.9% with audio
input only, visual input only, and both audio-visual input, respectively. The
highest classification accuracy of 93.9%, obtained from an ensemble of
audio-based and visual-based frameworks, shows an improvement of 16.5% compared
with DCASE baseline.
</summary>
    <author>
      <name>Lam Pham</name>
    </author>
    <author>
      <name>Alexander Schindler</name>
    </author>
    <author>
      <name>Mina Schütz</name>
    </author>
    <author>
      <name>Jasmin Lampert</name>
    </author>
    <author>
      <name>Sven Schlarb</name>
    </author>
    <author>
      <name>Ross King</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.06840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.06840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.06863v1</id>
    <updated>2021-06-12T20:55:44Z</updated>
    <published>2021-06-12T20:55:44Z</published>
    <title>Continuous Wavelet Vocoder-based Decomposition of Parametric Speech
  Waveform Synthesis</title>
    <summary>  To date, various speech technology systems have adopted the vocoder approach,
a method for synthesizing speech waveform that shows a major role in the
performance of statistical parametric speech synthesis. WaveNet one of the best
models that nearly resembles the human voice, has to generate a waveform in a
time consuming sequential manner with an extremely complex structure of its
neural networks.
</summary>
    <author>
      <name>Mohammed Salah Al-Radhi</name>
    </author>
    <author>
      <name>Tamás Gábor Csapó</name>
    </author>
    <author>
      <name>Csaba Zainkó</name>
    </author>
    <author>
      <name>Géza Németh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures, accepted to the conference of Interspeech 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.06863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.06863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.07157v2</id>
    <updated>2021-08-15T05:16:19Z</updated>
    <published>2021-06-14T04:32:36Z</published>
    <title>Multiple scattering ambisonics: three-dimensional sound field estimation
  using interacting spheres</title>
    <summary>  Rigid spherical microphone arrays (RSMAs) have been widely used in ambisonics
sound field recording. While it is desired to combine the information captured
by a grid of densely arranged RSMAs for expanding the area of accurate
reconstruction, or sweet-spots, this is not trivial due to inter-array
interference. Here we propose multiple scattering ambisonics, a method for
three-dimensional ambisonics sound field recording using multiple acoustically
interacting RSMAs. Numerical experiments demonstrate the sweet-spot expansion
realized by the proposed method. The proposed method can be used with existing
RSMAs as building blocks and opens possibilities including higher
degrees-of-freedom spatial audio.
</summary>
    <author>
      <name>Shoken Kaneko</name>
    </author>
    <author>
      <name>Ramani Duraiswami</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1121/10.0005832</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1121/10.0005832" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JASA Express Lett. 1 (8), 084801 (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2106.07157v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.07157v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.03041v1</id>
    <updated>2021-08-06T10:43:14Z</updated>
    <published>2021-08-06T10:43:14Z</published>
    <title>The EIHW-GLAM Deep Attentive Multi-model Fusion System for Cough-based
  COVID-19 Recognition in the DiCOVA 2021 Challenge</title>
    <summary>  Aiming to automatically detect COVID-19 from cough sounds, we propose a deep
attentive multi-model fusion system evaluated on the Track-1 dataset of the
DiCOVA 2021 challenge. Three kinds of representations are extracted, including
hand-crafted features, image-from-audio-based deep representations, and
audio-based deep representations. Afterwards, the best models on the three
types of features are fused at both the feature level and the decision level.
The experimental results demonstrate that the proposed attention-based fusion
at the feature level achieves the best performance (AUC: 77.96%) on the test
set, resulting in an 8.05% improvement over the official baseline.
</summary>
    <author>
      <name>Zhao Ren</name>
    </author>
    <author>
      <name>Yi Chang</name>
    </author>
    <author>
      <name>Björn W. Schuller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.03041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.03041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.04449v1</id>
    <updated>2021-08-10T05:02:38Z</updated>
    <published>2021-08-10T05:02:38Z</published>
    <title>An empirical investigation into audio pipeline approaches for
  classifying bird species</title>
    <summary>  This paper is an investigation into aspects of an audio classification
pipeline that will be appropriate for the monitoring of bird species on edges
devices. These aspects include transfer learning, data augmentation and model
optimization. The hope is that the resulting models will be good candidates to
deploy on edge devices to monitor bird populations. Two classification
approaches will be taken into consideration, one which explores the
effectiveness of a traditional Deep Neural Network(DNN) and another that makes
use of Convolutional layers.This study aims to contribute empirical evidence of
the merits and demerits of each approach.
</summary>
    <author>
      <name>David Behr</name>
    </author>
    <author>
      <name>Ciira wa Maina</name>
    </author>
    <author>
      <name>Vukosi Marivate</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, Accepted and to be published in AFRICON 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.04449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.04449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.11637v1</id>
    <updated>2021-08-26T08:05:07Z</updated>
    <published>2021-08-26T08:05:07Z</published>
    <title>Self-Attention for Audio Super-Resolution</title>
    <summary>  Convolutions operate only locally, thus failing to model global interactions.
Self-attention is, however, able to learn representations that capture
long-range dependencies in sequences. We propose a network architecture for
audio super-resolution that combines convolution and self-attention.
Attention-based Feature-Wise Linear Modulation (AFiLM) uses self-attention
mechanism instead of recurrent neural networks to modulate the activations of
the convolutional model. Extensive experiments show that our model outperforms
existing approaches on standard benchmarks. Moreover, it allows for more
parallelization resulting in significantly faster training.
</summary>
    <author>
      <name>Nathanaël Carraz Rakotonirina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MLSP 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.11637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.11637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.12105v1</id>
    <updated>2021-08-27T03:19:07Z</updated>
    <published>2021-08-27T03:19:07Z</published>
    <title>Full Attention Bidirectional Deep Learning Structure for Single Channel
  Speech Enhancement</title>
    <summary>  As the cornerstone of other important technologies, such as speech
recognition and speech synthesis, speech enhancement is a critical area in
audio signal processing. In this paper, a new deep learning structure for
speech enhancement is demonstrated. The model introduces a "full" attention
mechanism to a bidirectional sequence-to-sequence method to make use of latent
information after each focal frame. This is an extension of the previous
attention-based RNN method. The proposed bidirectional attention-based
architecture achieves better performance in terms of speech quality (PESQ),
compared with OM-LSA, CNN-LSTM, T-GSA and the unidirectional attention-based
LSTM baseline.
</summary>
    <author>
      <name>Yuzi Yan</name>
    </author>
    <author>
      <name>Wei-Qiang Zhang</name>
    </author>
    <author>
      <name>Michael T. Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.12105v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.12105v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.01948v1</id>
    <updated>2021-09-04T22:58:08Z</updated>
    <published>2021-09-04T22:58:08Z</published>
    <title>Network Modulation Synthesis: New Algorithms for Generating Musical
  Audio Using Autoencoder Networks</title>
    <summary>  A new framework is presented for generating musical audio using autoencoder
neural networks. With the presented framework, called network modulation
synthesis, users can create synthesis architectures and use novel generative
algorithms to more easily move through the complex latent parameter space of an
autoencoder model to create audio.
  Implementations of the new algorithms are provided for the open-source CANNe
synthesizer network, and can be applied to other autoencoder networks for audio
synthesis. Spectrograms and time-series encoding analysis demonstrate that the
new algorithms provide simple mechanisms for users to generate time-varying
parameter combinations, and therefore auditory possibilities, that are
difficult to create by generating audio from handcrafted encodings.
</summary>
    <author>
      <name>Jeremy Hyrkas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to the International Computer Music Conference 2021 (2020
  Selected Papers)</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.01948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.01948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.01989v1</id>
    <updated>2021-09-05T04:07:05Z</updated>
    <published>2021-09-05T04:07:05Z</published>
    <title>The SpeakIn System for VoxCeleb Speaker Recognition Challange 2021</title>
    <summary>  This report describes our submission to the track 1 and track 2 of the
VoxCeleb Speaker Recognition Challenge 2021 (VoxSRC 2021). Both track 1 and
track 2 share the same speaker verification system, which only uses
VoxCeleb2-dev as our training set. This report explores several parts,
including data augmentation, network structures, domain-based large margin
fine-tuning, and back-end refinement. Our system is a fusion of 9 models and
achieves first place in these two tracks of VoxSRC 2021. The minDCF of our
submission is 0.1034, and the corresponding EER is 1.8460%.
</summary>
    <author>
      <name>Miao Zhao</name>
    </author>
    <author>
      <name>Yufeng Ma</name>
    </author>
    <author>
      <name>Min Liu</name>
    </author>
    <author>
      <name>Minqiang Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to INTERSPEECH2021 VoxSRC2021 Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.01989v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.01989v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.04081v1</id>
    <updated>2021-09-09T07:51:57Z</updated>
    <published>2021-09-09T07:51:57Z</published>
    <title>DeepEMO: Deep Learning for Speech Emotion Recognition</title>
    <summary>  We proposed the industry level deep learning approach for speech emotion
recognition task. In industry, carefully proposed deep transfer learning
technology shows real results due to mostly low amount of training data
availability, machine training cost, and specialized learning on dedicated AI
tasks. The proposed speech recognition framework, called DeepEMO, consists of
two main pipelines such that preprocessing to extract efficient main features
and deep transfer learning model to train and recognize. Main source code is in
https://github.com/enkhtogtokh/deepemo repository
</summary>
    <author>
      <name>Enkhtogtokh Togootogtokh</name>
    </author>
    <author>
      <name>Christian Klasen</name>
    </author>
    <link href="http://arxiv.org/abs/2109.04081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.04081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.14508v1</id>
    <updated>2021-09-29T15:43:12Z</updated>
    <published>2021-09-29T15:43:12Z</published>
    <title>Cross-domain Semi-Supervised Audio Event Classification Using
  Contrastive Regularization</title>
    <summary>  In this study, we proposed a novel semi-supervised training method that uses
unlabeled data with a class distribution that is completely different from the
target data or data without a target label. To this end, we introduce a
contrastive regularization that is designed to be target task-oriented and
trained simultaneously. In addition, we propose an audio mixing based simple
augmentation strategy that performed in batch samples. Experimental results
validate that the proposed method successfully contributed to the performance
improvement, and particularly showed that it has advantages in stable training
and generalization.
</summary>
    <author>
      <name>Donmoon Lee</name>
    </author>
    <author>
      <name>Kyogu Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, and 2 tables. Accepted paper at IEEE Workshop on
  Applications of Signal Processing to Audio and Acoustics (WASPAA) 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.14508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.14508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.04678v1</id>
    <updated>2021-10-10T01:39:28Z</updated>
    <published>2021-10-10T01:39:28Z</published>
    <title>An Overview of Techniques for Biomarker Discovery in Voice Signal</title>
    <summary>  This paper reflects on the effect of several categories of medical conditions
on human voice, focusing on those that may be hypothesized to have effects on
voice, but for which the changes themselves may be subtle enough to have eluded
observation in standard analytical examinations of the voice signal. It
presents three categories of techniques that can potentially uncover such
elusive biomarkers and allow them to be measured and used for predictive and
diagnostic purposes. These approaches include proxy techniques, model-based
analytical techniques and data-driven AI techniques.
</summary>
    <author>
      <name>Rita Singh</name>
    </author>
    <author>
      <name>Ankit Shah</name>
    </author>
    <author>
      <name>Hira Dhamyal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Last two authors contributed equally to the paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.04678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.04678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.05587v1</id>
    <updated>2021-10-11T20:01:14Z</updated>
    <published>2021-10-11T20:01:14Z</published>
    <title>Evaluation of Latent Space Disentanglement in the Presence of
  Interdependent Attributes</title>
    <summary>  Controllable music generation with deep generative models has become
increasingly reliant on disentanglement learning techniques. However, current
disentanglement metrics, such as mutual information gap (MIG), are often
inadequate and misleading when used for evaluating latent representations in
the presence of interdependent semantic attributes often encountered in
real-world music datasets. In this work, we propose a dependency-aware
information metric as a drop-in replacement for MIG that accounts for the
inherent relationship between semantic attributes.
</summary>
    <author>
      <name>Karn N. Watcharasupat</name>
    </author>
    <author>
      <name>Alexander Lerch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the Late-Breaking Demo Session of the 22nd International
  Society for Music Information Retrieval Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.05587v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.05587v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.06323v1</id>
    <updated>2021-10-12T20:16:53Z</updated>
    <published>2021-10-12T20:16:53Z</published>
    <title>An Annihilating Filter-Based DOA Estimation for Uniform Linear Array</title>
    <summary>  In this paper, we propose a new method to design an annihilating filter (AF)
for direction-of-arrival (DOA) estimation of multiple snapshots within an
uniform linear array. To evaluate the proposed method, we firstly design a DOA
estimation using multiple signal classification (MUSIC) algorithm, referred to
as the MUSIC baseline. We then compare the proposed method with the MUSIC
baseline in two environmental noise conditions: Only white noise, or both white
noise and diffusion. The experimental results highlight two main contributions;
the first is to modify conventional MUSIC algorithm for adapting different
noise conditions, and the second is to propose an AF-based method that shows
competitive accuracy of arrival angles detected and low complexity compared
with the MUSIC baseline.
</summary>
    <author>
      <name>Son Phan</name>
    </author>
    <author>
      <name>Lam Pham</name>
    </author>
    <link href="http://arxiv.org/abs/2110.06323v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.06323v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.06371v1</id>
    <updated>2021-10-12T21:32:17Z</updated>
    <published>2021-10-12T21:32:17Z</published>
    <title>Algorithmic Composition by Autonomous Systems with Multiple Time-Scales</title>
    <summary>  Dynamic systems have found their use in sound synthesis as well as score
synthesis. These levels can be integrated in monolithic autonomous systems in a
novel approach to algorithmic composition that shares certain aesthetic
motivations with some work with autonomous music systems, such as the search
for emergence. We discuss various strategies for achieving variation on
multiple time-scales by using slow-fast, hybrid dynamic systems, and
statistical feedback. The ideas are illustrated with a case study.
</summary>
    <author>
      <name>Risto Holopainen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 3 figures. Submitted to Divergence Press</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.06371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.06371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.06999v1</id>
    <updated>2021-10-13T19:20:20Z</updated>
    <published>2021-10-13T19:20:20Z</published>
    <title>Study of positional encoding approaches for Audio Spectrogram
  Transformers</title>
    <summary>  Transformers have revolutionized the world of deep learning, specially in the
field of natural language processing. Recently, the Audio Spectrogram
Transformer (AST) was proposed for audio classification, leading to state of
the art results in several datasets. However, in order for ASTs to outperform
CNNs, pretraining with ImageNet is needed. In this paper, we study one
component of the AST, the positional encoding, and propose several variants to
improve the performance of ASTs trained from scratch, without ImageNet
pretraining. Our best model, which incorporates conditional positional
encodings, significantly improves performance on Audioset and ESC-50 compared
to the original AST.
</summary>
    <author>
      <name>Leonardo Pepino</name>
    </author>
    <author>
      <name>Pablo Riera</name>
    </author>
    <author>
      <name>Luciana Ferrer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP43922.2022.9747742</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP43922.2022.9747742" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICASSP 2022. 5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.06999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.06999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.09605v2</id>
    <updated>2021-12-10T12:11:13Z</updated>
    <published>2021-10-18T20:04:46Z</published>
    <title>Neural Synthesis of Footsteps Sound Effects with Generative Adversarial
  Networks</title>
    <summary>  Footsteps are among the most ubiquitous sound effects in multimedia
applications. There is substantial research into understanding the acoustic
features and developing synthesis models for footstep sound effects. In this
paper, we present a first attempt at adopting neural synthesis for this task.
We implemented two GAN-based architectures and compared the results with real
recordings as well as six traditional sound synthesis methods. Our
architectures reached realism scores as high as recorded samples, showing
encouraging results for the task at hand.
</summary>
    <author>
      <name>Marco Comunità</name>
    </author>
    <author>
      <name>Huy Phan</name>
    </author>
    <author>
      <name>Joshua D. Reiss</name>
    </author>
    <link href="http://arxiv.org/abs/2110.09605v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.09605v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.11807v1</id>
    <updated>2021-10-22T14:27:02Z</updated>
    <published>2021-10-22T14:27:02Z</published>
    <title>Signal-Envelope: A C++ library with Python bindings for temporal
  envelope estimation</title>
    <summary>  Signals can be interpreted as composed of a rapidly varying component
modulated by a slower varying envelope. Identifying this envelope is an
essential operation in signal processing, with applications in areas ranging
from seismology to medicine. Conventional envelope detection approaches based
on classic methods tend to lack generality, however, and need to be tailored to
each specific application in order to yield reasonable results. Taking
inspiration from geometric concepts, most notably the theory of alpha-shapes,
we introduce a general-purpose library to efficiently extract the envelope of
arbitrary signals.
</summary>
    <author>
      <name>Carlos Tarjano</name>
    </author>
    <author>
      <name>Valdecy Pereira</name>
    </author>
    <link href="http://arxiv.org/abs/2110.11807v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.11807v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.13323v2</id>
    <updated>2021-10-28T22:57:34Z</updated>
    <published>2021-10-25T23:56:38Z</published>
    <title>Deep Learning Tools for Audacity: Helping Researchers Expand the
  Artist's Toolkit</title>
    <summary>  We present a software framework that integrates neural networks into the
popular open-source audio editing software, Audacity, with a minimal amount of
developer effort. In this paper, we showcase some example use cases for both
end-users and neural network developers. We hope that this work fosters a new
level of interactivity between deep learning practitioners and end-users.
</summary>
    <author>
      <name>Hugo Flores Garcia</name>
    </author>
    <author>
      <name>Aldo Aguilar</name>
    </author>
    <author>
      <name>Ethan Manilow</name>
    </author>
    <author>
      <name>Dmitry Vedenko</name>
    </author>
    <author>
      <name>Bryan Pardo</name>
    </author>
    <link href="http://arxiv.org/abs/2110.13323v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.13323v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.15316v1</id>
    <updated>2021-10-18T08:42:31Z</updated>
    <published>2021-10-18T08:42:31Z</published>
    <title>VRM-Phase I VKW system description of long-short video customizable
  keyword wakeup challenge</title>
    <summary>  Keyword wakeup technology has always been a research hotspot in speech
processing, but many related works were done on different datasets. We
organized a Chinese long-short video keyword wakeup challenge (Video Keyword
Wakeup Challenge, VKW) for testing the ability of each participating team to
build a keyword wakeup system under the public dataset. All submitted systems
not only need to support the setting of multiple different keywords, but also
need to support the wakeup of any costumed keyword.This paper mainly describes
the basic situation of the VKW challenge and the experimental results of some
participating teams.
</summary>
    <author>
      <name>Yougen Yuan</name>
    </author>
    <author>
      <name>Zhiqiang Lv</name>
    </author>
    <author>
      <name>Shen Huang</name>
    </author>
    <author>
      <name>Pengfei Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, in Chinese language, 3 tables, NCMMC 2021 conference paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.15316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.15316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.00436v1</id>
    <updated>2021-10-31T09:05:48Z</updated>
    <published>2021-10-31T09:05:48Z</published>
    <title>Analysis of North Indian Classical Ragas Using Tonnetz</title>
    <summary>  In North Indian Classical music, each raga has been traditionally associated
with a performance time, which supposedly maximizes its aesthetic and emotional
effects on the listener. The objective of this work was to investigate the
structural basis, if any, for the association of ragas with different times of
the 24-hour span. The tonnetz framework has been used to analyze the pitch sets
of 65 North Indian Classical ragas, and structural similarities have been
observed between ragas associated with (1) times of transition between day and
night, i.e., dawn and dusk, and (2) times between these transitions. These
findings could provide some insight into the scientific basis of the age-old
raga-time relation, and their effects on the perception of the listener.
</summary>
    <author>
      <name>Ananya Giri</name>
    </author>
    <link href="http://arxiv.org/abs/2111.00436v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.00436v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.00868v2</id>
    <updated>2021-11-02T10:37:13Z</updated>
    <published>2021-10-19T07:59:23Z</published>
    <title>A mathematical model of the vowel space</title>
    <summary>  The articulatory-acoustic relationship is many-to-one and non linear and this
is a great limitation for studying speech production. A simplification is
proposed to set a bijection between the vowel space (f1, f2) and the parametric
space of different vocal tract models. The generic area function model is based
on mixtures of cosines allowing the generation of main vowels with two
formulas. Then the mixture function is transformed into a coordination function
able to deal with articulatory parameters. This is shown that the coordination
function acts similarly with the Fant's model and with the 4-Tube DRM derived
from the generic model.
</summary>
    <author>
      <name>Frédéric Berthommier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GIPSA-PCMD</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2111.00868v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.00868v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.class-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.10235v1</id>
    <updated>2021-11-19T14:15:45Z</updated>
    <published>2021-11-19T14:15:45Z</published>
    <title>Interpreting deep urban sound classification using Layer-wise Relevance
  Propagation</title>
    <summary>  After constructing a deep neural network for urban sound classification, this
work focuses on the sensitive application of assisting drivers suffering from
hearing loss. As such, clear etiology justifying and interpreting model
predictions comprise a strong requirement. To this end, we used two different
representations of audio signals, i.e. Mel and constant-Q spectrograms, while
the decisions made by the deep neural network are explained via layer-wise
relevance propagation. At the same time, frequency content assigned with high
relevance in both feature sets, indicates extremely discriminative information
characterizing the present classification task. Overall, we present an
explainable AI framework for understanding deep urban sound classification.
</summary>
    <author>
      <name>Marco Colussi</name>
    </author>
    <author>
      <name>Stavros Ntalampiras</name>
    </author>
    <link href="http://arxiv.org/abs/2111.10235v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.10235v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.07349v1</id>
    <updated>2021-12-14T13:07:53Z</updated>
    <published>2021-12-14T13:07:53Z</published>
    <title>Supervised Learning for Multi Zone Sound Field Reproduction under Harsh
  Environmental Conditions</title>
    <summary>  This manuscript presents an approach for multi zone sound field reproduction
using supervised learning. Traditional multi zone sound field reproduction
methods assume constant speed of sound, neglecting nonlinear effects like wind
and temperature stratification. We show how to overcome these restrictions
using supervised learning of transfer functions. The quality of the solution is
measured by the acoustic contrast and the reproduction error. Our results show
that for the chosen setup, even with relatively small wind speeds, the acoustic
contrast and reproduction error can be improved by up to 16 dB, when wind is
considered in the trained model.
</summary>
    <author>
      <name>Henry Sallandt</name>
    </author>
    <author>
      <name>Philipp Krah</name>
    </author>
    <author>
      <name>Mathias Lemke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint submitted for publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.07349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.07349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.08027v1</id>
    <updated>2021-12-15T10:48:10Z</updated>
    <published>2021-12-15T10:48:10Z</published>
    <title>Speech frame implementation for speech analysis and recognition</title>
    <summary>  Distinctive features of the created speech frame are: the ability to take
into account the emotional state of the speaker, sup-port for working with
diseases of the speech-forming tract of speakers and the presence of manual
segmentation of a num-ber of speech signals. In addition, the system is focused
on Russian-language speech material, unlike most analogs.
</summary>
    <author>
      <name>A. A. Konev</name>
    </author>
    <author>
      <name>V. S. Khlebnikov</name>
    </author>
    <author>
      <name>A. Yu. Yakimuk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 27 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.08027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.08027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.00513v1</id>
    <updated>2022-02-24T00:31:27Z</updated>
    <published>2022-02-24T00:31:27Z</published>
    <title>A comparative study of several parameterizations for speaker recognition</title>
    <summary>  This paper presents an exhaustive study about the robustness of several
parameterizations, in speaker verification and identification tasks. We have
studied several mismatch conditions: different recording sessions, microphones,
and different languages (it has been obtained from a bilingual set of
speakers). This study reveals that the combination of several parameterizations
can improve the robustness in all the scenarios for both tasks, identification
and verification. In addition, two different methods have been evaluated:
vector quantization, and covariance matrices with an arithmetic-harmonic
sphericity measure.
</summary>
    <author>
      <name>Marcos Faundez-Zanuy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2000 10th European Signal Processing Conference, 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2203.00513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.00513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.01164v1</id>
    <updated>2022-02-23T23:49:41Z</updated>
    <published>2022-02-23T23:49:41Z</published>
    <title>Speaker recognition improvement using blind inversion of distortions</title>
    <summary>  In this paper we propose the inversion of nonlinear distortions in order to
improve the recognition rates of a speaker recognizer system. We study the
effect of saturations on the test signals, trying to take into account real
situations where the training material has been recorded in a controlled
situation but the testing signals present some mismatch with the input signal
level (saturations). The experimental results shows that a combination of data
fusion with and without nonlinear distortion compensation can improve the
recognition rates with saturated test sentences from 80% to 88.57%, while the
results with clean speech (without saturation) is 87.76% for one microphone.
</summary>
    <author>
      <name>Marcos Faundez-Zanuy</name>
    </author>
    <author>
      <name>Jordi Sole-Casals</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EUSIPCO 2004, Vienna</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2203.01164v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.01164v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.02020v1</id>
    <updated>2022-03-03T21:03:38Z</updated>
    <published>2022-03-03T21:03:38Z</published>
    <title>Nonlinear predictive models computation in ADPCM schemes</title>
    <summary>  Recently several papers have been published on nonlinear prediction applied
to speech coding. At ICASSP98 we presented a system based on an ADPCM scheme
with a nonlinear predictor based on a neural net. The most critical parameter
was the training procedure in order to achieve good generalization capability
and robustness against mismatch between training and testing conditions. In
this paper, we propose several new approaches that improve the performance of
the original system in up to 1.2dB of SEGSNR (using bayesian regularization).
The variance of the SEGSNR between frames is also minimized, so the new scheme
produces a more stable quality of the output.
</summary>
    <author>
      <name>Marcos Faundez-Zanuy</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2000 10th European Signal Processing Conference, 2000, pp. 1-4</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2203.02020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.02020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.03190v1</id>
    <updated>2022-03-07T07:57:54Z</updated>
    <published>2022-03-07T07:57:54Z</published>
    <title>Speaker recognition by means of a combination of linear and nonlinear
  predictive models</title>
    <summary>  This paper deals the combination of nonlinear predictive models with
classical LPCC parameterization for speaker recognition. It is shown that the
combination of both a measure defined over LPCC coefficients and a measure
defined over predictive analysis residual signal gives rise to an improvement
over the classical method that considers only the LPCC coefficients. If the
residual signal is obtained from a linear prediction analysis, the improvement
is 2.63% (error rate drops from 6.31% to 3.68%) and if it is computed through a
nonlinear predictive neural nets based model, the improvement is 3.68%. An
efficient algorithm for reducing the computational burden is also proposed.
</summary>
    <author>
      <name>Marcos Faundez-Zanuy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">6th European Conference on EUROSPEEECH 1999 Budapest, Hungary,
  September 5-9, 1999</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2203.03190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.03190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.03932v1</id>
    <updated>2022-03-08T08:57:11Z</updated>
    <published>2022-03-08T08:57:11Z</published>
    <title>Digital Speech Algorithms for Speaker De-Identification</title>
    <summary>  The present work is based on the COST Action IC1206 for De-identification in
multimedia content. It was performed to test four algorithms of voice
modifications on a speech gender recognizer to find the degree of modification
of pitch when the speech recognizer have the probability of success equal to
the probability of failure. The purpose of this analysis is to assess the
intensity of the speech tone modification, the quality, the reversibility and
not-reversibility of the changes made.
</summary>
    <author>
      <name>Stefano Marinozzi</name>
    </author>
    <author>
      <name>Marcos Faundez-Zanuy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CogInfoCom.2014.7020470</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CogInfoCom.2014.7020470" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2014 5th IEEE Conference on Cognitive Infocommunications
  (CogInfoCom), 2014, pp. 317-320</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2203.03932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.03932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.04638v1</id>
    <updated>2022-03-09T10:47:23Z</updated>
    <published>2022-03-09T10:47:23Z</published>
    <title>Speaker Identification Experiments Under Gender De-Identification</title>
    <summary>  The present work is based on the COST Action IC1206 for De-identification in
multimedia content. It was performed to test four algorithms of voice
modifications on a speech gender recognizer to find the degree of modification
of pitch when the speech recognizer have the probability of success equal to
the probability of failure. The purpose of this analysis is to assess the
intensity of the speech tone modification, the quality, the reversibility and
not-reversibility of the changes made. Keywords DeIdentification; Speech
Algorithms
</summary>
    <author>
      <name>Marcos Faundez-Zanuy</name>
    </author>
    <author>
      <name>Enric Sesa-Nogueras</name>
    </author>
    <author>
      <name>Stefano Marinozzi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CCST.2015.7389702</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CCST.2015.7389702" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages. arXiv admin note: substantial text overlap with
  arXiv:2203.03932</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2015 International Carnahan Conference on Security Technology
  (ICCST), 2015, pp. 1-6</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2203.04638v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.04638v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.11614v1</id>
    <updated>2022-03-22T11:11:02Z</updated>
    <published>2022-03-22T11:11:02Z</published>
    <title>Speaker recognition with a MLP classifier and LPCC codebook</title>
    <summary>  This paper improves the speaker recognition rates of a MLP classifier and
LPCC codebook alone, using a linear combination between both methods. In
simulations we have obtained an improvement of 4.7% over a LPCC codebook of 32
vectors and 1.5% for a codebook of 128 vectors (error rate drops from 3.68% to
2.1%). Also we propose an efficient algorithm that reduces the computational
complexity of the LPCC-VQ system by a factor of 4.
</summary>
    <author>
      <name>Daniel Rodriguez-Porcheron</name>
    </author>
    <author>
      <name>Marcos Faundez-Zanuy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.1999.759872</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.1999.759872" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, published in 1999 IEEE International Conference on
  Acoustics, Speech, and Signal Processing. Proceedings. ICASSP99 (Cat.
  No.99CH36258) Phoenix, AZ, USA</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">1999 IEEE International Conference on Acoustics, Speech, and
  Signal Processing. Proceedings. ICASSP99 (Cat. No.99CH36258), 1999, pp.
  1005-1008 vol.2</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2203.11614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.11614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.12306v1</id>
    <updated>2022-03-23T10:06:41Z</updated>
    <published>2022-03-23T10:06:41Z</published>
    <title>A combination between VQ and covariance matrices for speaker recognition</title>
    <summary>  This paper presents a new algorithm for speaker recognition based on the
combination between the classical Vector Quantization (VQ) and Covariance
Matrix (CM) methods. The combined VQ-CM method improves the identification
rates of each method alone, with comparable computational burden. It offers a
straightforward procedure to obtain a model similar to GMM with full covariance
matrices. Experimental results also show that it is more robust against noise
than VQ or CM alone.
</summary>
    <author>
      <name>Marcos Faundez-Zanuy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2001.940865</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2001.940865" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, published in 2001 IEEE International Conference on
  Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221),
  Salt Lake City, UT, USA</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2001 IEEE International Conference on Acoustics, Speech, and
  Signal Processing. Proceedings (Cat. No.01CH37221), 2001, pp. 453-456 vol.1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2203.12306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.12306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.12896v1</id>
    <updated>2022-03-24T07:28:15Z</updated>
    <published>2022-03-24T07:28:15Z</published>
    <title>Wide band sub-band speech coding using nonlinear prediction</title>
    <summary>  We compare a wide band sub-band speech coder using ADPCM schemes with linear
prediction against the same scheme with nonlinear prediction based on
multi-layer perceptrons. Exhaustive results are presented in each band, and the
full signal. Our proposed scheme with non-linear neural net prediction
outperforms the linear scheme up to 2 dB in SEGSNR. In addition, we propose a
simple method based on a non-linearity in order to obtain a synthetic wide band
signal from a narrow band signal.
</summary>
    <author>
      <name>Marcos Faundez-Zanuy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2003.1202324</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2003.1202324" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, published in 2003 IEEE International Conference on
  Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03)
  Hong Kong, China</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2003 IEEE International Conference on Acoustics, Speech, and
  Signal Processing, 2003. Proceedings. (ICASSP '03)., 2003, pp. II-181</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2203.12896v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.12896v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.14409v1</id>
    <updated>2022-03-27T22:44:16Z</updated>
    <published>2022-03-27T22:44:16Z</published>
    <title>SMP-PHAT: Lightweight DoA Estimation by Merging Microphone Pairs</title>
    <summary>  This paper introduces SMP-PHAT, which performs direction of arrival (DoA) of
sound estimation with a microphone array by merging pairs of microphones that
are parallel in space. This approach reduces the number of pairwise
cross-correlation computations, and brings down the number of flops and memory
lookups when searching for DoA. Experiments on low-cost hardware with commonly
used microphone arrays show that the proposed method provides the same accuracy
as the former SRP-PHAT approach, while reducing the computational load by 39%
in some cases.
</summary>
    <author>
      <name>François Grondin</name>
    </author>
    <author>
      <name>Marc-Antoine Maheux</name>
    </author>
    <author>
      <name>Jean-Samuel Lauzon</name>
    </author>
    <author>
      <name>Jonathan Vincent</name>
    </author>
    <author>
      <name>François Michaud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Interspeech 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.14409v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.14409v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.01009v1</id>
    <updated>2022-04-03T06:54:13Z</updated>
    <published>2022-04-03T06:54:13Z</published>
    <title>A Computational Analysis of Pitch Drift in Unaccompanied Solo Singing
  using DBSCAN Clustering</title>
    <summary>  Unaccompanied vocalists usually change the tuning unintentionally and end up
with a higher or lower pitch than the starting point during a long performance.
This phenomenon is called pitch drift, which is dependent on various elements,
such as the skill of the performer, and the length and difficulty of the
performance. In this paper, we propose a computational method for measuring
pitch drift in the course of an unaccompanied vocal performance, using pitch
histogram and DBSCAN clustering.
</summary>
    <author>
      <name>Sepideh Shafiei</name>
    </author>
    <author>
      <name>S. Hakam</name>
    </author>
    <link href="http://arxiv.org/abs/2204.01009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.01009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.01294v1</id>
    <updated>2022-04-04T08:04:04Z</updated>
    <published>2022-04-04T08:04:04Z</published>
    <title>On The Model Size Selection For Speaker Identification</title>
    <summary>  In this paper we evaluate the relevance of the model size for speaker
identification. We show that it is possible to improve the identification rates
if a different model size is used for each speaker. We also present some
criteria for selecting the model size, and a new algorithm that outperforms the
classical system with a fixed model size.
</summary>
    <author>
      <name>Marcos Faundez-Zanuy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, published in Speaker odyssey 2001, The speaker recognition
  workshop. 189-194 Crete (Greece)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2001 A Speaker Odyssey - The Speaker Recognition Workshop June
  18-22, 2001, Crete, Greece</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2204.01294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.01294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.01295v1</id>
    <updated>2022-04-04T08:04:16Z</updated>
    <published>2022-04-04T08:04:16Z</published>
    <title>Nonlinear Vectorial Prediction with Neural Nets</title>
    <summary>  In this paper we propose a nonlinear vectorial prediction scheme based on a
Multi Layer Perceptron. This system is applied to speech coding in an ADPCM
backward scheme. In addition a procedure to obtain a vectorial quantizer is
given, in order to achieve a fully vectorial speech encoder. We also present
several results with the proposed system
</summary>
    <author>
      <name>Marcos Faundez-Zanuy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5555/646370.688874</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5555/646370.688874" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, published in Proceedings of the 6th International Work
  Conference on Artificial and Natural Neural Networks: Bio inspired
  Applications of Connectionism Part II June 2001 Pages 754 761</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Computer Science LNCS 2085 Vol. II, pages
  754-761. IWANN 2001, Granada (Spain) ISSN 0302-9743</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2204.01295v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.01295v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.02101v1</id>
    <updated>2022-04-05T10:48:13Z</updated>
    <published>2022-04-05T10:48:13Z</published>
    <title>Non-Linear Speech coding with MLP, RBF and Elman based prediction</title>
    <summary>  In this paper we propose a nonlinear scalar predictor based on a combination
of Multi Layer Perceptron, Radial Basis Functions and Elman networks. This
system is applied to speech coding in an ADPCM backward scheme. The combination
of this predictors improves the results of one predictor alone. A comparative
study of this three neural networks for speech prediction is also presented.
</summary>
    <author>
      <name>Marcos Faundez-Zanuy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/3-540-44869-1_85</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/3-540-44869-1_85" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, published in Mira, J., \'Alvarez, J.R. (eds) Artificial
  Neural Nets Problem Solving Methods. IWANN 2003. Lecture Notes in Computer
  Science, vol 2687. Springer, Berlin, Heidelberg</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Work-Conference on Artificial Neural Networks IWANN
  2003, LNCS 2687 Menorca (Spain)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2204.02101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.02101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.02400v1</id>
    <updated>2022-04-05T10:57:46Z</updated>
    <published>2022-04-05T10:57:46Z</published>
    <title>What can predictive speech coders learn from speaker recognizers?</title>
    <summary>  This paper compares the speech coder and speaker recognizer applications,
showing some parallelism between them. In this paper, some approaches used for
speaker recognition are applied to speech coding in order to improve the
prediction accuracy. Experimental results show an improvement in Segmental SNR
(SEGSNR).
</summary>
    <author>
      <name>Marcos Faundez-Zanuy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, published in ITRW on Non-Linear Speech Processing (NOLISP
  03), May 20-23, 2003, Le Croisic, France, paper 001. arXiv admin note: text
  overlap with arXiv:2204.02101</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Non-Linear Speech Processing (NOLISP) 2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2204.02400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.02400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.03967v1</id>
    <updated>2022-04-08T09:50:39Z</updated>
    <published>2022-04-08T09:50:39Z</published>
    <title>The Sillwood Technologies System for the VoiceMOS Challenge 2022</title>
    <summary>  In this paper we describe our entry for the VoiceMOS Challenge 2022 for both
the main and out-of-domain (OOD) track of the competition. Our system is based
on finetuning pre-trained self-supervised waveform prediction models, while
improving its generalisation ability through stochastic weight averaging.
Further, we use influence functions to identity possible low-quality data
within the training set to further increase our model's performance for the OOD
track. Our system ranked 5th and joint 7th for the main track and OOD track,
respectively.
</summary>
    <author>
      <name>Jiameng Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Interspeech 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.03967v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.03967v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.04756v1</id>
    <updated>2022-04-10T19:42:52Z</updated>
    <published>2022-04-10T19:42:52Z</published>
    <title>Towards Evaluation of Autonomously Generated Musical Compositions: A
  Comprehensive Survey</title>
    <summary>  There are many applications that aim to create a complete model for an
autonomously generated composition; systems are able to generate muzak songs,
assist singers in transcribing songs or can imitate long-dead authors.
Subjective understanding of creativity or aesthetics differs not only within
preferences (popular authors or genres), but also differs on the basis of
experienced experience or socio-cultural environment. So, what do we want to
achieve with such an adaptation? What is the benefit of the resulting work for
the author, who can no longer evaluate this composition? And in what ways
should we evaluate such a composition at all?
</summary>
    <author>
      <name>Daniel Kvak</name>
    </author>
    <link href="http://arxiv.org/abs/2204.04756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.04756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.08977v2</id>
    <updated>2022-06-08T13:19:17Z</updated>
    <published>2022-04-19T16:26:34Z</published>
    <title>Disappeared Command: Spoofing Attack On Automatic Speech Recognition
  Systems with Sound Masking</title>
    <summary>  The development of deep learning technology has greatly promoted the
performance improvement of automatic speech recognition (ASR) technology, which
has demonstrated an ability comparable to human hearing in many tasks. Voice
interfaces are becoming more and more widely used as input for many
applications and smart devices. However, existing research has shown that DNN
is easily disturbed by slight disturbances and makes false recognition, which
is extremely dangerous for intelligent voice applications controlled by voice.
</summary>
    <author>
      <name>Jinghui Xu</name>
    </author>
    <author>
      <name>Jifeng Zhu</name>
    </author>
    <author>
      <name>Yong Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1903.10346 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.08977v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.08977v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.09381v2</id>
    <updated>2022-06-30T15:49:26Z</updated>
    <published>2022-04-20T10:47:28Z</published>
    <title>Exploration strategies for articulatory synthesis of complex syllable
  onsets</title>
    <summary>  High-quality articulatory speech synthesis has many potential applications in
speech science and technology. However, developing appropriate mappings from
linguistic specification to articulatory gestures is difficult and time
consuming. In this paper we construct an optimisation-based framework as a
first step towards learning these mappings without manual intervention. We
demonstrate the production of syllables with complex onsets and discuss the
quality of the articulatory gestures with reference to coarticulation.
</summary>
    <author>
      <name>Daniel R. van Niekerk</name>
    </author>
    <author>
      <name>Anqi Xu</name>
    </author>
    <author>
      <name>Branislav Gerazov</name>
    </author>
    <author>
      <name>Paul K. Krug</name>
    </author>
    <author>
      <name>Peter Birkholz</name>
    </author>
    <author>
      <name>Yi Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at Interspeech 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.09381v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.09381v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.1383v1</id>
    <updated>2011-05-06T20:25:36Z</updated>
    <published>2011-05-06T20:25:36Z</published>
    <title>Topological Considerations for Tuning and Fingering Stringed Instruments</title>
    <summary>  We present a formal language for assigning pitches to strings for fingered
multi-string instruments, particularly the six-string guitar. Given the
instrument's tuning (the strings' open pitches) and the compass of the fingers
of the hand stopping the strings, the formalism yields a framework for
simultaneously optimizing three things: the mapping of pitches to strings, the
choice of instrument tuning, and the key of the composition. Final optimization
relies on heuristics idiomatic to the tuning, the particular musical style, and
the performer's proficiency.
</summary>
    <author>
      <name>Terry Allen</name>
    </author>
    <author>
      <name>Camille Goudeseune</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.1383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.1383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="14P10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.0; H.5.5; G.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.05849v1</id>
    <updated>2015-03-19T17:24:16Z</updated>
    <published>2015-03-19T17:24:16Z</published>
    <title>Deep Transform: Time-Domain Audio Error Correction via Probabilistic
  Re-Synthesis</title>
    <summary>  In the process of recording, storage and transmission of time-domain audio
signals, errors may be introduced that are difficult to correct in an
unsupervised way. Here, we train a convolutional deep neural network to
re-synthesize input time-domain speech signals at its output layer. We then use
this abstract transformation, which we call a deep transform (DT), to perform
probabilistic re-synthesis on further speech (of the same speaker) which has
been degraded. Using the convolutive DT, we demonstrate the recovery of speech
audio that has been subject to extreme degradation. This approach may be useful
for correction of errors in communications devices.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1503.05849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.05849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06046v1</id>
    <updated>2015-03-20T12:00:44Z</updated>
    <published>2015-03-20T12:00:44Z</published>
    <title>Deep Transform: Cocktail Party Source Separation via Probabilistic
  Re-Synthesis</title>
    <summary>  In cocktail party listening scenarios, the human brain is able to separate
competing speech signals. However, the signal processing implemented by the
brain to perform cocktail party listening is not well understood. Here, we
trained two separate convolutive autoencoder deep neural networks (DNN) to
separate monaural and binaural mixtures of two concurrent speech streams. We
then used these DNNs as convolutive deep transform (CDT) devices to perform
probabilistic re-synthesis. The CDTs operated directly in the time-domain. Our
simulations demonstrate that very simple neural networks are capable of
exploiting monaural and binaural information available in a cocktail party
listening scenario.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1503.06046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06962v1</id>
    <updated>2015-03-24T09:34:51Z</updated>
    <published>2015-03-24T09:34:51Z</published>
    <title>Probabilistic Binary-Mask Cocktail-Party Source Separation in a
  Convolutional Deep Neural Network</title>
    <summary>  Separation of competing speech is a key challenge in signal processing and a
feat routinely performed by the human auditory brain. A long standing benchmark
of the spectrogram approach to source separation is known as the ideal binary
mask. Here, we train a convolutional deep neural network, on a two-speaker
cocktail party problem, to make probabilistic predictions about binary masks.
Our results approach ideal binary mask performance, illustrating that
relatively simple deep neural networks are capable of robust binary mask
prediction. We also illustrate the trade-off between prediction statistics and
separation quality.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1503.06962v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06962v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05260v1</id>
    <updated>2017-11-14T16:47:23Z</updated>
    <published>2017-11-14T16:47:23Z</published>
    <title>Optimal Tuning of Two-Dimensional Keyboards</title>
    <summary>  We give a new analysis of a tuning problem in music theory, pertaining
specifically to the approximation of harmonics on a two-dimensional keyboard.
We formulate the question as a linear programming problem on families of
constraints and provide exact solutions for many new keyboard dimensions. We
also show that an optimal tuning for harmonic approximation can be obtained for
any keyboard of given width, provided sufficiently many rows of octaves.
</summary>
    <author>
      <name>Aricca Bannerman</name>
    </author>
    <author>
      <name>James Emington</name>
    </author>
    <author>
      <name>Anil Venkatesh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 page, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.05260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="00A65" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05447v2</id>
    <updated>2017-11-28T02:07:43Z</updated>
    <published>2017-11-15T08:27:35Z</published>
    <title>Emotional End-to-End Neural Speech Synthesizer</title>
    <summary>  In this paper, we introduce an emotional speech synthesizer based on the
recent end-to-end neural model, named Tacotron. Despite its benefits, we found
that the original Tacotron suffers from the exposure bias problem and
irregularity of the attention alignment. Later, we address the problem by
utilization of context vector and residual connection at recurrent neural
networks (RNNs). Our experiments showed that the model could successfully train
and generate speech for given emotion labels.
</summary>
    <author>
      <name>Younggun Lee</name>
    </author>
    <author>
      <name>Azam Rabiee</name>
    </author>
    <author>
      <name>Soo-Young Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.05447v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05447v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.10271v1</id>
    <updated>2017-11-28T13:13:41Z</updated>
    <published>2017-11-28T13:13:41Z</published>
    <title>Exploiting Nontrivial Connectivity for Automatic Speech Recognition</title>
    <summary>  Nontrivial connectivity has allowed the training of very deep networks by
addressing the problem of vanishing gradients and offering a more efficient
method of reusing parameters. In this paper we make a comparison between
residual networks, densely-connected networks and highway networks on an image
classification task. Next, we show that these methodologies can easily be
deployed into automatic speech recognition and provide significant improvements
to existing models.
</summary>
    <author>
      <name>Marius Paraschiv</name>
    </author>
    <author>
      <name>Lasse Borgholt</name>
    </author>
    <author>
      <name>Tycho Max Sylvester Tax</name>
    </author>
    <author>
      <name>Marco Singh</name>
    </author>
    <author>
      <name>Lars Maaløe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the ML4Audio workshop at the NIPS 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.10271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.10271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.11141v1</id>
    <updated>2017-11-29T22:45:05Z</updated>
    <published>2017-11-29T22:45:05Z</published>
    <title>Stream Attention for far-field multi-microphone ASR</title>
    <summary>  A stream attention framework has been applied to the posterior probabilities
of the deep neural network (DNN) to improve the far-field automatic speech
recognition (ASR) performance in the multi-microphone configuration. The stream
attention scheme has been realized through an attention vector, which is
derived by predicting the ASR performance from the phoneme posterior
distribution of individual microphone stream, focusing the recognizer's
attention to more reliable microphones. Investigation on the various ASR
performance measures has been carried out using the real recorded dataset.
Experiments results show that the proposed framework has yielded substantial
improvements in word error rate (WER).
</summary>
    <author>
      <name>Xiaofei Wang</name>
    </author>
    <author>
      <name>Yonghong Yan</name>
    </author>
    <author>
      <name>Hynek Hermansky</name>
    </author>
    <link href="http://arxiv.org/abs/1711.11141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.11141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.11160v1</id>
    <updated>2017-11-29T23:53:52Z</updated>
    <published>2017-11-29T23:53:52Z</published>
    <title>Time Domain Neural Audio Style Transfer</title>
    <summary>  A recently published method for audio style transfer has shown how to extend
the process of image style transfer to audio. This method synthesizes audio
"content" and "style" independently using the magnitudes of a short time
Fourier transform, shallow convolutional networks with randomly initialized
filters, and iterative phase reconstruction with Griffin-Lim. In this work, we
explore whether it is possible to directly optimize a time domain audio signal,
removing the process of phase reconstruction and opening up possibilities for
real-time applications and higher quality syntheses. We explore a variety of
style transfer processes on neural networks that operate directly on time
domain audio signals and demonstrate one such network capable of audio
stylization.
</summary>
    <author>
      <name>Parag K. Mital</name>
    </author>
    <link href="http://arxiv.org/abs/1711.11160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.11160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.07628v2</id>
    <updated>2018-08-09T22:42:24Z</updated>
    <published>2018-05-19T17:35:14Z</published>
    <title>Sparse Architectures for Text-Independent Speaker Verification Using
  Deep Neural Networks</title>
    <summary>  Network pruning is of great importance due to the elimination of the
unimportant weights or features activated due to the network
over-parametrization. Advantages of sparsity enforcement include preventing the
overfitting and speedup. Considering a large number of parameters in deep
architectures, network compression becomes of critical importance due to the
required huge amount of computational power. In this work, we impose structured
sparsity for speaker verification which is the validation of the query speaker
compared to the speaker gallery. We will show that the mere sparsity
enforcement can improve the verification results due to the possible initial
overfitting in the network.
</summary>
    <author>
      <name>Sara Sedighi</name>
    </author>
    <author>
      <name>Shayan Ramhormozi</name>
    </author>
    <link href="http://arxiv.org/abs/1805.07628v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.07628v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.00381v1</id>
    <updated>2018-09-02T20:03:09Z</updated>
    <published>2018-09-02T20:03:09Z</published>
    <title>Multitask Learning for Fundamental Frequency Estimation in Music</title>
    <summary>  Fundamental frequency (f0) estimation from polyphonic music includes the
tasks of multiple-f0, melody, vocal, and bass line estimation. Historically
these problems have been approached separately, and only recently, using
learning-based approaches. We present a multitask deep learning architecture
that jointly estimates outputs for various tasks including multiple-f0, melody,
vocal and bass line estimation, and is trained using a large,
semi-automatically annotated dataset. We show that the multitask model
outperforms its single-task counterparts, and explore the effect of various
design decisions in our approach, and show that it performs better or at least
competitively when compared against strong baseline methods.
</summary>
    <author>
      <name>Rachel M. Bittner</name>
    </author>
    <author>
      <name>Brian McFee</name>
    </author>
    <author>
      <name>Juan P. Bello</name>
    </author>
    <link href="http://arxiv.org/abs/1809.00381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.00381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.01060v1</id>
    <updated>2018-12-03T20:09:05Z</updated>
    <published>2018-12-03T20:09:05Z</published>
    <title>Bach2Bach: Generating Music Using A Deep Reinforcement Learning Approach</title>
    <summary>  A model of music needs to have the ability to recall past details and have a
clear, coherent understanding of musical structure. Detailed in the paper is a
deep reinforcement learning architecture that predicts and generates polyphonic
music aligned with musical rules. The probabilistic model presented is a
Bi-axial LSTM trained with a pseudo-kernel reminiscent of a convolutional
kernel. To encourage exploration and impose greater global coherence on the
generated music, a deep reinforcement learning approach DQN is adopted. When
analyzed quantitatively and qualitatively, this approach performs well in
composing polyphonic music.
</summary>
    <author>
      <name>Nikhil Kotecha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.01060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.01060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.01731v1</id>
    <updated>2018-12-04T22:43:33Z</updated>
    <published>2018-12-04T22:43:33Z</published>
    <title>Domain Mismatch Robust Acoustic Scene Classification using Channel
  Information Conversion</title>
    <summary>  In a recent acoustic scene classification (ASC) research field, training and
test device channel mismatch have become an issue for the real world
implementation. To address the issue, this paper proposes a channel domain
conversion using factorized hierarchical variational autoencoder. Proposed
method adapts both the source and target domain to a pre-defined specific
domain. Unlike the conventional approach, the relationship between the target
and source domain and information of each domain are not required in the
adaptation process. Based on the experimental results using the IEEE detection
and classification of acoustic scenes and event 2018 task 1-B dataset and the
baseline system, it is shown that the proposed approach can mitigate the
channel mismatching issue of different recording devices.
</summary>
    <author>
      <name>Seongkyu Mun</name>
    </author>
    <author>
      <name>Suwon Shon</name>
    </author>
    <link href="http://arxiv.org/abs/1812.01731v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.01731v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.05901v1</id>
    <updated>2018-12-14T13:15:45Z</updated>
    <published>2018-12-14T13:15:45Z</published>
    <title>Evaluation of an open-source implementation of the SRP-PHAT algorithm
  within the 2018 LOCATA challenge</title>
    <summary>  This short paper presents an efficient, flexible implementation of the
SRP-PHAT multichannel sound source localization method. The method is evaluated
on the single-source tasks of the LOCATA 2018 development dataset, and an
associated Matlab toolbox is made available online.
</summary>
    <author>
      <name>Romain Lebarbenchon</name>
    </author>
    <author>
      <name>Ewen Camberlein</name>
    </author>
    <author>
      <name>Diego di Carlo</name>
    </author>
    <author>
      <name>Clément Gaultier</name>
    </author>
    <author>
      <name>Antoine Deleforge</name>
    </author>
    <author>
      <name>Nancy Bertin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the LOCATA Challenge Workshop - a satellite event
  of IWAENC 2018 (arXiv:1811.08482 )</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.05901v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.05901v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.01453v4</id>
    <updated>2020-07-21T15:32:41Z</updated>
    <published>2019-06-03T06:58:20Z</published>
    <title>MUSICNTWRK: data tools for music theory, analysis and composition</title>
    <summary>  We present the API for MUSICNTWRK, a python library for pitch class set and
rhythmic sequences classification and manipulation, the generation of networks
in generalized music and sound spaces, deep learning algorithms for timbre
recognition, and the sonification of arbitrary data. The software is freely
available under GPL 3.0 and can be downloaded at www.musicntwrk.com or
installed as a PyPi project (pip install musicntwrk).
</summary>
    <author>
      <name>Marco Buongiorno Nardelli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1905.01842</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.01453v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.01453v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.02618v1</id>
    <updated>2019-06-06T14:44:36Z</updated>
    <published>2019-06-06T14:44:36Z</published>
    <title>Singing voice separation: a study on training data</title>
    <summary>  In the recent years, singing voice separation systems showed increased
performance due to the use of supervised training. The design of training
datasets is known as a crucial factor in the performance of such systems. We
investigate on how the characteristics of the training dataset impacts the
separation performances of state-of-the-art singing voice separation
algorithms. We show that the separation quality and diversity are two important
and complementary assets of a good training dataset. We also provide insights
on possible transforms to perform data augmentation for this task.
</summary>
    <author>
      <name>Laure Prétet</name>
    </author>
    <author>
      <name>Romain Hennequin</name>
    </author>
    <author>
      <name>Jimena Royo-Letelier</name>
    </author>
    <author>
      <name>Andrea Vaglio</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2019.8683555</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2019.8683555" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICASSP 2019 - 2019 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1906.02618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.02618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.03697v2</id>
    <updated>2019-06-28T22:59:34Z</updated>
    <published>2019-06-09T19:35:55Z</published>
    <title>Deep Unsupervised Drum Transcription</title>
    <summary>  We introduce DrummerNet, a drum transcription system that is trained in an
unsupervised manner. DrummerNet does not require any ground-truth transcription
and, with the data-scalability of deep neural networks, learns from a large
unlabeled dataset. In DrummerNet, the target drum signal is first passed to a
(trainable) transcriber, then reconstructed in a (fixed) synthesizer according
to the transcription estimate. By training the system to minimize the distance
between the input and the output audio signals, the transcriber learns to
transcribe without ground truth transcription. Our experiment shows that
DrummerNet performs favorably compared to many other recent drum transcription
systems, both supervised and unsupervised.
</summary>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISMIR 2019 camera-ready</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.03697v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.03697v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.06746v1</id>
    <updated>2019-06-16T18:16:21Z</updated>
    <published>2019-06-16T18:16:21Z</published>
    <title>Multi-scale Embedded CNN for Music Tagging (MsE-CNN)</title>
    <summary>  Convolutional neural networks (CNN) recently gained notable attraction in a
variety of machine learning tasks: including music classification and style
tagging. In this work, we propose implementing intermediate connections to the
CNN architecture to facilitate the transfer of multi-scale/level knowledge
between different layers. Our novel model for music tagging shows significant
improvement in comparison to the proposed approaches in the literature, due to
its ability to carry low-level timbral features to the last layer.
</summary>
    <author>
      <name>Nima Hamidi</name>
    </author>
    <author>
      <name>Mohsen Vahidzadeh</name>
    </author>
    <author>
      <name>Stephen Baek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 36th International Conference on Machine Learning
  (ICML)</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.06746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.06746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.00459v1</id>
    <updated>2019-12-28T13:45:29Z</updated>
    <published>2019-12-28T13:45:29Z</published>
    <title>Joint Robust Voicing Detection and Pitch Estimation Based on Residual
  Harmonics</title>
    <summary>  This paper focuses on the problem of pitch tracking in noisy conditions. A
method using harmonic information in the residual signal is presented. The
proposed criterion is used both for pitch estimation, as well as for
determining the voicing segments of speech. In the experiments, the method is
compared to six state-of-the-art pitch trackers on the Keele and CSTR
databases. The proposed technique is shown to be particularly robust to
additive noise, leading to a significant improvement in adverse conditions.
</summary>
    <author>
      <name>Thomas Drugman</name>
    </author>
    <author>
      <name>Abeer Alwan</name>
    </author>
    <link href="http://arxiv.org/abs/2001.00459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.00579v1</id>
    <updated>2020-01-02T09:25:30Z</updated>
    <published>2020-01-02T09:25:30Z</published>
    <title>A Comparative Evaluation of Pitch Modification Techniques</title>
    <summary>  This paper addresses the problem of pitch modification, as an important
module for an efficient voice transformation system. The Deterministic plus
Stochastic Model of the residual signal we proposed in a previous work is
compared to TDPSOLA, HNM and STRAIGHT. The four methods are compared through an
important subjective test. The influence of the speaker gender and of the pitch
modification ratio is analyzed. Despite its higher compression level, the DSM
technique is shown to give similar or better results than other methods,
especially for male speakers and important ratios of modification. The DSM
turns out to be only outperformed by STRAIGHT for female voices.
</summary>
    <author>
      <name>Thomas Drugman</name>
    </author>
    <author>
      <name>Thierry Dutoit</name>
    </author>
    <link href="http://arxiv.org/abs/2001.00579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.00580v1</id>
    <updated>2020-01-02T09:30:23Z</updated>
    <published>2020-01-02T09:30:23Z</published>
    <title>Assessment of Audio Features for Automatic Cough Detection</title>
    <summary>  This paper addresses the issue of cough detection using only audio
recordings, with the ultimate goal of quantifying and qualifying the degree of
pathology for patients suffering from respiratory diseases, notably
mucoviscidosis. A large set of audio features describing various aspects of the
audio signal is proposed. These features are assessed in two steps. First,
their intrisic potential and redundancy are evaluated using mutual
information-based measures. Secondly, their efficiency is confirmed relying on
three classifiers: Artificial Neural Network, Gaussian Mixture Model and
Support Vector Machine. The influence of both the feature dimension and the
classifier complexity are also investigated.
</summary>
    <author>
      <name>Thomas Drugman</name>
    </author>
    <author>
      <name>Jerome Urbain</name>
    </author>
    <author>
      <name>Thierry Dutoit</name>
    </author>
    <link href="http://arxiv.org/abs/2001.00580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.00582v1</id>
    <updated>2020-01-02T09:44:52Z</updated>
    <published>2020-01-02T09:44:52Z</published>
    <title>Excitation-based Voice Quality Analysis and Modification</title>
    <summary>  This paper investigates the differences occuring in the excitation for
different voice qualities. Its goal is two-fold. First a large corpus
containing three voice qualities (modal, soft and loud) uttered by the same
speaker is analyzed and significant differences in characteristics extracted
from the excitation are observed. Secondly rules of modification derived from
the analysis are used to build a voice quality transformation system applied as
a post-process to HMM-based speech synthesis. The system is shown to
effectively achieve the transformations while maintaining the delivered
quality.
</summary>
    <author>
      <name>Thomas Drugman</name>
    </author>
    <author>
      <name>Thierry Dutoit</name>
    </author>
    <author>
      <name>Baris Bozkurt</name>
    </author>
    <link href="http://arxiv.org/abs/2001.00582v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00582v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.00583v1</id>
    <updated>2020-01-02T10:04:37Z</updated>
    <published>2020-01-02T10:04:37Z</published>
    <title>On the Mutual Information between Source and Filter Contributions for
  Voice Pathology Detection</title>
    <summary>  This paper addresses the problem of automatic detection of voice pathologies
directly from the speech signal. For this, we investigate the use of the
glottal source estimation as a means to detect voice disorders. Three sets of
features are proposed, depending on whether they are related to the speech or
the glottal signal, or to prosody. The relevancy of these features is assessed
through mutual information-based measures. This allows an intuitive
interpretation in terms of discrimation power and redundancy between the
features, independently of any subsequent classifier. It is discussed which
characteristics are interestingly informative or complementary for detecting
voice pathologies.
</summary>
    <author>
      <name>Thomas Drugman</name>
    </author>
    <author>
      <name>Thomas Dubuisson</name>
    </author>
    <author>
      <name>Thierry Dutoit</name>
    </author>
    <link href="http://arxiv.org/abs/2001.00583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.00841v1</id>
    <updated>2019-12-28T19:27:45Z</updated>
    <published>2019-12-28T19:27:45Z</published>
    <title>Glottal Closure and Opening Instant Detection from Speech Signals</title>
    <summary>  This paper proposes a new procedure to detect Glottal Closure and Opening
Instants (GCIs and GOIs) directly from speech waveforms. The procedure is
divided into two successive steps. First a mean-based signal is computed, and
intervals where speech events are expected to occur are extracted from it.
Secondly, at each interval a precise position of the speech event is assigned
by locating a discontinuity in the Linear Prediction residual. The proposed
method is compared to the DYPSA algorithm on the CMU ARCTIC database. A
significant improvement as well as a better noise robustness are reported.
Besides, results of GOI identification accuracy are promising for the glottal
source characterization.
</summary>
    <author>
      <name>Thomas Drugman</name>
    </author>
    <author>
      <name>Thierry Dutoit</name>
    </author>
    <link href="http://arxiv.org/abs/2001.00841v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.00841v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.08830v1</id>
    <updated>2020-01-23T22:11:38Z</updated>
    <published>2020-01-23T22:11:38Z</published>
    <title>Scattering Features for Multimodal Gait Recognition</title>
    <summary>  We consider the problem of identifying people on the basis of their walk
(gait) pattern. Classical approaches to tackle this problem are based on, e.g.,
video recordings or piezoelectric sensors embedded in the floor. In this work,
we rely on acoustic and vibration measurements, obtained from a microphone and
a geophone sensor, respectively. The contribution of this work is twofold.
First, we propose a feature extraction method based on an (untrained) shallow
scattering network, specially tailored for the gait signals. Second, we
demonstrate that fusing the two modalities improves identification in the
practically relevant open set scenario.
</summary>
    <author>
      <name>Srđan Kitić</name>
    </author>
    <author>
      <name>Gilles Puy</name>
    </author>
    <author>
      <name>Patrick Pérez</name>
    </author>
    <author>
      <name>Philippe Gilberton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at IEEE GlobalSIP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.08830v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.08830v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.00143v1</id>
    <updated>2020-08-01T01:23:36Z</updated>
    <published>2020-08-01T01:23:36Z</published>
    <title>Efficient Independent Vector Extraction of Dominant Target Speech</title>
    <summary>  The complete decomposition performed by blind source separation is
computationally demanding and superfluous when only the speech of one specific
target speaker is desired. In this paper, we propose a computationally
efficient blind speech extraction method based on a proper modification of the
commonly utilized independent vector analysis algorithm, under the mild
assumption that the average power of signal of interest outweighs interfering
speech sources. Considering that the minimum distortion principle cannot be
implemented since the full demixing matrix is not available, we also design a
one-unit scaling operation to solve the scaling ambiguity. Simulations validate
the efficacy of the proposed method in extracting the dominant speech.
</summary>
    <author>
      <name>Lele Liao</name>
    </author>
    <author>
      <name>Zhaoyi Gu</name>
    </author>
    <author>
      <name>Jing Lu</name>
    </author>
    <link href="http://arxiv.org/abs/2008.00143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.00143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.03436v2</id>
    <updated>2020-10-13T07:35:05Z</updated>
    <published>2020-08-08T03:44:28Z</published>
    <title>Symbolic Music Playing Techniques Generation as a Tagging Problem</title>
    <summary>  Music generation has always been a hot topic. When discussing symbolic music,
melody or harmonies are usually seen as the only generating targets. But in
fact, playing techniques are also quite an important part of the music. In this
paper, we discuss the playing techniques generation problem by seeing it as a
tagging problem. We propose a model that can use both the current data and
external knowledge. Experiments were carried out by applying the proposed model
in Chinese bamboo flute music, and results show that our method can make
generated music more lively.
</summary>
    <author>
      <name>Yifan Xie</name>
    </author>
    <author>
      <name>Rongfeng Li</name>
    </author>
    <link href="http://arxiv.org/abs/2008.03436v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.03436v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.09960v1</id>
    <updated>2020-08-23T05:08:39Z</updated>
    <published>2020-08-23T05:08:39Z</published>
    <title>Translating Paintings Into Music Using Neural Networks</title>
    <summary>  We propose a system that learns from artistic pairings of music and
corresponding album cover art. The goal is to 'translate' paintings into music
and, in further stages of development, the converse. We aim to deploy this
system as an artistic tool for real time 'translations' between musicians and
painters. The system's outputs serve as elements to be employed in a joint live
performance of music and painting, or as generative material to be used by the
artists as inspiration for their improvisation.
</summary>
    <author>
      <name>Prateek Verma</name>
    </author>
    <author>
      <name>Constantin Basica</name>
    </author>
    <author>
      <name>Pamela Davis Kivelson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.09960v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.09960v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.11159v1</id>
    <updated>2020-08-25T16:46:56Z</updated>
    <published>2020-08-25T16:46:56Z</published>
    <title>Medley2K: A Dataset of Medley Transitions</title>
    <summary>  The automatic generation of medleys, i.e., musical pieces formed by different
songs concatenated via smooth transitions, is not well studied in the current
literature. To facilitate research on this topic, we make available a dataset
called Medley2K that consists of 2,000 medleys and 7,712 labeled transitions.
Our dataset features a rich variety of song transitions across different music
genres. We provide a detailed description of this dataset and validate it by
training a state-of-the-art generative model in the task of generating
transitions between songs.
</summary>
    <author>
      <name>Lukas Faber</name>
    </author>
    <author>
      <name>Sandro Luck</name>
    </author>
    <author>
      <name>Damian Pascual</name>
    </author>
    <author>
      <name>Andreas Roth</name>
    </author>
    <author>
      <name>Gino Brunner</name>
    </author>
    <author>
      <name>Roger Wattenhofer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MML 2020 - 13th Int. Workshop on Machine Learning and Music at
  ECML-PKDD 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.11159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.11706v1</id>
    <updated>2020-09-24T14:04:51Z</updated>
    <published>2020-09-24T14:04:51Z</published>
    <title>Timbre Space Representation of a Subtractive Synthesizer</title>
    <summary>  In this study, we produce a geometrically scaled perceptual timbre space from
dissimilarity ratings of subtractive synthesized sounds and correlate the
resulting dimensions with a set of acoustic descriptors. We curate a set of 15
sounds, produced by a synthesis model that uses varying source waveforms,
frequency modulation (FM) and a lowpass filter with an enveloped cutoff
frequency. Pairwise dissimilarity ratings were collected within an online
browser-based experiment. We hypothesized that a varied waveform input source
and enveloped filter would act as the main vehicles for timbral variation,
providing novel acoustic correlates for the perception of synthesized timbres.
</summary>
    <author>
      <name>Cyrus Vahidi</name>
    </author>
    <author>
      <name>George Fazekas</name>
    </author>
    <author>
      <name>Charalampos Saitis</name>
    </author>
    <author>
      <name>Alessandro Palladini</name>
    </author>
    <link href="http://arxiv.org/abs/2009.11706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.13931v2</id>
    <updated>2020-11-06T03:33:36Z</updated>
    <published>2020-09-29T11:26:25Z</published>
    <title>Residual acoustic echo suppression based on efficient multi-task
  convolutional neural network</title>
    <summary>  Acoustic echo degrades the user experience in voice communication systems
thus needs to be suppressed completely. We propose a real-time residual
acoustic echo suppression (RAES) method using an efficient convolutional neural
network. The double talk detector is used as an auxiliary task to improve the
performance of RAES in the context of multi-task learning. The training
criterion is based on a novel loss function, which we call as the suppression
loss, to balance the suppression of residual echo and the distortion of
near-end signals. The experimental results show that the proposed method can
efficiently suppress the residual echo under different circumstances.
</summary>
    <author>
      <name>Xinquan Zhou</name>
    </author>
    <author>
      <name>Yanhong Leng</name>
    </author>
    <link href="http://arxiv.org/abs/2009.13931v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.13931v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.14374v1</id>
    <updated>2020-09-30T01:22:37Z</updated>
    <published>2020-09-30T01:22:37Z</published>
    <title>Rethinking Evaluation Methodology for Audio-to-Score Alignment</title>
    <summary>  This paper offers a precise, formal definition of an audio-to-score
alignment. While the concept of an alignment is intuitively grasped, this
precision affords us new insight into the evaluation of audio-to-score
alignment algorithms. Motivated by these insights, we introduce new evaluation
metrics for audio-to-score alignment. Using an alignment evaluation dataset
derived from pairs of KernScores and MAESTRO performances, we study the
behavior of our new metrics and the standard metrics on several classical
alignment algorithms.
</summary>
    <author>
      <name>John Thickstun</name>
    </author>
    <author>
      <name>Jennifer Brennan</name>
    </author>
    <author>
      <name>Harsh Verma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.14374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.14374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.00823v2</id>
    <updated>2020-10-26T14:03:26Z</updated>
    <published>2020-10-02T07:40:44Z</published>
    <title>Deep Composer Classification Using Symbolic Representation</title>
    <summary>  In this study, we train deep neural networks to classify composer on a
symbolic domain. The model takes a two-channel two-dimensional input, i.e.,
onset and note activations of time-pitch representation, which is converted
from MIDI recordings and performs a single-label classification. On the
experiments conducted on MAESTRO dataset, we report an F1 value of 0.8333 for
the classification of 13~classical composers.
</summary>
    <author>
      <name>Sunghyeon Kim</name>
    </author>
    <author>
      <name>Hyeyoon Lee</name>
    </author>
    <author>
      <name>Sunjong Park</name>
    </author>
    <author>
      <name>Jinho Lee</name>
    </author>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <link href="http://arxiv.org/abs/2010.00823v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.00823v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01572v1</id>
    <updated>2020-10-04T12:56:41Z</updated>
    <published>2020-10-04T12:56:41Z</published>
    <title>Resonant Processing of Instrumental Sound Controlled by Spatial Position</title>
    <summary>  We present an acoustic musical instrument played through a resonance model of
another sound. The resonance model is controlled in real time as part of the
composite instrument. Our implementation uses an electric violin, whose spatial
position modifies filter parameters of the resonance model. Simplicial
interpolation defines the mapping from spatial position to filter parameters.
With some effort, pitch tracking can also control the filter parameters. The
individual technologies -- motion tracking, pitch tracking, resonance models --
are easily adapted to other instruments.
</summary>
    <author>
      <name>Camille Goudeseune</name>
    </author>
    <author>
      <name>Guy Garnett</name>
    </author>
    <author>
      <name>Timothy Johnson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1176362</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1176362" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01572v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01572v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.04546v1</id>
    <updated>2020-10-08T07:50:15Z</updated>
    <published>2020-10-08T07:50:15Z</published>
    <title>Dataset Augmentation and Dimensionality Reduction of Pinna-Related
  Transfer Functions</title>
    <summary>  Efficient modeling of the inter-individual variations of head-related
transfer functions (HRTFs) is a key matterto the individualization of binaural
synthesis. In previous work, we augmented a dataset of 119 pairs of earshapes
and pinna-related transfer functions (PRTFs), thus creating a wide dataset of
1005 ear shapes and PRTFsgenerated by random ear drawings (WiDESPREaD) and
acoustical simulations. In this article, we investigate thedimensionality
reduction capacity of two principal component analysis (PCA) models of
magnitude PRTFs, trainedon WiDESPREaD and on the original dataset,
respectively. We find that the model trained on the WiDESPREaDdataset performs
best, regardless of the number of retained principal components.
</summary>
    <author>
      <name>Corentin Guezenoc</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IETR</arxiv:affiliation>
    </author>
    <author>
      <name>Renaud Seguier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IETR</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.17743/aesconv.2020.978-1-942220-32-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.17743/aesconv.2020.978-1-942220-32-9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Audio Engineering Society Convention, May 2020, Vienna, Austria</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.04546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.04546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.07518v1</id>
    <updated>2020-10-15T04:52:02Z</updated>
    <published>2020-10-15T04:52:02Z</published>
    <title>Automatic Analysis and Influence of Hierarchical Structure on Melody,
  Rhythm and Harmony in Popular Music</title>
    <summary>  Repetition is a basic indicator of musical structure. This study introduces
new algorithms for identifying musical phrases based on repetition. Phrases
combine to form sections yielding a two-level hierarchical structure.
Automatically detected hierarchical repetition structures reveal significant
interactions between structure and chord progressions, melody and rhythm.
Different levels of hierarchy interact differently, providing evidence that
structural hierarchy plays an important role in music beyond simple notions of
repetition or similarity. Our work suggests new applications for music
generation and music evaluation.
</summary>
    <author>
      <name>Shuqi Dai</name>
    </author>
    <author>
      <name>Huan Zhang</name>
    </author>
    <author>
      <name>Roger B. Dannenberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 2020 Joint Conference on AI Music Creativity
  (CSMC-MuMe 2020), Stockholm, Sweden, October 21-24, 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.07518v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.07518v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.07562v2</id>
    <updated>2020-10-16T03:50:27Z</updated>
    <published>2020-10-15T07:21:58Z</published>
    <title>Melody Classification based on Performance Event Vector and BRNN</title>
    <summary>  We proposed a model for the Conference of Music and Technology (CSMT2020)
data challenge of melody classification. Our model used the Performance Event
Vector as the input sequence to build a Bidirectional RNN network for
classfication. The model achieved a satisfying performance on the development
dataset and Wikifonia dataset. We also discussed the effect of several
hyper-parameters, and created multiple prediction outputs for the evaluation
dataset.
</summary>
    <author>
      <name>Jinyue Guo</name>
    </author>
    <author>
      <name>Aozhi Liu</name>
    </author>
    <author>
      <name>Jing Xiao</name>
    </author>
    <link href="http://arxiv.org/abs/2010.07562v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.07562v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.07739v1</id>
    <updated>2020-10-15T13:30:40Z</updated>
    <published>2020-10-15T13:30:40Z</published>
    <title>Music Classification in MIDI Format based on LSTM Mdel</title>
    <summary>  Music classification between music made by AI or human composers can be done
by deep learning networks. We first transformed music samples in midi format to
natural language sequences, then classified these samples by mLSTM
(multiplicative Long Short Term Memory) + logistic regression. The accuracy of
the result evaluated by 10-fold cross validation can reach 90%. Our work
indicates that music generated by AI and human composers do have different
characteristics, which can be learned by deep learning networks.
</summary>
    <author>
      <name>Yiting Xia</name>
    </author>
    <author>
      <name>Yiwei Jiang</name>
    </author>
    <author>
      <name>Tao Ye</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Chinese</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.07739v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.07739v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.08123v2</id>
    <updated>2020-11-24T20:37:52Z</updated>
    <published>2020-10-16T03:01:14Z</published>
    <title>Melody Classifier with Stacked-LSTM</title>
    <summary>  Attempts to use generative models for music generation have been common in
recent years, and some of them have achieved good results. Pieces generated by
some of these models are almost indistinguishable from those being composed by
human composers. However, the research on the evaluation system for
machine-generated music is still at a relatively early stage, and there is no
uniform standard for such tasks. This paper proposes a stacked-LSTM binary
classifier based on a language model, which can be used to distinguish the
human composer's work from the machine-generated melody by learning the MIDI
file's pitch, position, and duration.
</summary>
    <author>
      <name>You Li</name>
    </author>
    <author>
      <name>Zhuowen Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2010.08123v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.08123v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.09489v1</id>
    <updated>2020-10-16T06:42:40Z</updated>
    <published>2020-10-16T06:42:40Z</published>
    <title>Hit Song Prediction Based on Early Adopter Data and Audio Features</title>
    <summary>  Billions of USD are invested in new artists and songs by the music industry
every year. This research provides a new strategy for assessing the hit
potential of songs, which can help record companies support their investment
decisions. A number of models were developed that use both audio data, and a
novel feature based on social media listening behaviour. The results show that
models based on early adopter behaviour perform well when predicting top 20
dance hits.
</summary>
    <author>
      <name>Dorien Herremans</name>
    </author>
    <author>
      <name>Tom Bergmans</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 18th International Society for Music Information Retrieval
  Conference (ISMIR)2018 - LBD</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.09489v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09489v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.09930v1</id>
    <updated>2020-10-19T23:55:48Z</updated>
    <published>2020-10-19T23:55:48Z</published>
    <title>BIRD: Big Impulse Response Dataset</title>
    <summary>  This paper introduces BIRD, the Big Impulse Response Dataset. This open
dataset consists of 100,000 multichannel room impulse responses (RIRs)
generated from simulations using the Image Method, making it the largest
multichannel open dataset currently available. These RIRs can be used toperform
efficient online data augmentation for scenarios that involve two microphones
and multiple sound sources. The paper also introduces use cases to illustrate
how BIRD can perform data augmentation with existing speech corpora.
</summary>
    <author>
      <name>François Grondin</name>
    </author>
    <author>
      <name>Jean-Samuel Lauzon</name>
    </author>
    <author>
      <name>Simon Michaud</name>
    </author>
    <author>
      <name>Mirco Ravanelli</name>
    </author>
    <author>
      <name>François Michaud</name>
    </author>
    <link href="http://arxiv.org/abs/2010.09930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.10145v1</id>
    <updated>2020-10-20T09:25:40Z</updated>
    <published>2020-10-20T09:25:40Z</published>
    <title>Tongji University Undergraduate Team for the VoxCeleb Speaker
  Recognition Challenge2020</title>
    <summary>  In this report, we discribe the submission of Tongji University undergraduate
team to the CLOSE track of the VoxCeleb Speaker Recognition Challenge (VoxSRC)
2020 at Interspeech 2020. We applied the RSBU-CW module to the ResNet34
framework to improve the denoising ability of the network and better complete
the speaker verification task in a complex environment.We trained two variants
of ResNet,used score fusion and data-augmentation methods to improve the
performance of the model. Our fusion of two selected systems for the CLOSE
track achieves 0.2973 DCF and 4.9700\% EER on the challenge evaluation set.
</summary>
    <author>
      <name>Shufan Shen</name>
    </author>
    <author>
      <name>Ran Miao</name>
    </author>
    <author>
      <name>Yi Wang</name>
    </author>
    <author>
      <name>Zhihua Wei</name>
    </author>
    <link href="http://arxiv.org/abs/2010.10145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.10145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.10556v1</id>
    <updated>2020-10-20T18:15:45Z</updated>
    <published>2020-10-20T18:15:45Z</published>
    <title>Speaker Separation Using Speaker Inventories and Estimated Speech</title>
    <summary>  We propose speaker separation using speaker inventories and estimated speech
(SSUSIES), a framework leveraging speaker profiles and estimated speech for
speaker separation. SSUSIES contains two methods, speaker separation using
speaker inventories (SSUSI) and speaker separation using estimated speech
(SSUES). SSUSI performs speaker separation with the help of speaker inventory.
By combining the advantages of permutation invariant training (PIT) and speech
extraction, SSUSI significantly outperforms conventional approaches. SSUES is a
widely applicable technique that can substantially improve speaker separation
performance using the output of first-pass separation. We evaluate the models
on both speaker separation and speech recognition metrics.
</summary>
    <author>
      <name>Peidong Wang</name>
    </author>
    <author>
      <name>Zhuo Chen</name>
    </author>
    <author>
      <name>DeLiang Wang</name>
    </author>
    <author>
      <name>Jinyu Li</name>
    </author>
    <author>
      <name>Yifan Gong</name>
    </author>
    <link href="http://arxiv.org/abs/2010.10556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.10556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.12497v1</id>
    <updated>2020-10-23T16:01:28Z</updated>
    <published>2020-10-23T16:01:28Z</published>
    <title>EML System Description for VoxCeleb Speaker Diarization Challenge 2020</title>
    <summary>  This technical report describes the EML submission to the first VoxCeleb
speaker diarization challenge. Although the aim of the challenge has been the
offline processing of the signals, the submitted system is basically the EML
online algorithm which decides about the speaker labels in runtime
approximately every 1.2 sec. For the first phase of the challenge, only
VoxCeleb2 dev dataset was used for training. The results on the provided
VoxConverse dev set show much better accuracy in terms of both DER and JER
compared to the offline baseline provided in the challenge. The real-time
factor of the whole diarization process is about 0.01 using a single CPU
machine.
</summary>
    <author>
      <name>Omid Ghahabi</name>
    </author>
    <author>
      <name>Volker Fischer</name>
    </author>
    <link href="http://arxiv.org/abs/2010.12497v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.12497v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.14565v1</id>
    <updated>2020-10-27T19:12:08Z</updated>
    <published>2020-10-27T19:12:08Z</published>
    <title>Remixing Music with Visual Conditioning</title>
    <summary>  We propose a visually conditioned music remixing system by incorporating deep
visual and audio models. The method is based on a state of the art audio-visual
source separation model which performs music instrument source separation with
video information. We modified the model to work with user-selected images
instead of videos as visual input during inference to enable separation of
audio-only content. Furthermore, we propose a remixing engine that generalizes
the task of source separation into music remixing. The proposed method is able
to achieve improved audio quality compared to remixing performed by the
separate-and-add method with a state-of-the-art audio-visual source separation
model.
</summary>
    <author>
      <name>Li-Chia Yang</name>
    </author>
    <author>
      <name>Alexander Lerch</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2020 IEEE International Symposium on Multimedia</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.14565v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.14565v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.15772v1</id>
    <updated>2020-10-29T17:16:22Z</updated>
    <published>2020-10-29T17:16:22Z</published>
    <title>GANs &amp; Reels: Creating Irish Music using a Generative Adversarial
  Network</title>
    <summary>  In this paper we present a method for algorithmic melody generation using a
generative adversarial network without recurrent components. Music generation
has been successfully done using recurrent neural networks, where the model
learns sequence information that can help create authentic sounding melodies.
Here, we use DC-GAN architecture with dilated convolutions and towers to
capture sequential information as spatial image information, and learn
long-range dependencies in fixed-length melody forms such as Irish traditional
reel.
</summary>
    <author>
      <name>Antonina Kolokolova</name>
    </author>
    <author>
      <name>Mitchell Billard</name>
    </author>
    <author>
      <name>Robert Bishop</name>
    </author>
    <author>
      <name>Moustafa Elsisy</name>
    </author>
    <author>
      <name>Zachary Northcott</name>
    </author>
    <author>
      <name>Laura Graves</name>
    </author>
    <author>
      <name>Vineel Nagisetty</name>
    </author>
    <author>
      <name>Heather Patey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, (+ 2 pages of references)</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.15772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.15772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.00200v1</id>
    <updated>2020-10-31T06:36:26Z</updated>
    <published>2020-10-31T06:36:26Z</published>
    <title>The xx205 System for the VoxCeleb Speaker Recognition Challenge 2020</title>
    <summary>  This report describes the systems submitted to the first and second tracks of
the VoxCeleb Speaker Recognition Challenge (VoxSRC) 2020, which ranked second
in both tracks. Three key points of the system pipeline are explored: (1)
investigating multiple CNN architectures including ResNet, Res2Net and dual
path network (DPN) to extract the x-vectors, (2) using a composite angular
margin softmax loss to train the speaker models, and (3) applying score
normalization and system fusion to boost the performance. Measured on the
VoxSRC-20 Eval set, the best submitted systems achieve an EER of $3.808\%$ and
a MinDCF of $0.1958$ in the close-condition track 1, and an EER of $3.798\%$
and a MinDCF of $0.1942$ in the open-condition track 2, respectively.
</summary>
    <author>
      <name>Xu Xiang</name>
    </author>
    <link href="http://arxiv.org/abs/2011.00200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.00200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.01518v1</id>
    <updated>2020-11-03T07:26:21Z</updated>
    <published>2020-11-03T07:26:21Z</published>
    <title>ShaneRun System Description to VoxCeleb Speaker Recognition Challenge
  2020</title>
    <summary>  In this report, we describe the submission of ShaneRun's team to the VoxCeleb
Speaker Recognition Challenge (VoxSRC) 2020. We use ResNet-34 as encoder to
extract the speaker embeddings, which is referenced from the open-source
voxceleb-trainer. We also provide a simple method to implement optimum fusion
using t-SNE normalized distance of testing utterance pairs instead of original
negative Euclidean distance from the encoder. The final submitted system got
0.3098 minDCF and 5.076 % ERR for Fixed data track, which outperformed the
baseline by 1.3 % minDCF and 2.2 % ERR respectively.
</summary>
    <author>
      <name>Shen Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2011.01518v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.01518v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.02882v1</id>
    <updated>2020-11-04T05:24:18Z</updated>
    <published>2020-11-04T05:24:18Z</published>
    <title>Query Expansion System for the VoxCeleb Speaker Recognition Challenge
  2020</title>
    <summary>  In this report, we describe our submission to the VoxCeleb Speaker
Recognition Challenge (VoxSRC) 2020. Two approaches are adopted. One is to
apply query expansion on speaker verification, which shows significant progress
compared to baseline in the study. Another is to use Kaldi extract x-vector and
to combine its Probabilistic Linear Discriminant Analysis (PLDA) score with
ResNet score.
</summary>
    <author>
      <name>Yu-Sen Cheng</name>
    </author>
    <author>
      <name>Chun-Liang Shih</name>
    </author>
    <author>
      <name>Tien-Hong Lo</name>
    </author>
    <author>
      <name>Wen-Ting Tseng</name>
    </author>
    <author>
      <name>Berlin Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2011.02882v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.02882v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.04568v1</id>
    <updated>2020-11-09T17:13:21Z</updated>
    <published>2020-11-09T17:13:21Z</published>
    <title>Musical analysis of Stravinski's "The Rite of Spring" based on
  computational methods</title>
    <summary>  Stravinski's "The Rite of Spring" is one of the most well-known pieces from
the classical contemporary music repertoire. However, its analysis has aroused
different opinions within its construction and compositional foundations. In
this sense, I here proposed my own manual analysis and a computational approach
which aims to find a similar analysis, giving the opportunity of discovering
new possible points of view and supplying the current deficiencies of the
Musi"c Computing common analysis systems.
</summary>
    <author>
      <name>Germán Ruiz-Marcos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Audio and Music Processing Lab, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.04568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.04568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.08548v1</id>
    <updated>2020-11-17T10:27:39Z</updated>
    <published>2020-11-17T10:27:39Z</published>
    <title>Optimizing voice conversion network with cycle consistency loss of
  speaker identity</title>
    <summary>  We propose a novel training scheme to optimize voice conversion network with
a speaker identity loss function. The training scheme not only minimizes
frame-level spectral loss, but also speaker identity loss. We introduce a cycle
consistency loss that constrains the converted speech to maintain the same
speaker identity as reference speech at utterance level. While the proposed
training scheme is applicable to any voice conversion networks, we formulate
the study under the average model voice conversion framework in this paper.
Experiments conducted on CMU-ARCTIC and CSTR-VCTK corpus confirm that the
proposed method outperforms baseline methods in terms of speaker similarity.
</summary>
    <author>
      <name>Hongqiang Du</name>
    </author>
    <author>
      <name>Xiaohai Tian</name>
    </author>
    <author>
      <name>Lei Xie</name>
    </author>
    <author>
      <name>Haizhou Li</name>
    </author>
    <link href="http://arxiv.org/abs/2011.08548v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.08548v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.11970v1</id>
    <updated>2020-11-24T09:02:35Z</updated>
    <published>2020-11-24T09:02:35Z</published>
    <title>A Novel Multimodal Music Genre Classifier using Hierarchical Attention
  and Convolutional Neural Network</title>
    <summary>  Music genre classification is one of the trending topics in regards to the
current Music Information Retrieval (MIR) Research. Since, the dependency of
genre is not only limited to the audio profile, we also make use of textual
content provided as lyrics of the corresponding song. We implemented a CNN
based feature extractor for spectrograms in order to incorporate the acoustic
features and a Hierarchical Attention Network based feature extractor for
lyrics. We then go on to classify the music track based upon the resulting
fused feature vector.
</summary>
    <author>
      <name>Manish Agrawal</name>
    </author>
    <author>
      <name>Abhilash Nandy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.11970v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.11970v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.12596v1</id>
    <updated>2020-11-25T09:13:53Z</updated>
    <published>2020-11-25T09:13:53Z</published>
    <title>MTCRNN: A multi-scale RNN for directed audio texture synthesis</title>
    <summary>  Audio textures are a subset of environmental sounds, often defined as having
stable statistical characteristics within an adequately large window of time
but may be unstructured locally. They include common everyday sounds such as
from rain, wind, and engines. Given that these complex sounds contain patterns
on multiple timescales, they are a challenge to model with traditional methods.
We introduce a novel modelling approach for textures, combining recurrent
neural networks trained at different levels of abstraction with a conditioning
strategy that allows for user-directed synthesis. We demonstrate the model's
performance on a variety of datasets, examine its performance on various
metrics, and discuss some potential applications.
</summary>
    <author>
      <name>M. Huzaifah</name>
    </author>
    <author>
      <name>L. Wyse</name>
    </author>
    <link href="http://arxiv.org/abs/2011.12596v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.12596v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.12818v1</id>
    <updated>2020-11-25T15:21:39Z</updated>
    <published>2020-11-25T15:21:39Z</published>
    <title>Phase retrieval with Bregman divergences: Application to audio signal
  recovery</title>
    <summary>  Phase retrieval aims to recover a signal from magnitude or power spectra
measurements. It is often addressed by considering a minimization problem
involving a quadratic cost function. We propose a different formulation based
on Bregman divergences, which encompass divergences that are appropriate for
audio signal processing applications. We derive a fast gradient algorithm to
solve this problem.
</summary>
    <author>
      <name>Pierre-Hugo Vial</name>
    </author>
    <author>
      <name>Paul Magron</name>
    </author>
    <author>
      <name>Thomas Oberlin</name>
    </author>
    <author>
      <name>Cédric Févotte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Proceedings of iTWIST'20, Paper-ID: 16, Nantes, France, December,
  2-4, 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.12818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.12818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.13453v1</id>
    <updated>2020-11-26T19:33:08Z</updated>
    <published>2020-11-26T19:33:08Z</published>
    <title>Towards Movement Generation with Audio Features</title>
    <summary>  Sound and movement are closely coupled, particularly in dance. Certain audio
features have been found to affect the way we move to music. Is this
relationship between sound and movement something which can be modelled using
machine learning? This work presents initial experiments wherein high-level
audio features calculated from a set of music pieces are included in a movement
generation model trained on motion capture recordings of improvised dance. Our
results indicate that the model learns to generate realistic dance movements
which vary depending on the audio features.
</summary>
    <author>
      <name>Benedikte Wallace</name>
    </author>
    <author>
      <name>Charles P. Martin</name>
    </author>
    <author>
      <name>Jim Torresen</name>
    </author>
    <author>
      <name>Kristian Nymoen</name>
    </author>
    <link href="http://arxiv.org/abs/2011.13453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.13453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.00313v2</id>
    <updated>2021-11-17T17:53:40Z</updated>
    <published>2021-01-30T21:21:46Z</published>
    <title>Cortical Features for Defense Against Adversarial Audio Attacks</title>
    <summary>  We propose using a computational model of the auditory cortex as a defense
against adversarial attacks on audio. We apply several white-box iterative
optimization-based adversarial attacks to an implementation of Amazon Alexa's
HW network, and a modified version of this network with an integrated cortical
representation, and show that the cortical features help defend against
universal adversarial examples. At the same level of distortion, the
adversarial noises found for the cortical network are always less effective for
universal audio attacks. We make our code publicly available at
https://github.com/ilyakava/py3fst.
</summary>
    <author>
      <name>Ilya Kavalerov</name>
    </author>
    <author>
      <name>Ruijie Zheng</name>
    </author>
    <author>
      <name>Wojciech Czaja</name>
    </author>
    <author>
      <name>Rama Chellappa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Co-author legal name changed</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.00313v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.00313v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.07259v1</id>
    <updated>2021-02-14T22:28:55Z</updated>
    <published>2021-02-14T22:28:55Z</published>
    <title>Thank you for Attention: A survey on Attention-based Artificial Neural
  Networks for Automatic Speech Recognition</title>
    <summary>  Attention is a very popular and effective mechanism in artificial neural
network-based sequence-to-sequence models. In this survey paper, a
comprehensive review of the different attention models used in developing
automatic speech recognition systems is provided. The paper focuses on the
development and evolution of attention models for offline and streaming speech
recognition within recurrent neural network- and Transformer- based
architectures.
</summary>
    <author>
      <name>Priyabrata Karmakar</name>
    </author>
    <author>
      <name>Shyh Wei Teng</name>
    </author>
    <author>
      <name>Guojun Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE/ACM Trans. on Audio, Speech, and Language
  Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.07259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.07259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.10322v1</id>
    <updated>2021-02-20T12:16:35Z</updated>
    <published>2021-02-20T12:16:35Z</published>
    <title>Learnable MFCCs for Speaker Verification</title>
    <summary>  We propose a learnable mel-frequency cepstral coefficient (MFCC) frontend
architecture for deep neural network (DNN) based automatic speaker
verification. Our architecture retains the simplicity and interpretability of
MFCC-based features while allowing the model to be adapted to data flexibly. In
practice, we formulate data-driven versions of the four linear transforms of a
standard MFCC extractor -- windowing, discrete Fourier transform (DFT), mel
filterbank and discrete cosine transform (DCT). Results reported reach up to
6.7\% (VoxCeleb1) and 9.7\% (SITW) relative improvement in term of equal error
rate (EER) from static MFCCs, without additional tuning effort.
</summary>
    <author>
      <name>Xuechen Liu</name>
    </author>
    <author>
      <name>Md Sahidullah</name>
    </author>
    <author>
      <name>Tomi Kinnunen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ISCAS 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.10322v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.10322v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.03443v1</id>
    <updated>2021-07-07T19:08:16Z</updated>
    <published>2021-07-07T19:08:16Z</published>
    <title>BumbleBee: A Transformer for Music</title>
    <summary>  We will introduce BumbleBee, a transformer model that will generate MIDI
music data . We will tackle the issue of transformers applied to long sequences
by implementing a longformer generative model that uses dilating sliding
windows to compute the attention layers. We will compare our results to that of
the music transformer and Long-Short term memory (LSTM) to benchmark our
results. This analysis will be performed using piano MIDI files, in particular
, the JSB Chorales dataset that has already been used for other research works
(Huang et al., 2018)
</summary>
    <author>
      <name>Lucas Fenaux</name>
    </author>
    <author>
      <name>Maria Juliana Quintero</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.03443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.03443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T07" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.05546v1</id>
    <updated>2021-07-08T08:18:57Z</updated>
    <published>2021-07-08T08:18:57Z</published>
    <title>Calliope -- A Polyphonic Music Transformer</title>
    <summary>  The polyphonic nature of music makes the application of deep learning to
music modelling a challenging task. On the other hand, the Transformer
architecture seems to be a good fit for this kind of data. In this work, we
present Calliope, a novel autoencoder model based on Transformers for the
efficient modelling of multi-track sequences of polyphonic music. The
experiments show that our model is able to improve the state of the art on
musical sequence reconstruction and generation, with remarkably good results
especially on long sequences.
</summary>
    <author>
      <name>Andrea Valenti</name>
    </author>
    <author>
      <name>Stefano Berti</name>
    </author>
    <author>
      <name>Davide Bacciu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ESANN2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.05546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.05546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.07728v1</id>
    <updated>2021-07-16T06:54:38Z</updated>
    <published>2021-07-16T06:54:38Z</published>
    <title>Recognizing bird species in diverse soundscapes under weak supervision</title>
    <summary>  We present a robust classification approach for avian vocalization in complex
and diverse soundscapes, achieving second place in the BirdCLEF2021 challenge.
We illustrate how to make full use of pre-trained convolutional neural
networks, by using an efficient modeling and training routine supplemented by
novel augmentation methods. Thereby, we improve the generalization of weakly
labeled crowd-sourced data to productive data collected by autonomous recording
units. As such, we illustrate how to progress towards an accurate automated
assessment of avian population which would enable global biodiversity
monitoring at scale, impossible by manual annotation.
</summary>
    <author>
      <name>Christof Henkel</name>
    </author>
    <author>
      <name>Pascal Pfeiffer</name>
    </author>
    <author>
      <name>Philipp Singer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">All authors contributed equally, 8 pages, 4 figures, submitted to
  CEUR-WS</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.07728v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.07728v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.09311v1</id>
    <updated>2021-07-20T08:03:04Z</updated>
    <published>2021-07-20T08:03:04Z</published>
    <title>PERSA+: A Deep Learning Front-End for Context-Agnostic Audio
  Classification</title>
    <summary>  Deep learning has been applied to diverse audio semantics tasks, enabling the
construction of models that learn hierarchical levels of features from
high-dimensional raw data, delivering state-of-the-art performance. But do
these algorithms perform similarly in real-world conditions, or just at the
benchmark, where their high learning capability assures the complete
memorization of the employed datasets? This work presents a deep learning
front-end, aiming at discarding detrimental information before entering the
modeling stage, bringing the learning process closer to the point, anticipating
the development of robust and context-agnostic classification algorithms.
</summary>
    <author>
      <name>Lazaros Vrysis</name>
    </author>
    <author>
      <name>Iordanis Thoidis</name>
    </author>
    <author>
      <name>Charalampos Dimoulas</name>
    </author>
    <author>
      <name>George Papanikolaou</name>
    </author>
    <link href="http://arxiv.org/abs/2107.09311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.09311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.13144v1</id>
    <updated>2022-01-31T11:40:17Z</updated>
    <published>2022-01-31T11:40:17Z</published>
    <title>partitura: A Python Package for Handling Symbolic Musical Data</title>
    <summary>  This demo paper introduces partitura, a Python package for handling symbolic
musical information. The principal aim of this package is to handle richly
structured musical information as conveyed by modern staff music notation. It
provides a much wider range of possibilities to deal with music than the more
reductive (but very common) piano roll-oriented approach inspired by the MIDI
standard. The package is an open source project and is available on GitHub.
</summary>
    <author>
      <name>Maarten Grachten</name>
    </author>
    <author>
      <name>Carlos Cancino-Chacón</name>
    </author>
    <author>
      <name>Thassilo Gadermaier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This preprint is a slightly updated and reformatted version of the
  work presented at the Late Breaking/Demo Session of the 20th International
  Society for Music Information Retrieval Conference (ISMIR 2019), Delft, The
  Netherlands</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.13144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.13144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.02112v1</id>
    <updated>2022-02-04T12:51:16Z</updated>
    <published>2022-02-04T12:51:16Z</published>
    <title>Musical Audio Similarity with Self-supervised Convolutional Neural
  Networks</title>
    <summary>  We have built a music similarity search engine that lets video producers
search by listenable music excerpts, as a complement to traditional full-text
search. Our system suggests similar sounding track segments in a large music
catalog by training a self-supervised convolutional neural network with triplet
loss terms and musical transformations. Semi-structured user interviews
demonstrate that we can successfully impress professional video producers with
the quality of the search experience, and perceived similarities to query
tracks averaged 7.8/10 in user testing. We believe this search tool will make
for a more natural search experience that is easier to find music to soundtrack
videos with.
</summary>
    <author>
      <name>Carl Thomé</name>
    </author>
    <author>
      <name>Sebastian Piwell</name>
    </author>
    <author>
      <name>Oscar Utterbäck</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISMIR LBD 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.02112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.02112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.03514v1</id>
    <updated>2022-02-07T20:57:40Z</updated>
    <published>2022-02-07T20:57:40Z</published>
    <title>Maximizing Audio Event Detection Model Performance on Small Datasets
  Through Knowledge Transfer, Data Augmentation, And Pretraining: An Ablation
  Study</title>
    <summary>  An Xception model reaches state-of-the-art (SOTA) accuracy on the ESC-50
dataset for audio event detection through knowledge transfer from ImageNet
weights, pretraining on AudioSet, and an on-the-fly data augmentation pipeline.
This paper presents an ablation study that analyzes which components contribute
to the boost in performance and training time. A smaller Xception model is also
presented which nears SOTA performance with almost a third of the parameters.
</summary>
    <author>
      <name>Daniel Tompkins</name>
    </author>
    <author>
      <name>Kshitiz Kumar</name>
    </author>
    <author>
      <name>Jian Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2202.03514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.03514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.04393v2</id>
    <updated>2022-09-14T10:56:20Z</updated>
    <published>2022-02-09T11:01:37Z</published>
    <title>Binaural Audio Rendering in the Spherical Harmonic Domain: A Summary of
  the Mathematics and its Pitfalls</title>
    <summary>  The present document reviews the mathematics behind binaural rendering of
sound fields that are available as spherical harmonic expansion coefficients.
This process is also known as binaural ambisonic decoding. We highlight that
the details entail some amount peculiarity so that one has to be well aware of
the precise definitions that are chosen for some of the involved quantities to
obtain a consistent formulation. We also discuss what sets of definitions
produce ambisonic signals that are compatible with the most common software
tools that are available.
</summary>
    <author>
      <name>Jens Ahrens</name>
    </author>
    <link href="http://arxiv.org/abs/2202.04393v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.04393v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.07273v2</id>
    <updated>2022-03-30T17:23:15Z</updated>
    <published>2022-02-15T09:33:30Z</published>
    <title>SpeechPainter: Text-conditioned Speech Inpainting</title>
    <summary>  We propose SpeechPainter, a model for filling in gaps of up to one second in
speech samples by leveraging an auxiliary textual input. We demonstrate that
the model performs speech inpainting with the appropriate content, while
maintaining speaker identity, prosody and recording environment conditions, and
generalizing to unseen speakers. Our approach significantly outperforms
baselines constructed using adaptive TTS, as judged by human raters in
side-by-side preference and MOS tests.
</summary>
    <author>
      <name>Zalán Borsos</name>
    </author>
    <author>
      <name>Matt Sharifi</name>
    </author>
    <author>
      <name>Marco Tagliasacchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Interspeech 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.07273v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.07273v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.07382v1</id>
    <updated>2022-02-15T13:20:14Z</updated>
    <published>2022-02-15T13:20:14Z</published>
    <title>Phase Vocoder Done Right</title>
    <summary>  The phase vocoder (PV) is a widely spread technique for processing audio
signals. It employs a short-time Fourier transform (STFT)
analysis-modify-synthesis loop and is typically used for time-scaling of
signals by means of using different time steps for STFT analysis and synthesis.
The main challenge of PV used for that purpose is the correction of the STFT
phase. In this paper, we introduce a novel method for phase correction based on
phase gradient estimation and its integration. The method does not require
explicit peak picking and tracking nor does it require detection of transients
and their separate treatment. Yet, the method does not suffer from the typical
phase vocoder artifacts even for extreme time stretching factors.
</summary>
    <author>
      <name>Zdenek Prusa</name>
    </author>
    <author>
      <name>Nicki Holighaus</name>
    </author>
    <link href="http://arxiv.org/abs/2202.07382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.07382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.07498v1</id>
    <updated>2022-02-15T15:11:54Z</updated>
    <published>2022-02-15T15:11:54Z</published>
    <title>Non-iterative Filter Bank Phase (Re)Construction</title>
    <summary>  Signal reconstruction from magnitude-only measurements presents a
long-standing problem in signal processing. In this contribution, we propose a
phase (re)construction method for filter banks with uniform decimation and
controlled frequency variation. The suggested procedure extends the recently
introduced phase-gradient heap integration and relies on a phase-magnitude
relationship for filter bank coefficients obtained from Gaussian filters.
Admissible filter banks are modeled as the discretization of certain
generalized translation-invariant systems, for which we derive the
phase-magnitude relationship explicitly. The implementation for discrete
signals is described and the performance of the algorithm is evaluated on a
range of real and synthetic signals.
</summary>
    <author>
      <name>Zdeněk Průša</name>
    </author>
    <author>
      <name>Nicki Holighaus</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.23919/EUSIPCO.2017.8081342</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.23919/EUSIPCO.2017.8081342" rel="related"/>
    <link href="http://arxiv.org/abs/2202.07498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.07498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.07968v1</id>
    <updated>2022-02-16T10:22:40Z</updated>
    <published>2022-02-16T10:22:40Z</published>
    <title>On loss functions and evaluation metrics for music source separation</title>
    <summary>  We investigate which loss functions provide better separations via
benchmarking an extensive set of those for music source separation. To that
end, we first survey the most representative audio source separation losses we
identified, to later consistently benchmark them in a controlled experimental
setup. We also explore using such losses as evaluation metrics, via
cross-correlating them with the results of a subjective test. Based on the
observation that the standard signal-to-distortion ratio metric can be
misleading in some scenarios, we study alternative evaluation metrics based on
the considered losses.
</summary>
    <author>
      <name>Enric Gusó</name>
    </author>
    <author>
      <name>Jordi Pons</name>
    </author>
    <author>
      <name>Santiago Pascual</name>
    </author>
    <author>
      <name>Joan Serrà</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICASSP 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2202.07968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.07968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.13865v1</id>
    <updated>2022-02-24T09:14:25Z</updated>
    <published>2022-02-24T09:14:25Z</published>
    <title>On the relevance of bandwidth extension for speaker identification</title>
    <summary>  In this paper we discuss the relevance of bandwidth extension for speaker
identification tasks. Mainly we want to study if it is possible to recognize
voices that have been bandwith extended. For this purpose, we created two
different databases (microphonic and ISDN) of speech signals that were
bandwidth extended from telephone bandwidth ([300, 3400] Hz) to full bandwidth
([100, 8000] Hz). We have evaluated different parameterizations, and we have
found that the MELCEPST parameterization can take advantage of the bandwidth
extension algorithms in several situations.
</summary>
    <author>
      <name>Marcos Faundez-Zanuy</name>
    </author>
    <author>
      <name>Mattias Nilsson</name>
    </author>
    <author>
      <name>W. Bastiaan Kleijn</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2002 11th European Signal Processing Conference, 2002, pp. 1-4</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2202.13865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.13865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.05580v1</id>
    <updated>2022-05-11T15:48:56Z</updated>
    <published>2022-05-11T15:48:56Z</published>
    <title>Scream Detection in Heavy Metal Music</title>
    <summary>  Harsh vocal effects such as screams or growls are far more common in heavy
metal vocals than the traditionally sung vocal. This paper explores the problem
of detection and classification of extreme vocal techniques in heavy metal
music, specifically the identification of different scream techniques. We
investigate the suitability of various feature representations, including
cepstral, spectral, and temporal features as input representations for
classification. The main contributions of this work are (i) a manually
annotated dataset comprised of over 280 minutes of heavy metal songs of various
genres with a statistical analysis of occurrences of different extreme vocal
techniques in heavy metal music, and (ii) a systematic study of different input
feature representations for the classification of heavy metal vocals
</summary>
    <author>
      <name>Vedant Kalbag</name>
    </author>
    <author>
      <name>Alexander Lerch</name>
    </author>
    <link href="http://arxiv.org/abs/2205.05580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.05580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.14649v1</id>
    <updated>2022-05-29T13:03:42Z</updated>
    <published>2022-05-29T13:03:42Z</published>
    <title>Speaker Identification using Speech Recognition</title>
    <summary>  The audio data is increasing day by day throughout the globe with the
increase of telephonic conversations, video conferences and voice messages.
This research provides a mechanism for identifying a speaker in an audio file,
based on the human voice biometric features like pitch, amplitude, frequency
etc. We proposed an unsupervised learning model where the model can learn
speech representation with limited dataset. Librispeech dataset was used in
this research and we were able to achieve word error rate of 1.8.
</summary>
    <author>
      <name>Syeda Rabia Arshad</name>
    </author>
    <author>
      <name>Syed Mujtaba Haider</name>
    </author>
    <author>
      <name>Abdul Basit Mughal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.14649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.14649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.04805v1</id>
    <updated>2022-06-08T06:58:54Z</updated>
    <published>2022-06-08T06:58:54Z</published>
    <title>Motif Mining and Unsupervised Representation Learning for BirdCLEF 2022</title>
    <summary>  We build a classification model for the BirdCLEF 2022 challenge using
unsupervised methods. We implement an unsupervised representation of the
training dataset using a triplet loss on spectrogram representation of audio
motifs. Our best model performs with a score of 0.48 on the public leaderboard.
</summary>
    <author>
      <name>Anthony Miyaguchi</name>
    </author>
    <author>
      <name>Jiangyue Yu</name>
    </author>
    <author>
      <name>Bryan Cheungvivatpant</name>
    </author>
    <author>
      <name>Dakota Dudley</name>
    </author>
    <author>
      <name>Aniketh Swain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to CEUR-WS under LifeCLEF for the BirdCLEF 2022 challenge
  as a working note</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CEUR-WS Vol-3180 (2022) 2159-2167</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2206.04805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.04805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.12469v2</id>
    <updated>2022-10-18T05:48:23Z</updated>
    <published>2022-06-24T18:57:41Z</published>
    <title>Burst2Vec: An Adversarial Multi-Task Approach for Predicting Emotion,
  Age, and Origin from Vocal Bursts</title>
    <summary>  We present Burst2Vec, our multi-task learning approach to predict emotion,
age, and origin (i.e., native country/language) from vocal bursts. Burst2Vec
utilises pre-trained speech representations to capture acoustic information
from raw waveforms and incorporates the concept of model debiasing via
adversarial training. Our models achieve a relative 30 % performance gain over
baselines using pre-extracted features and score the highest amongst all
participants in the ICML ExVo 2022 Multi-Task Challenge.
</summary>
    <author>
      <name>Atijit Anuchitanukul</name>
    </author>
    <author>
      <name>Lucia Specia</name>
    </author>
    <link href="http://arxiv.org/abs/2206.12469v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.12469v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.12662v1</id>
    <updated>2022-06-25T14:27:10Z</updated>
    <published>2022-06-25T14:27:10Z</published>
    <title>Synthesizing Personalized Non-speech Vocalization from Discrete Speech
  Representations</title>
    <summary>  We formulated non-speech vocalization (NSV) modeling as a text-to-speech task
and verified its viability. Specifically, we evaluated the phonetic
expressivity of HUBERT speech units on NSVs and verified our model's ability to
control over speaker timbre even though the training data is speaker few-shot.
In addition, we substantiated that the heterogeneity in recording conditions is
the major obstacle for NSV modeling. Finally, we discussed five improvements
over our method for future research. Audio samples of synthesized NSVs are
available on our demo page: https://resemble-ai.github.io/reLaugh.
</summary>
    <author>
      <name>Chin-Cheng Hsu</name>
    </author>
    <link href="http://arxiv.org/abs/2206.12662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.12662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.14659v1</id>
    <updated>2022-06-29T13:59:19Z</updated>
    <published>2022-06-29T13:59:19Z</published>
    <title>Language-Based Audio Retrieval with Converging Tied Layers and
  Contrastive Loss</title>
    <summary>  In this paper, we tackle the new Language-Based Audio Retrieval task proposed
in DCASE 2022. Firstly, we introduce a simple, scalable architecture which ties
both the audio and text encoder together. Secondly, we show that using this
architecture along with contrastive loss allows the model to significantly beat
the performance of the baseline model. Finally, in addition to having an
extremely low training memory requirement, we are able to use pretrained models
as it is without needing to finetune them. We test our methods and show that
using a combination of our methods beats the baseline scores significantly.
</summary>
    <author>
      <name>Andrew Koh</name>
    </author>
    <author>
      <name>Eng Siong Chng</name>
    </author>
    <link href="http://arxiv.org/abs/2206.14659v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.14659v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.15423v1</id>
    <updated>2022-06-30T17:13:01Z</updated>
    <published>2022-06-30T17:13:01Z</published>
    <title>Implicit Neural Spatial Filtering for Multichannel Source Separation in
  the Waveform Domain</title>
    <summary>  We present a single-stage casual waveform-to-waveform multichannel model that
can separate moving sound sources based on their broad spatial locations in a
dynamic acoustic scene. We divide the scene into two spatial regions
containing, respectively, the target and the interfering sound sources. The
model is trained end-to-end and performs spatial processing implicitly, without
any components based on traditional processing or use of hand-crafted spatial
features. We evaluate the proposed model on a real-world dataset and show that
the model matches the performance of an oracle beamformer followed by a
state-of-the-art single-channel enhancement network.
</summary>
    <author>
      <name>Dejan Markovic</name>
    </author>
    <author>
      <name>Alexandre Defossez</name>
    </author>
    <author>
      <name>Alexander Richard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Interspeech 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.15423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.15423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.05508v1</id>
    <updated>2022-07-12T13:04:37Z</updated>
    <published>2022-07-12T13:04:37Z</published>
    <title>EfficientLEAF: A Faster LEarnable Audio Frontend of Questionable Use</title>
    <summary>  In audio classification, differentiable auditory filterbanks with few
parameters cover the middle ground between hard-coded spectrograms and raw
audio. LEAF (arXiv:2101.08596), a Gabor-based filterbank combined with
Per-Channel Energy Normalization (PCEN), has shown promising results, but is
computationally expensive. With inhomogeneous convolution kernel sizes and
strides, and by replacing PCEN with better parallelizable operations, we can
reach similar results more efficiently. In experiments on six audio
classification tasks, our frontend matches the accuracy of LEAF at 3% of the
cost, but both fail to consistently outperform a fixed mel filterbank. The
quest for learnable audio frontends is not solved.
</summary>
    <author>
      <name>Jan Schlüter</name>
    </author>
    <author>
      <name>Gerald Gutenbrunner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at EUSIPCO 2022. Code at
  https://github.com/CPJKU/EfficientLEAF</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.05508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.05508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.12121v1</id>
    <updated>2022-07-20T10:00:49Z</updated>
    <published>2022-07-20T10:00:49Z</published>
    <title>Cross-Modal Contrastive Representation Learning for Audio-to-Image
  Generation</title>
    <summary>  Multiple modalities for certain information provide a variety of perspectives
on that information, which can improve the understanding of the information.
Thus, it may be crucial to generate data of different modality from the
existing data to enhance the understanding. In this paper, we investigate the
cross-modal audio-to-image generation problem and propose Cross-Modal
Contrastive Representation Learning (CMCRL) to extract useful features from
audios and use it in the generation phase. Experimental results show that CMCRL
enhances quality of images generated than previous research.
</summary>
    <author>
      <name>HaeChun Chung</name>
    </author>
    <author>
      <name>JooYong Shim</name>
    </author>
    <author>
      <name>Jong-Kook Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, Accepted to MUE 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.12121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.12121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.04877v1</id>
    <updated>2022-08-09T16:11:45Z</updated>
    <published>2022-08-09T16:11:45Z</published>
    <title>Pure Data and INScore: Animated notation for new music</title>
    <summary>  New music is made with computers, taking advantage of its graphics displays
rather than its audio algorithms. Pure Data can be used to compose them. This
essay will show a case study that uses Pure Data, in connection with INScore,
for making a new type of score that uses animated notation or dynamic
musicography for making music with performers. This sample was made by the
author of the text, and it will show a number of notation possibilities that
can be done using the combination of software. This will be accompanied by a
simple prediction of what a musician could perform with it.
</summary>
    <author>
      <name>Patricio F. Calatayud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.04877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.04877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.14734v1</id>
    <updated>2022-08-31T09:34:27Z</updated>
    <published>2022-08-31T09:34:27Z</published>
    <title>Open Challenges in Musical Metacreation</title>
    <summary>  Musical Metacreation tries to obtain creative behaviors from computers
algorithms composing music. In this paper I briefly analyze how this field
evolved from algorithmic composition to be focused on the search for
creativity, and I point out some issues in pursuing this goal. Finally, I argue
that hybridization of algorithms can be a useful direction for research.
</summary>
    <author>
      <name>Filippo Carnovalini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3342428.3342678</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3342428.3342678" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In EAI International Conference on Smart Objects and Technologies
  for Social Good (GoodTechs '19), September 25-27, 2019, Valencia, Spain. ACM,
  New York, NY, USA, 2 pages</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2208.14734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.14734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.02696v1</id>
    <updated>2022-09-05T04:50:38Z</updated>
    <published>2022-09-05T04:50:38Z</published>
    <title>Instrument Separation of Symbolic Music by Explicitly Guided Diffusion
  Model</title>
    <summary>  Similar to colorization in computer vision, instrument separation is to
assign instrument labels (e.g. piano, guitar...) to notes from unlabeled
mixtures which contain only performance information. To address the problem, we
adopt diffusion models and explicitly guide them to preserve consistency
between mixtures and music. The quantitative results show that our proposed
model can generate high-fidelity samples for multitrack symbolic music with
creativity.
</summary>
    <author>
      <name>Sangjun Han</name>
    </author>
    <author>
      <name>Hyeongrae Ihm</name>
    </author>
    <author>
      <name>DaeHan Ahn</name>
    </author>
    <author>
      <name>Woohyung Lim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to NeurIPS 2022 Workshop on Machine Learning for Creativity
  and Design</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.02696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.02696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.03711v1</id>
    <updated>2022-09-08T11:02:13Z</updated>
    <published>2022-09-08T11:02:13Z</published>
    <title>What Did I Just Hear? Detecting Pornographic Sounds in Adult Videos
  Using Neural Networks</title>
    <summary>  Audio-based pornographic detection enables efficient adult content filtering
without sacrificing performance by exploiting distinct spectral
characteristics. To improve it, we explore pornographic sound modeling based on
different neural architectures and acoustic features. We find that CNN trained
on log mel spectrogram achieves the best performance on Pornography-800
dataset. Our experiment results also show that log mel spectrogram allows
better representations for the models to recognize pornographic sounds.
Finally, to classify whole audio waveforms rather than segments, we employ
voting segment-to-audio technique that yields the best audio-level detection
results.
</summary>
    <author>
      <name>Holy Lovenia</name>
    </author>
    <author>
      <name>Dessi Puji Lestari</name>
    </author>
    <author>
      <name>Rita Frieske</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3561212.3561244</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3561212.3561244" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in AudioMostly 2022, ACM</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.03711v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.03711v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.07974v1</id>
    <updated>2022-09-16T14:42:47Z</updated>
    <published>2022-09-16T14:42:47Z</published>
    <title>musicaiz: A Python Library for Symbolic Music Generation, Analysis and
  Visualization</title>
    <summary>  In this article, we present musicaiz, an object-oriented library for
analyzing, generating and evaluating symbolic music. The submodules of the
package allow the user to create symbolic music data from scratch, build
algorithms to analyze symbolic music, encode MIDI data as tokens to train deep
learning sequence models, modify existing music data and evaluate music
generation systems. The evaluation submodule builds on previous work to
objectively measure music generation systems and to be able to reproduce the
results of music generation models. The library is publicly available online.
We encourage the community to contribute and provide feedback.
</summary>
    <author>
      <name>Carlos Hernandez-Olivan</name>
    </author>
    <author>
      <name>Jose R. Beltran</name>
    </author>
    <link href="http://arxiv.org/abs/2209.07974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.07974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.11585v2</id>
    <updated>2022-09-26T13:16:22Z</updated>
    <published>2022-09-23T13:32:15Z</published>
    <title>Synthetic Voice Spoofing Detection Based On Online Hard Example Mining</title>
    <summary>  The automatic speaker verification spoofing (ASVspoof) challenge series is
crucial for enhancing the spoofing consideration and the countermeasures
growth. Although the recent ASVspoof 2019 validation results indicate the
significant capability to identify most attacks, the model's recognition effect
is still poor for some attacks. This paper presents the Online Hard Example
Mining (OHEM) algorithm for detecting unknown voice spoofing attacks. The OHEM
is utilized to overcome the imbalance between simple and hard samples in the
dataset. The presented system provides an equal error rate (EER) of 0.77% on
the ASVspoof 2019 Challenge logical access scenario's evaluation set.
</summary>
    <author>
      <name>Chenlei Hu</name>
    </author>
    <author>
      <name>Ruohua Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/2209.11585v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.11585v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.13932v1</id>
    <updated>2022-10-25T11:37:43Z</updated>
    <published>2022-10-25T11:37:43Z</published>
    <title>CoLoC: Conditioned Localizer and Classifier for Sound Event Localization
  and Detection</title>
    <summary>  In this article, we describe Conditioned Localizer and Classifier (CoLoC)
which is a novel solution for Sound Event Localization and Detection (SELD).
The solution constitutes of two stages: the localization is done first and is
followed by classification conditioned by the output of the localizer. In order
to resolve the problem of the unknown number of sources we incorporate the idea
borrowed from Sequential Set Generation (SSG). Models from both stages are
SELDnet-like CRNNs, but with single outputs. Conducted reasoning shows that
such two single-output models are fit for SELD task. We show that our solution
improves on the baseline system in most metrics on the STARSS22 Dataset.
</summary>
    <author>
      <name>Sławomir Kapka</name>
    </author>
    <author>
      <name>Jakub Tkaczuk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.13932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.13932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.14602v2</id>
    <updated>2023-02-01T08:01:32Z</updated>
    <published>2022-10-26T10:20:50Z</published>
    <title>Efficient Data Mosaicing with Simulation-based Inference</title>
    <summary>  We introduce an efficient algorithm for general data mosaicing, based on the
simulation-based inference paradigm. Our algorithm takes as input a target
datum, source data, and partitions of the target and source data into
fragments, learning distributions over averages of fragments of the source data
such that samples from those distributions approximate fragments of the target
datum. We utilize a model that can be trivially parallelized in conjunction
with the latest advances in efficient simulation-based inference in order to
find approximate posteriors fast enough for use in practical applications. We
demonstrate our technique is effective in both audio and image mosaicing
problems.
</summary>
    <author>
      <name>Andrew Gambardella</name>
    </author>
    <author>
      <name>Youngjun Choi</name>
    </author>
    <author>
      <name>Doyo Choi</name>
    </author>
    <author>
      <name>Jinjoon Lee</name>
    </author>
    <link href="http://arxiv.org/abs/2210.14602v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.14602v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.06508v1</id>
    <updated>2022-11-11T23:06:24Z</updated>
    <published>2022-11-11T23:06:24Z</published>
    <title>On the robustness of non-intrusive speech quality model by adversarial
  examples</title>
    <summary>  It has been shown recently that deep learning based models are effective on
speech quality prediction and could outperform traditional metrics in various
perspectives. Although network models have potential to be a surrogate for
complex human hearing perception, they may contain instabilities in
predictions. This work shows that deep speech quality predictors can be
vulnerable to adversarial perturbations, where the prediction can be changed
drastically by unnoticeable perturbations as small as $-30$ dB compared with
speech inputs. In addition to exposing the vulnerability of deep speech quality
predictors, we further explore and confirm the viability of adversarial
training for strengthening robustness of models.
</summary>
    <author>
      <name>Hsin-Yi Lin</name>
    </author>
    <author>
      <name>Huan-Hsin Tseng</name>
    </author>
    <author>
      <name>Yu Tsao</name>
    </author>
    <link href="http://arxiv.org/abs/2211.06508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.06508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.08141v1</id>
    <updated>2022-11-15T13:48:11Z</updated>
    <published>2022-11-15T13:48:11Z</published>
    <title>SSM-Net: feature learning for Music Structure Analysis using a
  Self-Similarity-Matrix based loss</title>
    <summary>  In this paper, we propose a new paradigm to learn audio features for Music
Structure Analysis (MSA). We train a deep encoder to learn features such that
the Self-Similarity-Matrix (SSM) resulting from those approximates a
ground-truth SSM. This is done by minimizing a loss between both SSMs. Since
this loss is differentiable w.r.t. its input features we can train the encoder
in a straightforward way. We successfully demonstrate the use of this training
paradigm using the Area Under the Curve ROC (AUC) on the RWC-Pop dataset.
</summary>
    <author>
      <name>Geoffroy Peeters</name>
    </author>
    <author>
      <name>Florian Angulo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended Abstracts for the Late-Breaking Demo Session of the 23rd
  Int. Society for Music Information Retrieval Conf., Bengaluru, India, 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.08141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.08141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.13016v1</id>
    <updated>2022-11-23T16:05:28Z</updated>
    <published>2022-11-23T16:05:28Z</published>
    <title>On the Typicality of Musical Sequences</title>
    <summary>  It has been shown in a recent publication that words in human-produced
English language tend to have an information content close to the conditional
entropy. In this paper, we show that the same is true for events in
human-produced monophonic musical sequences. We also show how "typical
sampling" influences the distribution of information around the entropy for
single events and sequences.
</summary>
    <author>
      <name>Mathias Rose Bjare</name>
    </author>
    <author>
      <name>Stefan Lattner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure, Accepted at the Extended Abstracts for the
  Late-Breaking Demo Session of the 23rd Int. Society for Music Information
  Retrieval Conf., Bengaluru, India, 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.13016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.13016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.15834v1</id>
    <updated>2022-11-29T00:08:31Z</updated>
    <published>2022-11-29T00:08:31Z</published>
    <title>OK Computer Analysis: An Audio Corpus Study of Radiohead</title>
    <summary>  The application of music information retrieval techniques in popular music
studies has great promise. In the present work, a corpus of Radiohead songs
across their career from 1992 to 2017 are subjected to automated audio
analysis. We examine findings from a number of granularities and perspectives,
including within song and between song examination of both timbral-rhythmic and
harmonic features. Chronological changes include possible career spanning
effects for a band's releases such as slowing tempi and reduced brightness, and
the timbral markers of Radiohead's expanding approach to instrumental resources
most identified with the Kid A and Amnesiac era. We conclude with a discussion
highlighting some challenges for this approach, and the potential for a field
of audio file based career analysis.
</summary>
    <author>
      <name>Nick Collins</name>
    </author>
    <link href="http://arxiv.org/abs/2211.15834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.15834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.10093v1</id>
    <updated>2022-12-20T09:10:25Z</updated>
    <published>2022-12-20T09:10:25Z</published>
    <title>Visual Transformers for Primates Classification and Covid Detection</title>
    <summary>  We apply the vision transformer, a deep machine learning model build around
the attention mechanism, on mel-spectrogram representations of raw audio
recordings. When adding mel-based data augmentation techniques and
sample-weighting, we achieve comparable performance on both (PRS and CCS
challenge) tasks of ComParE21, outperforming most single model baselines. We
further introduce overlapping vertical patching and evaluate the influence of
parameter configurations. Index Terms: audio classification, attention,
mel-spectrogram, unbalanced data-sets, computational paralinguistics
</summary>
    <author>
      <name>Steffen Illium</name>
    </author>
    <author>
      <name>Robert Müller</name>
    </author>
    <author>
      <name>Andreas Sedlmeier</name>
    </author>
    <author>
      <name>Claudia-Linnhoff Popien</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.21437/Interspeech.2021-273</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.21437/Interspeech.2021-273" rel="related"/>
    <link href="http://arxiv.org/abs/2212.10093v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.10093v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.07491v1</id>
    <updated>2023-01-17T15:52:39Z</updated>
    <published>2023-01-17T15:52:39Z</published>
    <title>The Newsbridge -Telecom SudParis VoxCeleb Speaker Recognition Challenge
  2022 System Description</title>
    <summary>  We describe the system used by our team for the VoxCeleb Speaker Recognition
Challenge 2022 (VoxSRC 2022) in the speaker diarization track. Our solution was
designed around a new combination of voice activity detection algorithms that
uses the strengths of several systems. We introduce a novel multi stream
approach with a decision protocol based on classifiers entropy. We called this
method a multi-stream voice activity detection and used it with standard
baseline diarization embeddings, clustering and resegmentation. With this work,
we successfully demonstrated that using a strong baseline and working only on
voice activity detection, one can achieved close to state-of-theart results.
</summary>
    <author>
      <name>Yannis Tevissen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ARMEDIA-SAMOVAR</arxiv:affiliation>
    </author>
    <author>
      <name>Jérôme Boudy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ARMEDIA-SAMOVAR</arxiv:affiliation>
    </author>
    <author>
      <name>Frédéric Petitpont</name>
    </author>
    <link href="http://arxiv.org/abs/2301.07491v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.07491v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.07978v1</id>
    <updated>2023-01-19T10:13:52Z</updated>
    <published>2023-01-19T10:13:52Z</published>
    <title>SpotHitPy: A Study For ML-Based Song Hit Prediction Using Spotify</title>
    <summary>  In this study, we approached the Hit Song Prediction problem, which aims to
predict which songs will become Billboard hits. We gathered a dataset of nearly
18500 hit and non-hit songs and extracted their audio features using the
Spotify Web API. We test four machine-learning models on our dataset. We were
able to predict the Billboard success of a song with approximately 86\%
accuracy. The most succesful algorithms were Random Forest and Support Vector
Machine.
</summary>
    <author>
      <name>Ioannis Dimolitsas</name>
    </author>
    <author>
      <name>Spyridon Kantarelis</name>
    </author>
    <author>
      <name>Afroditi Fouka</name>
    </author>
    <link href="http://arxiv.org/abs/2301.07978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.07978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.08620v1</id>
    <updated>2023-01-20T15:01:46Z</updated>
    <published>2023-01-20T15:01:46Z</published>
    <title>Adjoint-Based Identification of Sound Sources for Sound Reinforcement
  and Source Localization</title>
    <summary>  The identification of sound sources is a common problem in acoustics.
Different parameters are sought, among these are signal and position of the
sources. We present an adjoint-based approach for sound source identification,
which employs computational aeroacoustic techniques. Two different applications
are presented as a proof-of-concept: optimization of a sound reinforcement
setup and the localization of (moving) sound sources.
</summary>
    <author>
      <name>Mathias Lemke</name>
    </author>
    <author>
      <name>Lewin Stein</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-52429-6_17</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-52429-6_17" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Notes on Numerical Fluid Mechanics and Multidisciplinary Design,
  vol 145. Springer (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2301.08620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.08620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="76Q05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.13380v1</id>
    <updated>2023-01-31T03:05:48Z</updated>
    <published>2023-01-31T03:05:48Z</published>
    <title>Automated Time-frequency Domain Audio Crossfades using Graph Cuts</title>
    <summary>  The problem of transitioning smoothly from one audio clip to another arises
in many music consumption scenarios, especially as music consumption has moved
from professionally curated and live-streamed radios to personal playback
devices and services. we present the first steps toward a new method of
automatically transitioning from one audio clip to another by discretizing the
frequency spectrum into bins and then finding transition times for each bin. We
phrase the problem as one of graph flow optimization; specifically
min-cut/max-flow.
</summary>
    <author>
      <name>Kyle Robinson</name>
    </author>
    <author>
      <name>Dan Brown</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Late Breaking/Demo at the 20th International Society for Music
  Information Retrieval, Delft, The Netherlands, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2301.13380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.13380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.05690v1</id>
    <updated>2023-02-11T13:17:59Z</updated>
    <published>2023-02-11T13:17:59Z</published>
    <title>Attention does not guarantee best performance in speech enhancement</title>
    <summary>  Attention mechanism has been widely utilized in speech enhancement (SE)
because theoretically it can effectively model the long-term inherent
connection of signal both in time domain and spectrum domain. However, the
generally used global attention mechanism might not be the best choice since
the adjacent information naturally imposes more influence than the far-apart
information in speech enhancement. In this paper, we validate this conjecture
by replacing attention with RNN in two typical state-of-the-art (SOTA) models,
multi-scale temporal frequency convolutional network (MTFAA) with axial
attention and conformer-based metric-GAN network (CMGAN).
</summary>
    <author>
      <name>Zhongshu Hou</name>
    </author>
    <author>
      <name>Qinwen Hu</name>
    </author>
    <author>
      <name>Kai Chen</name>
    </author>
    <author>
      <name>Jing Lu</name>
    </author>
    <link href="http://arxiv.org/abs/2302.05690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.05690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.13290v1</id>
    <updated>2023-02-26T10:46:15Z</updated>
    <published>2023-02-26T10:46:15Z</published>
    <title>Implementation of an aeroacoustic simulation pipeline using
  openCFS-Acoustics and openCFS-Data applied to human phonation</title>
    <summary>  The human phonation process be modeled using the Finite Element Method (FEM)
which provides a detailed representation of the voice production process. A
software implementation in C++ using FEM (openCFS) has been used to simulate
the phonation process. The FEM model consists of a 3D mesh of the upper human
airways. The simVoice model provides an accurate representation of the
phonation process and was valid in several publications. In this article, we
show how to set up the model using openCFS and openCFS-Data.
</summary>
    <author>
      <name>Stefan Schoder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.13290v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.13290v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.01875v1</id>
    <updated>2023-03-03T12:10:54Z</updated>
    <published>2023-03-03T12:10:54Z</published>
    <title>Decoding and Visualising Intended Emotion in an Expressive Piano
  Performance</title>
    <summary>  Expert musicians can mould a musical piece to convey specific emotions that
they intend to communicate. In this paper, we place a mid-level features based
music emotion model in this performer-to-listener communication scenario, and
demonstrate via a small visualisation music emotion decoding in real time. We
also extend the existing set of mid-level features using analogues of
perceptual speed and perceived dynamics.
</summary>
    <author>
      <name>Shreyan Chowdhury</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of Late-Breaking Demo Session paper accepted at
  ISMIR 2022 (23rd Int. Society for Music Information Retrieval Conf.,
  Bengaluru, India, 2022)</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.01875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.01875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.02599v1</id>
    <updated>2023-03-05T07:54:49Z</updated>
    <published>2023-03-05T07:54:49Z</published>
    <title>Hybrid Y-Net Architecture for Singing Voice Separation</title>
    <summary>  This research paper presents a novel deep learning-based neural network
architecture, named Y-Net, for achieving music source separation. The proposed
architecture performs end-to-end hybrid source separation by extracting
features from both spectrogram and waveform domains. Inspired by the U-Net
architecture, Y-Net predicts a spectrogram mask to separate vocal sources from
a mixture signal. Our results demonstrate the effectiveness of the proposed
architecture for music source separation with fewer parameters. Overall, our
work presents a promising approach for improving the accuracy and efficiency of
music source separation.
</summary>
    <author>
      <name>Rashen Fernando</name>
    </author>
    <author>
      <name>Pamudu Ranasinghe</name>
    </author>
    <author>
      <name>Udula Ranasinghe</name>
    </author>
    <author>
      <name>Janaka Wijayakulasooriya</name>
    </author>
    <author>
      <name>Pantaleon Perera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted for EUSIPCO23: 5 Pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.02599v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.02599v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.06379v1</id>
    <updated>2023-03-11T11:12:49Z</updated>
    <published>2023-03-11T11:12:49Z</published>
    <title>TaylorAECNet: A Taylor Style Neural Network for Full-Band Echo
  Cancellation</title>
    <summary>  This paper describes aecX team's entry to the ICASSP 2023 acoustic echo
cancellation (AEC) challenge. Our system consists of an adaptive filter and a
proposed full-band Taylor-style acoustic echo cancellation neural network
(TaylorAECNet) as a post-filter. Specifically, we leverage the recent advances
in Taylor expansion based decoupling-style interpretable speech enhancement and
explore its feasibility in the AEC task. Our TaylorAECNet based approach
achieves an overall mean opinion score (MOS) of 4.241, a word accuracy (WAcc)
ratio of 0.767, and ranks 5th in the non-personalized track (track 1).
</summary>
    <author>
      <name>Weiming Xu</name>
    </author>
    <author>
      <name>Zhihao Guo</name>
    </author>
    <link href="http://arxiv.org/abs/2303.06379v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.06379v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.03530v2</id>
    <updated>2023-05-11T04:44:12Z</updated>
    <published>2023-05-05T13:37:04Z</published>
    <title>Exploring Softly Masked Language Modelling for Controllable Symbolic
  Music Generation</title>
    <summary>  This document presents some early explorations of applying Softly Masked
Language Modelling (SMLM) to symbolic music generation. SMLM can be seen as a
generalisation of masked language modelling (MLM), where instead of each
element of the input set being either known or unknown, each element can be
known, unknown or partly known. We demonstrate some results of applying SMLM to
constrained symbolic music generation using a transformer encoder architecture.
Several audio examples are available at
https://erl-j.github.io/smlm-web-supplement/
</summary>
    <author>
      <name>Nicolas Jonason</name>
    </author>
    <author>
      <name>Bob L. T. Sturm</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 1.1</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.03530v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.03530v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.03982v1</id>
    <updated>2023-05-06T09:05:11Z</updated>
    <published>2023-05-06T09:05:11Z</published>
    <title>Pitch Estimation by Denoising Preprocessor and Hybrid Estimation Model</title>
    <summary>  Pitch estimation is to estimate the fundamental frequency and the midi number
and plays a critical role in music signal analysis and vocal signal processing.
In this work, we proposed a new architecture based on a learning-based
enhancement preprocessor and a combination of several traditional and deep
learning pitch estimation methods to achieve better pitch estimation
performance in both noisy and clean scenarios. We test 17 different types of
noise and 4 SNRdb noise levels. The results show that the proposed pitch
estimation can perform better in both noisy and clean scenarios with short
response time.
</summary>
    <author>
      <name>Yu Cheng Hung</name>
    </author>
    <author>
      <name>Ping Hung Chen</name>
    </author>
    <author>
      <name>Jian Jiun Ding</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">From ICCE-Taiwan</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.03982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.03982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.07243v2</id>
    <updated>2023-05-23T21:41:54Z</updated>
    <published>2023-05-12T04:19:49Z</published>
    <title>Better speech synthesis through scaling</title>
    <summary>  In recent years, the field of image generation has been revolutionized by the
application of autoregressive transformers and DDPMs. These approaches model
the process of image generation as a step-wise probabilistic processes and
leverage large amounts of compute and data to learn the image distribution.
This methodology of improving performance need not be confined to images. This
paper describes a way to apply advances in the image generative domain to
speech synthesis. The result is TorToise -- an expressive, multi-voice
text-to-speech system.
  All model code and trained weights have been open-sourced at
https://github.com/neonbjb/tortoise-tts.
</summary>
    <author>
      <name>James Betker</name>
    </author>
    <link href="http://arxiv.org/abs/2305.07243v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.07243v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.11605v1</id>
    <updated>2023-05-19T11:31:33Z</updated>
    <published>2023-05-19T11:31:33Z</published>
    <title>MIDI-Draw: Sketching to Control Melody Generation</title>
    <summary>  We describe a proof-of-principle implementation of a system for drawing
melodies that abstracts away from a note-level input representation via melodic
contours. The aim is to allow users to express their musical intentions without
requiring prior knowledge of how notes fit together melodiously. Current
approaches to controllable melody generation often require users to choose
parameters that are static across a whole sequence, via buttons or sliders. In
contrast, our method allows users to quickly specify how parameters should
change over time by drawing a contour.
</summary>
    <author>
      <name>Tashi Namgyal</name>
    </author>
    <author>
      <name>Peter Flach</name>
    </author>
    <author>
      <name>Raul Santos-Rodriguez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Late-Breaking / Demo Session Extended Abstract, ISMIR 2022 Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.11605v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.11605v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.14023v1</id>
    <updated>2023-05-23T12:56:35Z</updated>
    <published>2023-05-23T12:56:35Z</published>
    <title>Happy or Evil Laughter? Analysing a Database of Natural Audio Samples</title>
    <summary>  We conducted a data collection on the basis of the Google AudioSet database
by selecting a subset of the samples annotated with \textit{laughter}. The
selection criterion was to be present a communicative act with clear
connotation of being either positive (laughing with) or negative (being laughed
at). On the basis of this annotated data, we performed two experiments: on the
one hand, we manually extract and analyze phonetic features. On the other hand,
we conduct several machine learning experiments by systematically combining
several automatically extracted acoustic feature sets with machine learning
algorithms. This shows that the best performing models can achieve and
unweighted average recall of .7.
</summary>
    <author>
      <name>Aljoscha Düsterhöft</name>
    </author>
    <author>
      <name>Felix Burkhardt</name>
    </author>
    <author>
      <name>Björn W. Schuller</name>
    </author>
    <link href="http://arxiv.org/abs/2305.14023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.14023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.14867v1</id>
    <updated>2023-05-24T08:19:42Z</updated>
    <published>2023-05-24T08:19:42Z</published>
    <title>Interactive Neural Resonators</title>
    <summary>  In this work, we propose a method for the controllable synthesis of real-time
contact sounds using neural resonators. Previous works have used physically
inspired statistical methods and physical modelling for object materials and
excitation signals. Our method incorporates differentiable second-order
resonators and estimates their coefficients using a neural network that is
conditioned on physical parameters. This allows for interactive dynamic control
and the generation of novel sounds in an intuitive manner. We demonstrate the
practical implementation of our method and explore its potential creative
applications.
</summary>
    <author>
      <name>Rodrigo Diaz</name>
    </author>
    <author>
      <name>Charalampos Saitis</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <link href="http://arxiv.org/abs/2305.14867v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.14867v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.16043v1</id>
    <updated>2023-05-25T13:21:00Z</updated>
    <published>2023-05-25T13:21:00Z</published>
    <title>Ordered and Binary Speaker Embedding</title>
    <summary>  Modern speaker recognition systems represent utterances by embedding vectors.
Conventional embedding vectors are dense and non-structural. In this paper, we
propose an ordered binary embedding approach that sorts the dimensions of the
embedding vector via a nested dropout and converts the sorted vectors to binary
codes via Bernoulli sampling. The resultant ordered binary codes offer some
important merits such as hierarchical clustering, reduced memory usage, and
fast retrieval. These merits were empirically verified by comprehensive
experiments on a speaker identification task with the VoxCeleb and CN-Celeb
datasets.
</summary>
    <author>
      <name>Jiaying Wang</name>
    </author>
    <author>
      <name>Xianglong Wang</name>
    </author>
    <author>
      <name>Namin Wang</name>
    </author>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to be published in INTERSPEECH 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.16043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.16043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.17749v1</id>
    <updated>2023-05-28T15:14:46Z</updated>
    <published>2023-05-28T15:14:46Z</published>
    <title>Bayesian inference and neural estimation of acoustic wave propagation</title>
    <summary>  In this work, we introduce a novel framework which combines physics and
machine learning methods to analyse acoustic signals. Three methods are
developed for this task: a Bayesian inference approach for inferring the
spectral acoustics characteristics, a neural-physical model which equips a
neural network with forward and backward physical losses, and the non-linear
least squares approach which serves as benchmark. The inferred propagation
coefficient leads to the room impulse response (RIR) quantity which can be used
for relocalisation with uncertainty. The simplicity and efficiency of this
framework is empirically validated on simulated data.
</summary>
    <author>
      <name>Yongchao Huang</name>
    </author>
    <author>
      <name>Yuhang He</name>
    </author>
    <author>
      <name>Hong Ge</name>
    </author>
    <link href="http://arxiv.org/abs/2305.17749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.17749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.00614v1</id>
    <updated>2023-06-01T12:38:11Z</updated>
    <published>2023-06-01T12:38:11Z</published>
    <title>Adaptation and Optimization of Automatic Speech Recognition (ASR) for
  the Maritime Domain in the Field of VHF Communication</title>
    <summary>  This paper introduces a multilingual automatic speech recognizer (ASR) for
maritime radio communi-cation that automatically converts received VHF radio
signals into text. The challenges of maritime radio communication are described
at first, and the deep learning architecture of marFM consisting of audio
processing techniques and machine learning algorithms is presented.
Subsequently, maritime radio data of interest is analyzed and then used to
evaluate the transcription performance of our ASR model for various maritime
radio data.
</summary>
    <author>
      <name>Emin Cagatay Nakilcioglu</name>
    </author>
    <author>
      <name>Maximilian Reimann</name>
    </author>
    <author>
      <name>Ole John</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the COMPIT Conference 22 (2023) 345-354</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2306.00614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.00614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.02750v1</id>
    <updated>2023-06-05T10:12:41Z</updated>
    <published>2023-06-05T10:12:41Z</published>
    <title>The Learning Prescription, A Neural Network Hearing Aid Core</title>
    <summary>  The definition of a hearing aid core which is based on a prescription neural
network (such as NAL-NL2) is defined here. This hearing aid core replaces a
traditional compressor hearing aid core which mimics the said hearing aid
prescription. Whilst the replacement of the compressors for a neural network
may seem simple, the implications are vast in terms of the "learning
prescription" where the topology of the neural network may be increased to make
available more free parameters and allow great personalisation of the hearing
aid prescription.
</summary>
    <author>
      <name>Matt R. Flax</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">https://github.com/flatmax/hearing.aid-neural.network-core</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.02750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.02750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.04628v1</id>
    <updated>2023-06-06T13:26:55Z</updated>
    <published>2023-06-06T13:26:55Z</published>
    <title>Systematic Analysis of Music Representations from BERT</title>
    <summary>  There have been numerous attempts to represent raw data as numerical vectors
that effectively capture semantic and contextual information. However, in the
field of symbolic music, previous works have attempted to validate their music
embeddings by observing the performance improvement of various fine-tuning
tasks. In this work, we directly analyze embeddings from BERT and BERT with
contrastive learning trained on bar-level MIDI, inspecting their musical
information that can be obtained from MIDI events. We observe that the
embeddings exhibit distinct characteristics of information depending on the
contrastive objectives and the choice of layers. Our code is available at
https://github.com/sjhan91/MusicBERT.
</summary>
    <author>
      <name>Sangjun Han</name>
    </author>
    <author>
      <name>Hyeongrae Ihm</name>
    </author>
    <author>
      <name>Woohyung Lim</name>
    </author>
    <link href="http://arxiv.org/abs/2306.04628v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.04628v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.06284v1</id>
    <updated>2023-06-09T22:24:05Z</updated>
    <published>2023-06-09T22:24:05Z</published>
    <title>Everybody Compose: Deep Beats To Music</title>
    <summary>  This project presents a deep learning approach to generate monophonic
melodies based on input beats, allowing even amateurs to create their own music
compositions. Three effective methods - LSTM with Full Attention, LSTM with
Local Attention, and Transformer with Relative Position Representation - are
proposed for this novel task, providing great variation, harmony, and structure
in the generated music. This project allows anyone to compose their own music
by tapping their keyboards or ``recoloring'' beat sequences from existing
works.
</summary>
    <author>
      <name>Conghao Shen</name>
    </author>
    <author>
      <name>Violet Z. Yao</name>
    </author>
    <author>
      <name>Yixin Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3587819.3592542</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3587819.3592542" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted MMSys '23</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 14th Conference on ACM Multimedia Systems
  (2023)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2306.06284v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.06284v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.08012v1</id>
    <updated>2023-06-13T11:16:52Z</updated>
    <published>2023-06-13T11:16:52Z</published>
    <title>A Novel Scheme to classify Read and Spontaneous Speech</title>
    <summary>  The COVID-19 pandemic has led to an increased use of remote telephonic
interviews, making it important to distinguish between scripted and spontaneous
speech in audio recordings. In this paper, we propose a novel scheme for
identifying read and spontaneous speech. Our approach uses a pre-trained
DeepSpeech audio-to-alphabet recognition engine to generate a sequence of
alphabets from the audio. From these alphabets, we derive features that allow
us to discriminate between read and spontaneous speech. Our experimental
results show that even a small set of self-explanatory features can effectively
classify the two types of speech very effectively.
</summary>
    <author>
      <name>Sunil Kumar Kopparapu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.08012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.08012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.09382v3</id>
    <updated>2023-07-21T07:59:06Z</updated>
    <published>2023-06-15T12:59:04Z</published>
    <title>Sound Demixing Challenge 2023 Music Demixing Track Technical Report:
  TFC-TDF-UNet v3</title>
    <summary>  In this report, we present our award-winning solutions for the Music Demixing
Track of Sound Demixing Challenge 2023. First, we propose TFC-TDF-UNet v3, a
time-efficient music source separation model that achieves state-of-the-art
results on the MUSDB benchmark. We then give full details regarding our
solutions for each Leaderboard, including a loss masking approach for
noise-robust training. Code for reproducing model training and final
submissions is available at github.com/kuielab/sdx23.
</summary>
    <author>
      <name>Minseok Kim</name>
    </author>
    <author>
      <name>Jun Hyung Lee</name>
    </author>
    <author>
      <name>Soonyoung Jung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.09382v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.09382v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.12957v1</id>
    <updated>2023-06-22T15:16:06Z</updated>
    <published>2023-06-22T15:16:06Z</published>
    <title>Siamese SIREN: Audio Compression with Implicit Neural Representations</title>
    <summary>  Implicit Neural Representations (INRs) have emerged as a promising method for
representing diverse data modalities, including 3D shapes, images, and audio.
While recent research has demonstrated successful applications of INRs in image
and 3D shape compression, their potential for audio compression remains largely
unexplored. Motivated by this, we present a preliminary investigation into the
use of INRs for audio compression. Our study introduces Siamese SIREN, a novel
approach based on the popular SIREN architecture. Our experimental results
indicate that Siamese SIREN achieves superior audio reconstruction fidelity
while utilizing fewer network parameters compared to previous INR
architectures.
</summary>
    <author>
      <name>Luca A. Lanzendörfer</name>
    </author>
    <author>
      <name>Roger Wattenhofer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a workshop paper at ICML 2023 neural compression
  workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.12957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.12957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.16081v1</id>
    <updated>2023-06-28T10:27:53Z</updated>
    <published>2023-06-28T10:27:53Z</published>
    <title>Graph neural networks for sound source localization on distributed
  microphone networks</title>
    <summary>  Distributed Microphone Arrays (DMAs) present many challenges with respect to
centralized microphone arrays. An important requirement of applications on
these arrays is handling a variable number of input channels. We consider the
use of Graph Neural Networks (GNNs) as a solution to this challenge. We present
a localization method using the Relation Network GNN, which we show shares many
similarities to classical signal processing algorithms for Sound Source
Localization (SSL). We apply our method for the task of SSL and validate it
experimentally using an unseen number of microphones. We test different feature
extractors and show that our approach significantly outperforms classical
baselines.
</summary>
    <author>
      <name>Eric Grinstein</name>
    </author>
    <author>
      <name>Mike Brookes</name>
    </author>
    <author>
      <name>Patrick A. Naylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented as a poster at ICASSP 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.16081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.16081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.02132v1</id>
    <updated>2023-07-05T09:20:46Z</updated>
    <published>2023-07-05T09:20:46Z</published>
    <title>Going Retro: Astonishingly Simple Yet Effective Rule-based Prosody
  Modelling for Speech Synthesis Simulating Emotion Dimensions</title>
    <summary>  We introduce two rule-based models to modify the prosody of speech synthesis
in order to modulate the emotion to be expressed. The prosody modulation is
based on speech synthesis markup language (SSML) and can be used with any
commercial speech synthesizer. The models as well as the optimization result
are evaluated against human emotion annotations. Results indicate that with a
very simple method both dimensions arousal (.76 UAR) and valence (.43 UAR) can
be simulated.
</summary>
    <author>
      <name>Felix Burkhardt</name>
    </author>
    <author>
      <name>Uwe Reichel</name>
    </author>
    <author>
      <name>Florian Eyben</name>
    </author>
    <author>
      <name>Björn Schuller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted at 34th ESSV 2023, Munich 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.02132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.02132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.06181v2</id>
    <updated>2023-08-17T10:34:48Z</updated>
    <published>2023-07-12T14:12:19Z</published>
    <title>B-CLEAN-SC: CLEAN-SC for broadband sources</title>
    <summary>  This paper presents B-CLEAN-SC, a variation of CLEAN-SC for broadband
sources. Opposed to CLEAN-SC, which ``deconvolves'' the beamforming map for
each frequency individually, B-CLEAN-SC processes frequency intervals. Instead
of performing a deconvolution iteration at the location of the maximum level,
B-CLEAN-SC performs it at the location of the over-frequency-averaged maximum
to improve the location estimation. The method is validated and compared to
standard CLEAN-SC on synthetic cases, and real-world experiments, for broad-
and narrowband sources. It improves the source reconstruction at low and high
frequencies and suppresses noise, while it only increases the need for memory
but not computational effort.
</summary>
    <author>
      <name>Armin Goudarzi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">revision 1</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.06181v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.06181v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.flu-dyn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.09006v1</id>
    <updated>2023-07-18T06:48:39Z</updated>
    <published>2023-07-18T06:48:39Z</published>
    <title>OxfordVGG Submission to the EGO4D AV Transcription Challenge</title>
    <summary>  This report presents the technical details of our submission on the EGO4D
Audio-Visual (AV) Automatic Speech Recognition Challenge 2023 from the
OxfordVGG team. We present WhisperX, a system for efficient speech
transcription of long-form audio with word-level time alignment, along with two
text normalisers which are publicly available. Our final submission obtained
56.0% of the Word Error Rate (WER) on the challenge test set, ranked 1st on the
leaderboard. All baseline codes and models are available on
https://github.com/m-bain/whisperX.
</summary>
    <author>
      <name>Jaesung Huh</name>
    </author>
    <author>
      <name>Max Bain</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.09006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.09006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.09723v1</id>
    <updated>2023-07-19T02:21:44Z</updated>
    <published>2023-07-19T02:21:44Z</published>
    <title>Improving Domain Generalization for Sound Classification with Sparse
  Frequency-Regularized Transformer</title>
    <summary>  Sound classification models' performance suffers from generalizing on
out-of-distribution (OOD) data. Numerous methods have been proposed to help the
model generalize. However, most either introduce inference overheads or focus
on long-lasting CNN-variants, while Transformers has been proven to outperform
CNNs on numerous natural language processing and computer vision tasks. We
propose FRITO, an effective regularization technique on Transformer's
self-attention, to improve the model's generalization ability by limiting each
sequence position's attention receptive field along the frequency dimension on
the spectrogram. Experiments show that our method helps Transformer models
achieve SOTA generalization performance on TAU 2020 and Nsynth datasets while
saving 20% inference time.
</summary>
    <author>
      <name>Honglin Mu</name>
    </author>
    <author>
      <name>Wentian Xia</name>
    </author>
    <author>
      <name>Wanxiang Che</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICME 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.09723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.09723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.10283v1</id>
    <updated>2023-07-18T11:46:13Z</updated>
    <published>2023-07-18T11:46:13Z</published>
    <title>Interpretable Timbre Synthesis using Variational Autoencoders
  Regularized on Timbre Descriptors</title>
    <summary>  Controllable timbre synthesis has been a subject of research for several
decades, and deep neural networks have been the most successful in this area.
Deep generative models such as Variational Autoencoders (VAEs) have the ability
to generate a high-level representation of audio while providing a structured
latent space. Despite their advantages, the interpretability of these latent
spaces in terms of human perception is often limited. To address this
limitation and enhance the control over timbre generation, we propose a
regularized VAE-based latent space that incorporates timbre descriptors.
Moreover, we suggest a more concise representation of sound by utilizing its
harmonic content, in order to minimize the dimensionality of the latent space.
</summary>
    <author>
      <name>Anastasia Natsiou</name>
    </author>
    <author>
      <name>Luca Longo</name>
    </author>
    <author>
      <name>Sean O'Leary</name>
    </author>
    <link href="http://arxiv.org/abs/2307.10283v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.10283v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.15400v1</id>
    <updated>2023-07-28T08:50:29Z</updated>
    <published>2023-07-28T08:50:29Z</published>
    <title>The FlySpeech Audio-Visual Speaker Diarization System for MISP Challenge
  2022</title>
    <summary>  This paper describes the FlySpeech speaker diarization system submitted to
the second \textbf{M}ultimodal \textbf{I}nformation Based \textbf{S}peech
\textbf{P}rocessing~(\textbf{MISP}) Challenge held in ICASSP 2022. We develop
an end-to-end audio-visual speaker diarization~(AVSD) system, which consists of
a lip encoder, a speaker encoder, and an audio-visual decoder. Specifically, to
mitigate the degradation of diarization performance caused by separate
training, we jointly train the speaker encoder and the audio-visual decoder. In
addition, we leverage the large-data pretrained speaker extractor to initialize
the speaker encoder.
</summary>
    <author>
      <name>Li Zhang</name>
    </author>
    <author>
      <name>Huan Zhao</name>
    </author>
    <author>
      <name>Yue Li</name>
    </author>
    <author>
      <name>Bowen Pang</name>
    </author>
    <author>
      <name>Yannan Wang</name>
    </author>
    <author>
      <name>Hongji Wang</name>
    </author>
    <author>
      <name>Wei Rao</name>
    </author>
    <author>
      <name>Qing Wang</name>
    </author>
    <author>
      <name>Lei Xie</name>
    </author>
    <link href="http://arxiv.org/abs/2307.15400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.15400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.07170v2</id>
    <updated>2023-12-17T17:46:27Z</updated>
    <published>2023-08-14T14:26:52Z</published>
    <title>Human Voice Pitch Estimation: A Convolutional Network with Auto-Labeled
  and Synthetic Data</title>
    <summary>  In the domain of music and sound processing, pitch extraction plays a pivotal
role. Our research presents a specialized convolutional neural network designed
for pitch extraction, particularly from the human singing voice in acapella
performances. Notably, our approach combines synthetic data with auto-labeled
acapella sung audio, creating a robust training environment. Evaluation across
datasets comprising synthetic sounds, opera recordings, and time-stretched
vowels demonstrates its efficacy. This work paves the way for enhanced pitch
extraction in both music and voice settings.
</summary>
    <author>
      <name>Jeremy Cochoy</name>
    </author>
    <link href="http://arxiv.org/abs/2308.07170v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.07170v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.08181v1</id>
    <updated>2023-08-16T07:21:01Z</updated>
    <published>2023-08-16T07:21:01Z</published>
    <title>ChinaTelecom System Description to VoxCeleb Speaker Recognition
  Challenge 2023</title>
    <summary>  This technical report describes ChinaTelecom system for Track 1 (closed) of
the VoxCeleb2023 Speaker Recognition Challenge (VoxSRC 2023). Our system
consists of several ResNet variants trained only on VoxCeleb2, which were fused
for better performance later. Score calibration was also applied for each
variant and the fused system. The final submission achieved minDCF of 0.1066
and EER of 1.980%.
</summary>
    <author>
      <name>Mengjie Du</name>
    </author>
    <author>
      <name>Xiang Fang</name>
    </author>
    <author>
      <name>Jie Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">System description of VoxSRC 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.08181v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.08181v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.13941v2</id>
    <updated>2023-10-14T14:24:23Z</updated>
    <published>2023-08-26T18:58:10Z</published>
    <title>A small vocabulary database of ultrasound image sequences of vocal tract
  dynamics</title>
    <summary>  This paper presents a new database consisting of concurrent articulatory and
acoustic speech data. The articulatory data correspond to ultrasound videos of
the vocal tract dynamics, which allow the visualization of the tongue upper
contour during the speech production process. Acoustic data is composed of 30
short sentences that were acquired by a directional cardioid microphone. This
database includes data from 17 young subjects (8 male and 9 female) from the
Santander region in Colombia, who reported not having any speech pathology.
</summary>
    <author>
      <name>Margareth Castillo</name>
    </author>
    <author>
      <name>Felipe Rubio</name>
    </author>
    <author>
      <name>Dagoberto Porras</name>
    </author>
    <author>
      <name>Sonia H. Contreras-Ortiz</name>
    </author>
    <author>
      <name>Alexander Sepúlveda</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/STSIVA.2019.8730224</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/STSIVA.2019.8730224" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">STSIVA-2019, Bucaramanga, Colombia, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2308.13941v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.13941v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.16678v1</id>
    <updated>2023-08-31T12:29:24Z</updated>
    <published>2023-08-31T12:29:24Z</published>
    <title>Dynamic nsNet2: Efficient Deep Noise Suppression with Early Exiting</title>
    <summary>  Although deep learning has made strides in the field of deep noise
suppression, leveraging deep architectures on resource-constrained devices
still proved challenging. Therefore, we present an early-exiting model based on
nsNet2 that provides several levels of accuracy and resource savings by halting
computations at different stages. Moreover, we adapt the original architecture
by splitting the information flow to take into account the injected dynamism.
We show the trade-offs between performance and computational complexity based
on established metrics.
</summary>
    <author>
      <name>Riccardo Miccini</name>
    </author>
    <author>
      <name>Alaa Zniber</name>
    </author>
    <author>
      <name>Clément Laroche</name>
    </author>
    <author>
      <name>Tobias Piechowiak</name>
    </author>
    <author>
      <name>Martin Schoeberl</name>
    </author>
    <author>
      <name>Luca Pezzarossa</name>
    </author>
    <author>
      <name>Ouassim Karrakchou</name>
    </author>
    <author>
      <name>Jens Sparsø</name>
    </author>
    <author>
      <name>Mounir Ghogho</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MLSP55844.2023.10285925</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MLSP55844.2023.10285925" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the MLSP 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.16678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.16678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.03516v1</id>
    <updated>2023-09-07T06:56:56Z</updated>
    <published>2023-09-07T06:56:56Z</published>
    <title>Topological fingerprints for audio identification</title>
    <summary>  We present a topological audio fingerprinting approach for robustly
identifying duplicate audio tracks. Our method applies persistent homology on
local spectral decompositions of audio signals, using filtered cubical
complexes computed from mel-spectrograms. By encoding the audio content in
terms of local Betti curves, our topological audio fingerprints enable accurate
detection of time-aligned audio matchings. Experimental results demonstrate the
accuracy of our algorithm in the detection of tracks with the same audio
content, even when subjected to various obfuscations. Our approach outperforms
existing methods in scenarios involving topological distortions, such as time
stretching and pitch shifting.
</summary>
    <author>
      <name>Wojciech Reise</name>
    </author>
    <author>
      <name>Ximena Fernández</name>
    </author>
    <author>
      <name>Maria Dominguez</name>
    </author>
    <author>
      <name>Heather A. Harrington</name>
    </author>
    <author>
      <name>Mariano Beguerisse-Díaz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.03516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.03516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="55N31, 68U10, 62R40" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.03641v2</id>
    <updated>2024-04-21T03:39:55Z</updated>
    <published>2023-09-07T11:21:10Z</published>
    <title>Spiking Structured State Space Model for Monaural Speech Enhancement</title>
    <summary>  Speech enhancement seeks to extract clean speech from noisy signals.
Traditional deep learning methods face two challenges: efficiently using
information in long speech sequences and high computational costs. To address
these, we introduce the Spiking Structured State Space Model (Spiking-S4). This
approach merges the energy efficiency of Spiking Neural Networks (SNN) with the
long-range sequence modeling capabilities of Structured State Space Models
(S4), offering a compelling solution. Evaluation on the DNS Challenge and
VoiceBank+Demand Datasets confirms that Spiking-S4 rivals existing Artificial
Neural Network (ANN) methods but with fewer computational resources, as
evidenced by reduced parameters and Floating Point Operations (FLOPs).
</summary>
    <author>
      <name>Yu Du</name>
    </author>
    <author>
      <name>Xu Liu</name>
    </author>
    <author>
      <name>Yansong Chua</name>
    </author>
    <link href="http://arxiv.org/abs/2309.03641v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.03641v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.09522v1</id>
    <updated>2023-10-14T07:25:52Z</updated>
    <published>2023-10-14T07:25:52Z</published>
    <title>Dynamic Prediction of Full-Ocean Depth SSP by Hierarchical LSTM: An
  Experimental Result</title>
    <summary>  SSP distribution is an important parameter for underwater positioning,
navigation and timing (PNT) because it affects the propagation mode of
underwater acoustic signals. To accurate predict future sound speed
distribution, we propose a hierarchical long short--term memory (H--LSTM)
neural network for future sound speed prediction, which explore the
distribution pattern of sound velocity in the time dimension. To verify the
feasibility and effectiveness, we conducted both simulations and real
experiments. The ocean experiment was held in the South China Sea in April,
2023. Results show that the accuracy of the proposed method outperforms the
state--of--the--art methods.
</summary>
    <author>
      <name>Jiajun Lu</name>
    </author>
    <author>
      <name>Wei Huang</name>
    </author>
    <author>
      <name>Hao Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LGRS.2024.3356552</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LGRS.2024.3356552" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Geoscience and Remote Sensing Letters, 2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2310.09522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.09522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.16109v1</id>
    <updated>2023-10-24T18:21:03Z</updated>
    <published>2023-10-24T18:21:03Z</published>
    <title>Complex Image Generation SwinTransformer Network for Audio Denoising</title>
    <summary>  Achieving high-performance audio denoising is still a challenging task in
real-world applications. Existing time-frequency methods often ignore the
quality of generated frequency domain images. This paper converts the audio
denoising problem into an image generation task. We first develop a complex
image generation SwinTransformer network to capture more information from the
complex Fourier domain. We then impose structure similarity and detailed loss
functions to generate high-quality images and develop an SDR loss to minimize
the difference between denoised and clean audios. Extensive experiments on two
benchmark datasets demonstrate that our proposed model is better than
state-of-the-art methods.
</summary>
    <author>
      <name>Youshan Zhang</name>
    </author>
    <author>
      <name>Jialu Li</name>
    </author>
    <link href="http://arxiv.org/abs/2310.16109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.16109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.19842v1</id>
    <updated>2023-10-30T08:02:08Z</updated>
    <published>2023-10-30T08:02:08Z</published>
    <title>Musical Form Generation</title>
    <summary>  While recent generative models can produce engaging music, their utility is
limited. The variation in the music is often left to chance, resulting in
compositions that lack structure. Pieces extending beyond a minute can become
incoherent or repetitive. This paper introduces an approach for generating
structured, arbitrarily long musical pieces. Central to this approach is the
creation of musical segments using a conditional generative model, with
transitions between these segments. The generation of prompts that determine
the high-level composition is distinct from the creation of finer, lower-level
details. A large language model is then used to suggest the musical form.
</summary>
    <author>
      <name>Lilac Atassi</name>
    </author>
    <link href="http://arxiv.org/abs/2310.19842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.19842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.02087v1</id>
    <updated>2023-10-24T14:43:42Z</updated>
    <published>2023-10-24T14:43:42Z</published>
    <title>Design Of Rubble Analyzer Probe Using ML For Earthquake</title>
    <summary>  The earthquake rubble analyzer uses machine learning to detect human presence
via ambient sounds, achieving 97.45% accuracy. It also provides real-time
environmental data, aiding in assessing survival prospects for trapped
individuals, crucial for post-earthquake rescue efforts
</summary>
    <author>
      <name>Abhishek Sebastian</name>
    </author>
    <author>
      <name>R Pragna</name>
    </author>
    <author>
      <name>K Vishal Vythianathan</name>
    </author>
    <author>
      <name>Dasaraju Sohan Sai</name>
    </author>
    <author>
      <name>U Shiva Sri Hari Al</name>
    </author>
    <author>
      <name>R Anirudh</name>
    </author>
    <author>
      <name>Apurv Choudhary</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/5.0178244</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/5.0178244" rel="related"/>
    <link href="http://arxiv.org/abs/2311.02087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.02087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.05609v1</id>
    <updated>2023-11-09T18:59:24Z</updated>
    <published>2023-11-09T18:59:24Z</published>
    <title>What Do I Hear? Generating Sounds for Visuals with ChatGPT</title>
    <summary>  This short paper introduces a workflow for generating realistic soundscapes
for visual media. In contrast to prior work, which primarily focus on matching
sounds for on-screen visuals, our approach extends to suggesting sounds that
may not be immediately visible but are essential to crafting a convincing and
immersive auditory environment. Our key insight is leveraging the reasoning
capabilities of language models, such as ChatGPT. In this paper, we describe
our workflow, which includes creating a scene context, brainstorming sounds,
and generating the sounds.
</summary>
    <author>
      <name>David Chuan-En Lin</name>
    </author>
    <author>
      <name>Nikolas Martelaro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Demo: http://soundify.cc</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.05609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.05609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.10384v2</id>
    <updated>2023-12-28T15:08:07Z</updated>
    <published>2023-11-17T08:21:56Z</published>
    <title>Retrieval Augmented Generation of Symbolic Music with LLMs</title>
    <summary>  We explore the use of large language models (LLMs) for music generation using
a retrieval system to select relevant examples. We find promising initial
results for music generation in a dialogue with the user, especially
considering the ease with which such a system can be implemented. The code is
available online.
</summary>
    <author>
      <name>Nicolas Jonason</name>
    </author>
    <author>
      <name>Luca Casini</name>
    </author>
    <author>
      <name>Carl Thomé</name>
    </author>
    <author>
      <name>Bob L. T. Sturm</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LBD @ ISMIR 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.10384v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.10384v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.11363v1</id>
    <updated>2023-11-19T16:18:58Z</updated>
    <published>2023-11-19T16:18:58Z</published>
    <title>Encoding Performance Data in MEI with the Automatic Music Performance
  Analysis and Comparison Toolkit (AMPACT)</title>
    <summary>  This paper presents a new method of encoding performance data in MEI using
the recently added \texttt{&lt;extData&gt;} element. Performance data was extracted
using the Automatic Music Performance Analysis and Comparison Toolkit (AMPACT)
and encoded as a JSON object within an \texttt{&lt;extData&gt;} element linked to a
specific musical note. A set of pop music vocals has was encoded to demonstrate
both the range of descriptors that can be encoded in &lt;extData&gt; and how AMPACT
can be used for extracting performance data in the absence of a fully specified
musical score.
</summary>
    <author>
      <name>Johanna Devaney</name>
    </author>
    <author>
      <name>Cecilia Beauchamp</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Late-Breaking Demo Session of the 24th International Society for
  Music Information Retrieval Conference (2023)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2311.11363v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.11363v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.06055v1</id>
    <updated>2023-12-11T01:23:50Z</updated>
    <published>2023-12-11T01:23:50Z</published>
    <title>Speaker-Text Retrieval via Contrastive Learning</title>
    <summary>  In this study, we introduce a novel cross-modal retrieval task involving
speaker descriptions and their corresponding audio samples. Utilizing
pre-trained speaker and text encoders, we present a simple learning framework
based on contrastive learning. Additionally, we explore the impact of
incorporating speaker labels into the training process. Our findings establish
the effectiveness of linking speaker and text information for the task for both
English and Japanese languages, across diverse data configurations. Additional
visual analysis unveils potential nuanced associations between speaker
clustering and retrieval performance.
</summary>
    <author>
      <name>Xuechen Liu</name>
    </author>
    <author>
      <name>Xin Wang</name>
    </author>
    <author>
      <name>Erica Cooper</name>
    </author>
    <author>
      <name>Xiaoxiao Miao</name>
    </author>
    <author>
      <name>Junichi Yamagishi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE Signal Processing Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.06055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.06055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.14806v1</id>
    <updated>2023-12-22T16:27:12Z</updated>
    <published>2023-12-22T16:27:12Z</published>
    <title>The Effects of Signal-to-Noise Ratio on Generative Adversarial Networks
  Applied to Marine Bioacoustic Data</title>
    <summary>  In recent years generative adversarial networks (GANs) have been used to
supplement datasets within the field of marine bioacoustics. This is driven by
factors such as the cost to collect data, data sparsity and aid preprocessing.
One notable challenge with marine bioacoustic data is the low signal-to-noise
ratio (SNR) posing difficulty when applying deep learning techniques such as
GANs. This work investigates the effect SNR has on the audio-based GAN
performance and examines three different evaluation methodologies for GAN
performance, yielding interesting results on the effects of SNR on GANs,
specifically WaveGAN.
</summary>
    <author>
      <name>Georgia Atkinson</name>
    </author>
    <author>
      <name>Nick Wright</name>
    </author>
    <author>
      <name>A. Stephen McGough</name>
    </author>
    <author>
      <name>Per Berggren</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.14806v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.14806v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.12266v1</id>
    <updated>2024-01-22T10:32:18Z</updated>
    <published>2024-01-22T10:32:18Z</published>
    <title>An Exploratory Study of Multimodal Physiological Data in Jazz
  Improvisation Using Basic Machine Learning Techniques</title>
    <summary>  Our study delves into the "Embodied Musicking Dataset," exploring the
intertwined relationships and correlations between physiological and
psychological dimensions during improvisational music performances. The primary
objective is to ascertain the presence of a definitive causal or correlational
relationship between these states and comprehend their manifestation in musical
compositions. This rich dataset provides a perspective on how musicians
coordinate their physicality with sonic events in real-time improvisational
scenarios, emphasizing the concept of "Embodied Musicking."
</summary>
    <author>
      <name>Yawen Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Master's thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.12266v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.12266v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.14890v1</id>
    <updated>2024-01-26T14:15:20Z</updated>
    <published>2024-01-26T14:15:20Z</published>
    <title>Comparison of parameters of vowel sounds of russian and english
  languages</title>
    <summary>  In multilingual speech recognition systems, a situation can often arise when
the language is not known in advance, but the signal has already been received
and is being processed. For such cases, some generalized model is needed that
will be able to respond to phonetic differences and, depending on them,
correctly recog-nize speech in the desired language. To build such a model, it
is necessary to set the values of phonetic parameters, and then compare similar
sounds, establishing significant differences.
</summary>
    <author>
      <name>V. I. Fedoseev</name>
    </author>
    <author>
      <name>A. A. Konev</name>
    </author>
    <author>
      <name>A. Yu. Yakimuk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.14890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.14890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.17129v1</id>
    <updated>2024-01-29T06:05:23Z</updated>
    <published>2024-01-29T06:05:23Z</published>
    <title>Enhanced Sound Event Localization and Detection in Real 360-degree
  audio-visual soundscapes</title>
    <summary>  This technical report details our work towards building an enhanced
audio-visual sound event localization and detection (SELD) network. We build on
top of the audio-only SELDnet23 model and adapt it to be audio-visual by
merging both audio and video information prior to the gated recurrent unit
(GRU) of the audio-only network. Our model leverages YOLO and DETIC object
detectors. We also build a framework that implements audio-visual data
augmentation and audio-visual synthetic data generation. We deliver an
audio-visual SELDnet system that outperforms the existing audio-visual SELD
baseline.
</summary>
    <author>
      <name>Adrian S. Roman</name>
    </author>
    <author>
      <name>Baladithya Balamurugan</name>
    </author>
    <author>
      <name>Rithik Pothuganti</name>
    </author>
    <link href="http://arxiv.org/abs/2401.17129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.17129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.17738v2</id>
    <updated>2024-04-20T07:50:27Z</updated>
    <published>2024-01-31T10:58:59Z</published>
    <title>Harnessing Smartwatch Microphone Sensors for Cough Detection and
  Classification</title>
    <summary>  This study investigates the potential of using smartwatches with built-in
microphone sensors for monitoring coughs and detecting various cough types. We
conducted a study involving 32 participants and collected 9 hours of audio data
in a controlled manner. Afterward, we processed this data using a structured
approach, resulting in 223 positive cough samples. We further improved the
dataset through augmentation techniques and employed a specialized 1D CNN
model. This model achieved an impressive accuracy rate of 98.49% while
non-walking and 98.2% while walking, showing smartwatches can detect cough.
Moreover, our research successfully identified four distinct types of coughs
using clustering techniques.
</summary>
    <author>
      <name>Pranay Jaiswal</name>
    </author>
    <author>
      <name>Haroon R. Lone</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.17738v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.17738v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.10218v1</id>
    <updated>2024-01-04T08:11:47Z</updated>
    <published>2024-01-04T08:11:47Z</published>
    <title>AntiDeepFake: AI for Deep Fake Speech Recognition</title>
    <summary>  In this research study, we propose a modern artificial intelligence (AI)
approach to recognize deepfake voice, also known as generative AI cloned
synthetic voice. Our proposed AI technology, called AntiDeepFake, consists of
all main pipelines from data to evaluation in the whole picture. We provide
experimental results and scores for all our proposed methods. The main source
code for our approach is available in the provided link:
https://github.com/enkhtogtokh/antideepfake repository.
</summary>
    <author>
      <name>Enkhtogtokh Togootogtokh</name>
    </author>
    <author>
      <name>Christian Klasen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2308.12734 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.10218v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.10218v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.10547v1</id>
    <updated>2024-02-16T10:20:42Z</updated>
    <published>2024-02-16T10:20:42Z</published>
    <title>Learning Disentangled Audio Representations through Controlled Synthesis</title>
    <summary>  This paper tackles the scarcity of benchmarking data in disentangled auditory
representation learning. We introduce SynTone, a synthetic dataset with
explicit ground truth explanatory factors for evaluating disentanglement
techniques. Benchmarking state-of-the-art methods on SynTone highlights its
utility for method evaluation. Our results underscore strengths and limitations
in audio disentanglement, motivating future research.
</summary>
    <author>
      <name>Yusuf Brima</name>
    </author>
    <author>
      <name>Ulf Krumnack</name>
    </author>
    <author>
      <name>Simone Pika</name>
    </author>
    <author>
      <name>Gunther Heidemann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 12 figures, accepted as a Tiny paper at ICLR 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.10547v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.10547v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.11931v1</id>
    <updated>2024-02-19T08:18:52Z</updated>
    <published>2024-02-19T08:18:52Z</published>
    <title>Soft-Weighted CrossEntropy Loss for Continous Alzheimer's Disease
  Detection</title>
    <summary>  Alzheimer's disease is a common cognitive disorder in the elderly. Early and
accurate diagnosis of Alzheimer's disease (AD) has a major impact on the
progress of research on dementia. At present, researchers have used machine
learning methods to detect Alzheimer's disease from the speech of participants.
However, the recognition accuracy of current methods is unsatisfactory, and
most of them focus on using low-dimensional handcrafted features to extract
relevant information from audios. This paper proposes an Alzheimer's disease
detection system based on the pre-trained framework Wav2vec 2.0 (Wav2vec2). In
addition, by replacing the loss function with the Soft-Weighted CrossEntropy
loss function, we achieved 85.45\% recognition accuracy on the same test
dataset.
</summary>
    <author>
      <name>Xiaohui Zhang</name>
    </author>
    <author>
      <name>Wenjie Fu</name>
    </author>
    <author>
      <name>Mangui Liang</name>
    </author>
    <link href="http://arxiv.org/abs/2402.11931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.11931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.14982v3</id>
    <updated>2024-07-09T02:21:21Z</updated>
    <published>2024-02-22T21:44:58Z</published>
    <title>Human Brain Exhibits Distinct Patterns When Listening to Fake Versus
  Real Audio: Preliminary Evidence</title>
    <summary>  In this paper we study the variations in human brain activity when listening
to real and fake audio. Our preliminary results suggest that the
representations learned by a state-of-the-art deepfake audio detection
algorithm, do not exhibit clear distinct patterns between real and fake audio.
In contrast, human brain activity, as measured by EEG, displays distinct
patterns when individuals are exposed to fake versus real audio. This
preliminary evidence enables future research directions in areas such as
deepfake audio detection.
</summary>
    <author>
      <name>Mahsa Salehi</name>
    </author>
    <author>
      <name>Kalin Stefanov</name>
    </author>
    <author>
      <name>Ehsan Shareghi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.14982v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.14982v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.16927v1</id>
    <updated>2024-02-26T13:26:20Z</updated>
    <published>2024-02-26T13:26:20Z</published>
    <title>The ICASSP 2024 Audio Deep Packet Loss Concealment Challenge</title>
    <summary>  Audio packet loss concealment is the hiding of gaps in VoIP audio streams
caused by network packet loss. With the ICASSP 2024 Audio Deep Packet Loss
Concealment Grand Challenge, we build on the success of the previous Audio PLC
Challenge held at INTERSPEECH 2022. We evaluate models on an overall harder
dataset, and use the new ITU-T P.804 evaluation procedure to more closely
evaluate the performance of systems specifically on the PLC task. We evaluate a
total of 9 systems, 8 of which satisfy the strict real-time performance
requirements of the challenge, using both P.804 and Word Accuracy evaluations.
</summary>
    <author>
      <name>Lorenz Diener</name>
    </author>
    <author>
      <name>Solomiya Branets</name>
    </author>
    <author>
      <name>Ando Saabas</name>
    </author>
    <author>
      <name>Ross Cutler</name>
    </author>
    <link href="http://arxiv.org/abs/2402.16927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.16927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.01785v1</id>
    <updated>2024-03-04T07:27:25Z</updated>
    <published>2024-03-04T07:27:25Z</published>
    <title>What do neural networks listen to? Exploring the crucial bands in Speech
  Enhancement using Sinc-convolution</title>
    <summary>  This study introduces a reformed Sinc-convolution (Sincconv) framework
tailored for the encoder component of deep networks for speech enhancement
(SE). The reformed Sincconv, based on parametrized sinc functions as band-pass
filters, offers notable advantages in terms of training efficiency, filter
diversity, and interpretability. The reformed Sinc-conv is evaluated in
conjunction with various SE models, showcasing its ability to boost SE
performance. Furthermore, the reformed Sincconv provides valuable insights into
the specific frequency components that are prioritized in an SE scenario. This
opens up a new direction of SE research and improving our knowledge of their
operating dynamics.
</summary>
    <author>
      <name>Kuan-Hsun Ho</name>
    </author>
    <author>
      <name>Jeih-weih Hung</name>
    </author>
    <author>
      <name>Berlin Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2403.01785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.01785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.01792v1</id>
    <updated>2024-03-04T07:34:24Z</updated>
    <published>2024-03-04T07:34:24Z</published>
    <title>ConSep: a Noise- and Reverberation-Robust Speech Separation Framework by
  Magnitude Conditioning</title>
    <summary>  Speech separation has recently made significant progress thanks to the
fine-grained vision used in time-domain methods. However, several studies have
shown that adopting Short-Time Fourier Transform (STFT) for feature extraction
could be beneficial when encountering harsher conditions, such as noise or
reverberation. Therefore, we propose a magnitude-conditioned time-domain
framework, ConSep, to inherit the beneficial characteristics. The experiment
shows that ConSep promotes performance in anechoic, noisy, and reverberant
settings compared to two celebrated methods, SepFormer and Bi-Sep. Furthermore,
we visualize the components of ConSep to strengthen the advantages and cohere
with the actualities we have found in preliminary studies.
</summary>
    <author>
      <name>Kuan-Hsun Ho</name>
    </author>
    <author>
      <name>Jeih-weih Hung</name>
    </author>
    <author>
      <name>Berlin Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2403.01792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.01792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.04111v2</id>
    <updated>2024-04-03T07:40:54Z</updated>
    <published>2024-03-06T23:47:48Z</published>
    <title>Multi-Level Attention Aggregation for Language-Agnostic Speaker
  Replication</title>
    <summary>  This paper explores the task of language-agnostic speaker replication, a
novel endeavor that seeks to replicate a speaker's voice irrespective of the
language they are speaking. Towards this end, we introduce a multi-level
attention aggregation approach that systematically probes and amplifies various
speaker-specific attributes in a hierarchical manner. Through rigorous
evaluations across a wide range of scenarios including seen and unseen speakers
conversing in seen and unseen lingua, we establish that our proposed model is
able to achieve substantial speaker similarity, and is able to generalize to
out-of-domain (OOD) cases.
</summary>
    <author>
      <name>Yejin Jeon</name>
    </author>
    <author>
      <name>Gary Geunbae Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EACL Main 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.04111v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.04111v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.17562v1</id>
    <updated>2024-03-26T10:10:56Z</updated>
    <published>2024-03-26T10:10:56Z</published>
    <title>Deep functional multiple index models with an application to SER</title>
    <summary>  Speech Emotion Recognition (SER) plays a crucial role in advancing
human-computer interaction and speech processing capabilities. We introduce a
novel deep-learning architecture designed specifically for the functional data
model known as the multiple-index functional model. Our key innovation lies in
integrating adaptive basis layers and an automated data transformation search
within the deep learning framework. Simulations for this new model show good
performances. This allows us to extract features tailored for chunk-level SER,
based on Mel Frequency Cepstral Coefficients (MFCCs). We demonstrate the
effectiveness of our approach on the benchmark IEMOCAP database, achieving good
performance compared to existing methods.
</summary>
    <author>
      <name>Matthieu Saumard</name>
    </author>
    <author>
      <name>Abir El Haj</name>
    </author>
    <author>
      <name>Thibault Napoleon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.17562v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.17562v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.19763v1</id>
    <updated>2024-03-28T18:24:09Z</updated>
    <published>2024-03-28T18:24:09Z</published>
    <title>Creating Aesthetic Sonifications on the Web with SIREN</title>
    <summary>  SIREN is a flexible, extensible, and customizable web-based general-purpose
interface for auditory data display (sonification). Designed as a digital audio
workstation for sonification, synthesizers written in JavaScript using the Web
Audio API facilitate intuitive mapping of data to auditory parameters for a
wide range of purposes.
  This paper explores the breadth of sound synthesis techniques supported by
SIREN, and details the structure and definition of a SIREN synthesizer module.
The paper proposes further development that will increase SIREN's utility.
</summary>
    <author>
      <name>Tristan Peng</name>
    </author>
    <author>
      <name>Hongchan Choi</name>
    </author>
    <author>
      <name>Jonathan Berger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure, 5 listings, submitted to the Web Audio Conference
  2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.19763v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.19763v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.03894v1</id>
    <updated>2024-04-05T05:03:39Z</updated>
    <published>2024-04-05T05:03:39Z</published>
    <title>Holon: a cybernetic interface for bio-semiotics</title>
    <summary>  This paper presents an interactive artwork, "Holon", a collection of 130
autonomous, cybernetic organisms that listen and make sound in collaboration
with the natural environment. The work was developed for installation on water
at a heritage-listed dock in Melbourne, Australia. Conceptual issues informing
the work are presented, along with a detailed technical overview of the
implementation. Individual holons are of three types, inspired by biological
models of animal communication: composer/generators, collector/critics and
disruptors. Collectively, Holon integrates and occupies elements of the
acoustic spectrum in collaboration with human and non-human agents.
</summary>
    <author>
      <name>Jon McCormack</name>
    </author>
    <author>
      <name>Elliott Wilson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper accepted at ISEA 24, The 29th International Symposium on
  Electronic Art, Brisbane, Australia, 21-29 June 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.03894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.03894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.11; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.11116v1</id>
    <updated>2024-04-17T07:01:29Z</updated>
    <published>2024-04-17T07:01:29Z</published>
    <title>Music Enhancement with Deep Filters: A Technical Report for The ICASSP
  2024 Cadenza Challenge</title>
    <summary>  In this challenge, we disentangle the deep filters from the original
DeepfilterNet and incorporate them into our Spec-UNet-based network to further
improve a hybrid Demucs (hdemucs) based remixing pipeline. The motivation
behind the use of the deep filter component lies at its potential in better
handling temporal fine structures. We demonstrate an incremental improvement in
both the Signal-to-Distortion Ratio (SDR) and the Hearing Aid Audio Quality
Index (HAAQI) metrics when comparing the performance of hdemucs against
different versions of our model.
</summary>
    <author>
      <name>Keren Shao</name>
    </author>
    <author>
      <name>Ke Chen</name>
    </author>
    <author>
      <name>Shlomo Dubnov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures, 1 tables, Proceedings of the International
  Conference on Acoustics, Speech, and Signal Processing, ICASSP 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.11116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.11116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.13286v1</id>
    <updated>2024-04-20T06:22:07Z</updated>
    <published>2024-04-20T06:22:07Z</published>
    <title>Track Role Prediction of Single-Instrumental Sequences</title>
    <summary>  In the composition process, selecting appropriate single-instrumental music
sequences and assigning their track-role is an indispensable task. However,
manually determining the track-role for a myriad of music samples can be
time-consuming and labor-intensive. This study introduces a deep learning model
designed to automatically predict the track-role of single-instrumental music
sequences. Our evaluations show a prediction accuracy of 87% in the symbolic
domain and 84% in the audio domain. The proposed track-role prediction methods
hold promise for future applications in AI music generation and analysis.
</summary>
    <author>
      <name>Changheon Han</name>
    </author>
    <author>
      <name>Suhyun Lee</name>
    </author>
    <author>
      <name>Minsam Ko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISMIR LBD 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.13286v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.13286v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.13428v1</id>
    <updated>2024-04-20T17:26:59Z</updated>
    <published>2024-04-20T17:26:59Z</published>
    <title>Text-dependent Speaker Verification (TdSV) Challenge 2024: Challenge
  Evaluation Plan</title>
    <summary>  This document outlines the Text-dependent Speaker Verification (TdSV)
Challenge 2024, which centers on analyzing and exploring novel approaches for
text-dependent speaker verification. The primary goal of this challenge is to
motive participants to develop single yet competitive systems, conduct thorough
analyses, and explore innovative concepts such as multi-task learning,
self-supervised learning, few-shot learning, and others, for text-dependent
speaker verification.
</summary>
    <author>
      <name>Zeinali Hossein</name>
    </author>
    <author>
      <name>Lee Kong Aik</name>
    </author>
    <author>
      <name>Alam Jahangir</name>
    </author>
    <author>
      <name>Burget Lukas</name>
    </author>
    <link href="http://arxiv.org/abs/2404.13428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.13428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.18002v2</id>
    <updated>2024-06-07T05:58:16Z</updated>
    <published>2024-04-27T20:36:52Z</published>
    <title>Towards Privacy-Preserving Audio Classification Systems</title>
    <summary>  Audio signals can reveal intimate details about a person's life, including
their conversations, health status, emotions, location, and personal
preferences. Unauthorized access or misuse of this information can have
profound personal and social implications. In an era increasingly populated by
devices capable of audio recording, safeguarding user privacy is a critical
obligation. This work studies the ethical and privacy concerns in current audio
classification systems. We discuss the challenges and research directions in
designing privacy-preserving audio sensing systems. We propose
privacy-preserving audio features that can be used to classify wide range of
audio classes, while being privacy preserving.
</summary>
    <author>
      <name>Bhawana Chhaglani</name>
    </author>
    <author>
      <name>Jeremy Gummeson</name>
    </author>
    <author>
      <name>Prashant Shenoy</name>
    </author>
    <link href="http://arxiv.org/abs/2404.18002v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.18002v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.02333v1</id>
    <updated>2024-04-26T09:44:26Z</updated>
    <published>2024-04-26T09:44:26Z</published>
    <title>Speech Technology Services for Oral History Research</title>
    <summary>  Oral history is about oral sources of witnesses and commentors on historical
events. Speech technology is an important instrument to process such recordings
in order to obtain transcription and further enhancements to structure the oral
account In this contribution we address the transcription portal and the
webservices associated with speech processing at BAS, speech solutions
developed at LINDAT, how to do it yourself with Whisper, remaining challenges,
and future developments.
</summary>
    <author>
      <name>Christoph Draxler</name>
    </author>
    <author>
      <name>Henk van den Heuvel</name>
    </author>
    <author>
      <name>Arjan van Hessen</name>
    </author>
    <author>
      <name>Pavel Ircing</name>
    </author>
    <author>
      <name>Jan Lehečka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages plus references, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.02333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.02333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.05240v1</id>
    <updated>2024-05-08T17:36:29Z</updated>
    <published>2024-05-08T17:36:29Z</published>
    <title>An LSTM-Based Chord Generation System Using Chroma Histogram
  Representations</title>
    <summary>  This paper proposes a system for chord generation to monophonic symbolic
melodies using an LSTM-based model trained on chroma histogram representations
of chords. Chroma representations promise more harmonically rich generation
than chord label-based approaches, whilst maintaining a small number of
dimensions in the dataset. This system is shown to be suitable for limited
real-time use. While it does not meet the state-of-the-art for coherent
long-term generation, it does show diatonic generation with cadential chord
relationships. The need for further study into chroma histograms as an
extracted feature in chord generation tasks is highlighted.
</summary>
    <author>
      <name>Jack Hardwick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.05240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.05240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.07034v1</id>
    <updated>2024-05-11T15:17:27Z</updated>
    <published>2024-05-11T15:17:27Z</published>
    <title>Towards an Accessible and Rapidly Trainable Rhythm Sequencer Using a
  Generative Stacked Autoencoder</title>
    <summary>  Neural networks and deep learning are often deployed for the sake of the most
comprehensive music generation with as little involvement as possible from the
human musician. Implementations in aid of, or being a tool for, music
practitioners are sparse. This paper proposes the integration of generative
stacked autoencoder structures for rhythm generation, within a conventional
melodic step-sequencer. It further aims to work towards its implementation
being accessible to the average electronic music practitioner. Several model
architectures have been trained and tested for their creative potential. While
the currently implementations do display limitations, they do represent viable
creative solutions for music practitioners.
</summary>
    <author>
      <name>Alex Wastnidge</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.07034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.07034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.09241v1</id>
    <updated>2024-05-15T10:41:47Z</updated>
    <published>2024-05-15T10:41:47Z</published>
    <title>SMUG-Explain: A Framework for Symbolic Music Graph Explanations</title>
    <summary>  In this work, we present Score MUsic Graph (SMUG)-Explain, a framework for
generating and visualizing explanations of graph neural networks applied to
arbitrary prediction tasks on musical scores. Our system allows the user to
visualize the contribution of input notes (and note features) to the network
output, directly in the context of the musical score. We provide an interactive
interface based on the music notation engraving library Verovio. We showcase
the usage of SMUG-Explain on the task of cadence detection in classical music.
All code is available on https://github.com/manoskary/SMUG-Explain.
</summary>
    <author>
      <name>Emmanouil Karystinaios</name>
    </author>
    <author>
      <name>Francesco Foscarin</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the Sound and Music Computing Conference 2024
  (SMC2024), Porto, Portugal</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.09241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.09241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.12666v1</id>
    <updated>2024-05-21T10:27:34Z</updated>
    <published>2024-05-21T10:27:34Z</published>
    <title>SYMPLEX: Controllable Symbolic Music Generation using Simplex Diffusion
  with Vocabulary Priors</title>
    <summary>  We present a new approach for fast and controllable generation of symbolic
music based on the simplex diffusion, which is essentially a diffusion process
operating on probabilities rather than the signal space. This objective has
been applied in domains such as natural language processing but here we apply
it to generating 4-bar multi-instrument music loops using an orderless
representation. We show that our model can be steered with vocabulary priors,
which affords a considerable level control over the music generation process,
for instance, infilling in time and pitch and choice of instrumentation -- all
without task-specific model adaptation or applying extrinsic control.
</summary>
    <author>
      <name>Nicolas Jonason</name>
    </author>
    <author>
      <name>Luca Casini</name>
    </author>
    <author>
      <name>Bob L. T. Sturm</name>
    </author>
    <link href="http://arxiv.org/abs/2405.12666v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.12666v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.13636v1</id>
    <updated>2024-05-22T13:35:56Z</updated>
    <published>2024-05-22T13:35:56Z</published>
    <title>Audio Mamba: Pretrained Audio State Space Model For Audio Tagging</title>
    <summary>  Audio tagging is an important task of mapping audio samples to their
corresponding categories. Recently endeavours that exploit transformer models
in this field have achieved great success. However, the quadratic
self-attention cost limits the scaling of audio transformer models and further
constrains the development of more universal audio models. In this paper, we
attempt to solve this problem by proposing Audio Mamba, a self-attention-free
approach that captures long audio spectrogram dependency with state space
models. Our experimental results on two audio-tagging datasets demonstrate the
parameter efficiency of Audio Mamba, it achieves comparable results to SOTA
audio spectrogram transformers with one third parameters.
</summary>
    <author>
      <name>Jiaju Lin</name>
    </author>
    <author>
      <name>Haoxuan Hu</name>
    </author>
    <link href="http://arxiv.org/abs/2405.13636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.13636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.15338v1</id>
    <updated>2024-05-24T08:18:58Z</updated>
    <published>2024-05-24T08:18:58Z</published>
    <title>SoundLoCD: An Efficient Conditional Discrete Contrastive Latent
  Diffusion Model for Text-to-Sound Generation</title>
    <summary>  We present SoundLoCD, a novel text-to-sound generation framework, which
incorporates a LoRA-based conditional discrete contrastive latent diffusion
model. Unlike recent large-scale sound generation models, our model can be
efficiently trained under limited computational resources. The integration of a
contrastive learning strategy further enhances the connection between text
conditions and the generated outputs, resulting in coherent and high-fidelity
performance. Our experiments demonstrate that SoundLoCD outperforms the
baseline with greatly reduced computational resources. A comprehensive ablation
study further validates the contribution of each component within SoundLoCD.
Demo page: \url{https://XinleiNIU.github.io/demo-SoundLoCD/}.
</summary>
    <author>
      <name>Xinlei Niu</name>
    </author>
    <author>
      <name>Jing Zhang</name>
    </author>
    <author>
      <name>Christian Walder</name>
    </author>
    <author>
      <name>Charles Patrick Martin</name>
    </author>
    <link href="http://arxiv.org/abs/2405.15338v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.15338v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.04595v1</id>
    <updated>2024-06-07T02:59:58Z</updated>
    <published>2024-06-07T02:59:58Z</published>
    <title>Pitch-Aware RNN-T for Mandarin Chinese Mispronunciation Detection and
  Diagnosis</title>
    <summary>  Mispronunciation Detection and Diagnosis (MDD) systems, leveraging Automatic
Speech Recognition (ASR), face two main challenges in Mandarin Chinese: 1) The
two-stage models create an information gap between the phoneme or tone
classification stage and the MDD stage. 2) The scarcity of Mandarin MDD
datasets limits model training. In this paper, we introduce a stateless RNN-T
model for Mandarin MDD, utilizing HuBERT features with pitch embedding through
a Pitch Fusion Block. Our model, trained solely on native speaker data, shows a
3% improvement in Phone Error Rate and a 7% increase in False Acceptance Rate
over the state-of-the-art baseline in non-native scenarios
</summary>
    <author>
      <name>Xintong Wang</name>
    </author>
    <author>
      <name>Mingqian Shi</name>
    </author>
    <author>
      <name>Ye Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at Interspeech 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.04595v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.04595v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.05653v1</id>
    <updated>2024-06-09T05:30:05Z</updated>
    <published>2024-06-09T05:30:05Z</published>
    <title>Heart Sound Segmentation Using Deep Learning Techniques</title>
    <summary>  Heart disease remains a leading cause of mortality worldwide. Auscultation,
the process of listening to heart sounds, can be enhanced through
computer-aided analysis using Phonocardiogram (PCG) signals. This paper
presents a novel approach for heart sound segmentation and classification into
S1 (LUB) and S2 (DUB) sounds. We employ FFT-based filtering, dynamic
programming for event detection, and a Siamese network for robust
classification. Our method demonstrates superior performance on the PASCAL
heart sound dataset compared to existing approaches.
</summary>
    <author>
      <name>Manas Madine</name>
    </author>
    <link href="http://arxiv.org/abs/2406.05653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.05653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.05863v1</id>
    <updated>2024-06-09T17:27:20Z</updated>
    <published>2024-06-09T17:27:20Z</published>
    <title>Source -Free Domain Adaptation for Speaker Verification in Data-Scarce
  Languages and Noisy Channels</title>
    <summary>  Domain adaptation is often hampered by exceedingly small target datasets and
inaccessible source data. These conditions are prevalent in speech
verification, where privacy policies and/or languages with scarce speech
resources limit the availability of sufficient data. This paper explored
techniques of sourcefree domain adaptation unto a limited target speech dataset
for speaker verificationin data-scarce languages. Both language and channel
mis-match between source and target were investigated. Fine-tuning methods were
evaluated and compared across different sizes of labeled target data. A novel
iterative cluster-learn algorithm was studied for unlabeled target datasets.
</summary>
    <author>
      <name>Shlomo Salo Elia</name>
    </author>
    <author>
      <name>Aviad Malachi</name>
    </author>
    <author>
      <name>Vered Aharonson</name>
    </author>
    <author>
      <name>Gadi Pinkas</name>
    </author>
    <link href="http://arxiv.org/abs/2406.05863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.05863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.17103v2</id>
    <updated>2024-07-15T03:22:05Z</updated>
    <published>2024-06-24T19:42:22Z</published>
    <title>Maximum Likelihood Estimation of the Direction of Sound In A Reverberant
  Noisy Environment</title>
    <summary>  We describe a new method for estimating the direction of sound in a
reverberant environment from basic principles of sound propagation. The method
utilizes SNR-adaptive features from time-delay and energy of the directional
components after acoustic wave decomposition of the observed sound field to
estimate the line-of-sight direction under noisy and reverberant conditions.
The effectiveness of the approach is established with measured data of
different microphone array configurations under various usage scenarios.
</summary>
    <author>
      <name>Mohamed F. Mansour</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.17103v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.17103v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.17111v2</id>
    <updated>2024-07-13T02:50:50Z</updated>
    <published>2024-06-24T19:52:48Z</published>
    <title>Sound Field Synthesis with Acoustic Waves</title>
    <summary>  We propose a practical framework to synthesize the broadband sound-field on a
small rigid surface based on the physics of sound propagation. The sound-field
is generated as a composite map of two components: the room component and the
device component, with acoustic plane waves as the core tool for the
generation. This decoupling of room and device components significantly reduces
the problem complexity and provides accurate rendering of the sound-field.
  We describe in detail the theoretical foundations, and efficient procedures
of the implementation. The effectiveness of the proposed framework is
established through rigorous validation under different environment setups.
</summary>
    <author>
      <name>Mohamed F. Mansour</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, double-spaced, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.17111v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.17111v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.01452v1</id>
    <updated>2024-07-01T16:46:39Z</updated>
    <published>2024-07-01T16:46:39Z</published>
    <title>On Feature Learning for Titi Monkey Activity Detection</title>
    <summary>  This paper, a technical summary of our preceding publication, introduces a
robust machine learning framework for the detection of vocal activities of
Coppery titi monkeys. Utilizing a combination of MFCC features and a
bidirectional LSTM-based classifier, we effectively address the challenges
posed by the small amount of expert-annotated vocal data available. Our
approach significantly reduces false positives and improves the accuracy of
call detection in bioacoustic research. Initial results demonstrate an accuracy
of 95\% on instance predictions, highlighting the effectiveness of our model in
identifying and classifying complex vocal patterns in environmental audio
recordings. Moreover, we show how call classification can be done downstream,
paving the way for real-world monitoring.
</summary>
    <author>
      <name>Aditya Ravuri</name>
    </author>
    <author>
      <name>Jen Muir</name>
    </author>
    <author>
      <name>Neil D. Lawrence</name>
    </author>
    <link href="http://arxiv.org/abs/2407.01452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.01452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.01860v3</id>
    <updated>2024-11-23T14:43:33Z</updated>
    <published>2024-07-02T00:13:07Z</published>
    <title>Constant Directivity Loudspeaker Beamforming</title>
    <summary>  Loudspeaker array beamforming is a common signal processing technique for
acoustic directivity control and robust audio reproduction. Unlike their
microphone counterpart, loudspeaker constraints are often heterogeneous due to
arrayed transducers with varying operating ranges in frequency,
acoustic-electrical sensitivity, efficiency, and directivity. This work
proposes a frequency-regularization method for generalized Rayleigh quotient
directivity specifications and two novel beamformer designs that optimize for
maximum efficiency constant directivity (MECD) and maximum sensitivity constant
directivity (MSCD). We derive fast converging and analytic solutions from their
quadratic equality constrained quadratic program formulations. Experiments
optimize generalized directivity index constrained beamformer designs for a
full-band heterogeneous array.
</summary>
    <author>
      <name>Yuancheng Luo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.23919/EUSIPCO63174.2024.10715159</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.23919/EUSIPCO63174.2024.10715159" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at EUSIPCO 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.01860v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.01860v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.07598v1</id>
    <updated>2024-07-10T12:31:53Z</updated>
    <published>2024-07-10T12:31:53Z</published>
    <title>Targeted Augmented Data for Audio Deepfake Detection</title>
    <summary>  The availability of highly convincing audio deepfake generators highlights
the need for designing robust audio deepfake detectors. Existing works often
rely solely on real and fake data available in the training set, which may lead
to overfitting, thereby reducing the robustness to unseen manipulations. To
enhance the generalization capabilities of audio deepfake detectors, we propose
a novel augmentation method for generating audio pseudo-fakes targeting the
decision boundary of the model. Inspired by adversarial attacks, we perturb
original real data to synthesize pseudo-fakes with ambiguous prediction
probabilities. Comprehensive experiments on two well-known architectures
demonstrate that the proposed augmentation contributes to improving the
generalization capabilities of these architectures.
</summary>
    <author>
      <name>Marcella Astrid</name>
    </author>
    <author>
      <name>Enjie Ghorbel</name>
    </author>
    <author>
      <name>Djamila Aouada</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in EUSIPCO 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.07598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.07598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.08951v1</id>
    <updated>2024-07-12T03:10:25Z</updated>
    <published>2024-07-12T03:10:25Z</published>
    <title>Audio Spotforming Using Nonnegative Tensor Factorization with
  Attractor-Based Regularization</title>
    <summary>  Spotforming is a target-speaker extraction technique that uses multiple
microphone arrays. This method applies beamforming (BF) to each microphone
array, and the common components among the BF outputs are estimated as the
target source. This study proposes a new common component extraction method
based on nonnegative tensor factorization (NTF) for higher model
interpretability and more robust spotforming against hyperparameters. Moreover,
attractor-based regularization was introduced to facilitate the automatic
selection of optimal target bases in the NTF. Experimental results show that
the proposed method performs better than conventional methods in spotforming
performance and also shows some characteristics suitable for practical use.
</summary>
    <author>
      <name>Shoma Ayano</name>
    </author>
    <author>
      <name>Li Li</name>
    </author>
    <author>
      <name>Shogo Seki</name>
    </author>
    <author>
      <name>Daichi Kitamura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at EUSIPCO2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.08951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.08951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.10182v1</id>
    <updated>2024-07-14T12:52:35Z</updated>
    <published>2024-07-14T12:52:35Z</published>
    <title>Few-Shot Bioacoustic Event Detection with Frame-Level Embedding Learning
  System</title>
    <summary>  This technical report presents our frame-level embedding learning system for
the DCASE2024 challenge for few-shot bioacoustic event detection (Task 5).In
this work, we used log-mel and PCEN for feature extraction of the input audio,
Netmamba Encoder as the information interaction network, and adopted data
augmentation strategies to improve the generalizability of the trained model as
well as multiple post-processing methods. Our final system achieved an
F-measure score of 56.4%, securing the 2nd rank in the few-shot bioacoustic
event detection category of the Detection and Classification of Acoustic Scenes
and Events Challenge 2024.
</summary>
    <author>
      <name>PengYuan Zhao</name>
    </author>
    <author>
      <name>ChengWei Lu</name>
    </author>
    <author>
      <name>Liang Zou</name>
    </author>
    <link href="http://arxiv.org/abs/2407.10182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.10182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.10328v1</id>
    <updated>2024-07-14T20:51:08Z</updated>
    <published>2024-07-14T20:51:08Z</published>
    <title>The Interpretation Gap in Text-to-Music Generation Models</title>
    <summary>  Large-scale text-to-music generation models have significantly enhanced music
creation capabilities, offering unprecedented creative freedom. However, their
ability to collaborate effectively with human musicians remains limited. In
this paper, we propose a framework to describe the musical interaction process,
which includes expression, interpretation, and execution of controls. Following
this framework, we argue that the primary gap between existing text-to-music
models and musicians lies in the interpretation stage, where models lack the
ability to interpret controls from musicians. We also propose two strategies to
address this gap and call on the music information retrieval community to
tackle the interpretation challenge to improve human-AI musical collaboration.
</summary>
    <author>
      <name>Yongyi Zang</name>
    </author>
    <author>
      <name>Yixiao Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.10328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.10328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.17172v1</id>
    <updated>2024-07-24T11:22:57Z</updated>
    <published>2024-07-24T11:22:57Z</published>
    <title>Speech Editing -- a Summary</title>
    <summary>  With the rise of video production and social media, speech editing has become
crucial for creators to address issues like mispronunciations, missing words,
or stuttering in audio recordings. This paper explores text-based speech
editing methods that modify audio via text transcripts without manual waveform
editing. These approaches ensure edited audio is indistinguishable from the
original by altering the mel-spectrogram. Recent advancements, such as
context-aware prosody correction and advanced attention mechanisms, have
improved speech editing quality. This paper reviews state-of-the-art methods,
compares key metrics, and examines widely used datasets. The aim is to
highlight ongoing issues and inspire further research and innovation in speech
editing.
</summary>
    <author>
      <name>Tobias Kässmann</name>
    </author>
    <author>
      <name>Yining Liu</name>
    </author>
    <author>
      <name>Danni Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2407.17172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.17172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.03204v1</id>
    <updated>2024-08-06T14:03:06Z</updated>
    <published>2024-08-06T14:03:06Z</published>
    <title>GRAFX: An Open-Source Library for Audio Processing Graphs in PyTorch</title>
    <summary>  We present GRAFX, an open-source library designed for handling audio
processing graphs in PyTorch. Along with various library functionalities, we
describe technical details on the efficient parallel computation of input
graphs, signals, and processor parameters in GPU. Then, we show its example use
under a music mixing scenario, where parameters of every differentiable
processor in a large graph are optimized via gradient descent. The code is
available at https://github.com/sh-lee97/grafx.
</summary>
    <author>
      <name>Sungho Lee</name>
    </author>
    <author>
      <name>Marco Martínez-Ramírez</name>
    </author>
    <author>
      <name>Wei-Hsiang Liao</name>
    </author>
    <author>
      <name>Stefan Uhlich</name>
    </author>
    <author>
      <name>Giorgio Fabbro</name>
    </author>
    <author>
      <name>Kyogu Lee</name>
    </author>
    <author>
      <name>Yuki Mitsufuji</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to DAFx 2024 demo</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.03204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.03204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.06053v1</id>
    <updated>2024-08-12T10:59:45Z</updated>
    <published>2024-08-12T10:59:45Z</published>
    <title>PyNeuralFx: A Python Package for Neural Audio Effect Modeling</title>
    <summary>  We present PyNeuralFx, an open-source Python toolkit designed for research on
neural audio effect modeling. The toolkit provides an intuitive framework and
offers a comprehensive suite of features, including standardized implementation
of well-established model architectures, loss functions, and easy-to-use
visualization tools. As such, it helps promote reproducibility for research on
neural audio effect modeling, and enable in-depth performance comparison of
different models, offering insight into the behavior and operational
characteristics of models through DSP methodology. The toolkit can be found at
https://github.com/ytsrt66589/pyneuralfx.
</summary>
    <author>
      <name>Yen-Tung Yeh</name>
    </author>
    <author>
      <name>Wen-Yi Hsiao</name>
    </author>
    <author>
      <name>Yi-Hsuan Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">toolkit paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.06053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.06053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.06804v1</id>
    <updated>2024-08-13T10:46:50Z</updated>
    <published>2024-08-13T10:46:50Z</published>
    <title>Deep Learning for Speaker Identification: Architectural Insights from
  AB-1 Corpus Analysis and Performance Evaluation</title>
    <summary>  In the fields of security systems, forensic investigations, and personalized
services, the importance of speech as a fundamental human input outweighs
text-based interactions. This research delves deeply into the complex field of
Speaker Identification (SID), examining its essential components and
emphasising Mel Spectrogram and Mel Frequency Cepstral Coefficients (MFCC) for
feature extraction. Moreover, this study evaluates six slightly distinct model
architectures using extensive analysis to evaluate their performance, with
hyperparameter tuning applied to the best-performing model. This work performs
a linguistic analysis to verify accent and gender accuracy, in addition to bias
evaluation within the AB-1 Corpus dataset.
</summary>
    <author>
      <name>Matthias Bartolo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Resultant work from Assignment, Department of AI, University of
  Malta. Code available at: https://github.com/mbar0075/Speech-Technology</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.06804v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.06804v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.07020v1</id>
    <updated>2024-08-12T17:30:17Z</updated>
    <published>2024-08-12T17:30:17Z</published>
    <title>Source Separation of Multi-source Raw Music using a Residual Quantized
  Variational Autoencoder</title>
    <summary>  I developed a neural audio codec model based on the residual quantized
variational autoencoder architecture. I train the model on the Slakh2100
dataset, a standard dataset for musical source separation, composed of
multi-track audio. The model can separate audio sources, achieving almost SoTA
results with much less computing power. The code is publicly available at
github.com/LeonardoBerti00/Source-Separation-of-Multi-source-Music-using-Residual-Quantizad-Variational-Autoencoder
</summary>
    <author>
      <name>Leonardo Berti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.07020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.07020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.09802v1</id>
    <updated>2024-08-19T08:47:03Z</updated>
    <published>2024-08-19T08:47:03Z</published>
    <title>Hear Your Face: Face-based voice conversion with F0 estimation</title>
    <summary>  This paper delves into the emerging field of face-based voice conversion,
leveraging the unique relationship between an individual's facial features and
their vocal characteristics. We present a novel face-based voice conversion
framework that particularly utilizes the average fundamental frequency of the
target speaker, derived solely from their facial images. Through extensive
analysis, our framework demonstrates superior speech generation quality and the
ability to align facial features with voice characteristics, including tracking
of the target speaker's fundamental frequency.
</summary>
    <author>
      <name>Jaejun Lee</name>
    </author>
    <author>
      <name>Yoori Oh</name>
    </author>
    <author>
      <name>Injune Hwang</name>
    </author>
    <author>
      <name>Kyogu Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Interspeech 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.09802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.09802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.10864v1</id>
    <updated>2024-08-20T13:55:49Z</updated>
    <published>2024-08-20T13:55:49Z</published>
    <title>Rage Music Classification and Analysis using K-Nearest Neighbour, Random
  Forest, Support Vector Machine, Convolutional Neural Networks, and Gradient
  Boosting</title>
    <summary>  We classify rage music (a subgenre of rap well-known for disagreements on
whether a particular song is part of the genre) with an extensive feature set
through algorithms including Random Forest, Support Vector Machine, K-nearest
Neighbour, Gradient Boosting, and Convolutional Neural Networks. We compare
methods of classification in the application of audio analysis with machine
learning and identify optimal models. We then analyze the significant audio
features present in and most effective in categorizing rage music, while also
identifying key audio features as well as broader separating sonic variations
and trends.
</summary>
    <author>
      <name>Akul Kumar</name>
    </author>
    <link href="http://arxiv.org/abs/2408.10864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.10864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.13522v1</id>
    <updated>2024-08-24T08:54:26Z</updated>
    <published>2024-08-24T08:54:26Z</published>
    <title>StreamAAD: Decoding Spatial Auditory Attention with a Streaming
  Architecture</title>
    <summary>  In this paper, we present our approach for the Track 1 of the Chinese
Auditory Attention Decoding (Chinese AAD) Challenge at ISCSLP 2024. Most
existing spatial auditory attention decoding (Sp-AAD) methods employ an
isolated window architecture, focusing solely on global invariant features
without considering relationships between different decision windows, which can
lead to suboptimal performance. To address this issue, we propose a novel
streaming decoding architecture, termed StreamAAD. In StreamAAD, decision
windows are input to the network as a sequential stream and decoded in order,
allowing for the modeling of inter-window relationships. Additionally, we
employ a model ensemble strategy, achieving significant better performance than
the baseline, ranking First in the challenge.
</summary>
    <author>
      <name>Zelin Qiu</name>
    </author>
    <author>
      <name>Dingding Yao</name>
    </author>
    <author>
      <name>Junfeng Li</name>
    </author>
    <link href="http://arxiv.org/abs/2408.13522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.13522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.14086v1</id>
    <updated>2024-09-21T09:51:22Z</updated>
    <published>2024-09-21T09:51:22Z</published>
    <title>AMT-APC: Automatic Piano Cover by Fine-Tuning an Automatic Music
  Transcription Model</title>
    <summary>  There have been several studies on automatically generating piano covers, and
recent advancements in deep learning have enabled the creation of more
sophisticated covers. However, existing automatic piano cover models still have
room for improvement in terms of expressiveness and fidelity to the original.
To address these issues, we propose a learning algorithm called AMT-APC, which
leverages the capabilities of automatic music transcription models. By
utilizing the strengths of well-established automatic music transcription
models, we aim to improve the accuracy of piano cover generation. Our
experiments demonstrate that the AMT-APC model reproduces original tracks more
accurately than any existing models.
</summary>
    <author>
      <name>Kazuma Komiya</name>
    </author>
    <author>
      <name>Yoshihisa Fukuhara</name>
    </author>
    <link href="http://arxiv.org/abs/2409.14086v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.14086v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.19575v1</id>
    <updated>2024-09-29T06:30:46Z</updated>
    <published>2024-09-29T06:30:46Z</published>
    <title>Quantitative Analysis of Audio-Visual Tasks: An Information-Theoretic
  Perspective</title>
    <summary>  In the field of spoken language processing, audio-visual speech processing is
receiving increasing research attention. Key components of this research
include tasks such as lip reading, audio-visual speech recognition, and
visual-to-speech synthesis. Although significant success has been achieved,
theoretical analysis is still insufficient for audio-visual tasks. This paper
presents a quantitative analysis based on information theory, focusing on
information intersection between different modalities. Our results show that
this analysis is valuable for understanding the difficulties of audio-visual
processing tasks as well as the benefits that could be obtained by modality
integration.
</summary>
    <author>
      <name>Chen Chen</name>
    </author>
    <author>
      <name>Xiaolou Li</name>
    </author>
    <author>
      <name>Zehua Liu</name>
    </author>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ISCSLP2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.19575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.19575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.05019v1</id>
    <updated>2024-10-07T13:19:10Z</updated>
    <published>2024-10-07T13:19:10Z</published>
    <title>RelUNet: Relative Channel Fusion U-Net for Multichannel Speech
  Enhancement</title>
    <summary>  Neural multi-channel speech enhancement models, in particular those based on
the U-Net architecture, demonstrate promising performance and generalization
potential. These models typically encode input channels independently, and
integrate the channels during later stages of the network. In this paper, we
propose a novel modification of these models by incorporating relative
information from the outset, where each channel is processed in conjunction
with a reference channel through stacking. This input strategy exploits
comparative differences to adaptively fuse information between channels,
thereby capturing crucial spatial information and enhancing the overall
performance. The experiments conducted on the CHiME-3 dataset demonstrate
improvements in speech enhancement metrics across various architectures.
</summary>
    <author>
      <name>Ibrahim Aldarmaki</name>
    </author>
    <author>
      <name>Thamar Solorio</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <author>
      <name>Hanan Aldarmaki</name>
    </author>
    <link href="http://arxiv.org/abs/2410.05019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.05019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.09360v1</id>
    <updated>2024-10-12T04:00:56Z</updated>
    <published>2024-10-12T04:00:56Z</published>
    <title>Towards the Synthesis of Non-speech Vocalizations</title>
    <summary>  In this report, we focus on the unconditional generation of infant cry sounds
using the DiffWave framework, which has shown great promise in generating
high-quality audio from noise. We use two distinct datasets of infant cries:
the Baby Chillanto and the deBarbaro cry dataset. These datasets are used to
train the DiffWave model to generate new cry sounds that maintain high fidelity
and diversity. The focus here is on DiffWave's capability to handle the
unconditional generation task.
</summary>
    <author>
      <name>Enjamamul Hoq</name>
    </author>
    <author>
      <name>Ifeoma Nwogu</name>
    </author>
    <link href="http://arxiv.org/abs/2410.09360v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.09360v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.12956v1</id>
    <updated>2024-10-16T18:44:28Z</updated>
    <published>2024-10-16T18:44:28Z</published>
    <title>Towards Computational Analysis of Pansori Singing</title>
    <summary>  Pansori is one of the most representative vocal genres of Korean traditional
music, which has an elaborated vocal melody line with strong vibrato. Although
the music is transmitted orally without any music notation, transcribing
pansori music in Western staff notation has been introduced for several
purposes, such as documentation of music, education, or research. In this
paper, we introduce computational analysis of pansori based on both audio and
corresponding transcription, how modern Music Information Retrieval tasks can
be used in analyzing traditional music and how it revealed different audio
characteristics of what pansori contains.
</summary>
    <author>
      <name>Sangheon Park</name>
    </author>
    <author>
      <name>Danbinaerin Han</name>
    </author>
    <author>
      <name>Dasaem Jeong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Late-Breaking Demo Session of the 25th International Society for
  Music Information Retrieval (ISMIR) Conference, 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.12956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.12956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.14122v1</id>
    <updated>2024-10-18T02:31:36Z</updated>
    <published>2024-10-18T02:31:36Z</published>
    <title>Towards Robust Transcription: Exploring Noise Injection Strategies for
  Training Data Augmentation</title>
    <summary>  Recent advancements in Automatic Piano Transcription (APT) have significantly
improved system performance, but the impact of noisy environments on the system
performance remains largely unexplored. This study investigates the impact of
white noise at various Signal-to-Noise Ratio (SNR) levels on state-of-the-art
APT models and evaluates the performance of the Onsets and Frames model when
trained on noise-augmented data. We hope this research provides valuable
insights as preliminary work toward developing transcription models that
maintain consistent performance across a range of acoustic conditions.
</summary>
    <author>
      <name>Yonghyun Kim</name>
    </author>
    <author>
      <name>Alexander Lerch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the Late-Breaking Demo Session of the 25th International
  Society for Music Information Retrieval (ISMIR) Conference, 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.14122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.14122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.17584v1</id>
    <updated>2024-10-23T06:19:48Z</updated>
    <published>2024-10-23T06:19:48Z</published>
    <title>Exploring Tokenization Methods for Multitrack Sheet Music Generation</title>
    <summary>  This study explores the tokenization of multitrack sheet music in ABC
notation, introducing two methods--bar-stream and line-stream patching. We
compare these methods against existing techniques, including bar patching, byte
patching, and Byte Pair Encoding (BPE). In terms of both computational
efficiency and the musicality of the generated compositions, experimental
results show that bar-stream patching performs best overall compared to the
others, which makes it a promising tokenization strategy for sheet music
generation.
</summary>
    <author>
      <name>Yashan Wang</name>
    </author>
    <author>
      <name>Shangda Wu</name>
    </author>
    <author>
      <name>Xingjian Du</name>
    </author>
    <author>
      <name>Maosong Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 figure, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.17584v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.17584v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.19540v1</id>
    <updated>2024-10-25T13:11:19Z</updated>
    <published>2024-10-25T13:11:19Z</published>
    <title>CloserMusicDB: A Modern Multipurpose Dataset of High Quality Music</title>
    <summary>  In this paper, we introduce CloserMusicDB, a collection of full length studio
quality tracks annotated by a team of human experts. We describe the selected
qualities of our dataset, along with three example tasks possible to perform
using this dataset: hook detection, contextual tagging and artist
identification. We conduct baseline experiments and provide initial benchmarks
for these tasks.
</summary>
    <author>
      <name>Aleksandra Piekarzewicz</name>
    </author>
    <author>
      <name>Tomasz Sroka</name>
    </author>
    <author>
      <name>Aleksander Tym</name>
    </author>
    <author>
      <name>Mateusz Modrzejewski</name>
    </author>
    <link href="http://arxiv.org/abs/2410.19540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.19540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.20515v1</id>
    <updated>2024-10-27T16:54:58Z</updated>
    <published>2024-10-27T16:54:58Z</published>
    <title>Symbotunes: unified hub for symbolic music generative models</title>
    <summary>  Implementations of popular symbolic music generative models often differ
significantly in terms of the libraries utilized and overall project structure.
Therefore, directly comparing the methods or becoming acquainted with them may
present challenges. To mitigate this issue we introduce Symbotunes, an
open-source unified hub for symbolic music generative models. Symbotunes
contains modern Python implementations of well-known methods for symbolic music
generation, as well as a unified pipeline for generating and training.
</summary>
    <author>
      <name>Paweł Skierś</name>
    </author>
    <author>
      <name>Maksymilian Łazarski</name>
    </author>
    <author>
      <name>Michał Kopeć</name>
    </author>
    <author>
      <name>Mateusz Modrzejewski</name>
    </author>
    <link href="http://arxiv.org/abs/2410.20515v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.20515v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.20518v1</id>
    <updated>2024-10-27T17:00:55Z</updated>
    <published>2024-10-27T17:00:55Z</published>
    <title>MidiTok Visualizer: a tool for visualization and analysis of tokenized
  MIDI symbolic music</title>
    <summary>  Symbolic music research plays a crucial role in music-related machine
learning, but MIDI data can be complex for those without musical expertise. To
address this issue, we present MidiTok Visualizer, a web application designed
to facilitate the exploration and visualization of various MIDI tokenization
methods from the MidiTok Python package. MidiTok Visualizer offers numerous
customizable parameters, enabling users to upload MIDI files to visualize
tokenized data alongside an interactive piano roll.
</summary>
    <author>
      <name>Michał Wiszenko</name>
    </author>
    <author>
      <name>Kacper Stefański</name>
    </author>
    <author>
      <name>Piotr Malesa</name>
    </author>
    <author>
      <name>Łukasz Pokorzyński</name>
    </author>
    <author>
      <name>Mateusz Modrzejewski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Extended Abstracts for the Late-Breaking Demo Sessionof the 25th
  Int. Society for Music Information Retrieval Conf., San Francisco, United
  States, 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.20518v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.20518v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.02711v1</id>
    <updated>2024-11-05T01:21:28Z</updated>
    <published>2024-11-05T01:21:28Z</published>
    <title>Self-Supervised Multi-View Learning for Disentangled Music Audio
  Representations</title>
    <summary>  Self-supervised learning (SSL) offers a powerful way to learn robust,
generalizable representations without labeled data. In music, where labeled
data is scarce, existing SSL methods typically use generated supervision and
multi-view redundancy to create pretext tasks. However, these approaches often
produce entangled representations and lose view-specific information. We
propose a novel self-supervised multi-view learning framework for audio
designed to incentivize separation between private and shared representation
spaces. A case study on audio disentanglement in a controlled setting
demonstrates the effectiveness of our method.
</summary>
    <author>
      <name>Julia Wilkins</name>
    </author>
    <author>
      <name>Sivan Ding</name>
    </author>
    <author>
      <name>Magdalena Fuentes</name>
    </author>
    <author>
      <name>Juan Pablo Bello</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Late Breaking Demo at ISMIR 2024.
  https://juliawilkins.github.io/marlbymarl/</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.02711v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.02711v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.11692v1</id>
    <updated>2024-11-18T16:13:49Z</updated>
    <published>2024-11-18T16:13:49Z</published>
    <title>Do Captioning Metrics Reflect Music Semantic Alignment?</title>
    <summary>  Music captioning has emerged as a promising task, fueled by the advent of
advanced language generation models. However, the evaluation of music
captioning relies heavily on traditional metrics such as BLEU, METEOR, and
ROUGE which were developed for other domains, without proper justification for
their use in this new field. We present cases where traditional metrics are
vulnerable to syntactic changes, and show they do not correlate well with human
judgments. By addressing these issues, we aim to emphasize the need for a
critical reevaluation of how music captions are assessed.
</summary>
    <author>
      <name>Jinwoo Lee</name>
    </author>
    <author>
      <name>Kyogu Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Society for Music Information Retrieval (ISMIR) 2024,
  Late Breaking Demo (LBD)</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.11692v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.11692v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.12209v1</id>
    <updated>2024-11-19T03:57:00Z</updated>
    <published>2024-11-19T03:57:00Z</published>
    <title>Zero-Shot Crate Digging: DJ Tool Retrieval Using Speech Activity, Music
  Structure And CLAP Embeddings</title>
    <summary>  In genres like Hip-Hop, RnB, Reggae, Dancehall and just about every
Electronic/Dance/Club style, DJ tools are a special set of audio files curated
to heighten the DJ's musical performance and creative mixing choices. In this
work we demonstrate an approach to discovering DJ tools in personal music
collections. Leveraging open-source libraries for speech/music activity, music
boundary analysis and a Contrastive Language-Audio Pretraining (CLAP) model for
zero-shot audio classification, we demonstrate a novel system designed to
retrieve (or rediscover) compelling DJ tools for use live or in the studio.
</summary>
    <author>
      <name>Iroro Orife</name>
    </author>
    <link href="http://arxiv.org/abs/2411.12209v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.12209v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.15082v1</id>
    <updated>2024-11-22T17:18:08Z</updated>
    <published>2024-11-22T17:18:08Z</published>
    <title>Towards Speaker Identification with Minimal Dataset and Constrained
  Resources using 1D-Convolution Neural Network</title>
    <summary>  Voice recognition and speaker identification are vital for applications in
security and personal assistants. This paper presents a lightweight
1D-Convolutional Neural Network (1D-CNN) designed to perform speaker
identification on minimal datasets. Our approach achieves a validation accuracy
of 97.87%, leveraging data augmentation techniques to handle background noise
and limited training samples. Future improvements include testing on larger
datasets and integrating transfer learning methods to enhance generalizability.
We provide all code, the custom dataset, and the trained models to facilitate
reproducibility. These resources are available on our GitHub repository:
https://github.com/IrfanNafiz/RecMe.
</summary>
    <author>
      <name>Irfan Nafiz Shahan</name>
    </author>
    <author>
      <name>Pulok Ahmed Auvi</name>
    </author>
    <link href="http://arxiv.org/abs/2411.15082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.15082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.15913v1</id>
    <updated>2024-11-24T16:53:34Z</updated>
    <published>2024-11-24T16:53:34Z</published>
    <title>A Training-Free Approach for Music Style Transfer with Latent Diffusion
  Models</title>
    <summary>  Music style transfer, while offering exciting possibilities for personalized
music generation, often requires extensive training or detailed textual
descriptions. This paper introduces a novel training-free approach leveraging
pre-trained Latent Diffusion Models (LDMs). By manipulating the self-attention
features of the LDM, we effectively transfer the style of reference music onto
content music without additional training. Our method achieves superior style
transfer and melody preservation compared to existing methods. This work opens
new creative avenues for personalized music generation.
</summary>
    <author>
      <name>Sooyoung Kim</name>
    </author>
    <author>
      <name>Joonwoo Kwon</name>
    </author>
    <author>
      <name>Heehwan Wang</name>
    </author>
    <author>
      <name>Shinjae Yoo</name>
    </author>
    <author>
      <name>Yuewei Lin</name>
    </author>
    <author>
      <name>Jiook Cha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Codes will be released upon acceptance</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.15913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.15913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.16276v1</id>
    <updated>2024-11-25T10:53:45Z</updated>
    <published>2024-11-25T10:53:45Z</published>
    <title>The SVASR System for Text-dependent Speaker Verification (TdSV) AAIC
  Challenge 2024</title>
    <summary>  This paper introduces an efficient and accurate pipeline for text-dependent
speaker verification (TDSV), designed to address the need for high-performance
biometric systems. The proposed system incorporates a Fast-Conformer-based ASR
module to validate speech content, filtering out Target-Wrong (TW) and
Impostor-Wrong (IW) trials. For speaker verification, we propose a feature
fusion approach that combines speaker embeddings extracted from wav2vec-BERT
and ReDimNet models to create a unified speaker representation. This system
achieves competitive results on the TDSV 2024 Challenge test set, with a
normalized min-DCF of 0.0452 (rank 2), highlighting its effectiveness in
balancing accuracy and robustness.
</summary>
    <author>
      <name>Mohammadreza Molavi</name>
    </author>
    <author>
      <name>Reza Khodadadi</name>
    </author>
    <link href="http://arxiv.org/abs/2411.16276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.16276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.18636v1</id>
    <updated>2024-11-20T13:01:30Z</updated>
    <published>2024-11-20T13:01:30Z</published>
    <title>Towards Advanced Speech Signal Processing: A Statistical Perspective on
  Convolution-Based Architectures and its Applications</title>
    <summary>  This article surveys convolution-based models including convolutional neural
networks (CNNs), Conformers, ResNets, and CRNNs-as speech signal processing
models and provide their statistical backgrounds and speech recognition,
speaker identification, emotion recognition, and speech enhancement
applications. Through comparative training cost assessment, model size,
accuracy and speed assessment, we compare the strengths and weaknesses of each
model, identify potential errors and propose avenues for further research,
emphasizing the central role it plays in advancing applications of speech
technologies.
</summary>
    <author>
      <name>Nirmal Joshua Kapu</name>
    </author>
    <author>
      <name>Raghav Karan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.20944/preprints202411.1218.v1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.20944/preprints202411.1218.v1" rel="related"/>
    <link href="http://arxiv.org/abs/2411.18636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.18636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.19844v1</id>
    <updated>2024-11-29T17:03:41Z</updated>
    <published>2024-11-29T17:03:41Z</published>
    <title>Musical composition and 2D cellular automata based on music intervals</title>
    <summary>  This study is a theoretical approach for exploring the applicability of a 2D
cellular automaton based on melodic and harmonic intervals in random arrays of
musical notes. The aim of this study was to explore alternatives uses for a
cellular automaton in the musical context for better understanding the musical
creativity. We used the complex systems and humanities approaches as a
framework for capturing the essence of creating music based on rules of music
theory. Findings suggested that such rules matter for generating large-scale
patterns of organized notes. Therefore, our formulation provides a novel
approach for understanding and replicating aspects of the musical creativity.
</summary>
    <author>
      <name>Igor Lugo</name>
    </author>
    <author>
      <name>Martha G. Alatriste-Contreras</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.19844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.19844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.05123v1</id>
    <updated>2024-12-06T15:29:48Z</updated>
    <published>2024-12-06T15:29:48Z</published>
    <title>Applying Automatic Differentiation to Optimize Differential Microphone
  Array Designs</title>
    <summary>  This paper introduces a novel methodology leveraging differentiable
programming to design efficient, constrained adaptive non-uniform Linear
Differential Microphone Arrays (LDMAs) with reduced implementation costs.
Utilizing an automatic differentiation framework, we propose a differentiable
convex approach that enables the adaptive design of a filter with a
distortionless constraint in the desired sound direction, while also imposing
constraints on microphone positioning to ensure consistent performance. This
approach achieves the desired Directivity Factor (DF) over a wide frequency
range and facilitates effective recovery of wide-band speech signals at lower
implementation costs.
</summary>
    <author>
      <name>Siminfar Samakoush Galougah</name>
    </author>
    <author>
      <name>Ramani Duraiswami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.05123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.05123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.06965v1</id>
    <updated>2024-12-09T20:09:44Z</updated>
    <published>2024-12-09T20:09:44Z</published>
    <title>Improving Source Extraction with Diffusion and Consistency Models</title>
    <summary>  In this work, we demonstrate the integration of a score-matching diffusion
model into a deterministic architecture for time-domain musical source
extraction, resulting in enhanced audio quality. To address the typically slow
iterative sampling process of diffusion models, we apply consistency
distillation and reduce the sampling process to a single step, achieving
performance comparable to that of diffusion models, and with two or more steps,
even surpassing them. Trained on the Slakh2100 dataset for four instruments
(bass, drums, guitar, and piano), our model shows significant improvements
across objective metrics compared to baseline methods. Sound examples are
available at https://consistency-separation.github.io/.
</summary>
    <author>
      <name>Tornike Karchkhadze</name>
    </author>
    <author>
      <name>Mohammad Rasool Izadi</name>
    </author>
    <author>
      <name>Shuo Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2412.06965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.06965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.07948v2</id>
    <updated>2025-01-16T17:56:53Z</updated>
    <published>2024-12-10T22:22:19Z</published>
    <title>Frechet Music Distance: A Metric For Generative Symbolic Music
  Evaluation</title>
    <summary>  In this paper we introduce the Frechet Music Distance (FMD), a novel
evaluation metric for generative symbolic music models, inspired by the Frechet
Inception Distance (FID) in computer vision and Frechet Audio Distance (FAD) in
generative audio. FMD calculates the distance between distributions of
reference and generated symbolic music embeddings, capturing abstract musical
features. We validate FMD across several datasets and models. Results indicate
that FMD effectively differentiates model quality, providing a domain-specific
metric for evaluating symbolic music generation, and establishing a
reproducible standard for future research in symbolic music modeling.
</summary>
    <author>
      <name>Jan Retkowski</name>
    </author>
    <author>
      <name>Jakub Stępniak</name>
    </author>
    <author>
      <name>Mateusz Modrzejewski</name>
    </author>
    <link href="http://arxiv.org/abs/2412.07948v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.07948v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.11943v2</id>
    <updated>2025-04-10T13:51:44Z</updated>
    <published>2024-12-16T16:25:58Z</published>
    <title>autrainer: A Modular and Extensible Deep Learning Toolkit for Computer
  Audition Tasks</title>
    <summary>  This work introduces the key operating principles for autrainer, our new deep
learning training framework for computer audition tasks. autrainer is a
PyTorch-based toolkit that allows for rapid, reproducible, and easily
extensible training on a variety of different computer audition tasks.
Concretely, autrainer offers low-code training and supports a wide range of
neural networks as well as preprocessing routines. In this work, we present an
overview of its inner workings and key capabilities.
</summary>
    <author>
      <name>Simon Rampp</name>
    </author>
    <author>
      <name>Andreas Triantafyllopoulos</name>
    </author>
    <author>
      <name>Manuel Milling</name>
    </author>
    <author>
      <name>Björn W. Schuller</name>
    </author>
    <link href="http://arxiv.org/abs/2412.11943v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.11943v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.12111v1</id>
    <updated>2024-12-01T00:05:00Z</updated>
    <published>2024-12-01T00:05:00Z</published>
    <title>Voice Biomarker Analysis and Automated Severity Classification of
  Dysarthric Speech in a Multilingual Context</title>
    <summary>  Dysarthria, a motor speech disorder, severely impacts voice quality,
pronunciation, and prosody, leading to diminished speech intelligibility and
reduced quality of life. Accurate assessment is crucial for effective
treatment, but traditional perceptual assessments are limited by their
subjectivity and resource intensity. To mitigate the limitations, automatic
dysarthric speech assessment methods have been proposed to support clinicians
on their decision-making. While these methods have shown promising results,
most research has focused on monolingual environments. However, multilingual
approaches are necessary to address the global burden of dysarthria and ensure
equitable access to accurate diagnosis. This thesis proposes a novel
multilingual dysarthria severity classification method, by analyzing three
languages: English, Korean, and Tamil.
</summary>
    <author>
      <name>Eunjung Yeo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SNU Doctoral thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.12111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.12111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.17924v1</id>
    <updated>2024-12-23T19:32:53Z</updated>
    <published>2024-12-23T19:32:53Z</published>
    <title>Are audio DeepFake detection models polyglots?</title>
    <summary>  Since the majority of audio DeepFake (DF) detection methods are trained on
English-centric datasets, their applicability to non-English languages remains
largely unexplored. In this work, we present a benchmark for the multilingual
audio DF detection challenge by evaluating various adaptation strategies. Our
experiments focus on analyzing models trained on English benchmark datasets, as
well as intra-linguistic (same-language) and cross-linguistic adaptation
approaches. Our results indicate considerable variations in detection efficacy,
highlighting the difficulties of multilingual settings. We show that limiting
the dataset to English negatively impacts the efficacy, while stressing the
importance of the data in the target language.
</summary>
    <author>
      <name>Bartłomiej Marek</name>
    </author>
    <author>
      <name>Piotr Kawa</name>
    </author>
    <author>
      <name>Piotr Syga</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: Audio DeepFakes, DeepFake detection, multilingual audio
  DeepFakes</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.17924v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.17924v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.01650v1</id>
    <updated>2025-01-03T05:51:27Z</updated>
    <published>2025-01-03T05:51:27Z</published>
    <title>An efficient light-weighted signal reconstruction method consists of
  Fast Fourier Transform and Convolutional-based Autoencoder</title>
    <summary>  The main theme of this paper is to reconstruct audio signal from interrupted
measurements. We present a light-weighted model only consisting discrete
Fourier transform and Convolutional-based Autoencoder model (ConvAE), called
the FFT-ConvAE model for the Helsinki Speech Challenge 2024. The FFT-ConvAE
model is light-weighted (in terms of real-time factor) and efficient (in terms
of character error rate), which was verified by the organizers. Furthermore,
the FFT-ConvAE is a general-purpose model capable of handling all tasks with a
unified configuration.
</summary>
    <author>
      <name>Pu-Yun Kow</name>
    </author>
    <author>
      <name>Pu-Zhao Kow</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.01650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.01650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="35R25, 35R30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.05332v1</id>
    <updated>2025-01-09T15:58:37Z</updated>
    <published>2025-01-09T15:58:37Z</published>
    <title>AnCoGen: Analysis, Control and Generation of Speech with a Masked
  Autoencoder</title>
    <summary>  This article introduces AnCoGen, a novel method that leverages a masked
autoencoder to unify the analysis, control, and generation of speech signals
within a single model. AnCoGen can analyze speech by estimating key attributes,
such as speaker identity, pitch, content, loudness, signal-to-noise ratio, and
clarity index. In addition, it can generate speech from these attributes and
allow precise control of the synthesized speech by modifying them. Extensive
experiments demonstrated the effectiveness of AnCoGen across speech
analysis-resynthesis, pitch estimation, pitch modification, and speech
enhancement.
</summary>
    <author>
      <name>Samir Sadok</name>
    </author>
    <author>
      <name>Simon Leglaive</name>
    </author>
    <author>
      <name>Laurent Girin</name>
    </author>
    <author>
      <name>Gaël Richard</name>
    </author>
    <author>
      <name>Xavier Alameda-Pineda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, https://samsad35.github.io/site-ancogen</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.05332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.05332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.15613v1</id>
    <updated>2025-01-26T17:43:32Z</updated>
    <published>2025-01-26T17:43:32Z</published>
    <title>Stepback: Enhanced Disentanglement for Voice Conversion via Multi-Task
  Learning</title>
    <summary>  Voice conversion (VC) modifies voice characteristics while preserving
linguistic content. This paper presents the Stepback network, a novel model for
converting speaker identity using non-parallel data. Unlike traditional VC
methods that rely on parallel data, our approach leverages deep learning
techniques to enhance disentanglement completion and linguistic content
preservation. The Stepback network incorporates a dual flow of different domain
data inputs and uses constraints with self-destructive amendments to optimize
the content encoder. Extensive experiments show that our model significantly
improves VC performance, reducing training costs while achieving high-quality
voice conversion. The Stepback network's design offers a promising solution for
advanced voice conversion tasks.
</summary>
    <author>
      <name>Qian Yang</name>
    </author>
    <author>
      <name>Calbert Graham</name>
    </author>
    <link href="http://arxiv.org/abs/2501.15613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.15613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.18919v1</id>
    <updated>2025-01-31T06:43:50Z</updated>
    <published>2025-01-31T06:43:50Z</published>
    <title>Deepfake Detection of Singing Voices With Whisper Encodings</title>
    <summary>  The deepfake generation of singing vocals is a concerning issue for artists
in the music industry. In this work, we propose a singing voice deepfake
detection (SVDD) system, which uses noise-variant encodings of open-AI's
Whisper model. As counter-intuitive as it may sound, even though the Whisper
model is known to be noise-robust, the encodings are rich in non-speech
information, and are noise-variant. This leads us to evaluate Whisper encodings
as feature representations for the SVDD task. Therefore, in this work, the SVDD
task is performed on vocals and mixtures, and the performance is evaluated in
\%EER over varying Whisper model sizes and two classifiers- CNN and ResNet34,
under different testing conditions.
</summary>
    <author>
      <name>Falguni Sharma</name>
    </author>
    <author>
      <name>Priyanka Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in ICASSP,2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.18919v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.18919v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.06285v1</id>
    <updated>2025-02-10T09:27:44Z</updated>
    <published>2025-02-10T09:27:44Z</published>
    <title>End-to-End Multi-Microphone Speaker Extraction Using Relative Transfer
  Functions</title>
    <summary>  This paper introduces a multi-microphone method for extracting a desired
speaker from a mixture involving multiple speakers and directional noise in a
reverberant environment. In this work, we propose leveraging the instantaneous
relative transfer function (RTF), estimated from a reference utterance recorded
in the same position as the desired source. The effectiveness of the RTF-based
spatial cue is compared with direction of arrival (DOA)-based spatial cue and
the conventional spectral embedding. Experimental results in challenging
acoustic scenarios demonstrate that using spatial cues yields better
performance than the spectral-based cue and that the instantaneous RTF
outperforms the DOA-based spatial cue.
</summary>
    <author>
      <name>Aviad Eisenberg</name>
    </author>
    <author>
      <name>Sharon Gannot</name>
    </author>
    <author>
      <name>Shlomo E. Chazan</name>
    </author>
    <link href="http://arxiv.org/abs/2502.06285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.06285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.12972v1</id>
    <updated>2025-02-18T15:54:20Z</updated>
    <published>2025-02-18T15:54:20Z</published>
    <title>Skip That Beat: Augmenting Meter Tracking Models for Underrepresented
  Time Signatures</title>
    <summary>  Beat and downbeat tracking models are predominantly developed using datasets
with music in 4/4 meter, which decreases their generalization to repertories in
other time signatures, such as Brazilian samba which is in 2/4. In this work,
we propose a simple augmentation technique to increase the representation of
time signatures beyond 4/4, namely 2/4 and 3/4. Our augmentation procedure
works by removing beat intervals from 4/4 annotated tracks. We show that the
augmented data helps to improve downbeat tracking for underrepresented meters
while preserving the overall performance of beat tracking in two different
models. We also show that this technique helps improve downbeat tracking in an
unseen samba dataset.
</summary>
    <author>
      <name>Giovana Morais</name>
    </author>
    <author>
      <name>Brian McFee</name>
    </author>
    <author>
      <name>Magdalena Fuentes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages + references, 3 figures, 1st Latin American Music Information
  Retrieval (LAMIR) workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.12972v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.12972v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.16914v1</id>
    <updated>2025-02-24T07:19:28Z</updated>
    <published>2025-02-24T07:19:28Z</published>
    <title>ENACT-Heart -- ENsemble-based Assessment Using CNN and Transformer on
  Heart Sounds</title>
    <summary>  This study explores the application of Vision Transformer (ViT) principles in
audio analysis, specifically focusing on heart sounds. This paper introduces
ENACT-Heart - a novel ensemble approach that leverages the complementary
strengths of Convolutional Neural Networks (CNN) and ViT through a Mixture of
Experts (MoE) framework, achieving a remarkable classification accuracy of
97.52%. This outperforms the individual contributions of ViT (93.88%) and CNN
(95.45%), demonstrating the potential for enhanced diagnostic accuracy in
cardiovascular health monitoring. These results demonstrate the potential of
ensemble methods in enhancing classification performance for cardiovascular
health monitoring and diagnosis.
</summary>
    <author>
      <name>Jiho Han</name>
    </author>
    <author>
      <name>Adnan Shaout</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted but not published in Global Digital Health Knowledge
  Exchange &amp; Empowerment Conference (gDigiHealth.KEE)</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.16914v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.16914v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.19444v1</id>
    <updated>2025-02-24T19:18:10Z</updated>
    <published>2025-02-24T19:18:10Z</published>
    <title>Filtro Adaptativo y Modulo de Grabacion en Dispositivo Para Mejora en la
  Calidad de Audicion</title>
    <summary>  This project presents the development of a real-time auditory enhancement
system utilizing an ESP32, an LMS adaptive filter, and artificial intelligence
techniques. An I2S INMP44 microphone captures the sound, which is dynamically
processed to suppress noise before being played through a MAX98357 speaker. The
system continuously adapts to varying acoustic environments, ensuring improved
speech clarity and an optimized listening experience
</summary>
    <author>
      <name>Carlos Elihu Palomino Torres</name>
    </author>
    <author>
      <name>Francisco Claudio Chichipe Mondragon</name>
    </author>
    <author>
      <name>Frank Antonio Siesquen Rodriguez</name>
    </author>
    <author>
      <name>Mariana Alexandra Huaynate Leon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Spanish language</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.19444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.19444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.21097v1</id>
    <updated>2025-02-28T14:34:43Z</updated>
    <published>2025-02-28T14:34:43Z</published>
    <title>Deep learning-based filtering of cross-spectral matrices using
  generative adversarial networks</title>
    <summary>  In this paper, we present a deep-learning method to filter out effects such
as ambient noise, reflections, or source directivity from microphone array data
represented as cross-spectral matrices. Specifically, we focus on a generative
adversarial network (GAN) architecture designed to transform fixed-size
cross-spectral matrices. Theses models were trained using sound pressure
simulations of varying complexity developed for this purpose. Based on the
results from applying these methods in a hyperparameter optimization of an
auto-encoding task, we trained the optimized model to perform five distinct
transformation tasks derived from different complexities inherent in our sound
pressure simulations.
</summary>
    <author>
      <name>Christof Puhle</name>
    </author>
    <link href="http://arxiv.org/abs/2502.21097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.21097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.08806v1</id>
    <updated>2025-03-11T18:35:38Z</updated>
    <published>2025-03-11T18:35:38Z</published>
    <title>Learning Control of Neural Sound Effects Synthesis from Physically
  Inspired Models</title>
    <summary>  Sound effects model design commonly uses digital signal processing techniques
with full control ability, but it is difficult to achieve realism within a
limited number of parameters. Recently, neural sound effects synthesis methods
have emerged as a promising approach for generating high-quality and realistic
sounds, but the process of synthesizing the desired sound poses difficulties in
terms of control. This paper presents a real-time neural synthesis model guided
by a physically inspired model, enabling the generation of high-quality sounds
while inheriting the control interface of the physically inspired model. We
showcase the superior performance of our model in terms of sound quality and
control.
</summary>
    <author>
      <name>Yisu Zong</name>
    </author>
    <author>
      <name>Joshua Reiss</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICASSP 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.08806v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.08806v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.11627v1</id>
    <updated>2025-03-14T17:46:34Z</updated>
    <published>2025-03-14T17:46:34Z</published>
    <title>Are Deep Speech Denoising Models Robust to Adversarial Noise?</title>
    <summary>  Deep noise suppression (DNS) models enjoy widespread use throughout a variety
of high-stakes speech applications. However, in this paper, we show that four
recent DNS models can each be reduced to outputting unintelligible gibberish
through the addition of imperceptible adversarial noise. Furthermore, our
results show the near-term plausibility of targeted attacks, which could induce
models to output arbitrary utterances, and over-the-air attacks. While the
success of these attacks varies by model and setting, and attacks appear to be
strongest when model-specific (i.e., white-box and non-transferable), our
results highlight a pressing need for practical countermeasures in DNS systems.
</summary>
    <author>
      <name>Will Schwarzer</name>
    </author>
    <author>
      <name>Philip S. Thomas</name>
    </author>
    <author>
      <name>Andrea Fanelli</name>
    </author>
    <author>
      <name>Xiaoyu Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.11627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.11627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.11956v1</id>
    <updated>2025-03-15T01:59:47Z</updated>
    <published>2025-03-15T01:59:47Z</published>
    <title>Optimization-Based Analysis of Music Intervals and Tuning Systems in
  Oral Traditions Using Pitch Histograms: A Case Study of Iranian Vocal Music</title>
    <summary>  This paper presents a computational methodology for analyzing music intervals
and tunings in microtonal oral traditions, utilizing pitch histograms, Dynamic
Time Warping (DTW), and optimization techniques. By extracting pitch
frequencies directly from vocal performances and aligning them with MIDI notes
via DTW, we determine musical intervals using histograms. This approach offers
an efficient, performance-based, and instrument-independent alternative to
traditional tuning system analysis. Optimization techniques are then employed
to align intervals throughout the oral tradition repertoire, capturing the
specific tunings and modes involved. Our methodology demonstrates the potential
of computational techniques in advancing musicological and ethnomusicological
research, revealing new insights into the studied traditions.
</summary>
    <author>
      <name>Sepideh Shafiei</name>
    </author>
    <author>
      <name>Shapour Hakam</name>
    </author>
    <link href="http://arxiv.org/abs/2503.11956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.11956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.12506v1</id>
    <updated>2025-03-16T13:57:37Z</updated>
    <published>2025-03-16T13:57:37Z</published>
    <title>A General Close-loop Predictive Coding Framework for Auditory Working
  Memory</title>
    <summary>  Auditory working memory is essential for various daily activities, such as
language acquisition, conversation. It involves the temporary storage and
manipulation of information that is no longer present in the environment. While
extensively studied in neuroscience and cognitive science, research on its
modeling within neural networks remains limited. To address this gap, we
propose a general framework based on a close-loop predictive coding paradigm to
perform short auditory signal memory tasks. The framework is evaluated on two
widely used benchmark datasets for environmental sound and speech,
demonstrating high semantic similarity across both datasets.
</summary>
    <author>
      <name>Zhongju Yuan</name>
    </author>
    <author>
      <name>Geraint Wiggins</name>
    </author>
    <author>
      <name>Dick Botteldooren</name>
    </author>
    <link href="http://arxiv.org/abs/2503.12506v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.12506v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.18138v1</id>
    <updated>2025-03-23T16:58:03Z</updated>
    <published>2025-03-23T16:58:03Z</published>
    <title>Machine learning based animal emotion classification using audio signals</title>
    <summary>  This paper presents the machine learning approach to the automated
classification of a dog's emotional state based on the processing and
recognition of audio signals. It offers helpful information for improving
human-machine interfaces and developing more precise tools for classifying
emotions from acoustic data. The presented model demonstrates an overall
accuracy value above 70% for audio signals recorded for one dog.
</summary>
    <author>
      <name>Mariia Slobodian</name>
    </author>
    <author>
      <name>Mykola Kozlenko</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.7514136 10.5281/zenodo.7514137</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.7514136" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.7514137" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures. This paper was originally published in 2022
  International Conference on Innovative Solutions in Software Engineering
  (ICISSE), available: https://zenodo.org/records/7514136</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2022 International Conference on Innovative Solutions in Software
  Engineering (ICISSE), Vasyl Stefanyk Precarpathian National University,
  Ivano-Frankivsk, Ukraine, Nov. 29-30, 2022, pp. 277-281</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2503.18138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.18138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94A12 (Primary) 68T07 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.19677v1</id>
    <updated>2025-03-25T14:02:10Z</updated>
    <published>2025-03-25T14:02:10Z</published>
    <title>Deep Learning for Speech Emotion Recognition: A CNN Approach Utilizing
  Mel Spectrograms</title>
    <summary>  This paper explores the application of Convolutional Neural Networks CNNs for
classifying emotions in speech through Mel Spectrogram representations of audio
files. Traditional methods such as Gaussian Mixture Models and Hidden Markov
Models have proven insufficient for practical deployment, prompting a shift
towards deep learning techniques. By transforming audio data into a visual
format, the CNN model autonomously learns to identify intricate patterns,
enhancing classification accuracy. The developed model is integrated into a
user-friendly graphical interface, facilitating realtime predictions and
potential applications in educational environments. The study aims to advance
the understanding of deep learning in speech emotion recognition, assess the
models feasibility, and contribute to the integration of technology in learning
contexts
</summary>
    <author>
      <name>Niketa Penumajji</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2503.19677v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.19677v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.02988v1</id>
    <updated>2025-04-03T19:27:43Z</updated>
    <published>2025-04-03T19:27:43Z</published>
    <title>Generating Diverse Audio-Visual 360 Soundscapes for Sound Event
  Localization and Detection</title>
    <summary>  We present SELDVisualSynth, a tool for generating synthetic videos for
audio-visual sound event localization and detection (SELD). Our approach
incorporates real-world background images to improve realism in synthetic
audio-visual SELD data while also ensuring audio-visual spatial alignment. The
tool creates 360 synthetic videos where objects move matching synthetic SELD
audio data and its annotations. Experimental results demonstrate that a model
trained with this data attains performance gains across multiple metrics,
achieving superior localization recall (56.4 LR) and competitive localization
error (21.9deg LE). We open-source our data generation tool for maximal use by
members of the SELD research community.
</summary>
    <author>
      <name>Adrian S. Roman</name>
    </author>
    <author>
      <name>Aiden Chang</name>
    </author>
    <author>
      <name>Gerardo Meza</name>
    </author>
    <author>
      <name>Iran R. Roman</name>
    </author>
    <link href="http://arxiv.org/abs/2504.02988v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.02988v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.06165v1</id>
    <updated>2025-04-08T16:01:25Z</updated>
    <published>2025-04-08T16:01:25Z</published>
    <title>Real-Time Pitch/F0 Detection Using Spectrogram Images and Convolutional
  Neural Networks</title>
    <summary>  This paper presents a novel approach to detect F0 through Convolutional
Neural Networks and image processing techniques to directly estimate pitch from
spectrogram images. Our new approach demonstrates a very good detection
accuracy; a total of 92% of predicted pitch contours have strong or moderate
correlations to the true pitch contours. Furthermore, the experimental
comparison between our new approach and other state-of-the-art CNN methods
reveals that our approach can enhance the detection rate by approximately 5%
across various Signal-to-Noise Ratio conditions.
</summary>
    <author>
      <name>Xufang Zhao</name>
    </author>
    <author>
      <name>Omer Tsimhoni</name>
    </author>
    <link href="http://arxiv.org/abs/2504.06165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.06165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.08659v1</id>
    <updated>2025-04-11T16:01:04Z</updated>
    <published>2025-04-11T16:01:04Z</published>
    <title>BowelRCNN: Region-based Convolutional Neural Network System for Bowel
  Sound Auscultation</title>
    <summary>  Sound events representing intestinal activity detection is a diagnostic tool
with potential to identify gastrointestinal conditions. This article introduces
BowelRCNN, a novel bowel sound detection system that uses audio recording,
spectrogram analysys and region-based convolutional neural network (RCNN)
architecture. The system was trained and validated on a real recording dataset
gathered from 19 patients, comprising 60 minutes of prepared and annotated
audio data. BowelRCNN achieved a classification accuracy of 96% and an F1 score
of 71%. This research highlights the feasibility of using CNN architectures for
bowel sound auscultation, achieving results comparable to those of
recurrent-convolutional methods.
</summary>
    <author>
      <name>Igor Matynia</name>
    </author>
    <author>
      <name>Robert Nowak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2504.08659v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.08659v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.15071v1</id>
    <updated>2025-04-21T12:59:40Z</updated>
    <published>2025-04-21T12:59:40Z</published>
    <title>Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling</title>
    <summary>  We introduce an extensive new dataset of MIDI files, created by transcribing
audio recordings of piano performances into their constituent notes. The data
pipeline we use is multi-stage, employing a language model to autonomously
crawl and score audio recordings from the internet based on their metadata,
followed by a stage of pruning and segmentation using an audio classifier. The
resulting dataset contains over one million distinct MIDI files, comprising
roughly 100,000 hours of transcribed audio. We provide an in-depth analysis of
our techniques, offering statistical insights, and investigate the content by
extracting metadata tags, which we also provide. Dataset available at
https://github.com/loubbrad/aria-midi.
</summary>
    <author>
      <name>Louis Bradshaw</name>
    </author>
    <author>
      <name>Simon Colton</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Learning Representations (ICLR), 2025</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2504.15071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.15071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.16839v1</id>
    <updated>2025-04-23T16:02:25Z</updated>
    <published>2025-04-23T16:02:25Z</published>
    <title>SMART: Tuning a symbolic music generation system with an audio domain
  aesthetic reward</title>
    <summary>  Recent work has proposed training machine learning models to predict
aesthetic ratings for music audio. Our work explores whether such models can be
used to finetune a symbolic music generation system with reinforcement
learning, and what effect this has on the system outputs. To test this, we use
group relative policy optimization to finetune a piano MIDI model with Meta
Audiobox Aesthetics ratings of audio-rendered outputs as the reward. We find
that this optimization has effects on multiple low-level features of the
generated outputs, and improves the average subjective ratings in a preliminary
listening study with $14$ participants. We also find that over-optimization
dramatically reduces diversity of model outputs.
</summary>
    <author>
      <name>Nicolas Jonason</name>
    </author>
    <author>
      <name>Luca Casini</name>
    </author>
    <author>
      <name>Bob L. T. Sturm</name>
    </author>
    <link href="http://arxiv.org/abs/2504.16839v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.16839v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.00579v1</id>
    <updated>2025-05-01T15:10:29Z</updated>
    <published>2025-05-01T15:10:29Z</published>
    <title>Voice Cloning: Comprehensive Survey</title>
    <summary>  Voice Cloning has rapidly advanced in today's digital world, with many
researchers and corporations working to improve these algorithms for various
applications. This article aims to establish a standardized terminology for
voice cloning and explore its different variations. It will cover speaker
adaptation as the fundamental concept and then delve deeper into topics such as
few-shot, zero-shot, and multilingual TTS within that context. Finally, we will
explore the evaluation metrics commonly used in voice cloning research and
related datasets. This survey compiles the available voice cloning algorithms
to encourage research toward its generation and detection to limit its misuse.
</summary>
    <author>
      <name>Hussam Azzuni</name>
    </author>
    <author>
      <name>Abdulmotaleb El Saddik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.00579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.00579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.00750v1</id>
    <updated>2025-05-01T00:55:52Z</updated>
    <published>2025-05-01T00:55:52Z</published>
    <title>GVPT -- A software for guided visual pitch tracking</title>
    <summary>  GVPT (Guided visual pitch tracking) is a publicly available, real-time pitch
tracking software designed to guide and evaluate vocal pitch control using
visual feedback. Developed for clinical and research applications, the system
presents various visual target pitch contour and overlays the subject's pitch
in real-time to promote accurate vocal reproduction. GVPT supports difficulty
modification, session logging, and precise pitch tracking. The software enables
voice pitch control exercise in both experimental and therapeutic settings.
</summary>
    <author>
      <name>Hyunjin Cho</name>
    </author>
    <author>
      <name>Farhad Tabasi</name>
    </author>
    <author>
      <name>Jeremy D. Greenlee</name>
    </author>
    <author>
      <name>Rahul Singh</name>
    </author>
    <link href="http://arxiv.org/abs/2505.00750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.00750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.08203v1</id>
    <updated>2025-05-13T03:33:36Z</updated>
    <published>2025-05-13T03:33:36Z</published>
    <title>Not that Groove: Zero-Shot Symbolic Music Editing</title>
    <summary>  Most work in AI music generation focused on audio, which has seen limited use
in the music production industry due to its rigidity. To maximize flexibility
while assuming only textual instructions from producers, we are among the first
to tackle symbolic music editing. We circumvent the known challenge of lack of
labeled data by proving that LLMs with zero-shot prompting can effectively edit
drum grooves. The recipe of success is a creatively designed format that
interfaces LLMs and music, while we facilitate evaluation by providing an
evaluation dataset with annotated unit tests that highly aligns with musicians'
judgment.
</summary>
    <author>
      <name>Li Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2505.08203v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.08203v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.09616v1</id>
    <updated>2025-01-10T06:18:41Z</updated>
    <published>2025-01-10T06:18:41Z</published>
    <title>SpecWav-Attack: Leveraging Spectrogram Resizing and Wav2Vec 2.0 for
  Attacking Anonymized Speech</title>
    <summary>  This paper presents SpecWav-Attack, an adversarial model for detecting
speakers in anonymized speech. It leverages Wav2Vec2 for feature extraction and
incorporates spectrogram resizing and incremental training for improved
performance. Evaluated on librispeech-dev and librispeech-test, SpecWav-Attack
outperforms conventional attacks, revealing vulnerabilities in anonymized
speech systems and emphasizing the need for stronger defenses, benchmarked
against the ICASSP 2025 Attacker Challenge.
</summary>
    <author>
      <name>Yuqi Li</name>
    </author>
    <author>
      <name>Yuanzhong Zheng</name>
    </author>
    <author>
      <name>Zhongtian Guo</name>
    </author>
    <author>
      <name>Yaoxuan Wang</name>
    </author>
    <author>
      <name>Jianjun Yin</name>
    </author>
    <author>
      <name>Haojun Fei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages,3 figures,1 chart</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.09616v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.09616v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.09784v1</id>
    <updated>2025-05-14T20:22:30Z</updated>
    <published>2025-05-14T20:22:30Z</published>
    <title>Theoretical Model of Acoustic Power Transfer Through Solids</title>
    <summary>  Acoustic Power Transfer is a relatively new technology. It is a modern type
of a wireless interface, where data signals and supply voltages are
transmitted, with the use of mechanical waves, through a medium. The simplest
application of such systems is the measurement of frequency response for audio
speakers. It consists of a variable signal generator, a measuring amplifier
which drives an acoustic source and the loudspeaker driver. The receiver
contains a microphone circuit with a level recorder. Acoustic Power Transfer
could have many applications, such as: Cochlear Implants, Sonar Systems and
Wireless Charging. However, it is a new technology, thus it needs further
investigation.
</summary>
    <author>
      <name>Ippokratis Kochliaridis</name>
    </author>
    <author>
      <name>Michail E. Kiziroglou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8th International Workoshop on Microsystems, International Hellenic
  University</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.09784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.09784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.13847v1</id>
    <updated>2025-05-20T02:42:46Z</updated>
    <published>2025-05-20T02:42:46Z</published>
    <title>Forensic deepfake audio detection using segmental speech features</title>
    <summary>  This study explores the potential of using acoustic features of segmental
speech sounds to detect deepfake audio. These features are highly interpretable
because of their close relationship with human articulatory processes and are
expected to be more difficult for deepfake models to replicate. The results
demonstrate that certain segmental features commonly used in forensic voice
comparison are effective in identifying deep-fakes, whereas some global
features provide little value. These findings underscore the need to approach
audio deepfake detection differently for forensic voice comparison and offer a
new perspective on leveraging segmental features for this purpose.
</summary>
    <author>
      <name>Tianle Yang</name>
    </author>
    <author>
      <name>Chengzhe Sun</name>
    </author>
    <author>
      <name>Siwei Lyu</name>
    </author>
    <author>
      <name>Phil Rose</name>
    </author>
    <link href="http://arxiv.org/abs/2505.13847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.13847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.17023v1</id>
    <updated>2025-04-02T15:45:00Z</updated>
    <published>2025-04-02T15:45:00Z</published>
    <title>ReMi: A Random Recurrent Neural Network Approach to Music Production</title>
    <summary>  Generative artificial intelligence raises concerns related to energy
consumption, copyright infringement and creative atrophy. We show that randomly
initialized recurrent neural networks can produce arpeggios and low-frequency
oscillations that are rich and configurable. In contrast to end-to-end music
generation that aims to replace musicians, our approach expands their
creativity while requiring no data and much less computational power. More
information can be found at: https://allendia.com/
</summary>
    <author>
      <name>Hugo Chateau-Laurent</name>
    </author>
    <author>
      <name>Tara Vanhatalo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for an Innovation Showcase Demo at International Computer
  Music Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.17023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.17023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.17259v1</id>
    <updated>2025-05-22T20:14:53Z</updated>
    <published>2025-05-22T20:14:53Z</published>
    <title>Understanding the Algorithm Behind Audio Key Detection</title>
    <summary>  The determination of musical key is a fundamental aspect of music theory and
perception, providing a harmonic context for melodies and chord progressions.
Automating this process, known as automatic key detection, is a significant
task in the field of Music Information Retrieval (MIR). This article outlines
an algorithmic methodology for estimating the musical key of an audio recording
by analyzing its tonal content through digital signal processing techniques and
comparison with theoretical key profiles.
</summary>
    <author>
      <name>Henrique Perez G. Silva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint. Describes an algorithmic approach to musical key detection
  implemented in Python. Includes conceptual explanation of audio feature
  extraction and key profile matching</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.17259v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.17259v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94A12, 00A69, 68T10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.19480v1</id>
    <updated>2025-05-26T04:02:52Z</updated>
    <published>2025-05-26T04:02:52Z</published>
    <title>Room Impulse Response as a Prompt for Acoustic Echo Cancellation</title>
    <summary>  Data-driven acoustic echo cancellation (AEC) methods, predominantly trained
on synthetic or constrained real-world datasets, encounter performance declines
in unseen echo scenarios, especially in real environments where echo paths are
not directly observable. Our proposed method counters this limitation by
integrating room impulse response (RIR) as a pivotal training prompt, aiming to
improve the generalization of AEC models in such unforeseen conditions. We also
explore four RIR prompt fusion methods. Comprehensive evaluations, including
both simulated RIR under unknown conditions and recorded RIR in real,
demonstrate that the proposed approach significantly improves performance
compared to baseline models. These results substantiate the effectiveness of
our RIR-guided approach in strengthening the model's generalization
capabilities.
</summary>
    <author>
      <name>Fei Zhao</name>
    </author>
    <author>
      <name>Shulin He</name>
    </author>
    <author>
      <name>Xueliang Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by Interspeech 2025</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.19480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.19480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.2654v1</id>
    <updated>2012-01-12T19:43:06Z</updated>
    <published>2012-01-12T19:43:06Z</published>
    <title>Musical Modes, Their Associated Chords and Their Musicality</title>
    <summary>  In this paper we present a mathematical way of defining musical modes and we
define the musicality of a mode as a product of three different factors. We
conclude by classifying the modes which are most musical according to our
definition.
</summary>
    <author>
      <name>Mihail Cocos</name>
    </author>
    <author>
      <name>Kent Kidman</name>
    </author>
    <link href="http://arxiv.org/abs/1201.2654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.2654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.08335v1</id>
    <updated>2019-11-15T20:59:30Z</updated>
    <published>2019-11-15T20:59:30Z</published>
    <title>Generative Audio Synthesis with a Parametric Model</title>
    <summary>  Use a parametric representation of audio to train a generative model in the
interest of obtaining more flexible control over the generated sound.
</summary>
    <author>
      <name>Krishna Subramani</name>
    </author>
    <author>
      <name>Alexandre D'Hooge</name>
    </author>
    <author>
      <name>Preeti Rao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISMIR 2019 Late Breaking/Demo</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.08335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.08335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.09838v1</id>
    <updated>2020-06-16T04:44:30Z</updated>
    <published>2020-06-16T04:44:30Z</published>
    <title>LSTM Networks for Music Generation</title>
    <summary>  The paper presents a method of the music generation based on LSTM (Long
Short-Term Memory), contrasts the effects of different network structures on
the music generation and introduces other methods used by some researchers.
</summary>
    <author>
      <name>Xin Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2006.09838v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.09838v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.05821v1</id>
    <updated>2022-11-10T19:23:24Z</updated>
    <published>2022-11-10T19:23:24Z</published>
    <title>Topology in Sound Synthesis and Digital Signal Processing -- DAFx2022
  Lecture Notes</title>
    <summary>  Lecture notes of a tutorial on topology in sound synthesis and digital signal
processing held at international conference for digital audio effects (DAFx-22)
in Vienna, Austria.
</summary>
    <author>
      <name>Georg Essl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 45 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.05821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.05821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.15607v1</id>
    <updated>2025-02-21T17:22:48Z</updated>
    <published>2025-02-21T17:22:48Z</published>
    <title>Benchmarking machine learning for bowel sound pattern classification
  from tabular features to pretrained models</title>
    <summary>  The development of electronic stethoscopes and wearable recording sensors
opened the door to the automated analysis of bowel sound (BS) signals. This
enables a data-driven analysis of bowel sound patterns, their interrelations,
and their correlation to different pathologies. This work leverages a BS
dataset collected from 16 healthy subjects that was annotated according to four
established BS patterns. This dataset is used to evaluate the performance of
machine learning models to detect and/or classify BS patterns. The selection of
considered models covers models using tabular features, convolutional neural
networks based on spectrograms and models pre-trained on large audio datasets.
The results highlight the clear superiority of pre-trained models, particularly
in detecting classes with few samples, achieving an AUC of 0.89 in
distinguishing BS from non-BS using a HuBERT model and an AUC of 0.89 in
differentiating bowel sound patterns using a Wav2Vec 2.0 model. These results
pave the way for an improved understanding of bowel sounds in general and
future machine-learning-driven diagnostic applications for gastrointestinal
examinations
</summary>
    <author>
      <name>Zahra Mansour</name>
    </author>
    <author>
      <name>Verena Uslar</name>
    </author>
    <author>
      <name>Dirk Weyhe</name>
    </author>
    <author>
      <name>Danilo Hollosi</name>
    </author>
    <author>
      <name>Nils Strodthoff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures and 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.15607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.15607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD (Primary) cs.LG, eess.AS, eess.SP (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.0514v1</id>
    <updated>2007-07-03T22:41:11Z</updated>
    <published>2007-07-03T22:41:11Z</published>
    <title>Phase space methods and psychoacoustic models in lossy transform coding</title>
    <summary>  I present a method for lossy transform coding of digital audio that uses the
Weyl symbol calculus for constructing the encoding and decoding transformation.
The method establishes a direct connection between a time-frequency
representation of the signal dependent threshold of masked noise and the
encode/decode pair. The formalism also offers a time-frequency measure of
perceptual entropy.
</summary>
    <author>
      <name>Matthew Charles Cargo</name>
    </author>
    <link href="http://arxiv.org/abs/0707.0514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.0514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.4501v1</id>
    <updated>2008-09-25T20:54:29Z</updated>
    <published>2008-09-25T20:54:29Z</published>
    <title>Audio Classification from Time-Frequency Texture</title>
    <summary>  Time-frequency representations of audio signals often resemble texture
images. This paper derives a simple audio classification algorithm based on
treating sound spectrograms as texture images. The algorithm is inspired by an
earlier visual classification scheme particularly efficient at classifying
textures. While solely based on time-frequency texture features, the algorithm
achieves surprisingly good performance in musical instrument classification
experiments.
</summary>
    <author>
      <name>Guoshen Yu</name>
    </author>
    <author>
      <name>Jean-Jacques Slotine</name>
    </author>
    <link href="http://arxiv.org/abs/0809.4501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.4501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.4269v1</id>
    <updated>2012-02-20T09:44:47Z</updated>
    <published>2012-02-20T09:44:47Z</published>
    <title>Live-Musikprogrammierung in Haskell</title>
    <summary>  We aim to compose algorithmic music in an interactive way with multiple
participants. To this end we develop an interpreter for a sub-language of the
non-strict functional programming language Haskell that allows to modify the
program during its execution. Our system can be used both for musical
live-coding and for demonstration and education of functional programming.
</summary>
    <author>
      <name>Henning Thielemann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 figures, 5. Arbeitstagung Programmiersprachen 2012,
  ATPS'12</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.4269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.4269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.5091v1</id>
    <updated>2012-12-19T17:40:07Z</updated>
    <published>2012-12-19T17:40:07Z</published>
    <title>Maximally Informative Observables and Categorical Perception</title>
    <summary>  We formulate the problem of perception in the framework of information
theory, and prove that categorical perception is equivalent to the existence of
an observable that has the maximum possible information on the target of
perception. We call such an observable maximally informative. Regardless
whether categorical perception is real, maximally informative observables can
form the basis of a theory of perception. We conclude with the implications of
such a theory for the problem of speech perception.
</summary>
    <author>
      <name>Elaine Tsiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.5091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.5091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.5768v1</id>
    <updated>2013-03-22T20:02:38Z</updated>
    <published>2013-03-22T20:02:38Z</published>
    <title>Live music programming in Haskell</title>
    <summary>  We aim for composing algorithmic music in an interactive way with multiple
participants. To this end we have developed an interpreter for a sub-language
of the non-strict functional programming language Haskell that allows the
modification of a program during its execution. Our system can be used both for
musical live-coding and for demonstration and education of functional
programming.
</summary>
    <author>
      <name>Henning Thielemann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures, Linux Audio Conference 2013. This is a
  translation and update of the ATPS-2012 paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.5768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.5768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.7746v1</id>
    <updated>2014-03-30T12:22:36Z</updated>
    <published>2014-03-30T12:22:36Z</published>
    <title>Multi-label Ferns for Efficient Recognition of Musical Instruments in
  Recordings</title>
    <summary>  In this paper we introduce multi-label ferns, and apply this technique for
automatic classification of musical instruments in audio recordings. We compare
the performance of our proposed method to a set of binary random ferns, using
jazz recordings as input data. Our main result is obtaining much faster
classification and higher F-score. We also achieve substantial reduction of the
model size.
</summary>
    <author>
      <name>Miron B. Kursa</name>
    </author>
    <author>
      <name>Alicja A. Wieczorkowska</name>
    </author>
    <link href="http://arxiv.org/abs/1403.7746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.7746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06095v1</id>
    <updated>2015-09-21T02:28:44Z</updated>
    <published>2015-09-21T02:28:44Z</published>
    <title>Multilayer bootstrap network for unsupervised speaker recognition</title>
    <summary>  We apply multilayer bootstrap network (MBN), a recent proposed unsupervised
learning method, to unsupervised speaker recognition. The proposed method first
extracts supervectors from an unsupervised universal background model, then
reduces the dimension of the high-dimensional supervectors by multilayer
bootstrap network, and finally conducts unsupervised speaker recognition by
clustering the low-dimensional data. The comparison results with 2 unsupervised
and 1 supervised speaker recognition techniques demonstrate the effectiveness
and robustness of the proposed method.
</summary>
    <author>
      <name>Xiao-Lei Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1509.06095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01058v1</id>
    <updated>2016-12-04T03:36:51Z</updated>
    <published>2016-12-04T03:36:51Z</published>
    <title>Algorithmic Songwriting with ALYSIA</title>
    <summary>  This paper introduces ALYSIA: Automated LYrical SongwrIting Application.
ALYSIA is based on a machine learning model using Random Forests, and we
discuss its success at pitch and rhythm prediction. Next, we show how ALYSIA
was used to create original pop songs that were subsequently recorded and
produced. Finally, we discuss our vision for the future of Automated
Songwriting for both co-creative and autonomous systems.
</summary>
    <author>
      <name>Margareta Ackerman</name>
    </author>
    <author>
      <name>David Loker</name>
    </author>
    <link href="http://arxiv.org/abs/1612.01058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08675v1</id>
    <updated>2017-06-27T05:28:06Z</updated>
    <published>2017-06-27T05:28:06Z</published>
    <title>Proceedings of the First International Workshop on Deep Learning and
  Music</title>
    <summary>  Proceedings of the First International Workshop on Deep Learning and Music,
joint with IJCNN, Anchorage, US, May 17-18, 2017
</summary>
    <author>
      <name>Dorien Herremans</name>
    </author>
    <author>
      <name>Ching-Hua Chuan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.22227.99364/1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.22227.99364/1" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Workshop on Deep Learning
  and Music, joint with IJCNN, Anchorage, US, May 17-18, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.08675v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08675v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08928v1</id>
    <updated>2017-06-27T16:25:00Z</updated>
    <published>2017-06-27T16:25:00Z</published>
    <title>Classical Music Clustering Based on Acoustic Features</title>
    <summary>  In this paper we cluster 330 classical music pieces collected from MusicNet
database based on their musical note sequence. We use shingling and chord
trajectory matrices to create signature for each music piece and performed
spectral clustering to find the clusters. Based on different resolution, the
output clusters distinctively indicate composition from different classical
music era and different composing style of the musicians.
</summary>
    <author>
      <name>Xindi Wang</name>
    </author>
    <author>
      <name>Syed Arefinul Haque</name>
    </author>
    <link href="http://arxiv.org/abs/1706.08928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05193v1</id>
    <updated>2017-09-14T07:24:08Z</updated>
    <published>2017-09-14T07:24:08Z</published>
    <title>Clustering of Musical Pieces through Complex Networks: an Assessment
  over Guitar Solos</title>
    <summary>  Musical pieces can be modeled as complex networks. This fosters innovative
ways to categorize music, paving the way towards novel applications in
multimedia domains, such as music didactics, multimedia entertainment and
digital music generation. Clustering these networks through their main metrics
allows grouping similar musical tracks. To show the viability of the approach,
we provide results on a dataset of guitar solos.
</summary>
    <author>
      <name>Stefano Ferretti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in IEEE Multimedia magazine</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.05193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.01237v2</id>
    <updated>2019-10-31T01:13:51Z</updated>
    <published>2018-01-04T03:30:47Z</published>
    <title>A pairwise discriminative task for speech emotion recognition</title>
    <summary>  I have submitted a new version to arXiv:1910.11174. I forget to choose to
replace the old version, but submitted a new one. It's my mistake.
</summary>
    <author>
      <name>Zheng Lian</name>
    </author>
    <author>
      <name>Ya Li</name>
    </author>
    <author>
      <name>Jianhua Tao</name>
    </author>
    <author>
      <name>Jian Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">I have submitted a new version to arXiv:1910.11174. I forget to
  choose to replace the old version, but submitted a new one. It's my mistake</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.01237v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01237v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.03137v2</id>
    <updated>2020-08-10T13:13:56Z</updated>
    <published>2020-07-07T00:14:30Z</published>
    <title>Predicting Afrobeats Hit Songs Using Spotify Data</title>
    <summary>  This study approached the Hit Song Science problem with the aim of predicting
which songs in the Afrobeats genre will become popular among Spotify listeners.
A dataset of 2063 songs was generated through the Spotify Web API, with the
provided audio features. Random Forest and Gradient Boosting algorithms proved
to be successful with approximately F1 scores of 86%.
</summary>
    <author>
      <name>Adewale Adeagbo</name>
    </author>
    <link href="http://arxiv.org/abs/2007.03137v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.03137v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.01992v1</id>
    <updated>2022-03-04T10:31:19Z</updated>
    <published>2022-03-04T10:31:19Z</published>
    <title>On the relevance of language in speaker recognition</title>
    <summary>  This paper presents a new database collected from a bilingual speakers set
(49), in two different languages: Spanish and Catalan. Phonetically there are
significative differences between both languages. These differences have let us
to establish several conclusions on the relevance of language in speaker
recognition, using two methods: vector quantization and covariance matrices
</summary>
    <author>
      <name>Antonio Satue-Villar</name>
    </author>
    <author>
      <name>Marcos Faundez-Zanuy</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">6th European Conference on Speech Communication and Technology
  (EUROSPEECH 99) Budapest, Hungary, September 5-9, 1999</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2203.01992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.01992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02489v2</id>
    <updated>2018-05-30T13:03:59Z</updated>
    <published>2018-05-03T19:42:57Z</published>
    <title>Transformer for Emotion Recognition</title>
    <summary>  This paper describes the UMONS solution for the OMG-Emotion Challenge. We
explore a context-dependent architecture where the arousal and valence of an
utterance are predicted according to its surrounding context (i.e. the
preceding and following utterances of the video). We report an improvement when
taking into account context for both unimodal and multimodal predictions.
</summary>
    <author>
      <name>Jean-Benoit Delbrouck</name>
    </author>
    <link href="http://arxiv.org/abs/1805.02489v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02489v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11087v1</id>
    <updated>2018-05-26T05:05:48Z</updated>
    <published>2018-05-26T05:05:48Z</published>
    <title>Dodecatonic Cycles and Parsimonious Voice-Leading in the Mystic-Wozzeck
  Genus</title>
    <summary>  This paper develops a unified voice-leading model for the genus of mystic and
Wozzeck chords. These voice-leading regions are constructed by perturbing
symmetric partitions of the octave, and new Neo-Riemannian transformations
between nearly symmetric hexachords are defined. The behaviors of these
transformations are shown within visual representations of the voice-leading
regions for the mystic-Wozzeck genus.
</summary>
    <author>
      <name>Vaibhav Mohanty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 17 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.11087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.HO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="00A65" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01569v1</id>
    <updated>2020-10-04T12:55:18Z</updated>
    <published>2020-10-04T12:55:18Z</published>
    <title>A Course on Controllers</title>
    <summary>  Over the last four years, we have developed a series of lectures, labs and
project assignments aimed at introducing enough technology so that students
from a mix of disciplines can design and build innovative interface devices.
</summary>
    <author>
      <name>Bill Verplank</name>
    </author>
    <author>
      <name>Craig Sapp</name>
    </author>
    <author>
      <name>Max Mathews</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1176380</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1176380" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01569v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01569v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01575v1</id>
    <updated>2020-10-04T12:57:33Z</updated>
    <published>2020-10-04T12:57:33Z</published>
    <title>Tangible Music Interfaces Using Passive Magnetic Tags</title>
    <summary>  The technologies behind passive resonant magnetically coupled tags are
introduced and their application as a musical controller is illustrated for
solo or group performances, interactive installations, and music toys.
</summary>
    <author>
      <name>Joseph A. Paradiso</name>
    </author>
    <author>
      <name>Kai-yuh Hsiao</name>
    </author>
    <author>
      <name>Ari Benbasat</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.1176374</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.1176374" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on New Interfaces for
  Musical Expression, 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.00583v1</id>
    <updated>2022-09-18T18:58:58Z</updated>
    <published>2022-09-18T18:58:58Z</published>
    <title>Ambisonic Encoding of Signals From Spherical Microphone Arrays</title>
    <summary>  This document illustrates how to process the signals from the microphones of
a rigid-sphere higher-order ambisonic microphone array so that they are encoded
with N3D normalization and ACN channel order and thereby can be used with the
standard ambisonic software tools such as SPARTA and the IEM Plugin Suite. A
MATLAB script is provided.
</summary>
    <author>
      <name>Jens Ahrens</name>
    </author>
    <link href="http://arxiv.org/abs/2211.00583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.00583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.09645v1</id>
    <updated>2023-09-18T10:30:16Z</updated>
    <published>2023-09-18T10:30:16Z</published>
    <title>Scaling the time and Fourier domains to align periodically and their
  convolution</title>
    <summary>  This note shows how to align a periodic signal with its the Fourier transform
by means of frequency or time scaling. This may be useful in developing new
algorithms, e.g. for pitch estimation. This note also convolves the signals and
the frequency time convolution is denoted fxt.
</summary>
    <author>
      <name>Matthew R. Flax</name>
    </author>
    <author>
      <name>W. Harvey Holmes</name>
    </author>
    <link href="http://arxiv.org/abs/2309.09645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.09645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.03469v2</id>
    <updated>2023-11-23T09:49:59Z</updated>
    <published>2023-11-06T19:11:40Z</published>
    <title>Combinatorial Hodge Theory in Simplicial Signal Processing -- DAFx2023
  Lecture Notes</title>
    <summary>  Lecture notes of a tutorial on Combinatorial Hodge Theory in Simplicial
Signal Processing held at international conference for digital audio effects
(DAFx-23) in Copenhagen, Denmark.
</summary>
    <author>
      <name>Georg Essl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 40 figures. arXiv admin note: substantial text overlap with
  arXiv:2211.05821</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.03469v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.03469v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.08678v1</id>
    <updated>2024-01-11T02:57:37Z</updated>
    <published>2024-01-11T02:57:37Z</published>
    <title>Sub-band and Full-band Interactive U-Net with DPRNN for Demixing
  Cross-talk Stereo Music</title>
    <summary>  This paper presents a detailed description of our proposed methods for the
ICASSP 2024 Cadenza Challenge. Experimental results show that the proposed
system can achieve better performance than official baselines.
</summary>
    <author>
      <name>Han Yin</name>
    </author>
    <author>
      <name>Mou Wang</name>
    </author>
    <author>
      <name>Jisheng Bai</name>
    </author>
    <author>
      <name>Dongyuan Shi</name>
    </author>
    <author>
      <name>Woon-Seng Gan</name>
    </author>
    <author>
      <name>Jianfeng Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICASSP 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.08678v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.08678v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.01778v1</id>
    <updated>2024-02-01T17:54:15Z</updated>
    <published>2024-02-01T17:54:15Z</published>
    <title>Introduction to speech recognition</title>
    <summary>  This document contains lectures and practical experimentations using Matlab
and implementing a system which is actually correctly classifying three words
(one, two and three) with the help of a very small database. To achieve this
performance, it uses speech modeling specificities, powerful computer
algorithms (dynamic time warping and Dijktra's algorithm) and machine learning
(nearest neighbor). This document introduces also some machine learning
evaluation metrics.
</summary>
    <author>
      <name>Gabriel Dauphin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French language</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.01778v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.01778v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.04254v1</id>
    <updated>2023-10-15T13:07:41Z</updated>
    <published>2023-10-15T13:07:41Z</published>
    <title>Large Vocabulary Spontaneous Speech Recognition for Tigrigna</title>
    <summary>  This thesis proposes and describes a research attempt at designing and
developing a speaker independent spontaneous automatic speech recognition
system for Tigrigna The acoustic model of the Speech Recognition System is
developed using Carnegie Mellon University Automatic Speech Recognition
development tool (Sphinx) while the SRIM tool is used for the development of
the language model.
  Keywords Automatic Speech Recognition Tigrigna language
</summary>
    <author>
      <name>Ataklti Kahsu</name>
    </author>
    <author>
      <name>Solomon Teferra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 1 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.04254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.04254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50 (Primary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.12258v1</id>
    <updated>2024-03-18T21:14:18Z</updated>
    <published>2024-03-18T21:14:18Z</published>
    <title>A Multi-loudspeaker Binaural Room Impulse Response Dataset with
  High-Resolution Translational and Rotational Head Coordinates in a Listening
  Room</title>
    <summary>  Data report for the 3D3A Lab Binaural Room Impulse Response (BRIR) Dataset
(https://doi.org/10.34770/6gc9-5787).
</summary>
    <author>
      <name>Yue Qiao</name>
    </author>
    <author>
      <name>Ryan Miguel Gonzales</name>
    </author>
    <author>
      <name>Edgar Choueiri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3389/frsip.2024.1380060</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3389/frsip.2024.1380060" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Frontiers in Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.12258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.12258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.22903v1</id>
    <updated>2024-10-30T11:02:57Z</updated>
    <published>2024-10-30T11:02:57Z</published>
    <title>Augmenting Polish Automatic Speech Recognition System With Synthetic
  Data</title>
    <summary>  This paper presents a system developed for submission to Poleval 2024, Task
3: Polish Automatic Speech Recognition Challenge. We describe Voicebox-based
speech synthesis pipeline and utilize it to augment Conformer and Whisper
speech recognition models with synthetic data. We show that addition of
synthetic speech to training improves achieved results significantly. We also
present final results achieved by our models in the competition.
</summary>
    <author>
      <name>Łukasz Bondaruk</name>
    </author>
    <author>
      <name>Jakub Kubiak</name>
    </author>
    <author>
      <name>Mateusz Czyżnikiewicz</name>
    </author>
    <link href="http://arxiv.org/abs/2410.22903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.22903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.08978v1</id>
    <updated>2025-05-13T21:42:33Z</updated>
    <published>2025-05-13T21:42:33Z</published>
    <title>Inference Attacks for X-Vector Speaker Anonymization</title>
    <summary>  We revisit the privacy-utility tradeoff of x-vector speaker anonymization.
Existing approaches quantify privacy through training complex speaker
verification or identification models that are later used as attacks. Instead,
we propose a novel inference attack for de-anonymization. Our attack is simple
and ML-free yet we show experimentally that it outperforms existing approaches.
</summary>
    <author>
      <name>Luke Bauer</name>
    </author>
    <author>
      <name>Wenxuan Bao</name>
    </author>
    <author>
      <name>Malvika Jadhav</name>
    </author>
    <author>
      <name>Vincent Bindschaedler</name>
    </author>
    <link href="http://arxiv.org/abs/2505.08978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.08978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006026v2</id>
    <updated>2000-06-21T15:36:29Z</updated>
    <published>2000-06-12T06:08:57Z</published>
    <title>Online Correction of Dispersion Error in 2D Waveguide Meshes</title>
    <summary>  An elastic ideal 2D propagation medium, i.e., a membrane, can be simulated by
models discretizing the wave equation on the time-space grid (finite difference
methods), or locally discretizing the solution of the wave equation (waveguide
meshes). The two approaches provide equivalent computational structures, and
introduce numerical dispersion that induces a misalignment of the modes from
their theoretical positions. Prior literature shows that dispersion can be
arbitrarily reduced by oversizing and oversampling the mesh, or by adpting
offline warping techniques. In this paper we propose to reduce numerical
dispersion by embedding warping elements, i.e., properly tuned allpass filters,
in the structure. The resulting model exhibits a significant reduction in
dispersion, and requires less computational resources than a regular mesh
structure having comparable accuracy.
</summary>
    <author>
      <name>Federico Fontana</name>
    </author>
    <author>
      <name>Davide Rocchesso</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures, to appear in the Proceedings of the International
  Computer Music Conference, 2000. Corrected first reference</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0006026v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006026v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0007006v1</id>
    <updated>2000-07-05T18:20:51Z</updated>
    <published>2000-07-05T18:20:51Z</published>
    <title>DISCO: An object-oriented system for music composition and sound design</title>
    <summary>  This paper describes an object-oriented approach to music composition and
sound design. The approach unifies the processes of music making and instrument
building by using similar logic, objects, and procedures. The composition
modules use an abstract representation of musical data, which can be easily
mapped onto different synthesis languages or a traditionally notated score. An
abstract base class is used to derive classes on different time scales. Objects
can be related to act across time scales, as well as across an entire piece,
and relationships between similar objects can replicate traditional music
operations or introduce new ones. The DISCO (Digital Instrument for
Sonification and Composition) system is an open-ended work in progress.
</summary>
    <author>
      <name>Hans G. Kaper</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Argonne National Laboratory</arxiv:affiliation>
    </author>
    <author>
      <name>Sever Tipei</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Illinois at Urbana-Champaign</arxiv:affiliation>
    </author>
    <author>
      <name>Jeff M. Wright</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Illinois at Urbana-Champaign</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, no figures; to be published in Proc. Int'l Computer Music
  Conference 2000 (Berlin, August 2000)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0007006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0007006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0007007v1</id>
    <updated>2000-07-05T21:26:48Z</updated>
    <published>2000-07-05T21:26:48Z</published>
    <title>Data sonification and sound visualization</title>
    <summary>  This article describes a collaborative project between researchers in the
Mathematics and Computer Science Division at Argonne National Laboratory and
the Computer Music Project of the University of Illinois at Urbana-Champaign.
The project focuses on the use of sound for the exploration and analysis of
complex data sets in scientific computing. The article addresses digital sound
synthesis in the context of DIASS (Digital Instrument for Additive Sound
Synthesis) and sound visualization in a virtual-reality environment by means of
M4CAVE. It describes the procedures and preliminary results of some experiments
in scientific sonification and sound visualization.
</summary>
    <author>
      <name>Hans G. Kaper</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Argonne National Laboratory</arxiv:affiliation>
    </author>
    <author>
      <name>Sever Tipei</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Argonne National Laboratory</arxiv:affiliation>
    </author>
    <author>
      <name>Elizabeth Wiebel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Argonne National Laboratory</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 5 figures, 3 tables; preprint of the published paper
  (differing in details)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computing in Science and Engineering, Vol. 1 No. 4, July-August
  1999, pp. 48-58</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0007007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0007007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0010017v2</id>
    <updated>2001-01-08T07:48:08Z</updated>
    <published>2000-10-10T13:08:28Z</published>
    <title>Generalization of a 3-D resonator model for the simulation of spherical
  enclosures</title>
    <summary>  A rectangular enclosure has such an even distribution of resonances that it
can be accurately and efficiently modelled using a feedback delay network.
Conversely, a non rectangular shape such as a sphere has a distribution of
resonances that challenges the construction of an efficient model. This work
proposes an extension of the already known feedback delay network structure to
model the resonant properties of a sphere. A specific frequency distribution of
resonances can be approximated, up to a certain frequency, by inserting an
allpass filter of moderate order after each delay line of a feedback delay
network. The structure used for rectangular boxes is therefore augmented with a
set of allpass filters allowing parametric control over the enclosure size and
the boundary properties. This work was motivated by informal listening tests
which have shown that it is possible to identify a basic shape just from the
distribution of its audible resonances.
</summary>
    <author>
      <name>Davide Rocchesso</name>
    </author>
    <author>
      <name>Pierre Dutilleux</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1155/S1110865701000105</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1155/S1110865701000105" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, 16 figures, 6 tables. Accepted for publication in Applied
  Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0010017v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0010017v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0103005v1</id>
    <updated>2001-03-05T04:41:12Z</updated>
    <published>2001-03-05T04:41:12Z</published>
    <title>Source-Filter Decomposition of Harmonic Sounds</title>
    <summary>  This paper describes a method for decomposing steady-state instrument data
into excitation and formant filter components. The input data, taken from
several series of recordings of acoustical instruments is analyzed in the
frequency domain, and for each series a model is built, which most accurately
represents the data as a source-filter system. The source part is taken to be a
harmonic excitation system with frequency-invariant magnitudes, and the filter
part is considered to be responsible for all spectral inhomogenieties. This
method has been applied to the SHARC database of steady state instrument data
to create source-filter models for a large number of acoustical instruments.
Subsequent use of such models can have a wide variety of applications,
including improvements to wavetable and physical modeling synthesis, high
quality pitch shifting, and creation of "hybrid" instrument timbres.
</summary>
    <author>
      <name>Ilia Bisnovatyi</name>
    </author>
    <author>
      <name>Michael J. O'Donnell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary results reported at DAFx-99, in "Decomposition of Steady
  State Instrument Data into Excitation System and Formant Filter Components",
  http://www.tele.ntnu.no/akustikk/meetings/DAFx99/bisnovatyi.pdf 5 pages + 2
  appendices (6 graphs, 1 table)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0103005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0103005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H 5.5; G 1.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0103006v1</id>
    <updated>2001-03-05T05:05:15Z</updated>
    <published>2001-03-05T05:05:15Z</published>
    <title>Flexible Software Framework for Modal Synthesis</title>
    <summary>  Modal synthesis is an important area of physical modeling whose exploration
in the past has been held back by a large number of control parameters, the
scarcity of general-purpose design tools and the difficulty of obtaining the
computational power required for real-time synthesis. This paper presents an
overview of a flexible software framework facilitating the design and control
of instruments based on modal synthesis. The framework is designed as a
hierarchy of polymorphic synthesis objects, representing modal structures of
various complexity. As a method of generalizing all interactions among the
elements of a modal system, an abstract notion of {\it energy} is introduced,
and a set of energy transfer functions is provided. Such abstraction leads to a
design where the dynamics of interactions can be largely separated from the
specifics of particular modal structures, yielding an easily configurable and
expandable system. A real-time version of the framework has been implemented as
a set of C++ classes along with an integrating shell and a GUI, and is
currently being used to design and play modal instruments, as well as to survey
fundamental properties of various modal algorithms.
</summary>
    <author>
      <name>Ilia Bisnovatyi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at DAFx00,
  http://profs.sci.univr.it/~dafx/DAFx-final-papers.html</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">in Proceedings of COST-G6 Conference on Digital Audio Effects
  (DAFx-00), Dec 7-9, 2000, Verona, Italy</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0103006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0103006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H 5.5; I 6.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0203015v1</id>
    <updated>2002-03-12T09:21:39Z</updated>
    <published>2002-03-12T09:21:39Z</published>
    <title>Towards Experimental Nanosound Using Almost Disjoint Set Theory</title>
    <summary>  Music composition using digital audio sequence editors is increasingly
performed in a visual workspace where sound complexes are built from discrete
sound objects, called gestures that are arranged in time and space to generate
a continuous composition. The visual workspace, common to most industry
standard audio loop sequencing software, is premised on the arrangement of
gestures defined with geometric shape properties. Here, one aspect of fractal
set theory was validated using audio-frequency sets to evaluate self-affine
scaling behavior when new sound complexes are built through union and
intersection operations on discrete musical gestures. Results showed that
intersection of two sets revealed lower complexity compared with the union
operator, meaning that the intersection of two sound gestures is an almost
disjoint set, and in accord with formal logic. These results are also discussed
with reference to fuzzy sets, cellular automata, nanotechnology and
self-organization to further explore the link between sequenced notation and
complexity.
</summary>
    <author>
      <name>Cameron L Jones</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 4 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0203015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0203015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410027v1</id>
    <updated>2004-10-13T02:28:10Z</updated>
    <published>2004-10-13T02:28:10Z</published>
    <title>Detecting User Engagement in Everyday Conversations</title>
    <summary>  This paper presents a novel application of speech emotion recognition:
estimation of the level of conversational engagement between users of a voice
communication system. We begin by using machine learning techniques, such as
the support vector machine (SVM), to classify users' emotions as expressed in
individual utterances. However, this alone fails to model the temporal and
interactive aspects of conversational engagement. We therefore propose the use
of a multilevel structure based on coupled hidden Markov models (HMM) to
estimate engagement levels in continuous natural speech. The first level is
comprised of SVM-based classifiers that recognize emotional states, which could
be (e.g.) discrete emotion types or arousal/valence levels. A high-level HMM
then uses these emotional states as input, estimating users' engagement in
conversation by decoding the internal states of the HMM. We report experimental
results obtained by applying our algorithms to the LDC Emotional Prosody and
CallFriend speech corpora.
</summary>
    <author>
      <name>Chen Yu</name>
    </author>
    <author>
      <name>Paul M. Aoki</name>
    </author>
    <author>
      <name>Allison Woodruff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages (A4), 1 figure (EPS)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 8th Int'l Conf. on Spoken Language Processing (ICSLP) (Vol.
  2), Jeju Island, Republic of Korea, Oct. 2004, 1329-1332. ISCA.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0410027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.4; I.2.7; H.5.2; H.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612138v1</id>
    <updated>2006-12-28T06:39:55Z</updated>
    <published>2006-12-28T06:39:55Z</published>
    <title>Accommodating Sample Size Effect on Similarity Measures in Speaker
  Clustering</title>
    <summary>  We investigate the symmetric Kullback-Leibler (KL2) distance in speaker
clustering and its unreported effects for differently-sized feature matrices.
Speaker data is represented as Mel Frequency Cepstral Coefficient (MFCC)
vectors, and features are compared using the KL2 metric to form clusters of
speech segments for each speaker. We make two observations with respect to
clustering based on KL2: 1.) The accuracy of clustering is strongly dependent
on the absolute lengths of the speech segments and their extracted feature
vectors. 2.) The accuracy of the similarity measure strongly degrades with the
length of the shorter of the two speech segments. These effects of length can
be attributed to the measure of covariance used in KL2. We demonstrate an
empirical correction of this sample-size effect that increases clustering
accuracy. We draw parallels to two Vector Quantization-based (VQ) similarity
measures, one which exhibits an equivalent effect of sample size, and the
second being less influenced by it.
</summary>
    <author>
      <name>Alexander Haubold</name>
    </author>
    <author>
      <name>John R. Kender</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0612138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.5.1; H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.3241v2</id>
    <updated>2013-11-23T09:08:26Z</updated>
    <published>2008-04-21T06:32:23Z</published>
    <title>A Synthesizer Based on Frequency-Phase Analysis and Square Waves</title>
    <summary>  This article introduces an effective generalization of the polar flavor of
the Fourier Theorem based on a new method of analysis. Under the premises of
the new theory an ample class of functions become viable as bases, with the
further advantage of using the same basis for analysis and reconstruction. In
fact other tools, like the wavelets, admit specially built nonorthogonal bases
but require different bases for analysis and reconstruction (biorthogonal and
dual bases) and vectorial coordinates; this renders those systems unintuitive
and computing intensive. As an example of the advantages of the new
generalization of the Fourier Theorem, this paper introduces a novel method for
the synthesis that is based on frequency-phase series of square waves (the
equivalent of the polar Fourier Theorem but for nonorthogonal bases). The
resulting synthesizer is very efficient needing only few components, frugal in
terms of computing needs, and viable for many applications.
</summary>
    <author>
      <name>Sossio Vergara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages. Digital Signal Processing Journal 22 (2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.3241v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.3241v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.3080v1</id>
    <updated>2009-03-18T03:33:48Z</updated>
    <published>2009-03-18T03:33:48Z</published>
    <title>A Unified Theory of Time-Frequency Reassignment</title>
    <summary>  Time-frequency representations such as the spectrogram are commonly used to
analyze signals having a time-varying distribution of spectral energy, but the
spectrogram is constrained by an unfortunate tradeoff between resolution in
time and frequency. A method of achieving high-resolution spectral
representations has been independently introduced by several parties. The
technique has been variously named reassignment and remapping, but while the
implementations have differed in details, they are all based on the same
theoretical and mathematical foundation. In this work, we present a brief
history of work on the method we will call the method of time-frequency
reassignment, and present a unified mathematical description of the technique
and its derivation. We will focus on the development of time-frequency
reassignment in the context of the spectrogram, and conclude with a discussion
of some current applications of the reassigned spectrogram.
</summary>
    <author>
      <name>Kelly R. Fitz</name>
    </author>
    <author>
      <name>Sean A. Fulop</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 13 figures, draft of paper submitted to Digital Signal
  Processing (Elsevier) in 2005, still in review</arxiv:comment>
    <link href="http://arxiv.org/abs/0903.3080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.3080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.3220v1</id>
    <updated>2009-07-18T13:25:59Z</updated>
    <published>2009-07-18T13:25:59Z</published>
    <title>Inter Genre Similarity Modelling For Automatic Music Genre
  Classification</title>
    <summary>  Music genre classification is an essential tool for music information
retrieval systems and it has been finding critical applications in various
media platforms. Two important problems of the automatic music genre
classification are feature extraction and classifier design. This paper
investigates inter-genre similarity modelling (IGS) to improve the performance
of automatic music genre classification. Inter-genre similarity information is
extracted over the mis-classified feature population. Once the inter-genre
similarity is modelled, elimination of the inter-genre similarity reduces the
inter-genre confusion and improves the identification rates. Inter-genre
similarity modelling is further improved with iterative IGS modelling(IIGS) and
score modelling for IGS elimination(SMIGS). Experimental results with promising
classification improvements are provided.
</summary>
    <author>
      <name>Ulas Bagci</name>
    </author>
    <author>
      <name>Engin Erzin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Dafx 2006 submission</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">9th International Conference on Digital Audio Effects (DAFx-06),
  Montreal, Canada, September 18-20, 2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.3220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.3220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.0599v1</id>
    <updated>2009-09-03T08:49:54Z</updated>
    <published>2009-09-03T08:49:54Z</published>
    <title>Codebook Design Method for Noise Robust Speaker Identification based on
  Genetic Algorithm</title>
    <summary>  In this paper, a novel method of designing a codebook for noise robust
speaker identification purpose utilizing Genetic Algorithm has been proposed.
Wiener filter has been used to remove the background noises from the source
speech utterances. Speech features have been extracted using standard speech
parameterization method such as LPC, LPCC, RCC, MFCC, (delta)MFCC and
(delta)(delta) MFCC. For each of these techniques, the performance of the
proposed system has been compared. In this codebook design method, Genetic
Algorithm has the capability of getting global optimal result and hence
improves the quality of the codebook. Comparing with the NOIZEOUS speech
database, the experimental result shows that 79.62 percent accuracy has been
achieved.
</summary>
    <author>
      <name>Md. Rabiul Islam</name>
    </author>
    <author>
      <name>Md. Fayzur Rahman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact factor
  0.423,http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 4, No. 1 &amp; 2, August 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.0599v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.0599v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.2363v1</id>
    <updated>2009-09-12T20:36:35Z</updated>
    <published>2009-09-12T20:36:35Z</published>
    <title>Improvement of Text Dependent Speaker Identification System Using
  Neuro-Genetic Hybrid Algorithm in Office Environmental Conditions</title>
    <summary>  In this paper, an improved strategy for automated text dependent speaker
identification system has been proposed in noisy environment. The
identification process incorporates the Neuro- Genetic hybrid algorithm with
cepstral based features. To remove the background noise from the source
utterance, wiener filter has been used. Different speech pre-processing
techniques such as start-end point detection algorithm, pre-emphasis filtering,
frame blocking and windowing have been used to process the speech utterances.
RCC, MFCC, MFCC, MFCC, LPC and LPCC have been used to extract the features.
After feature extraction of the speech, Neuro-Genetic hybrid algorithm has been
used in the learning and identification purposes. Features are extracted by
using different techniques to optimize the performance of the identification.
According to the VALID speech database, the highest speaker identification rate
of 100.000 percent for studio environment and 82.33 percent for office
environmental conditions have been achieved in the close set text dependent
speaker identification system.
</summary>
    <author>
      <name>Md. Rabiul Islam</name>
    </author>
    <author>
      <name>Md. Fayzur Rahman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues (IJCSI), Volume 1,
  pp42-48, August 2009</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">R. Islam and F. Rahman," International Journal of Computer Science
  Issues (IJCSI), Volume 1, pp42-48,August 2009"</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.2363v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.2363v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.5171v1</id>
    <updated>2009-11-26T23:59:44Z</updated>
    <published>2009-11-26T23:59:44Z</published>
    <title>Untangling Phase and Time in Monophonic Sounds</title>
    <summary>  We are looking for a mathematical model of monophonic sounds with independent
time and phase dimensions. With such a model we can resynthesise a sound with
arbitrarily modulated frequency and progress of the timbre. We propose such a
model and show that it exactly fulfils some natural properties, like a kind of
time-invariance, robustness against non-harmonic frequencies, envelope
preservation, and inclusion of plain resampling as a special case. The
resulting algorithm is efficient and allows to process data in a streaming
manner with phase and shape modulation at sample rate, what we demonstrate with
an implementation in the functional language Haskell. It allows a wide range of
applications, namely pitch shifting and time scaling, creative FM synthesis
effects, compression of monophonic sounds, generating loops for sampled sounds,
synthesise sounds similar to wavetable synthesis, or making ultrasound audible.
</summary>
    <author>
      <name>Henning Thielemann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4236/jsip.2010.11001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4236/jsip.2010.11001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 22 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0911.5171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.5171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.2441v1</id>
    <updated>2010-03-11T23:00:15Z</updated>
    <published>2010-03-11T23:00:15Z</published>
    <title>Up-sampling and Natural Sample Value Computation for Digital Pulse Width
  Modulators</title>
    <summary>  Digital pulse width modulation has been considered for high-fidelity and
high-efficiency audio amplifiers for several years. It has been shown that the
distortion can be reduced and the implementation of the system can be
simplified if the switching frequency is much higher than the Nyquist rate of
the modulating waveform. Hence, the input digital source is normally upsampled
to a higher frequency. It was also proved that converting uniform samples to
natural samples will decrease the harmonic distortion. Thus, in this paper, we
examine a new approach that combines upsampling, digital interpolation and
natural sampling conversion. This approach uses poly-phase implementation of
the digital interpolation filter and digital differentiators. We will show that
the structure consists of an FIR-type linear stage and a nonlinear stage. Some
spectral simulation results of a pulse width modulation system based on this
approach will also be presented. Finally, we will discuss the improvement of
the new approach over old algorithms.
</summary>
    <author>
      <name>Kien C. Nguyen</name>
    </author>
    <author>
      <name>Dilip V. Sarwate</name>
    </author>
    <link href="http://arxiv.org/abs/1003.2441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.2441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4908v1</id>
    <updated>2010-03-25T14:24:52Z</updated>
    <published>2010-03-25T14:24:52Z</published>
    <title>Perceptual analyses of action-related impact sounds</title>
    <summary>  Among environmental sounds, we have chosen to study a class of action-related
impact sounds: automobile door closure sounds. We propose to describe these
sounds using a model composed of perceptual properties. The development of the
perceptual model was derived from the evaluation of many door closure sounds
measured under controlled laboratory listening conditions. However, listening
to such sounds normally occurs within a natural context, which probably
modifies their perception. We therefore need to study differences between the
real situation and the laboratory situation by following standard practices in
order to specify the precise listening conditions and observe the influence of
previous learning, expectations, action-perception interactions, and attention
given to sounds. Our process consists in doing in situ experiments that are
compared with specific laboratory experiments in order to isolate certain
influential, context dependent components.
</summary>
    <author>
      <name>Marie-Céline Bezat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PSA</arxiv:affiliation>
    </author>
    <author>
      <name>Vincent Roussarie</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PSA</arxiv:affiliation>
    </author>
    <author>
      <name>Kronland-Martinet Richard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LMA</arxiv:affiliation>
    </author>
    <author>
      <name>Solvi Ystad</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LMA</arxiv:affiliation>
    </author>
    <author>
      <name>Stephen Mcadams</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Euronoise 2006, Tampere : France (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.4908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.5623v1</id>
    <updated>2010-03-29T17:48:22Z</updated>
    <published>2010-03-29T17:48:22Z</published>
    <title>Spoken Language Identification Using Hybrid Feature Extraction Methods</title>
    <summary>  This paper introduces and motivates the use of hybrid robust feature
extraction technique for spoken language identification (LID) system. The
speech recognizers use a parametric form of a signal to get the most important
distinguishable features of speech signal for recognition task. In this paper
Mel-frequency cepstral coefficients (MFCC), Perceptual linear prediction
coefficients (PLP) along with two hybrid features are used for language
Identification. Two hybrid features, Bark Frequency Cepstral Coefficients
(BFCC) and Revised Perceptual Linear Prediction Coefficients (RPLP) were
obtained from combination of MFCC and PLP. Two different classifiers, Vector
Quantization (VQ) with Dynamic Time Warping (DTW) and Gaussian Mixture Model
(GMM) were used for classification. The experiment shows better identification
rate using hybrid feature extraction techniques compared to conventional
feature extraction methods.BFCC has shown better performance than MFCC with
both classifiers. RPLP along with GMM has shown best identification performance
among all feature extraction techniques.
</summary>
    <author>
      <name>Pawan Kumar</name>
    </author>
    <author>
      <name>Astik Biswas</name>
    </author>
    <author>
      <name>A . N. Mishra</name>
    </author>
    <author>
      <name>Mahesh Chandra</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Telecommunications, Volume 1, Issue 2, pp11-15, March
  2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.5623v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.5623v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.5627v1</id>
    <updated>2010-03-29T17:54:55Z</updated>
    <published>2010-03-29T17:54:55Z</published>
    <title>Wavelet-Based Mel-Frequency Cepstral Coefficients for Speaker
  Identification using Hidden Markov Models</title>
    <summary>  To improve the performance of speaker identification systems, an effective
and robust method is proposed to extract speech features, capable of operating
in noisy environment. Based on the time-frequency multi-resolution property of
wavelet transform, the input speech signal is decomposed into various frequency
channels. For capturing the characteristic of the signal, the Mel-Frequency
Cepstral Coefficients (MFCCs) of the wavelet channels are calculated. Hidden
Markov Models (HMMs) were used for the recognition stage as they give better
recognition for the speaker's features than Dynamic Time Warping (DTW).
Comparison of the proposed approach with the MFCCs conventional feature
extraction method shows that the proposed method not only effectively reduces
the influence of noise, but also improves recognition. A recognition rate of
99.3% was obtained using the proposed feature extraction technique compared to
98.7% using the MFCCs. When the test patterns were corrupted by additive white
Gaussian noise with 20 dB S/N ratio, the recognition rate was 97.3% using the
proposed method compared to 93.3% using the MFCCs.
</summary>
    <author>
      <name>Mahmoud I. Abdalla</name>
    </author>
    <author>
      <name>Hanaa S. Ali</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Telecommunications, Volume 1, Issue 2, pp16-21, March
  2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.5627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.5627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.0831v1</id>
    <updated>2010-06-04T09:58:27Z</updated>
    <published>2010-06-04T09:58:27Z</published>
    <title>Treatment the Effects of Studio Wall Resonance and Coincidence Phenomena
  for Recording Noisy Speech Via FPGA Digital Filter</title>
    <summary>  This work introduces an economic solution for the problems of sound
insulation of recording studios. Sound insulation at wall resonance frequency
is weak. Instead of acoustical treatment, a digital filter is used to eliminate
the effects of wall resonance and coincidence phenomena on recording of speech.
Sound insulation of studio is measured to calculate the wall resonance
frequency and the coincidence frequency. Pole /zero placement technique is used
to calculate the IIR filter coefficients. The digital filter is designed,
simulated and implemented. The proposed system is used to treat these problems
and it is shown to be effective in recording the noisy speech. In this work
digital signal processing is used instead of the acoustic treatment to
eliminate the effect of noise at the studio wall resonance. This technique is
cheap and effective in canceling the noise at the desired frequencies. Field
Programmable Gate Array (FPGA) is used for hardware implementation of the
proposed filter structure which provides fast and cheap solution for processing
real time audio signals. The implementation is carried out using Spartan chip
from Xinlinx achieving higher performance than commercially available software
solutions.
</summary>
    <author>
      <name>Mahmoud I. A. Abdalla</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Journal of Telecommunications, see
  http://sites.google.com/site/journaloftelecommunications/volume-2-issue-2-may-2010</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Telecommunications,Volume 2, Issue 2, pp42-48, May 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.0831v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.0831v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.0866v1</id>
    <updated>2010-06-04T11:20:08Z</updated>
    <published>2010-06-04T11:20:08Z</published>
    <title>A Study on the Interactive "HOPSCOTCH" Game for the Children Using
  Computer Music Techniques</title>
    <summary>  "Hopscotch" is a world-wide game for children to play since the times in the
ancient Roman Empire and China. Here we present a study mainly focused on the
research and discussion of the application on the children's well-know
edutainment via the physical interactive design to provide the sensing of the
times for the conventional hopscotch, which is a new type of experiment for the
technology aided edutainment. The innovated hopscotch music game involves the
sound samples of various animals and the characters of cartoon, and the
algorithmic composition via the development of the music technology based
interactive game, to gradually make the children perceive the world of digits,
sound, and music. It can guide the growing children's personality and character
from disorder into clarity. Furthermore, the traditional teaching materials can
be improved via the implementation of the electrical sensing devices,
electrical I/O module, and the computer music program Max/MSP, to integrate the
interactive computer music with the interactive and immersive soundscapes
composition, and the teaching tool with educational gaming is completely
accomplished eventually.
</summary>
    <author>
      <name>Shing-Kwei Tzeng</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Kainan University, Taiwan and</arxiv:affiliation>
    </author>
    <author>
      <name>Chih-Fang Huang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Yuan Ze University, Taiwan</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijma.2010.2203</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijma.2010.2203" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International journal of Multimedia &amp; Its Applications 2.2 (2010)
  32-44</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.0866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.0866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.5761v1</id>
    <updated>2010-09-29T03:20:40Z</updated>
    <published>2010-09-29T03:20:40Z</published>
    <title>Approximate Maximum A Posteriori Inference with Entropic Priors</title>
    <summary>  In certain applications it is useful to fit multinomial distributions to
observed data with a penalty term that encourages sparsity. For example, in
probabilistic latent audio source decomposition one may wish to encode the
assumption that only a few latent sources are active at any given time. The
standard heuristic of applying an L1 penalty is not an option when fitting the
parameters to a multinomial distribution, which are constrained to sum to 1. An
alternative is to use a penalty term that encourages low-entropy solutions,
which corresponds to maximum a posteriori (MAP) parameter estimation with an
entropic prior. The lack of conjugacy between the entropic prior and the
multinomial distribution complicates this approach. In this report I propose a
simple iterative algorithm for MAP estimation of multinomial distributions with
sparsity-inducing entropic priors.
</summary>
    <author>
      <name>Matthew D. Hoffman</name>
    </author>
    <link href="http://arxiv.org/abs/1009.5761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.5761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.1682v1</id>
    <updated>2011-01-09T23:02:52Z</updated>
    <published>2011-01-09T23:02:52Z</published>
    <title>Detecting gross alignment errors in the Spoken British National Corpus</title>
    <summary>  The paper presents methods for evaluating the accuracy of alignments between
transcriptions and audio recordings. The methods have been applied to the
Spoken British National Corpus, which is an extensive and varied corpus of
natural unscripted speech. Early results show good agreement with human ratings
of alignment accuracy. The methods also provide an indication of the location
of likely alignment problems; this should allow efficient manual examination of
large corpora. Automatic checking of such alignments is crucial when analysing
any very large corpus, since even the best current speech alignment systems
will occasionally make serious errors. The methods described here use a hybrid
approach based on statistics of the speech signal itself, statistics of the
labels being evaluated, and statistics linking the two.
</summary>
    <author>
      <name>Ladan Baghai-Ravary</name>
    </author>
    <author>
      <name>Sergio Grau</name>
    </author>
    <author>
      <name>Greg Kochanski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Four pages, 3 figures. Presented at "New Tools and Methods for
  Very-Large-Scale Phonetics Research", University of Pennsylvania, January
  28-31, 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.1682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.1682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.3544v2</id>
    <updated>2013-07-17T17:31:43Z</updated>
    <published>2011-03-29T22:29:39Z</published>
    <title>An automatic volume control for preserving intelligibility</title>
    <summary>  A new method has been developed to adjust volume automatically on all audio
devices equipped with at least one microphone, including mobile phones,
personal media players, headsets, and car radios, that might be used in noisy
environments, such as crowds, cars, and outdoors. The method uses a patented
set of algorithms, implemented on the chips in such devices, to preserve
constant intelligibility of speech in noisy environments, rather than constant
signal-to-noise ratio. The algorithms analyze the noise background in real time
and compensate only for fluctuating noise in the frequency domain and the time
domain that interferes with intelligibility of speech. Advantages of this
method of controlling volume include: Controlling volume without sacrificing
clarity; adjusting only for persistent speech-interference noise; smoothing
volume fluctuations; and eliminating static-like bursts caused by noise spikes.
Practical human-factors approaches to implementing these algorithms in mobile
phones are discussed.
</summary>
    <author>
      <name>Franklin Felber</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SARNOF.2011.5876448</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SARNOF.2011.5876448" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, published in Proceedings of 34th IEEE Sarnoff
  Symposium, Princeton, NJ, 3-4 May 2011. Version 2 has typographical changes
  only</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.3544v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.3544v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.0760v1</id>
    <updated>2011-06-03T20:22:03Z</updated>
    <published>2011-06-03T20:22:03Z</published>
    <title>Simulating the Electroweak Phase Transition: Sonification of Bubble
  Nucleation</title>
    <summary>  As an applicaton of sonification, a simulation of the early universe was
developed to portray a phase transition that occurred shortly after the Big
Bang. The Standard Model of particle physics postulates that a hypothetical
particle, the Higgs boson, is responsible for the breaking of the symmetry
between the electromagnetic force and the weak force. This phase transition may
have been responsible for triggering Baryogenesis, the generation of an
abundance of matter over anti-matter. This hypothesis is known as Electroweak
Baryogenesis. In this simulation, aspects of bubble nucleation in Standard
Model Electroweak Baryogenesis were examined and modeled using Mathematica, and
sonified using SuperCollider3. The resulting simulation, which has been used
for pedagogical purposes by one of the authors, suggests interesting
possibilities for the integration of science and aesthetics as well as auditory
perception. The sonification component in particular also had the unexpected
benefit of being useful in debugging the Mathematica code.
</summary>
    <author>
      <name>R. Michael Winters</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">College of Wooster</arxiv:affiliation>
    </author>
    <author>
      <name>Andrew Blaikie</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">College of Wooster</arxiv:affiliation>
    </author>
    <author>
      <name>Deva O'Neil</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Bridgewater College</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures. To appear in Proc. ICAD 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.0760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.0760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.0844v1</id>
    <updated>2011-06-04T18:10:09Z</updated>
    <published>2011-06-04T18:10:09Z</published>
    <title>A Fast Affine Projection Algorithm Based on Matching Pursuit in Adaptive
  Noise Cancellation for Speech Enhancement</title>
    <summary>  In many application of noise cancellation, the changes in signal
characteristics could be quite fast. This requires the utilization of adaptive
algorithms, which converge rapidly. Least Mean Squares (LMS) adaptive filters
have been used in a wide range of signal processing application. The Recursive
Least Squares (RLS) algorithm has established itself as the "ultimate" adaptive
filtering algorithm in the sense that it is the adaptive filter exhibiting the
best convergence behavior. Unfortunately, practical implementations of the
algorithm are often associated with high computational complexity and/or poor
numerical properties. Recently adaptive filtering was presented that was based
on Matching Pursuits, have a nice tradeoff between complexity and the
convergence speed. This paper describes a new approach for noise cancellation
in speech enhancement using the new adaptive filtering algorithm named fast
affine projection algorithm (FAPA). The simulation results demonstrate the good
performance of the FAPA in attenuating the noise.
</summary>
    <author>
      <name>Sayed A. Hadei</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Student Member, IEEE</arxiv:affiliation>
    </author>
    <author>
      <name>N. Sonbolestan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2010 International Conference on Intelligent Systems, Modelling and
  Simulation, Liverpool, UK</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.0844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.0844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.0846v1</id>
    <updated>2011-06-04T18:25:34Z</updated>
    <published>2011-06-04T18:25:34Z</published>
    <title>A Family of Adaptive Filter Algorithms in Noise Cancellation for Speech
  Enhancement</title>
    <summary>  In many application of noise cancellation, the changes in signal
characteristics could be quite fast. This requires the utilization of adaptive
algorithms, which converge rapidly. Least Mean Squares (LMS) and Normalized
Least Mean Squares (NLMS) adaptive filters have been used in a wide range of
signal processing application because of its simplicity in computation and
implementation. The Recursive Least Squares (RLS) algorithm has established
itself as the "ultimate" adaptive filtering algorithm in the sense that it is
the adaptive filter exhibiting the best convergence behavior. Unfortunately,
practical implementations of the algorithm are often associated with high
computational complexity and/or poor numerical properties. Recently adaptive
filtering was presented, have a nice tradeoff between complexity and the
convergence speed. This paper describes a new approach for noise cancellation
in speech enhancement using the two new adaptive filtering algorithms named
fast affine projection algorithm and fast Euclidean direction search algorithms
for attenuating noise in speech signals. The simulation results demonstrate the
good performance of the two new algorithms in attenuating the noise.
</summary>
    <author>
      <name>Sayed. A. Hadei</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Student Member IEEE</arxiv:affiliation>
    </author>
    <author>
      <name>M. lotfizad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer and Electrical Engineering, Vol. 2,
  No. 2, pp. 307-315, Apr. 2010. Singapore</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.0846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.0846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.1199v1</id>
    <updated>2011-06-06T21:03:31Z</updated>
    <published>2011-06-06T21:03:31Z</published>
    <title>Open-loop multi-channel inversion of room impulse response</title>
    <summary>  This paper considers methods for audio display in a CAVE-type virtual reality
theater, a 3 m cube with displays covering all six rigid faces. Headphones are
possible since the user's headgear continuously measures ear positions, but
loudspeakers are preferable since they enhance the sense of total immersion.
The proposed solution consists of open-loop acoustic point control. The
transfer function, a matrix of room frequency responses from the loudspeakers
to the ears of the user, is inverted using multi-channel inversion methods, to
create exactly the desired sound field at the user's ears. The inverse transfer
function is constructed from impulse responses simulated by the image source
method. This technique is validated by measuring a 2x2 matrix transfer
function, simulating a transfer function with the same geometry, and filtering
the measured transfer function through the inverse of the simulation. Since
accuracy of the image source method decreases with time, inversion performance
is improved by windowing the simulated response prior to inversion. Parameters
of the simulation and inversion are adjusted to minimize residual reverberant
energy; the best-case dereverberation ratio is 10 dB.
</summary>
    <author>
      <name>Bowon Lee</name>
    </author>
    <author>
      <name>Camille Goudeseune</name>
    </author>
    <author>
      <name>Mark A. Hasegawa-Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 24 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.1199v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.1199v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="76Q05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; G.2.3; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.4185v1</id>
    <updated>2011-07-21T07:17:34Z</updated>
    <published>2011-07-21T07:17:34Z</published>
    <title>Estimation of Severity of Speech Disability through Speech Envelope</title>
    <summary>  In this paper, envelope detection of speech is discussed to distinguish the
pathological cases of speech disabled children. The speech signal samples of
children of age between five to eight years are considered for the present
study. These speech signals are digitized and are used to determine the speech
envelope. The envelope is subjected to ratio mean analysis to estimate the
disability. This analysis is conducted on ten speech signal samples which are
related to both place of articulation and manner of articulation. Overall
speech disability of a pathological subject is estimated based on the results
of above analysis.
</summary>
    <author>
      <name>Anandthirtha B. Gudi</name>
    </author>
    <author>
      <name>H. K. Shreedhar</name>
    </author>
    <author>
      <name>H. C. Nagaraj</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/sipij.2011.2203</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/sipij.2011.2203" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,4 Figures,Signal &amp; Image Processing Journal AIRCC</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.2, No.2, June 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1107.4185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.4185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.5492v1</id>
    <updated>2011-07-27T14:42:52Z</updated>
    <published>2011-07-27T14:42:52Z</published>
    <title>Application of Gammachirp Auditory Filter as a Continuous Wavelet
  Analysis</title>
    <summary>  This paper presents a new method on the use of the gammachirp auditory filter
based on a continuous wavelet analysis. The gammachirp auditory filter is
designed to provide a spectrum reflecting the spectral properties of the
cochlea, which is responsible for frequency analysis in the human auditory
system. The impulse response of the theoretical gammachirp auditory filter that
has been developed by Irino and Patterson can be used as the kernel for wavelet
transform which approximates the frequency response of the cochlea. This study
implements the gammachirp auditory filter described by Irino as an analytical
wavelet and examines its application to a different speech signals.
  The obtained results will be compared with those obtained by two other
predefined wavelet families that are Morlet and Mexican Hat. The results show
that the gammachirp wavelet family gives results that are comparable to ones
obtained by Morlet and Mexican Hat wavelet family.
</summary>
    <author>
      <name>Lotfi Salhi</name>
    </author>
    <author>
      <name>Kais Ouni</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/sipij</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/sipij" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 17 figures;
  http://airccse.org/journal/sipij/papers/2011sipij10.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.5492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.5492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.5876v1</id>
    <updated>2011-09-27T13:09:11Z</updated>
    <published>2011-09-27T13:09:11Z</published>
    <title>Rényi Information Measures for Spectral Change Detection</title>
    <summary>  Change detection within an audio stream is an important task in several
domains, such as classification and segmentation of a sound or of a music
piece, as well as indexing of broadcast news or surveillance applications. In
this paper we propose two novel methods for spectral change detection without
any assumption about the input sound: they are both based on the evaluation of
information measures applied to a time- frequency representation of the signal,
and in particular to the spectrogram. The class of measures we consider, the
R\'enyi entropies, are obtained by extending the Shannon entropy definition: a
biasing of the spectrogram coefficients is realized through the dependence of
such measures on a parameter, which allows refined results compared to those
obtained with standard divergences. These methods provide a low computational
cost and are well-suited as a support for higher level analysis, segmentation
and classification algorithms.
</summary>
    <author>
      <name>Marco Liuni</name>
    </author>
    <author>
      <name>Axel Röbel</name>
    </author>
    <author>
      <name>Marco Romito</name>
    </author>
    <author>
      <name>Xavier Rodet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2011 IEEE Conference on Acoustics, Speech and Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.5876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.5876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.6270v1</id>
    <updated>2011-09-27T10:04:45Z</updated>
    <published>2011-09-27T10:04:45Z</published>
    <title>Fractal String Generation and Its Application in Music Composition</title>
    <summary>  Music is a string of some of the notes out of 12 notes (Sa, Komal_re, Re,
Komal_ga, Ga, Ma, Kari_ma, Pa, Komal_dha, Dha, Komal_ni, Ni) and their
harmonics. Each note corresponds to a particular frequency. When such strings
are encoded to form discrete sequences, different frequencies present in the
music corresponds to different amplitude levels (value) of the discrete
sequence. Initially, a class of discrete sequences has been generated using
logistic map. All these discrete sequences have at most n-different amplitude
levels (value) (depending on the particular raga). Without loss of generality,
we have chosen two discrete sequences of two types of Indian raga viz. Bhairabi
and Bhupali having same number of amplitude levels to obtain/search close
relatives from the class. The relative / closeness can be assured through
correlation coefficient.The search is unbiased, random and non-adaptive. The
obtained string is that which maximally resembles the given two sequences. The
same can be thought of as a music composition of the given two strings. It is
to be noted that all these string are fractal string which can be persuaded by
fractal dimension.
</summary>
    <author>
      <name>Avishek Ghosh</name>
    </author>
    <author>
      <name>Joydeep Banerjee</name>
    </author>
    <author>
      <name>Sk. S. Hassan</name>
    </author>
    <author>
      <name>P. Pal Choudhury</name>
    </author>
    <link href="http://arxiv.org/abs/1109.6270v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.6270v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.6313v1</id>
    <updated>2011-09-27T10:36:01Z</updated>
    <published>2011-09-27T10:36:01Z</published>
    <title>A Reduced Multiple Gabor Frame for Local Time Adaptation of the
  Spectrogram</title>
    <summary>  In this paper we propose a method for automatic local time adap- tation of
the spectrogram of an audio signal, based on its decomposition within a Gabor
multi-frame. The sparsity of the analyses within each individual frame is
evaluated through the R\'enyi entropies measures. According to the sparsity of
the decompositions, an optimal resolution and a reduced multi-frame are
determined, defining an adapted spectrogram with variable resolution and hop
size. The composition of such a reduced multi-frame allows an immediate
definition of a dual frame: re-synthesis techniques for this adapted analysis
are easily derived by the traditional phase vocoder scheme.
</summary>
    <author>
      <name>M. Liuni</name>
    </author>
    <author>
      <name>A. Röbel</name>
    </author>
    <author>
      <name>M. Romito</name>
    </author>
    <author>
      <name>X. Rodet</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of the 13th Int. Conference on Digital Audio Effects
  (DAFx-10), Graz, Austria , September 6-10, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.6313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.6313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.6314v1</id>
    <updated>2011-09-27T08:32:50Z</updated>
    <published>2011-09-27T08:32:50Z</published>
    <title>An Entropy Based Method for Local Time-Adaptation of the Spectrogram</title>
    <summary>  We propose a method for automatic local time-adaptation of the spectrogram of
audio signals: it is based on the decomposition of a signal within a Gabor
multi-frame through the STFT operator. The sparsity of the analysis in every
individual frame of the multi-frame is evaluated through the R\'enyi entropy
measures: the best local resolution is determined minimizing the entropy
values. The overall spectrogram of the signal we obtain thus provides local
optimal resolution adaptively evolving over time. We give examples of the
performance of our algorithm with an instrumental sound and a synthetic one,
showing the improvement in spectrogram displaying obtained with an automatic
adaptation of the resolution. The analysis operator is invertible, thus leading
to a perfect reconstruction of the original signal through the analysis
coefficients.
</summary>
    <author>
      <name>M. Liuni</name>
    </author>
    <author>
      <name>A. Röbel</name>
    </author>
    <author>
      <name>M. Romito</name>
    </author>
    <author>
      <name>X. Rodet</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CMMR 2010, LNCS 6684, pp. 60-75, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.6314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.6314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.6651v1</id>
    <updated>2011-09-29T14:06:15Z</updated>
    <published>2011-09-29T14:06:15Z</published>
    <title>Sound Analysis and Synthesis Adaptive in Time and Two Frequency Bands</title>
    <summary>  We present an algorithm for sound analysis and resynthesis with local
automatic adaptation of time-frequency resolution. There exists several
algorithms allowing to adapt the analysis window depending on its time or
frequency location; in what follows we propose a method which select the
optimal resolution depending on both time and frequency. We consider an
approach that we denote as analysis-weighting, from the point of view of Gabor
frame theory. We analyze in particular the case of different adaptive
time-varying resolutions within two complementary frequency bands; this is a
typical case where perfect signal reconstruction cannot in general be achieved
with fast algorithms, causing a certain error to be minimized. We provide
examples of adaptive analyses of a music sound, and outline several
possibilities that this work opens.
</summary>
    <author>
      <name>Marco Liuni</name>
    </author>
    <author>
      <name>Peter Balazs</name>
    </author>
    <author>
      <name>Axel Röbel</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of the 14th Int. Conference on Digital Audio Effects
  (DAFx-11), Paris, France, September 19-23, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1109.6651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.6651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2541v1</id>
    <updated>2012-04-11T07:01:31Z</updated>
    <published>2012-04-11T07:01:31Z</published>
    <title>Employing Subsequence Matching in Audio Data Processing</title>
    <summary>  We overview current problems of audio retrieval and time-series subsequence
matching. We discuss the usage of subsequence matching approaches in audio data
processing, especially in automatic speech recognition (ASR) area and we aim at
improving performance of the retrieval process. To overcome the problems known
from the time-series area like the occurrence of implementation bias and data
bias we present a Subsequence Matching Framework as a tool for fast
prototyping, building, and testing similarity search subsequence matching
applications. The framework is build on top of MESSIF (Metric Similarity Search
Implementation Framework) and thus the subsequence matching algorithms can
exploit advanced similarity indexes in order to significantly increase their
query processing performance. To prove our concept we provide a design of
query-by-example spoken term detection type of application with the usage of
phonetic posteriograms and subsequence matching approach.
</summary>
    <author>
      <name>Petr Volny</name>
    </author>
    <author>
      <name>David Novak</name>
    </author>
    <author>
      <name>Pavel Zezula</name>
    </author>
    <link href="http://arxiv.org/abs/1204.2541v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2541v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.5651v1</id>
    <updated>2012-05-25T09:54:24Z</updated>
    <published>2012-05-25T09:54:24Z</published>
    <title>Measuring the evolution of contemporary western popular music</title>
    <summary>  Popular music is a key cultural expression that has captured listeners'
attention for ages. Many of the structural regularities underlying musical
discourse are yet to be discovered and, accordingly, their historical evolution
remains formally unknown. Here we unveil a number of patterns and metrics
characterizing the generic usage of primary musical facets such as pitch,
timbre, and loudness in contemporary western popular music. Many of these
patterns and metrics have been consistently stable for a period of more than
fifty years, thus pointing towards a great degree of conventionalism.
Nonetheless, we prove important changes or trends related to the restriction of
pitch transitions, the homogenization of the timbral palette, and the growing
loudness levels. This suggests that our perception of the new would be rooted
on these changing characteristics. Hence, an old tune could perfectly sound
novel and fashionable, provided that it consisted of common harmonic
progressions, changed the instrumentation, and increased the average loudness.
</summary>
    <author>
      <name>Joan Serrà</name>
    </author>
    <author>
      <name>Álvaro Corral</name>
    </author>
    <author>
      <name>Marián Boguñá</name>
    </author>
    <author>
      <name>Martín Haro</name>
    </author>
    <author>
      <name>Josep Lluis Arcos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/srep00521</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/srep00521" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Supplementary materials not included. Please see the journal
  reference or contact the authors</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scientific Reports 2, 521 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1205.5651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.5651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.1450v1</id>
    <updated>2012-06-07T11:08:25Z</updated>
    <published>2012-06-07T11:08:25Z</published>
    <title>A comparative study of performance of fpga based mel filter bank &amp; bark
  filter bank</title>
    <summary>  The sensitivity of human ear is dependent on frequency which is nonlinearly
resolved across the audio spectrum .Now to improve the recognition performance
in a similar non linear approach requires a front -end design, suggested by
empirical evidences. A popular alternative to linear prediction based analysis
is therefore filter bank analysis since this provides a much more
straightforward route to obtain the desired non-linear frequency resolution.
MEL filter bank and BARK filter bank are two popular filter bank analysis
techniques. This paper presents FPGA based implementation of MEL filter bank
and BARK filter bank with different bandwidths and different signal spectrum
ranges. The designs have been implemented using VHDL, simulated and verified
using Xilinx 11.1.For each filter bank, the basic building block is implemented
in Spartan 3E. A comparative study among these two mentioned filter banks is
also done in this paper.
</summary>
    <author>
      <name>Debalina Ghosh</name>
    </author>
    <author>
      <name>Depanwita Sarkar Debnath</name>
    </author>
    <author>
      <name>Saikat Bose</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 20 figures, 6 tables; International Journal of Artificial
  Intelligence &amp; Applications (IJAIA), Vol.3, No.3, May 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.1450v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.1450v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.5104v1</id>
    <updated>2012-07-21T05:25:27Z</updated>
    <published>2012-07-21T05:25:27Z</published>
    <title>Analysis of speech under stress using Linear techniques and Non-Linear
  techniques for emotion recognition system</title>
    <summary>  Analysis of speech for recognition of stress is important for identification
of emotional state of person. This can be done using 'Linear Techniques', which
has different parameters like pitch, vocal tract spectrum, formant frequencies,
Duration, MFCC etc. which are used for extraction of features from speech.
TEO-CB-Auto-Env is the method which is non-linear method of features
extraction. Analysis is done using TU-Berlin (Technical University of Berlin)
German database. Here emotion recognition is done for different emotions like
neutral, happy, disgust, sad, boredom and anger. Emotion recognition is used in
lie detector, database access systems, and in military for recognition of
soldiers' emotion identification during the war.
</summary>
    <author>
      <name>A. A. Khulage</name>
    </author>
    <author>
      <name>Prof. B. V. Pathak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.5104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.5104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.5560v1</id>
    <updated>2012-07-23T23:25:36Z</updated>
    <published>2012-07-23T23:25:36Z</published>
    <title>Evolving Musical Counterpoint: The Chronopoint Musical Evolution System</title>
    <summary>  Musical counterpoint, a musical technique in which two or more independent
melodies are played simultaneously with the goal of creating harmony, has been
around since the baroque era. However, to our knowledge computational
generation of aesthetically pleasing linear counterpoint based on subjective
fitness assessment has not been explored by the evolutionary computation
community (although generation using objective fitness has been attempted in
quite a few cases). The independence of contrapuntal melodies and the
subjective nature of musical aesthetics provide an excellent platform for the
application of genetic algorithms. In this paper, a genetic algorithm approach
to generating contrapuntal melodies is explained, with a description of the
various musical heuristics used and of how variable-length chromosome strings
are used to avoid generating "jerky" rhythms and melodic phrases, as well as
how subjectivity is incorporated into the algorithm's fitness measures. Next,
results from empirical testing of the algorithm are presented, with a focus on
how a user's musical sophistication influences their experience. Lastly,
further musical and compositional applications of the algorithm are discussed
along with planned future work on the algorithm.
</summary>
    <author>
      <name>Jeffrey Power Jacobs</name>
    </author>
    <author>
      <name>James Reggia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Workshop on Evolutionary
  Music, 2011 IEEE Congress on Evolutionary Computation, 6-11 (2011)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1207.5560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.5560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0451v2</id>
    <updated>2015-01-25T02:31:09Z</updated>
    <published>2012-12-03T17:06:41Z</published>
    <title>Semi-blind Source Separation via Sparse Representations and Online
  Dictionary Learning</title>
    <summary>  This work examines a semi-blind single-channel source separation problem. Our
specific aim is to separate one source whose local structure is approximately
known, from another a priori unspecified background source, given only a single
linear combination of the two sources. We propose a separation technique based
on local sparse approximations along the lines of recent efforts in sparse
representations and dictionary learning. A key feature of our procedure is the
online learning of dictionaries (using only the data itself) to sparsely model
the background source, which facilitates its separation from the
partially-known source. Our approach is applicable to source separation
problems in various application domains; here, we demonstrate the performance
of our proposed approach via simulation on a stylized audio source separation
task.
</summary>
    <author>
      <name>Sirisha Rambhatla</name>
    </author>
    <author>
      <name>Jarvis D. Haupt</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACSSC.2013.6810587</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACSSC.2013.6810587" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, In Proceedings of the 47th Asilomar Conference on Signals
  Systems and Computers, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.0451v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.0451v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.6350v1</id>
    <updated>2012-12-27T11:31:16Z</updated>
    <published>2012-12-27T11:31:16Z</published>
    <title>Single-sided Real-time PESQ Score Estimation</title>
    <summary>  For several years now, the ITU-T's Perceptual Evaluation of Speech Quality
(PESQ) has been the reference for objective speech quality assessment. It is
widely deployed in commercial QoE measurement products, and it has been well
studied in the literature. While PESQ does provide reasonably good correlation
with subjective scores for VoIP applications, the algorithm itself is not
usable in a real-time context, since it requires a reference signal, which is
usually not available in normal conditions. In this paper we provide an
alternative technique for estimating PESQ scores in a single-sided fashion,
based on the Pseudo Subjective Quality Assessment (PSQA) technique.
</summary>
    <author>
      <name>Sebastián Basterrech</name>
    </author>
    <author>
      <name>Gerardo Rubino</name>
    </author>
    <author>
      <name>Martín Varela</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceeding of Measurement of Speech, Audio and Video Quality in
  Networks (MESAQIN'09), Prague, Czech Republic, June 2009, pp. 94-99</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.6350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.6350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="82C32, 62P30, 62M20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.4; D.4.4; I.5.1; B.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.6903v1</id>
    <updated>2012-12-31T15:01:37Z</updated>
    <published>2012-12-31T15:01:37Z</published>
    <title>About Multichannel Speech Signal Extraction and Separation Techniques</title>
    <summary>  The extraction of a desired speech signal from a noisy environment has become
a challenging issue. In the recent years, the scientific community has
particularly focused on multichannel techniques which are dealt with in this
review. In fact, this study tries to classify these multichannel techniques
into three main ones: Beamforming, Independent Com-ponent Analysis (ICA) and
Time Frequency (T-F) masking. This paper also highlights their advantages and
drawbacks. However these previously mentioned techniques could not afford
satisfactory results. This fact leads to the idea that a combination of those
techniques, which is depicted along this study, may probably provide more
efficient results. In-deed, giving the fact that those approaches are still be
considered as being not totally efficient, has led us to review these mentioned
above in the hope that further researches will provide this domain with
suitable innovations.
</summary>
    <author>
      <name>Adel Hidri</name>
    </author>
    <author>
      <name>Souad Meddeb</name>
    </author>
    <author>
      <name>Hamid Amiri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4236/jsip.2012.32032</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4236/jsip.2012.32032" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 11 Figures. arXiv admin note: substantial text overlap with
  arXiv:1212.6080</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Signal and Information Processing, Vol. 3 No. 2, 2012,
  pp. 238-247</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1212.6903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.6903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.0265v1</id>
    <updated>2013-01-02T17:06:58Z</updated>
    <published>2013-01-02T17:06:58Z</published>
    <title>Usable Speech Assignment for Speaker Identification under Co-Channel
  Situation</title>
    <summary>  Usable speech criteria are proposed to extract minimally corrupted speech for
speaker identification (SID) in co-channel speech. In co-channel speech, either
speaker can randomly appear as the stronger speaker or the weaker one at a
time. Hence, the extracted usable segments are separated in time and need to be
organized into speaker streams for SID. In this paper, we focus to organize
extracted usable speech segment into a single stream for the same speaker by
speaker assignment system. For this, we develop model-based speaker assignment
method based on posterior probability and exhaustive search algorithm.
Evaluation of this method is performed on TIMIT database. The system is
evaluated on co-channel speech and results show a significant improvement.
</summary>
    <author>
      <name>Wajdi Ghezaiel</name>
    </author>
    <author>
      <name>Amel Ben Slimane</name>
    </author>
    <author>
      <name>Ezzedine Ben Braiek</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/9646-4381</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/9646-4381" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications (IJCA)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications 59(18):7-11, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1301.0265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.1932v1</id>
    <updated>2013-01-09T17:42:59Z</updated>
    <published>2013-01-09T17:42:59Z</published>
    <title>An Approach for Classification of Dysfluent and Fluent Speech Using K-NN
  And SVM</title>
    <summary>  This paper presents a new approach for classification of dysfluent and fluent
speech using Mel-Frequency Cepstral Coefficient (MFCC). The speech is fluent
when person's speech flows easily and smoothly. Sounds combine into syllable,
syllables mix together into words and words link into sentences with little
effort. When someone's speech is dysfluent, it is irregular and does not flow
effortlessly. Therefore, a dysfluency is a break in the smooth, meaningful flow
of speech. Stuttering is one such disorder in which the fluent flow of speech
is disrupted by occurrences of dysfluencies such as repetitions, prolongations,
interjections and so on. In this work we have considered three types of
dysfluencies such as repetition, prolongation and interjection to characterize
dysfluent speech. After obtaining dysfluent and fluent speech, the speech
signals are analyzed in order to extract MFCC features. The k-Nearest Neighbor
(k-NN) and Support Vector Machine (SVM) classifiers are used to classify the
speech as dysfluent and fluent speech. The 80% of the data is used for training
and 20% for testing. The average accuracy of 86.67% and 93.34% is obtained for
dysfluent and fluent speech respectively.
</summary>
    <author>
      <name>P. Mahesha</name>
    </author>
    <author>
      <name>D. S. Vinod</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsea.2012.2603</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsea.2012.2603" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,4 figures; International Journal of Computer Science,
  Engineering and Applications (IJCSEA) Vol.2, No.6, December 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.1932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.1932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.4382v1</id>
    <updated>2013-02-15T16:44:17Z</updated>
    <published>2013-02-15T16:44:17Z</published>
    <title>Finite element computation of elliptical vocal tract impedances using
  the two-microphone transfer function method</title>
    <summary>  The experimental two-microphone transfer function method (TMTF) is adapted to
the numerical framework to compute the radiation and input impedances of
three-dimensional vocal tracts of elliptical cross section. In its simplest
version, the TMTF method only requires measuring the acoustic pressure at two
points in an impedance duct and the postprocessing of the corresponding
transfer function. However, some considerations are to be taken into account
when using the TMTF method in the numerical context, which constitute the main
objective of this paper. In particular, the importance of including absorption
at the impedance duct walls to avoid lengthy numerical simulations is discussed
and analytical complex axial wave numbers for elliptical ducts are derived for
this purpose. It is also shown how the plane wave restriction of the TMTF
method can be circumvented to some extent by appropriate location of the
virtual microphones, thus extending the method frequency range of validity.
Virtual microphone spacing is also discussed on the basis of the so called
singularity factor. Numerical examples include the computation of the radiation
impedance of vowels /a/, /i/ and /u/ and the input impedance of vowel /a/, for
simplified vocal tracts of circular and elliptical cross sections.
</summary>
    <author>
      <name>Marc Arnela</name>
    </author>
    <author>
      <name>Oriol Guasch</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1121/1.4803889</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1121/1.4803889" rel="related"/>
    <link href="http://arxiv.org/abs/1302.4382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.4382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.class-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.1023v2</id>
    <updated>2013-03-13T19:30:57Z</updated>
    <published>2013-03-05T13:15:16Z</published>
    <title>Consistent Iterative Hard Thresholding For Signal Declipping</title>
    <summary>  Clipping or saturation in audio signals is a very common problem in signal
processing, for which, in the severe case, there is still no satisfactory
solution. In such case, there is a tremendous loss of information, and
traditional methods fail to appropriately recover the signal. We propose a
novel approach for this signal restoration problem based on the framework of
Iterative Hard Thresholding. This approach, which enforces the consistency of
the reconstructed signal with the clipped observations, shows superior
performance in comparison to the state-of-the-art declipping algorithms. This
is confirmed on synthetic and on actual high-dimensional audio data processing,
both on SNR and on subjective user listening evaluations.
</summary>
    <author>
      <name>Srdjan Kitić</name>
    </author>
    <author>
      <name>Laurent Jacques</name>
    </author>
    <author>
      <name>Nilesh Madhu</name>
    </author>
    <author>
      <name>Michael Peter Hopwood</name>
    </author>
    <author>
      <name>Ann Spriet</name>
    </author>
    <author>
      <name>Christophe De Vleeschouwer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICASSP2013 conference paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.1023v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.1023v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.1141v1</id>
    <updated>2013-05-06T10:33:06Z</updated>
    <published>2013-05-06T10:33:06Z</published>
    <title>Acoustic Echo Cancellation Postfilter Design Issues For Speech
  Recognition System</title>
    <summary>  In this paper a generalized postfilter algorithm design issues are presented.
This postfilter is used to jointly suppress late reverberation, residual echo,
and background noise. When residual echo and noise are suppressed, the best
result obtains by suppressing both interferences together after the Acoustic
echo cancellation (AEC). The main advantage of this approach is that the
residual echo and noise suppression does not suffer from the existence of a
strong acoustic echo component. Furthermore, the Acoustic echo cancellation
(AEC) does not suffer from the time-varying noise suppression. A disadvantage
is that the input signal of the Acoustic echo cancellation (AEC) has a low
signal-to-noise ratio (SNR). To overcome this problem, algorithms have been
proposed where, apart from the joint suppression, a noise-reduced signal is
used to adapt the echo canceller.
</summary>
    <author>
      <name>Urmila Shrawankar</name>
    </author>
    <author>
      <name>V M Thakare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pages: 6</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Science and Advanced Technology (IJSAT),
  ISSN 2221-8386, Vol 1 No 5, July 2011, pp 38-43</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1305.1141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.1141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.1145v1</id>
    <updated>2013-05-06T10:42:34Z</updated>
    <published>2013-05-06T10:42:34Z</published>
    <title>Techniques for Feature Extraction In Speech Recognition System : A
  Comparative Study</title>
    <summary>  The time domain waveform of a speech signal carries all of the auditory
information. From the phonological point of view, it little can be said on the
basis of the waveform itself. However, past research in mathematics, acoustics,
and speech technology have provided many methods for converting data that can
be considered as information if interpreted correctly. In order to find some
statistically relevant information from incoming data, it is important to have
mechanisms for reducing the information of each segment in the audio signal
into a relatively small number of parameters, or features. These features
should describe each segment in such a characteristic way that other similar
segments can be grouped together by comparing their features. There are
enormous interesting and exceptional ways to describe the speech signal in
terms of parameters. Though, they all have their strengths and weaknesses, we
have presented some of the most used methods with their importance.
</summary>
    <author>
      <name>Urmila Shrawankar</name>
    </author>
    <author>
      <name>V M Thakare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pages: 9 Figures : 3</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal Of Computer Applications In Engineering,
  Technology and Sciences (IJCAETS),ISSN 0974-3596,2010,pp 412-418</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1305.1145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.1145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.1426v1</id>
    <updated>2013-05-07T07:21:06Z</updated>
    <published>2013-05-07T07:21:06Z</published>
    <title>Speech Enhancement Modeling Towards Robust Speech Recognition System</title>
    <summary>  Form about four decades human beings have been dreaming of an intelligent
machine which can master the natural speech. In its simplest form, this machine
should consist of two subsystems, namely automatic speech recognition (ASR) and
speech understanding (SU). The goal of ASR is to transcribe natural speech
while SU is to understand the meaning of the transcription. Recognizing and
understanding a spoken sentence is obviously a knowledge-intensive process,
which must take into account all variable information about the speech
communication process, from acoustics to semantics and pragmatics. While
developing an Automatic Speech Recognition System, it is observed that some
adverse conditions degrade the performance of the Speech Recognition System. In
this contribution, speech enhancement system is introduced for enhancing speech
signals corrupted by additive noise and improving the performance of Automatic
Speech Recognizers in noisy conditions. Automatic speech recognition
experiments show that replacing noisy speech signals by the corresponding
enhanced speech signals leads to an improvement in the recognition accuracies.
The amount of improvement varies with the type of the corrupting noise.
</summary>
    <author>
      <name>Urmila Shrawankar</name>
    </author>
    <author>
      <name>V. M. Thakare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pages: 04; Conference Proceedings International Conference on Advance
  Computing (ICAC-2008), India</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.1426v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.1426v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.0819v1</id>
    <updated>2013-11-04T19:22:39Z</updated>
    <published>2013-11-04T19:22:39Z</published>
    <title>Phoneme discrimination using neurons with symmetric nonlinear response
  over a spectral range</title>
    <summary>  We consider the ability of a very simple feed-forward neural network to
discriminate phonemes based on just relative power spectrum. The network
consists of two neurons with symmetric nonlinear response over a spectral
range. The output of the neurons is subsequently fed to a comparator. We show
that often this is enough to achieve complete separation of data. We compare
the performance of found discriminants with that of more general neurons. Our
conclusion is that not much is gained in passing to real-valued weights. More
likely higher number of neurons and preprocessing of input will yield better
discrimination results. The networks considered are directly amenable to
hardware (neuromorphic) designs. Other advantages include interpretability,
guarantees of performance on unseen data and low Kolmogorov complexity.
</summary>
    <author>
      <name>Ondrej Such</name>
    </author>
    <author>
      <name>Ondrej Skvarek</name>
    </author>
    <author>
      <name>Martin Klimo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, ICASSP 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.0819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.0819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.0842v1</id>
    <updated>2013-11-04T20:43:47Z</updated>
    <published>2013-11-04T20:43:47Z</published>
    <title>An Intuitive Design Approach For Implementing Real Time Audio Effects</title>
    <summary>  Audio effect implementation on random musical signal is a basic application
of digital signal processors. In this paper, the compatibility features of
MATLAB R2008a with Code Composer Studio 3.3 has been exploited to develop
Simulink models which when emulated on TMS320C6713 DSK generate real time audio
effects. Each design has been done by two different asynchronous scheduling
techniques: (i) Idle task Scheduling and (ii) DSP/BIOS task Scheduling. A basic
COCOMO analysis has been done for the generated code to justify the industrial
viability of this design approach.
  KEYWORDS: Musical signal processing, Real time Audio effects, Echo, Stress
Generation, Reverberation, Reverberated Chorus, Real Time Scheduling.
</summary>
    <author>
      <name>Mayukh Mukhopadhyay</name>
    </author>
    <author>
      <name>Om Ranjan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.7323/ijaet/v6_iss5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.7323/ijaet/v6_iss5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages,11 figures,4 tables,24 equations,9 references</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advances in Engineering &amp; Technology
  (IJAET), Volume 6 Issue 5(29), pp. 2216-2227, Nov. 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1311.0842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.0842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.2795v2</id>
    <updated>2013-12-11T07:27:01Z</updated>
    <published>2013-12-10T13:45:33Z</published>
    <title>Reverberant Audio Source Separation via Sparse and Low-Rank Modeling</title>
    <summary>  The performance of audio source separation from underdetermined convolutive
mixture assuming known mixing filters can be significantly improved by using an
analysis sparse prior optimized by a reweighting l1 scheme and a wideband
datafidelity term, as demonstrated by a recent article. In this letter, we show
that the performance can be improved even more significantly by exploiting a
low-rank prior on the source spectrograms.We present a new algorithm to
estimate the sources based on i) an analysis sparse prior, ii) a reweighting
scheme so as to increase the sparsity, iii) a wideband data-fidelity term in a
constrained form, and iv) a low-rank constraint on the source spectrograms.
Evaluation on reverberant music mixtures shows that the resulting algorithm
improves state-of-the-art methods by more than 2 dB of signal-to-distortion
ratio.
</summary>
    <author>
      <name>Simon Arberet</name>
    </author>
    <author>
      <name>Pierre Vandergheynst</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2014.2303135</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2014.2303135" rel="related"/>
    <link href="http://arxiv.org/abs/1312.2795v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.2795v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4127v1</id>
    <updated>2013-12-15T09:40:37Z</updated>
    <published>2013-12-15T09:40:37Z</published>
    <title>A Hybrid Approach for Co-Channel Speech Segregation based on CASA, HMM
  Multipitch Tracking, and Medium Frame Harmonic Model</title>
    <summary>  This paper proposes a hybrid approach for co-channel speech segregation. HMM
(hidden Markov model) is used to track the pitches of 2 talkers. The resulting
pitch tracks are then enriched with the prominent pitch. The enriched tracks
are correctly grouped using pitch continuity. Medium frame harmonics are used
to extract the second pitch for frames with only one pitch deduced using the
previous steps. Finally, the pitch tracks are input to CASA (computational
auditory scene analysis) to segregate the mixed speech. The center frequency
range of the gamma tone filter banks is maximized to reduce the overlap between
the channels filtered for better segregation. Experiments were conducted using
this hybrid approach on the speech separation challenge database and compared
to the single (non-hybrid) approaches, i.e. signal processing and CASA. Results
show that using the hybrid approach outperforms the single approaches.
</summary>
    <author>
      <name>Ashraf M. Mohy Eldin</name>
    </author>
    <author>
      <name>Aliaa A. A. Youssif</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14569/IJACSA.2013.040721</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14569/IJACSA.2013.040721" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: CASA (computational auditory scene analysis); co-channel
  speech segregation; HMM (hidden Markov model) tracking; hybrid speech
  segregation approach; medium frame harmonic model; multipitch tracking;
  prominent pitch</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Computer Science and
  Applications (IJACSA)Volume 4 Issue 7, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1312.4127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.1530v1</id>
    <updated>2014-02-06T23:43:28Z</updated>
    <published>2014-02-06T23:43:28Z</published>
    <title>TDOA--based localization in two dimensions: the bifurcation curve</title>
    <summary>  In this paper, we complete the study of the geometry of the TDOA map that
encodes the noiseless model for the localization of a source from the range
differences between three receivers in a plane, by computing the Cartesian
equation of the bifurcation curve in terms of the positions of the receivers.
From that equation, we can compute its real asymptotic lines. The present
manuscript completes the analysis of [Inverse Problems, Vol. 30, Number 3,
Pages 035004]. Our result is useful to check if a source belongs or is closed
to the bifurcation curve, where the localization in a noisy scenario is
ambiguous.
</summary>
    <author>
      <name>Marco Compagnoni</name>
    </author>
    <author>
      <name>Roberto Notari</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3233/FI-2014-1118</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3233/FI-2014-1118" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures, to appear in Fundamenta Informaticae</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Fundamenta Informaticae XXI (2014) 1001-1012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.1530v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.1530v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="gr-qc" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.4160v1</id>
    <updated>2014-02-14T03:11:53Z</updated>
    <published>2014-02-14T03:11:53Z</published>
    <title>Maximizing the Signal-to-Alias Ratio in Non-Uniform Filter Banks for
  Acoustic Echo Cancellation</title>
    <summary>  A new method for designing non-uniform filter-banks for acoustic echo
cancellation is proposed. In the method, the analysis prototype filter design
is framed as a convex optimization problem that maximizes the signal-to-alias
ratio (SAR) in the analysis banks. Since each sub-band has a different
bandwidth, the contribution to the overall SAR from each analysis bank is taken
into account during optimization. To increase the degrees of freedom during
optimization, no constraints are imposed on the phase or group delay of the
filters; at the same time, low delay is achieved by ensuring that the resulting
filters are minimum phase. Experimental results show that the filter bank
designed using the proposed method results in a sub-band adaptive filter with a
much better echo return loss enhancement (ERLE) when compared with existing
design methods.
</summary>
    <author>
      <name>R. C. Nongpiur</name>
    </author>
    <author>
      <name>D. J. Shpak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Circuits and Systems I: Regular Paper, vol. 59,
  no. 10, Oct. 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.4160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.4160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.6901v1</id>
    <updated>2014-03-27T01:32:09Z</updated>
    <published>2014-03-27T01:32:09Z</published>
    <title>Automatic Segmentation of Broadcast News Audio using Self Similarity
  Matrix</title>
    <summary>  Generally audio news broadcast on radio is com- posed of music, commercials,
news from correspondents and recorded statements in addition to the actual news
read by the newsreader. When news transcripts are available, automatic
segmentation of audio news broadcast to time align the audio with the text
transcription to build frugal speech corpora is essential. We address the
problem of identifying segmentation in the audio news broadcast corresponding
to the news read by the newsreader so that they can be mapped to the text
transcripts. The existing techniques produce sub-optimal solutions when used to
extract newsreader read segments. In this paper, we propose a new technique
which is able to identify the acoustic change points reliably using an acoustic
Self Similarity Matrix (SSM). We describe the two pass technique in detail and
verify its performance on real audio news broadcast of All India Radio for
different languages.
</summary>
    <author>
      <name>Sapna Soni</name>
    </author>
    <author>
      <name>Ahmed Imran</name>
    </author>
    <author>
      <name>Sunil Kumar Kopparapu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 images</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.6901v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.6901v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.0400v1</id>
    <updated>2014-04-01T21:15:32Z</updated>
    <published>2014-04-01T21:15:32Z</published>
    <title>A Deep Representation for Invariance And Music Classification</title>
    <summary>  Representations in the auditory cortex might be based on mechanisms similar
to the visual ventral stream; modules for building invariance to
transformations and multiple layers for compositionality and selectivity. In
this paper we propose the use of such computational modules for extracting
invariant and discriminative audio representations. Building on a theory of
invariance in hierarchical architectures, we propose a novel, mid-level
representation for acoustical signals, using the empirical distributions of
projections on a set of templates and their transformations. Under the
assumption that, by construction, this dictionary of templates is composed from
similar classes, and samples the orbit of variance-inducing signal
transformations (such as shift and scale), the resulting signature is
theoretically guaranteed to be unique, invariant to transformations and stable
to deformations. Modules of projection and pooling can then constitute layers
of deep networks, for learning composite representations. We present the main
theoretical and computational aspects of a framework for unsupervised learning
of invariant audio representations, empirically evaluated on music genre
classification.
</summary>
    <author>
      <name>Chiyuan Zhang</name>
    </author>
    <author>
      <name>Georgios Evangelopoulos</name>
    </author>
    <author>
      <name>Stephen Voinea</name>
    </author>
    <author>
      <name>Lorenzo Rosasco</name>
    </author>
    <author>
      <name>Tomaso Poggio</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2014.6854954</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2014.6854954" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, CBMM Memo No. 002, (to appear) IEEE 2014 International
  Conference on Acoustics, Speech, and Signal Processing (ICASSP 2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.0400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.0400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.1468v1</id>
    <updated>2014-04-05T11:51:35Z</updated>
    <published>2014-04-05T11:51:35Z</published>
    <title>High Throughput and Less Area AMP Architecture for Audio Signal
  Restoration</title>
    <summary>  Audio restoration is effectively achieved by using low complexity algorithm
called AMP. This algorithm has fast convergence and has lower computation
intensity making it suitable for audio recovery problems. This paper focuses on
restoring an audio signal by using VLSI architecture called AMP-M that
implements AMP algorithm. This architecture employs MAC unit with fixed bit
Wallace tree multiplier, FFT-MUX and various memory units (RAM) for audio
restoration. VLSI and FPGA implementation results shows that reduced area, high
throughput, low power is achieved making it suitable for real time audio
recovery problems. Prominent examples are Magnetic Resonance Imaging (MRI),
Radar and Wireless Communications.
</summary>
    <author>
      <name>Swetha. R</name>
    </author>
    <author>
      <name>Rukmani Devi. D</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V9P123</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V9P123" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages,6 figures,"Published with International Journal of Computer
  Trends and Technology (IJCTT)","National Conference on Modern Electronics and
  Signal Processing(2014)-Velammal Engineering College","Recent Trends in
  Information Technology(2014)-R.M.K College of Engineering and Technology"</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Swetha.R,Rukmani Devi.D Article: High Throughput and Less Area AMP
  Architecture for Audio Signal Restoration.International Journal of Computer
  Trends and Technology (IJCTT) 9(3):107-111,Mar 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.1468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.1468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1379v1</id>
    <updated>2014-05-05T14:37:47Z</updated>
    <published>2014-05-05T14:37:47Z</published>
    <title>Design and Optimization of a Speech Recognition Front-End for
  Distant-Talking Control of a Music Playback Device</title>
    <summary>  This paper addresses the challenging scenario for the distant-talking control
of a music playback device, a common portable speaker with four small
loudspeakers in close proximity to one microphone. The user controls the device
through voice, where the speech-to-music ratio can be as low as -30 dB during
music playback. We propose a speech enhancement front-end that relies on known
robust methods for echo cancellation, double-talk detection, and noise
suppression, as well as a novel adaptive quasi-binary mask that is well suited
for speech recognition. The optimization of the system is then formulated as a
large scale nonlinear programming problem where the recognition rate is
maximized and the optimal values for the system parameters are found through a
genetic algorithm. We validate our methodology by testing over the TIMIT
database for different music playback levels and noise types. Finally, we show
that the proposed front-end allows a natural interaction with the device for
limited-vocabulary voice commands.
</summary>
    <author>
      <name>Ramin Pichevar</name>
    </author>
    <author>
      <name>Jason Wung</name>
    </author>
    <author>
      <name>Daniele Giacobello</name>
    </author>
    <author>
      <name>Joshua Atkins</name>
    </author>
    <link href="http://arxiv.org/abs/1405.1379v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1379v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.4843v1</id>
    <updated>2014-05-19T19:10:15Z</updated>
    <published>2014-05-19T19:10:15Z</published>
    <title>Trends and Perspectives for Signal Processing in Consumer Audio</title>
    <summary>  The trend in media consumption towards streaming and portability offers new
challenges and opportunities for signal processing in audio and acoustics. The
most significant embodiment of this trend is that most music consumption now
happens on-the-go which has recently led to an explosion in headphone sales and
small portable speakers. In particular, premium headphones offer a gateway for
a younger generation to experience high quality sound. Additionally, through
technologies incorporating head-related transfer functions headphones can also
offer unique new experiences in gaming, augmented reality, and surround sound
listening. Home audio has also seen a transition to smaller sound systems in
the form of sound bars. This speaker configuration offers many exciting
challenges for surround sound reproduction which has traditionally used five
speakers surrounding the listener. Furthermore, modern home entertainment
systems offer more than just content delivery; users now expect wireless and
connected smart devices with video conferencing, gaming, and other interactive
capabilities. With this comes challenges for voice interaction at a distance
and in demanding conditions, e.g., during content playback, and opportunities
for new smart interactive experiences based on awareness of environment and
user biometrics.
</summary>
    <author>
      <name>Joshua Atkins</name>
    </author>
    <author>
      <name>Daniele Giacobello</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Audio and Acoustic Signal Processing Technical Committee
  Newsletter, May 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.4843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.4843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2464v1</id>
    <updated>2014-06-10T08:29:58Z</updated>
    <published>2014-06-10T08:29:58Z</published>
    <title>Music and Vocal Separation Using Multi-Band Modulation Based Features</title>
    <summary>  The potential use of non-linear speech features has not been investigated for
music analysis although other commonly used speech features like Mel Frequency
Ceptral Coefficients (MFCC) and pitch have been used extensively. In this
paper, we assume an audio signal to be a sum of modulated sinusoidal and then
use the energy separation algorithm to decompose the audio into amplitude and
frequency modulation components using the non-linear Teager-Kaiser energy
operator. We first identify the distribution of these non-linear features for
music only and voice only segments in the audio signal in different Mel spaced
frequency bands and show that they have the ability to discriminate. The
proposed method based on Kullback-Leibler divergence measure is evaluated using
a set of Indian classical songs from three different artists. Experimental
results show that the discrimination ability is evident in certain low and mid
frequency bands (200 - 1500 Hz).
</summary>
    <author>
      <name>Sunil Kumar Kopparapu</name>
    </author>
    <author>
      <name>Meghna Pandharipande</name>
    </author>
    <author>
      <name>G Sita</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISIEA.2010.5679370</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISIEA.2010.5679370" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, 2010 IEEE Symposium on Industrial Electronics
  Applications (ISIEA)</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2464v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2464v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.4447v1</id>
    <updated>2014-06-17T17:44:45Z</updated>
    <published>2014-06-17T17:44:45Z</published>
    <title>Automatic Fado Music Classification</title>
    <summary>  In late 2011, Fado was elevated to the oral and intangible heritage of
humanity by UNESCO. This study aims to develop a tool for automatic detection
of Fado music based on the audio signal. To do this, frequency spectrum-related
characteristics were captured form the audio signal: in addition to the Mel
Frequency Cepstral Coefficients (MFCCs) and the energy of the signal, the
signal was further analysed in two frequency ranges, providing additional
information. Tests were run both in a 10-fold cross-validation setup (97.6%
accuracy), and in a traditional train/test setup (95.8% accuracy). The good
results reflect the fact that Fado is a very distinctive musical style.
</summary>
    <author>
      <name>Pedro Girão Antunes</name>
    </author>
    <author>
      <name>David Martins de Matos</name>
    </author>
    <author>
      <name>Ricardo Ribeiro</name>
    </author>
    <author>
      <name>Isabel Trancoso</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.4447v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.4447v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0380v1</id>
    <updated>2014-06-27T20:34:05Z</updated>
    <published>2014-06-27T20:34:05Z</published>
    <title>A Multi Level Data Fusion Approach for Speaker Identification on
  Telephone Speech</title>
    <summary>  Several speaker identification systems are giving good performance with clean
speech but are affected by the degradations introduced by noisy audio
conditions. To deal with this problem, we investigate the use of complementary
information at different levels for computing a combined match score for the
unknown speaker. In this work, we observe the effect of two supervised machine
learning approaches including support vectors machines (SVM) and na\"ive bayes
(NB). We define two feature vector sets based on mel frequency cepstral
coefficients (MFCC) and relative spectral perceptual linear predictive
coefficients (RASTA-PLP). Each feature is modeled using the Gaussian Mixture
Model (GMM). Several ways of combining these information sources give
significant improvements in a text-independent speaker identification task
using a very large telephone degraded NTIMIT database.
</summary>
    <author>
      <name>Imen Trabelsi</name>
    </author>
    <author>
      <name>Dorra Ben Ayed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures, International Journal of Signal Processing,
  Image Processing and Pattern Recognition Vol. 6, No. 2, April, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.0380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.2351v3</id>
    <updated>2015-02-16T04:30:42Z</updated>
    <published>2014-07-09T04:42:25Z</published>
    <title>Efficient Steered-Response Power Methods for Sound Source Localization
  Using Microphone Arrays</title>
    <summary>  This paper proposes an efficient method based on the steered-response power
(SRP) technique for sound source localization using microphone arrays: the
volumetric SRP (V-SRP). As compared to the SRP, by deploying a sparser
volumetric grid, the V-SRP achieves a significant reduction of the
computational complexity without sacrificing the accuracy of the location
estimates. By appending a fine search step to the V-SRP, its refined version
(RV-SRP) improves on the compromise between complexity and accuracy.
Experiments conducted in both simulated- and real-data scenarios demonstrate
the benefits of the proposed approaches. Specifically, the RV-SRP is shown to
outperform the SRP in accuracy at a computational cost of about ten times
lower.
</summary>
    <author>
      <name>Markus V. S. Lima</name>
    </author>
    <author>
      <name>Wallace A. Martins</name>
    </author>
    <author>
      <name>Leonardo O. Nunes</name>
    </author>
    <author>
      <name>Luiz W. P. Biscainho</name>
    </author>
    <author>
      <name>Tadeu N. Ferreira</name>
    </author>
    <author>
      <name>Maurício V. M. Costa</name>
    </author>
    <author>
      <name>Bowon Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2014.2385864</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2014.2385864" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 9 figures, 5 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Signal Processing Letters (Volume:22 , Issue: 8 ), Aug. 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1407.2351v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.2351v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.3398v2</id>
    <updated>2014-07-17T05:23:38Z</updated>
    <published>2014-07-12T16:30:30Z</published>
    <title>Speech Polarity Detection Using Hilbert Phase Information</title>
    <summary>  The objective of the present work is to propose a method to automatically
detect polarity of the speech signals by estimating instants of significant
excitation of the vocaltract and the cosine phase of the analytic signal
representation. The phase changes in the analytic signal around the Hilbert
envelope (HE) peaks are found to vary according to the polarity of the given
speech signal. The relevant HE peaks for the Hilbert phase analysis are
selected by estimating the instants of significant excitation in speech. The
speech polarity identification rate obtained for the proposed method is almost
equal to the state of the art residual skewness method for speech polarity
detection. The proposed method also provides the same results for the polarity
detection in electro-glottogram signals. Finally, the robustness of the
proposed method is confirmed from the reduced detection error rates obtained in
noisy environments with various signal to noise ratios (SNRs). The MATLAB codes
used for implementing the proposed method are available for download from the
following link: http://nlp.amrita.edu:8080/TTS/polarityprograms.zip
</summary>
    <author>
      <name>D. Govind</name>
    </author>
    <author>
      <name>Anju Susan Biju</name>
    </author>
    <author>
      <name>Aguthu Smily</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures,
  http://nlp.amrita.edu:8080/TTS/polarityprograms.zip</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.3398v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.3398v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0203v1</id>
    <updated>2014-08-31T10:33:45Z</updated>
    <published>2014-08-31T10:33:45Z</published>
    <title>Ad Hoc Microphone Array Calibration: Euclidean Distance Matrix
  Completion Algorithm and Theoretical Guarantees</title>
    <summary>  This paper addresses the problem of ad hoc microphone array calibration where
only partial information about the distances between microphones is available.
We construct a matrix consisting of the pairwise distances and propose to
estimate the missing entries based on a novel Euclidean distance matrix
completion algorithm by alternative low-rank matrix completion and projection
onto the Euclidean distance space. This approach confines the recovered matrix
to the EDM cone at each iteration of the matrix completion algorithm. The
theoretical guarantees of the calibration performance are obtained considering
the random and locally structured missing entries as well as the measurement
noise on the known distances. This study elucidates the links between the
calibration error and the number of microphones along with the noise level and
the ratio of missing distances. Thorough experiments on real data recordings
and simulated setups are conducted to demonstrate these theoretical insights. A
significant improvement is achieved by the proposed Euclidean distance matrix
completion algorithm over the state-of-the-art techniques for ad hoc microphone
array calibration.
</summary>
    <author>
      <name>Mohammad J. Taghizadeh</name>
    </author>
    <author>
      <name>Reza Parhizkar</name>
    </author>
    <author>
      <name>Philip N. Garner</name>
    </author>
    <author>
      <name>Herve Bourlard</name>
    </author>
    <author>
      <name>Afsaneh Asaei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Press, available online, August 1, 2014.
  http://www.sciencedirect.com/science/article/pii/S0165168414003508, Signal
  Processing, 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.0203v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0203v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.7787v1</id>
    <updated>2014-09-27T09:47:16Z</updated>
    <published>2014-09-27T09:47:16Z</published>
    <title>Audio Surveillance: a Systematic Review</title>
    <summary>  Despite surveillance systems are becoming increasingly ubiquitous in our
living environment, automated surveillance, currently based on video sensory
modality and machine intelligence, lacks most of the time the robustness and
reliability required in several real applications. To tackle this issue, audio
sensory devices have been taken into account, both alone or in combination with
video, giving birth, in the last decade, to a considerable amount of research.
In this paper audio-based automated surveillance methods are organized into a
comprehensive survey: a general taxonomy, inspired by the more widespread video
surveillance field, is proposed in order to systematically describe the methods
covering background subtraction, event classification, object tracking and
situation analysis. For each of these tasks, all the significant works are
reviewed, detailing their pros and cons and the context for which they have
been proposed. Moreover, a specific section is devoted to audio features,
discussing their expressiveness and their employment in the above described
tasks. Differently, from other surveys on audio processing and analysis, the
present one is specifically targeted to automated surveillance, highlighting
the target applications of each described methods and providing the reader
tables and schemes useful to retrieve the most suited algorithms for a specific
requirement.
</summary>
    <author>
      <name>Marco Crocco</name>
    </author>
    <author>
      <name>Marco Cristani</name>
    </author>
    <author>
      <name>Andrea Trucco</name>
    </author>
    <author>
      <name>Vittorio Murino</name>
    </author>
    <link href="http://arxiv.org/abs/1409.7787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.7787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.6903v1</id>
    <updated>2014-10-25T09:40:46Z</updated>
    <published>2014-10-25T09:40:46Z</published>
    <title>Choice of Mel Filter Bank in Computing MFCC of a Resampled Speech</title>
    <summary>  Mel Frequency Cepstral Coefficients (MFCCs) are the most popularly used
speech features in most speech and speaker recognition applications. In this
paper, we study the effect of resampling a speech signal on these speech
features. We first derive a relationship between the MFCC param- eters of the
resampled speech and the MFCC parameters of the original speech. We propose six
methods of calculating the MFCC parameters of downsampled speech by
transforming the Mel filter bank used to com- pute MFCC of the original speech.
We then experimentally compute the MFCC parameters of the down sampled speech
using the proposed meth- ods and compute the Pearson coefficient between the
MFCC parameters of the downsampled speech and that of the original speech to
identify the most effective choice of Mel-filter band that enables the computed
MFCC of the resampled speech to be as close as possible to the original speech
sample MFCC.
</summary>
    <author>
      <name>Laxmi Narayana M.</name>
    </author>
    <author>
      <name>Sunil Kumar Kopparapu</name>
    </author>
    <link href="http://arxiv.org/abs/1410.6903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.6903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.6905v1</id>
    <updated>2014-10-25T09:53:39Z</updated>
    <published>2014-10-25T09:53:39Z</published>
    <title>On the use of Stress information in Speech for Speaker Recognition</title>
    <summary>  The performance of a speaker recognition system decreases when the speaker is
under stress or emotion. In this paper we explore and identify a mechanism that
enables use of inherent stress-in-speech or speaking style information present
in speech of a person as additional cues for speaker recognition. We quantify
the the inherent stress present in the speech of a speaker mainly using 3
features, namely, pitch, amplitude and duration (together called PAD) We
experimentally observe that the PAD vectors of similar phones in different
words of a speaker are close to each other in the three dimensional (PAD) space
confirming that the way a speaker stresses different syllables in their speech
is unique to them, thus we propose the use of PAD based speaking style of a
speaker as an additional feature for speaker recognition applications.
</summary>
    <author>
      <name>Laxmi Narayana M.</name>
    </author>
    <author>
      <name>Sunil Kumar Kopparapu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TENCON.2009.5396003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TENCON.2009.5396003" rel="related"/>
    <link href="http://arxiv.org/abs/1410.6905v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.6905v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.0370v1</id>
    <updated>2014-11-03T06:27:13Z</updated>
    <published>2014-11-03T06:27:13Z</published>
    <title>Detection of transitions between broad phonetic classes in a speech
  signal</title>
    <summary>  Detection of transitions between broad phonetic classes in a speech signal is
an important problem which has applications such as landmark detection and
segmentation. The proposed hierarchical method detects silence to non-silence
transitions, high amplitude (mostly sonorants) to low ampli- tude (mostly
fricatives/affricates/stop bursts) transitions and vice-versa. A subset of the
extremum (minimum or maximum) samples between every pair of successive
zero-crossings is selected above a second pass threshold, from each bandpass
filtered speech signal frame. Relative to the mid-point (reference) of a frame,
locations of the first and the last extrema lie on either side, if the speech
signal belongs to a homogeneous segment; else, both these locations lie on the
left or the right side of the reference, indicating a transition frame. When
tested on the entire TIMIT database, of the transitions detected, 93.6% are
within a tolerance of 20 ms from the hand labeled boundaries. Sonorant,
unvoiced non-sonorant and silence classes and their respective onsets are
detected with an accuracy of about 83.5% for the same tolerance. The results
are as good as, and in some respects better than the state-of-the-art methods
for similar tasks.
</summary>
    <author>
      <name>T V Ananthapadmanabha</name>
    </author>
    <author>
      <name>K V Vijay Girish</name>
    </author>
    <author>
      <name>A G Ramakrishnan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.0370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.0370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.1267v1</id>
    <updated>2014-11-05T13:20:56Z</updated>
    <published>2014-11-05T13:20:56Z</published>
    <title>An Interesting Property of LPCs for Sonorant Vs Fricative Discrimination</title>
    <summary>  Linear prediction (LP) technique estimates an optimum all-pole filter of a
given order for a frame of speech signal. The coefficients of the all-pole
filter, 1/A(z) are referred to as LP coefficients (LPCs). The gain of the
inverse of the all-pole filter, A(z) at z = 1, i.e, at frequency = 0, A(1)
corresponds to the sum of LPCs, which has the property of being lower (higher)
than a threshold for the sonorants (fricatives). When the inverse-tan of A(1),
denoted as T(1), is used a feature and tested on the sonorant and fricative
frames of the entire TIMIT database, an accuracy of 99.07% is obtained. Hence,
we refer to T(1) as sonorant-fricative discrimination index (SFDI). This
property has also been tested for its robustness for additive white noise and
on the telephone quality speech of the NTIMIT database. These results are
comparable to, or in some respects, better than the state-of-the-art methods
proposed for a similar task. Such a property may be used for segmenting a
speech signal or for non-uniform frame-rate analysis.
</summary>
    <author>
      <name>T. V. Ananthapadmanabha</name>
    </author>
    <author>
      <name>A. G. Ramakrishnan</name>
    </author>
    <author>
      <name>Pradeep Balachandran</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages including references</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.1267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.1267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.1898v1</id>
    <updated>2014-11-07T13:09:39Z</updated>
    <published>2014-11-07T13:09:39Z</published>
    <title>A Novel Uncertainty Parameter SR (Signal To Residual Spectrum Ratio)
  Evaluation Approach For Speech Enhancement</title>
    <summary>  Usually, hearing impaired people use hearing aids which are implemented with
speech enhancement algorithms. Estimation of speech and estimation of nose are
the components in single channel speech enhancement system. The main objective
of any speech enhancement algorithm is estimation of noise power spectrum for
non stationary environment. VAD (Voice Activity Detector) is used to identify
speech pauses and during these pauses only estimation of noise. MMSE (Minimum
Mean Square Error) speech enhancement algorithm did not enhance the
intelligibility, quality and listener fatigues are the perceptual aspects of
speech. Novel evaluation approach SR (Signal to Residual spectrum ratio) based
on uncertainty parameter introduced for the benefits of hearing impaired people
in non stationary environments to control distortions. By estimation and
updating of noise based on division of original pure signal into three parts
such as pure speech, quasi speech and non speech frames based on multiple
threshold conditions. Different values of SR and LLR demonstrate the amount of
attenuation and amplification distortions. The proposed method will compared
with any one method WAT(Weighted Average Technique) Hence by using parameters
SR (signal to residual spectrum ratio) and LLR (log like hood ratio), MMSE
(Minim Mean Square Error) in terms of segmented SNR and LLR.
</summary>
    <author>
      <name>M. Ravichandra Kumar</name>
    </author>
    <author>
      <name>B. Ravi Teja</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.1898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.1898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.3715v1</id>
    <updated>2014-11-13T16:03:09Z</updated>
    <published>2014-11-13T16:03:09Z</published>
    <title>Acoustic Scene Classification</title>
    <summary>  In this article we present an account of the state-of-the-art in acoustic
scene classification (ASC), the task of classifying environments from the
sounds they produce. Starting from a historical review of previous research in
this area, we define a general framework for ASC and present different imple-
mentations of its components. We then describe a range of different algorithms
submitted for a data challenge that was held to provide a general and fair
benchmark for ASC techniques. The dataset recorded for this purpose is
presented, along with the performance metrics that are used to evaluate the
algorithms and statistical significance tests to compare the submitted methods.
We use a baseline method that employs MFCCS, GMMS and a maximum likelihood
criterion as a benchmark, and only find sufficient evidence to conclude that
three algorithms significantly outperform it. We also evaluate the human
classification accuracy in performing a similar classification task. The best
performing algorithm achieves a mean accuracy that matches the median accuracy
obtained by humans, and common pairs of classes are misclassified by both
computers and humans. However, all acoustic scenes are correctly classified by
at least some individuals, while there are scenes that are misclassified by all
algorithms.
</summary>
    <author>
      <name>Daniele Barchiesi</name>
    </author>
    <author>
      <name>Dimitrios Giannoulis</name>
    </author>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MSP.2014.2326181</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MSP.2014.2326181" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Signal Processing Magazine 32(3) (May 2015) 16-34</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1411.3715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.3715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.4890v1</id>
    <updated>2014-11-17T05:06:50Z</updated>
    <published>2014-11-17T05:06:50Z</published>
    <title>Which Are You In A Photo?</title>
    <summary>  Automatic image tagging has been a long standing problem, it mainly relies on
image recognition techniques of which the accuracy is still not satisfying.
This paper attempts to explore out-of-band sensing base on the mobile phone to
sense the people in a picture while the picture is being taken and create name
tags on-the-fly. The major challenges pertain to two aspects - "Who" and
"Which". (1) "Who": discriminating people who are in the picture from those
that are not; (2) "Which": correlating each name tag with its corresponding
people in the picture. We propose an accurate acoustic scheme applying on the
mobile phones, which leverages the Doppler effect of sound wave to address
these two challenges. As a proof of concept, we implement the scheme on 7
android phones and take pictures in various real-life scenarios with people
positioning in different ways. Extensive experiments show that the accuracy of
tag correlation is above 85% within 3m for picturing.
</summary>
    <author>
      <name>Xing Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1411.4890v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.4890v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.6741v1</id>
    <updated>2014-11-25T06:18:45Z</updated>
    <published>2014-11-25T06:18:45Z</published>
    <title>A Complex Matrix Factorization approach to Joint Modeling of Magnitude
  and Phase for Source Separation</title>
    <summary>  Conventional NMF methods for source separation factorize the matrix of
spectral magnitudes. Spectral Phase is not included in the decomposition
process of these methods. However, phase of the speech mixture is generally
used in reconstructing the target speech signal. This results in undesired
traces of interfering sources in the target signal. In this paper the spectral
phase is incorporated in the decomposition process itself. Additionally, the
complex matrix factorization problem is reduced to an NMF problem using simple
transformations. This results in effective separation of speech mixtures since
both magnitude and phase are utilized jointly in the separation process.
Improvement in source separation results are demonstrated using objective
quality evaluations on the GRID corpus.
</summary>
    <author>
      <name>Chaitanya Ahuja</name>
    </author>
    <author>
      <name>Karan Nathwani</name>
    </author>
    <author>
      <name>Rajesh M. Hegde</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.6741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.6741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.4052v2</id>
    <updated>2016-06-12T08:08:41Z</updated>
    <published>2014-12-11T05:11:54Z</published>
    <title>The bag-of-frames approach: a not so sufficient model for urban
  soundscapes</title>
    <summary>  The "bag-of-frames" approach (BOF), which encodes audio signals as the
long-term statistical distribution of short-term spectral features, is commonly
regarded as an effective and sufficient way to represent environmental sound
recordings (soundscapes) since its introduction in an influential 2007 article.
The present paper describes a concep-tual replication of this seminal article
using several new soundscape datasets, with results strongly questioning the
adequacy of the BOF approach for the task. We show that the good accuracy
originally re-ported with BOF likely result from a particularly thankful
dataset with low within-class variability, and that for more realistic
datasets, BOF in fact does not perform significantly better than a mere
one-point av-erage of the signal's features. Soundscape modeling, therefore,
may not be the closed case it was once thought to be. Progress, we ar-gue,
could lie in reconsidering the problem of considering individual acoustical
events within each soundscape.
</summary>
    <author>
      <name>Mathieu Lagrange</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRCCyN</arxiv:affiliation>
    </author>
    <author>
      <name>Grégoire Lafay</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRCCyN</arxiv:affiliation>
    </author>
    <author>
      <name>Boris Defreville</name>
    </author>
    <author>
      <name>Jean-Julien Aucouturier</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1121/1.4935350</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1121/1.4935350" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JASA Express Letters, 2015, 138 (5), pp.487-492</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1412.4052v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.4052v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7022v3</id>
    <updated>2015-04-28T02:24:14Z</updated>
    <published>2014-12-22T15:15:44Z</published>
    <title>Audio Source Separation with Discriminative Scattering Networks</title>
    <summary>  In this report we describe an ongoing line of research for solving
single-channel source separation problems. Many monaural signal decomposition
techniques proposed in the literature operate on a feature space consisting of
a time-frequency representation of the input data. A challenge faced by these
approaches is to effectively exploit the temporal dependencies of the signals
at scales larger than the duration of a time-frame. In this work we propose to
tackle this problem by modeling the signals using a time-frequency
representation with multiple temporal resolutions. The proposed representation
consists of a pyramid of wavelet scattering operators, which generalizes
Constant Q Transforms (CQT) with extra layers of convolution and complex
modulus. We first show that learning standard models with this multi-resolution
setting improves source separation results over fixed-resolution methods. As
study case, we use Non-Negative Matrix Factorizations (NMF) that has been
widely considered in many audio application. Then, we investigate the inclusion
of the proposed multi-resolution setting into a discriminative training regime.
We discuss several alternatives using different deep neural network
architectures.
</summary>
    <author>
      <name>Pablo Sprechmann</name>
    </author>
    <author>
      <name>Joan Bruna</name>
    </author>
    <author>
      <name>Yann LeCun</name>
    </author>
    <link href="http://arxiv.org/abs/1412.7022v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7022v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7193v1</id>
    <updated>2014-12-22T22:38:06Z</updated>
    <published>2014-12-22T22:38:06Z</published>
    <title>Audio Source Separation Using a Deep Autoencoder</title>
    <summary>  This paper proposes a novel framework for unsupervised audio source
separation using a deep autoencoder. The characteristics of unknown source
signals mixed in the mixed input is automatically by properly configured
autoencoders implemented by a network with many layers, and separated by
clustering the coefficient vectors in the code layer. By investigating the
weight vectors to the final target, representation layer, the primitive
components of the audio signals in the frequency domain are observed. By
clustering the activation coefficients in the code layer, the previously
unknown source signals are segregated. The original source sounds are then
separated and reconstructed by using code vectors which belong to different
clusters. The restored sounds are not perfect but yield promising results for
the possibility in the success of many practical applications.
</summary>
    <author>
      <name>Giljin Jang</name>
    </author>
    <author>
      <name>Han-Gyu Kim</name>
    </author>
    <author>
      <name>Yung-Hwan Oh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures, ICLR 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.7193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.07496v1</id>
    <updated>2015-01-29T16:09:17Z</updated>
    <published>2015-01-29T16:09:17Z</published>
    <title>Implementation of an Automatic Syllabic Division Algorithm from Speech
  Files in Portuguese Language</title>
    <summary>  A new algorithm for voice automatic syllabic splitting in the Portuguese
language is proposed, which is based on the envelope of the speech signal of
the input audio file. A computational implementation in MatlabTM is presented
and made available at the URL
http://www2.ee.ufpe.br/codec/divisao_silabica.html. Due to its
straightforwardness, the proposed method is very attractive for embedded
systems (e.g. i-phones). It can also be used as a screen to assist more
sophisticated methods. Voice excerpts containing more than one syllable and
identified by the same envelope are named as super-syllables and they are
subsequently separated. The results indicate which samples corresponds to the
beginning and end of each detected syllable. Preliminary tests were performed
to fifty words at an identification rate circa 70% (further improvements may be
incorporated to treat particular phonemes). This algorithm is also useful in
voice command systems, as a tool in the teaching of Portuguese language or even
for patients with speech pathology.
</summary>
    <author>
      <name>E. L. F. Da Silva</name>
    </author>
    <author>
      <name>H. M. de Oliveira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures, 4 tables, conference: XIX Congresso Brasileiro de
  Automatica CBA, Campina Grande, Setembro, 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.07496v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.07496v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00524v2</id>
    <updated>2015-10-23T14:37:45Z</updated>
    <published>2015-02-02T15:45:38Z</published>
    <title>Unsupervised Incremental Learning and Prediction of Music Signals</title>
    <summary>  A system is presented that segments, clusters and predicts musical audio in
an unsupervised manner, adjusting the number of (timbre) clusters
instantaneously to the audio input. A sequence learning algorithm adapts its
structure to a dynamically changing clustering tree. The flow of the system is
as follows: 1) segmentation by onset detection, 2) timbre representation of
each segment by Mel frequency cepstrum coefficients, 3) discretization by
incremental clustering, yielding a tree of different sound classes (e.g.
instruments) that can grow or shrink on the fly driven by the instantaneous
sound events, resulting in a discrete symbol sequence, 4) extraction of
statistical regularities of the symbol sequence, using hierarchical N-grams and
the newly introduced conceptual Boltzmann machine, and 5) prediction of the
next sound event in the sequence. The system's robustness is assessed with
respect to complexity and noisiness of the signal. Clustering in isolation
yields an adjusted Rand index (ARI) of 82.7% / 85.7% for data sets of singing
voice and drums. Onset detection jointly with clustering achieve an ARI of
81.3% / 76.3% and the prediction of the entire system yields an ARI of 27.2% /
39.2%.
</summary>
    <author>
      <name>Ricard Marxer</name>
    </author>
    <author>
      <name>Hendrik Purwins</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2016.2530409</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2016.2530409" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.00524v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00524v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.01707v1</id>
    <updated>2015-02-05T20:40:41Z</updated>
    <published>2015-02-05T20:40:41Z</published>
    <title>CS reconstruction of the speech and musical signals</title>
    <summary>  The application of Compressive sensing approach to the speech and musical
signals is considered in this paper. Compressive sensing (CS) is a new approach
to the signal sampling that allows signal reconstruction from a small set of
randomly acquired samples. This method is developed for the signals that
exhibit the sparsity in a certain domain. Here we have observed two sparsity
domains: discrete Fourier and discrete cosine transform domain. Furthermore,
two different types of audio signals are analyzed in terms of sparsity and CS
performance - musical and speech signals. Comparative analysis of the CS
reconstruction using different number of signal samples is performed in the two
domains of sparsity. It is shown that the CS can be successfully applied to
both, musical and speech signals, but the speech signals are more demanding in
terms of the number of observations. Also, our results show that discrete
cosine transform domain allows better reconstruction using lower number of
observations, compared to the Fourier transform domain, for both types of
signals.
</summary>
    <author>
      <name>Trifun Savic</name>
    </author>
    <author>
      <name>Radoje Albijanic</name>
    </author>
    <link href="http://arxiv.org/abs/1502.01707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.01707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.03162v1</id>
    <updated>2015-02-11T00:47:22Z</updated>
    <published>2015-02-11T00:47:22Z</published>
    <title>Sparse Head-Related Impulse Response for Efficient Direct Convolution</title>
    <summary>  Head-related impulse responses (HRIRs) are subject-dependent and
direction-dependent filters used in spatial audio synthesis. They describe the
scattering response of the head, torso, and pinnae of the subject. We propose a
structural factorization of the HRIRs into a product of non-negative and
Toeplitz matrices; the factorization is based on a novel extension of a
non-negative matrix factorization algorithm. As a result, the HRIR becomes
expressible as a convolution between a direction-independent \emph{resonance}
filter and a direction-dependent \emph{reflection} filter. Further, the
reflection filter can be made \emph{sparse} with minimal HRIR distortion. The
described factorization is shown to be applicable to the arbitrary source
signal case and allows one to employ time-domain convolution at a computational
cost lower than using convolution in the frequency domain.
</summary>
    <author>
      <name>Yuancheng Luo</name>
    </author>
    <author>
      <name>Dmitry N. Zotkin</name>
    </author>
    <author>
      <name>Ramani Duraiswami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.03162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="15B05, 65F50, 42A85" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.03387v1</id>
    <updated>2015-02-11T17:30:48Z</updated>
    <published>2015-02-11T17:30:48Z</published>
    <title>A Full Frequency Masking Vocoder for Legal Eavesdropping Conversation
  Recording</title>
    <summary>  This paper presents a new approach for a vocoder design based on full
frequency masking by octaves in addition to a technique for spectral filling
via beta probability distribution. Some psycho-acoustic characteristics of
human hearing - inaudibility masking in frequency and phase - are used as a
basis for the proposed algorithm. The results confirm that this technique may
be useful to save bandwidth in applications requiring intelligibility. It is
recommended for the legal eavesdropping of long voice conversations.
</summary>
    <author>
      <name>R. F. B. Sotero Filho</name>
    </author>
    <author>
      <name>H. M. de Oliveira</name>
    </author>
    <author>
      <name>R. M. Campello de Souza</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5540/03.2015.003.01.0468</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5540/03.2015.003.01.0468" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, 3 tables, XXXV Cong. Nac. de Matematica Aplicada
  e Computacional, Natal, RN, Brazil 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.03387v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03387v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.03784v2</id>
    <updated>2015-02-13T14:05:12Z</updated>
    <published>2015-02-12T19:38:50Z</published>
    <title>Coherent-to-Diffuse Power Ratio Estimation for Dereverberation</title>
    <summary>  The estimation of the time- and frequency-dependent coherent-to-diffuse power
ratio (CDR) from the measured spatial coherence between two omnidirectional
microphones is investigated. Known CDR estimators are formulated in a common
framework, illustrated using a geometric interpretation in the complex plane,
and investigated with respect to bias and robustness towards model errors.
Several novel unbiased CDR estimators are proposed, and it is shown that
knowledge of either the direction of arrival (DOA) of the target source or the
coherence of the noise field is sufficient for unbiased CDR estimation. The
validity of the model for the application of CDR estimates to dereverberation
is investigated using measured and simulated impulse responses. A CDR-based
dereverberation system is presented and evaluated using signal-based quality
measures as well as automatic speech recognition accuracy. The results show
that the proposed unbiased estimators have a practical advantage over existing
estimators, and that the proposed DOA-independent estimator can be used for
effective blind dereverberation.
</summary>
    <author>
      <name>Andreas Schwarz</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2015.2418571</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2015.2418571" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to IEEE/ACM Transactions on Audio, Speech, and Language
  Processing, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.03784v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03784v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.04300v1</id>
    <updated>2015-02-15T10:22:47Z</updated>
    <published>2015-02-15T10:22:47Z</published>
    <title>Mandarin Singing Voice Synthesis Based on Harmonic Plus Noise Model and
  Singing Expression Analysis</title>
    <summary>  The purpose of this study is to investigate how humans interpret musical
scores expressively, and then design machines that sing like humans. We
consider six factors that have a strong influence on the expression of human
singing. The factors are related to the acoustic, phonetic, and musical
features of a real singing signal. Given real singing voices recorded following
the MIDI scores and lyrics, our analysis module can extract the expression
parameters from the real singing signals semi-automatically. The expression
parameters are used to control the singing voice synthesis (SVS) system for
Mandarin Chinese, which is based on the harmonic plus noise model (HNM). The
results of perceptual experiments show that integrating the expression factors
into the SVS system yields a notable improvement in perceptual naturalness,
clearness, and expressiveness. By one-to-one mapping of the real singing signal
and expression controls to the synthesizer, our SVS system can simulate the
interpretation of a real singer with the timbre of a speaker.
</summary>
    <author>
      <name>Ju-Chiang Wang</name>
    </author>
    <author>
      <name>Hung-Yan Gu</name>
    </author>
    <author>
      <name>Hsin-Min Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.04300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.04300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.05751v2</id>
    <updated>2015-07-09T14:27:05Z</updated>
    <published>2015-02-19T23:58:36Z</published>
    <title>Efficient Synthesis of Room Acoustics via Scattering Delay Networks</title>
    <summary>  An acoustic reverberator consisting of a network of delay lines connected via
scattering junctions is proposed. All parameters of the reverberator are
derived from physical properties of the enclosure it simulates. It allows for
simulation of unequal and frequency-dependent wall absorption, as well as
directional sources and microphones. The reverberator renders the first-order
reflections exactly, while making progressively coarser approximations of
higher-order reflections. The rate of energy decay is close to that obtained
with the image method (IM) and consistent with the predictions of Sabine and
Eyring equations. The time evolution of the normalized echo density, which was
previously shown to be correlated with the perceived texture of reverberation,
is also close to that of IM. However, its computational complexity is one to
two orders of magnitude lower, comparable to the computational complexity of a
feedback delay network (FDN), and its memory requirements are negligible.
</summary>
    <author>
      <name>Enzo De Sena</name>
    </author>
    <author>
      <name>Huseyin Hacihabiboglu</name>
    </author>
    <author>
      <name>Zoran Cvetkovic</name>
    </author>
    <author>
      <name>Julius O. Smith III</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2015.2438547</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2015.2438547" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE/ACM Transactions on Audio, Speech, and Language Processing,
  Vol. 23, No. 9, September 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1502.05751v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.05751v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.06811v1</id>
    <updated>2015-02-24T13:47:45Z</updated>
    <published>2015-02-24T13:47:45Z</published>
    <title>A Review of Audio Features and Statistical Models Exploited for Voice
  Pattern Design</title>
    <summary>  Audio fingerprinting, also named as audio hashing, has been well-known as a
powerful technique to perform audio identification and synchronization. It
basically involves two major steps: fingerprint (voice pattern) design and
matching search. While the first step concerns the derivation of a robust and
compact audio signature, the second step usually requires knowledge about
database and quick-search algorithms. Though this technique offers a wide range
of real-world applications, to the best of the authors' knowledge, a
comprehensive survey of existing algorithms appeared more than eight years ago.
Thus, in this paper, we present a more up-to-date review and, for emphasizing
on the audio signal processing aspect, we focus our state-of-the-art survey on
the fingerprint design step for which various audio features and their
tractable statistical models are discussed.
</summary>
    <author>
      <name>Ngoc Q. K. Duong</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">HUMG</arxiv:affiliation>
    </author>
    <author>
      <name>Hien-Thanh Duong</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">HUMG</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://www.iaria.org/conferences2015/PATTERNS15.html ; Seventh
  International Conferences on Pervasive Patterns and Applications (PATTERNS
  2015), Mar 2015, Nice, France</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.06811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.06811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.02945v1</id>
    <updated>2015-04-12T08:44:56Z</updated>
    <published>2015-04-12T08:44:56Z</published>
    <title>Deep Transform: Cocktail Party Source Separation via Complex Convolution
  in a Deep Neural Network</title>
    <summary>  Convolutional deep neural networks (DNN) are state of the art in many
engineering problems but have not yet addressed the issue of how to deal with
complex spectrograms. Here, we use circular statistics to provide a convenient
probabilistic estimate of spectrogram phase in a complex convolutional DNN. In
a typical cocktail party source separation scenario, we trained a convolutional
DNN to re-synthesize the complex spectrograms of two source speech signals
given a complex spectrogram of the monaural mixture - a discriminative deep
transform (DT). We then used this complex convolutional DT to obtain
probabilistic estimates of the magnitude and phase components of the source
spectrograms. Our separation results are on a par with equivalent binary-mask
based non-complex separation approaches.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1504.02945v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.02945v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.03128v1</id>
    <updated>2015-04-13T11:08:00Z</updated>
    <published>2015-04-13T11:08:00Z</published>
    <title>Absolute Geometry Calibration of Distributed Microphone Arrays in an
  Audio-Visual Sensor Network</title>
    <summary>  Joint audio-visual speaker tracking requires that the locations of
microphones and cameras are known and that they are given in a common
coordinate system. Sensor self-localization algorithms, however, are usually
separately developed for either the acoustic or the visual modality and return
their positions in a modality specific coordinate system, often with an unknown
rotation, scaling and translation between the two. In this paper we propose two
techniques to determine the positions of acoustic sensors in a common
coordinate system, based on audio-visual correlates, i.e., events that are
localized by both, microphones and cameras separately. The first approach maps
the output of an acoustic self-calibration algorithm by estimating rotation,
scale and translation to the visual coordinate system, while the second solves
a joint system of equations with acoustic and visual directions of arrival as
input. The evaluation of the two strategies reveals that joint calibration
outperforms the mapping approach and achieves an overall calibration error of
0.20m even in reverberant environments.
</summary>
    <author>
      <name>Florian Jacob</name>
    </author>
    <author>
      <name>Reinhold Haeb-Umbach</name>
    </author>
    <link href="http://arxiv.org/abs/1504.03128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.03128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.04658v1</id>
    <updated>2015-04-17T23:07:17Z</updated>
    <published>2015-04-17T23:07:17Z</published>
    <title>Deep Karaoke: Extracting Vocals from Musical Mixtures Using a
  Convolutional Deep Neural Network</title>
    <summary>  Identification and extraction of singing voice from within musical mixtures
is a key challenge in source separation and machine audition. Recently, deep
neural networks (DNN) have been used to estimate 'ideal' binary masks for
carefully controlled cocktail party speech separation problems. However, it is
not yet known whether these methods are capable of generalizing to the
discrimination of voice and non-voice in the context of musical mixtures. Here,
we trained a convolutional DNN (of around a billion parameters) to provide
probabilistic estimates of the ideal binary mask for separation of vocal sounds
from real-world musical mixtures. We contrast our DNN results with more
traditional linear methods. Our approach may be useful for automatic removal of
vocal sounds from musical mixtures for 'karaoke' type applications.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <author>
      <name>Gerard Roma</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <link href="http://arxiv.org/abs/1504.04658v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.04658v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.07372v1</id>
    <updated>2015-04-28T08:18:55Z</updated>
    <published>2015-04-28T08:18:55Z</published>
    <title>Time-Frequency Trade-offs for Audio Source Separation with Binary Masks</title>
    <summary>  The short-time Fourier transform (STFT) provides the foundation of
binary-mask based audio source separation approaches. In computing a
spectrogram, the STFT window size parameterizes the trade-off between time and
frequency resolution. However, it is not yet known how this parameter affects
the operation of the binary mask in terms of separation quality for real-world
signals such as speech or music. Here, we demonstrate that the trade-off
between time and frequency in the STFT, used to perform ideal binary mask
separation, depends upon the types of source that are to be separated. In
particular, we demonstrate that different window sizes are optimal for
separating different combinations of speech and musical signals. Our findings
have broad implications for machine audition and machine learning in general.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1504.07372v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.07372v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.08021v1</id>
    <updated>2015-04-29T20:56:42Z</updated>
    <published>2015-04-29T20:56:42Z</published>
    <title>Who Spoke What? A Latent Variable Framework for the Joint Decoding of
  Multiple Speakers and their Keywords</title>
    <summary>  In this paper, we present a latent variable (LV) framework to identify all
the speakers and their keywords given a multi-speaker mixture signal. We
introduce two separate LVs to denote active speakers and the keywords uttered.
The dependency of a spoken keyword on the speaker is modeled through a
conditional probability mass function. The distribution of the mixture signal
is expressed in terms of the LV mass functions and speaker-specific-keyword
models. The proposed framework admits stochastic models, representing the
probability density function of the observation vectors given that a particular
speaker uttered a specific keyword, as speaker-specific-keyword models. The LV
mass functions are estimated in a Maximum Likelihood framework using the
Expectation Maximization (EM) algorithm. The active speakers and their keywords
are detected as modes of the joint distribution of the two LVs. In mixture
signals, containing two speakers uttering the keywords simultaneously, the
proposed framework achieves an accuracy of 82% for detecting both the speakers
and their respective keywords, using Student's-t mixture models as
speaker-specific-keyword models.
</summary>
    <author>
      <name>Harshavardhan Sundar</name>
    </author>
    <author>
      <name>Thippur V. Sreenivas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures Submitted to : IEEE Signal Processing Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.08021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.08021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.08177v2</id>
    <updated>2015-05-30T02:21:43Z</updated>
    <published>2015-04-30T11:49:38Z</published>
    <title>Noise Sensitivity of Teager-Kaiser Energy Operators and Their Ratios</title>
    <summary>  The Teager-Kaiser energy operator (TKO) belongs to a class of autocorrelators
and their linear combination that can track the instantaneous energy of a
nonstationary sinusoidal signal source. TKO-based monocomponent AM-FM
demodulation algorithms work under the basic assumption that the operator
outputs are always positive. In the absence of noise, this is assured for pure
sinusoidal inputs and the instantaneous property is also guaranteed. Noise
invalidates both of these, particularly under small signal conditions.
Post-detection filtering and thresholding are of use to reestablish these at
the cost of some time to acquire. Key questions are: (a) how many samples must
one use and (b) how much noise power at the detector input can one tolerate.
Results of study of the role of delay and the limits imposed by additive
Gaussian noise are presented along with the computation of the cumulants and
probability density functions of the individual quadratic forms and their
ratios.
</summary>
    <author>
      <name>Pradeep Kr. Banerjee</name>
    </author>
    <author>
      <name>Nirmal B. Chakrabarti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.08177v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.08177v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00289v1</id>
    <updated>2015-05-01T22:10:58Z</updated>
    <published>2015-05-01T22:10:58Z</published>
    <title>Deep Remix: Remixing Musical Mixtures Using a Convolutional Deep Neural
  Network</title>
    <summary>  Audio source separation is a difficult machine learning problem and
performance is measured by comparing extracted signals with the component
source signals. However, if separation is motivated by the ultimate goal of
re-mixing then complete separation is not necessary and hence separation
difficulty and separation quality are dependent on the nature of the re-mix.
Here, we use a convolutional deep neural network (DNN), trained to estimate
'ideal' binary masks for separating voice from music, to perform re-mixing of
the vocal balance by operating directly on the individual magnitude components
of the musical mixture spectrogram. Our results demonstrate that small changes
in vocal gain may be applied with very little distortion to the ultimate
re-mix. Our method may be useful for re-mixing existing mixes.
</summary>
    <author>
      <name>Andrew J. R Simpson</name>
    </author>
    <author>
      <name>Gerard Roma</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <link href="http://arxiv.org/abs/1505.00289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.04385v1</id>
    <updated>2015-05-17T12:09:48Z</updated>
    <published>2015-05-17T12:09:48Z</published>
    <title>An Efficient Parameterization of the Room Transfer Function</title>
    <summary>  This paper proposes an efficient parameterization of the Room Transfer
Function (RTF). Typically, the RTF rapidly varies with varying source and
receiver positions, hence requires an impractical number of point to point
measurements to characterize a given room. Therefore, we derive a novel RTF
parameterization that is robust to both receiver and source variations with the
following salient features: (i) The parameterization is given in terms of a
modal expansion of 3D basis functions. (ii) The aforementioned modal expansion
can be truncated at a finite number of modes given that the source and receiver
locations are from two sizeable spatial regions, which are arbitrarily
distributed. (iii) The parameter weights/coefficients are independent of the
source/receiver positions. Therefore, a finite set of coefficients is shown to
be capable of accurately calculating the RTF between any two arbitrary points
from a predefined spatial region where the source(s) lie and a pre-defined
spatial region where the receiver(s) lie. A practical method to measure the RTF
coefficients is also provided, which only requires a single microphone unit and
a single loudspeaker unit, given that the room characteristics remain
stationary over time. The accuracy of the above parameterization is verified
using appropriate simulation examples.
</summary>
    <author>
      <name>Prasanga Samarasinghe</name>
    </author>
    <author>
      <name>Thushara Abhayapala</name>
    </author>
    <author>
      <name>Mark Poletti</name>
    </author>
    <author>
      <name>Terence Betlehem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.04385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.04385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.01830v2</id>
    <updated>2015-06-09T12:35:51Z</updated>
    <published>2015-06-05T09:20:00Z</published>
    <title>Sparsity and cosparsity for audio declipping: a flexible non-convex
  approach</title>
    <summary>  This work investigates the empirical performance of the sparse synthesis
versus sparse analysis regularization for the ill-posed inverse problem of
audio declipping. We develop a versatile non-convex heuristics which can be
readily used with both data models. Based on this algorithm, we report that, in
most cases, the two models perform almost similarly in terms of signal
enhancement. However, the analysis version is shown to be amenable for real
time audio processing, when certain analysis operators are considered. Both
versions outperform state-of-the-art methods in the field, especially for the
severely saturated signals.
</summary>
    <author>
      <name>Srđan Kitić</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA</arxiv:affiliation>
    </author>
    <author>
      <name>Nancy Bertin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA</arxiv:affiliation>
    </author>
    <author>
      <name>Rémi Gribonval</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LVA/ICA 2015 - The 12th International Conference on Latent
  Variable Analysis and Signal Separation, Aug 2015, Liberec, Czech Republic</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.01830v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.01830v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02170v1</id>
    <updated>2015-06-06T17:05:00Z</updated>
    <published>2015-06-06T17:05:00Z</published>
    <title>Hybridized Feature Extraction and Acoustic Modelling Approach for
  Dysarthric Speech Recognition</title>
    <summary>  Dysarthria is malfunctioning of motor speech caused by faintness in the human
nervous system. It is characterized by the slurred speech along with physical
impairment which restricts their communication and creates the lack of
confidence and affects the lifestyle. This paper attempt to increase the
efficiency of Automatic Speech Recognition (ASR) system for unimpaired speech
signal. It describes state of art of research into improving ASR for speakers
with dysarthria by means of incorporated knowledge of their speech production.
Hybridized approach for feature extraction and acoustic modelling technique
along with evolutionary algorithm is proposed for increasing the efficiency of
the overall system. Here number of feature vectors are varied and tested the
system performance. It is observed that system performance is boosted by
genetic algorithm. System with 16 acoustic features optimized with genetic
algorithm has obtained highest recognition rate of 98.28% with training time of
5:30:17.
</summary>
    <author>
      <name>Megha Rughani</name>
    </author>
    <author>
      <name>D. Shivakrishna</name>
    </author>
    <link href="http://arxiv.org/abs/1506.02170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.03604v1</id>
    <updated>2015-06-11T09:55:59Z</updated>
    <published>2015-06-11T09:55:59Z</published>
    <title>Binaural coherent-to-diffuse-ratio estimation for dereverberation using
  an ITD model</title>
    <summary>  Most previously proposed dual-channel coherent-to-diffuse-ratio (CDR)
estimators are based on a free-field model. When used for binaural signals,
e.g., for dereverberation in binaural hearing aids, their performance may
degrade due to the influence of the head, even when the direction-of-arrival of
the desired speaker is exactly known. In this paper, the head shadowing effect
is taken into account for CDR estimation by using a simplified model for the
frequency-dependent interaural time difference and a model for the binaural
coherence of the diffuse noise field. Evaluation of CDR-based dereverberation
with measured binaural impulse responses indicates that the proposed binaural
CDR estimators can improve PESQ scores.
</summary>
    <author>
      <name>Chengshi Zheng</name>
    </author>
    <author>
      <name>Andreas Schwarz</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <author>
      <name>Xiaodong Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for EUSIPCO 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.03604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.03604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.06832v1</id>
    <updated>2015-06-23T00:28:08Z</updated>
    <published>2015-06-23T00:28:08Z</published>
    <title>Detection and Analysis of Emotion From Speech Signals</title>
    <summary>  Recognizing emotion from speech has become one the active research themes in
speech processing and in applications based on human-computer interaction. This
paper conducts an experimental study on recognizing emotions from human speech.
The emotions considered for the experiments include neutral, anger, joy and
sadness. The distinuishability of emotional features in speech were studied
first followed by emotion classification performed on a custom dataset. The
classification was performed for different classifiers. One of the main feature
attribute considered in the prepared dataset was the peak-to-peak distance
obtained from the graphical representation of the speech signals. After
performing the classification tests on a dataset formed from 30 different
subjects, it was found that for getting better accuracy, one should consider
the data collected from one person rather than considering the data from a
group of people.
</summary>
    <author>
      <name>Assel Davletcharova</name>
    </author>
    <author>
      <name>Sherin Sugathan</name>
    </author>
    <author>
      <name>Bibia Abraham</name>
    </author>
    <author>
      <name>Alex Pappachen James</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2nd International Symposium on Computer Vision and the Internet,
  2015; to appear in Procedia Computer Science Journal, Elsevier, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.06832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.00201v1</id>
    <updated>2015-07-01T12:13:10Z</updated>
    <published>2015-07-01T12:13:10Z</published>
    <title>Towards a Generalization of Relative Transfer Functions to More Than One
  Source</title>
    <summary>  We propose a natural way to generalize relative transfer functions (RTFs) to
more than one source. We first prove that such a generalization is not possible
using a single multichannel spectro-temporal observation, regardless of the
number of microphones. We then introduce a new transform for multichannel
multi-frame spectrograms, i.e., containing several channels and time frames in
each time-frequency bin. This transform allows a natural generalization which
satisfies the three key properties of RTFs, namely, they can be directly
estimated from observed signals, they capture spatial properties of the sources
and they do not depend on emitted signals. Through simulated experiments, we
show how this new method can localize multiple simultaneously active sound
sources using short spectro-temporal windows, without relying on source
separation.
</summary>
    <author>
      <name>Antoine Deleforge</name>
    </author>
    <author>
      <name>Sharon Gannot</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <link href="http://arxiv.org/abs/1507.00201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.00201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05546v1</id>
    <updated>2015-07-20T16:06:23Z</updated>
    <published>2015-07-20T16:06:23Z</published>
    <title>Automatic Identification of Animal Breeds and Species Using Bioacoustics
  and Artificial Neural Networks</title>
    <summary>  In this research endeavor, it was hypothesized that the sound produced by
animals during their vocalizations can be used as identifiers of the animal
breed or species even if they sound the same to unaided human ear. To test this
hypothesis, three artificial neural networks (ANNs) were developed using
bioacoustics properties as inputs for the respective automatic identification
of 13 bird species, eight dog breeds, and 11 frog species. Recorded
vocalizations of these animals were collected and processed using several known
signal processing techniques to convert the respective sounds into computable
bioacoustics values. The converted values of the vocalizations, together with
the breed or species identifications, were used to train the ANNs following a
ten-fold cross validation technique. Tests show that the respective ANNs can
correctly identify 71.43\% of the birds, 94.44\% of the dogs, and 90.91\% of
the frogs. This result show that bioacoustics and ANN can be used to
automatically determine animal breeds and species, which together could be a
promising automated tool for animal identification, biodiversity determination,
animal conservation, and other animal welfare efforts.
</summary>
    <author>
      <name>Jaderick P. Pabico</name>
    </author>
    <author>
      <name>Anne Muriel V. Gonzales</name>
    </author>
    <author>
      <name>Mariann Jocel S. Villanueva</name>
    </author>
    <author>
      <name>Arlene A. Mendoza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.05546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08074v1</id>
    <updated>2015-07-29T09:22:58Z</updated>
    <published>2015-07-29T09:22:58Z</published>
    <title>STC Anti-spoofing Systems for the ASVspoof 2015 Challenge</title>
    <summary>  This paper presents the Speech Technology Center (STC) systems submitted to
Automatic Speaker Verification Spoofing and Countermeasures (ASVspoof)
Challenge 2015. In this work we investigate different acoustic feature spaces
to determine reliable and robust countermeasures against spoofing attacks. In
addition to the commonly used front-end MFCC features we explored features
derived from phase spectrum and features based on applying the multiresolution
wavelet transform. Similar to state-of-the-art ASV systems, we used the
standard TV-JFA approach for probability modelling in spoofing detection
systems. Experiments performed on the development and evaluation datasets of
the Challenge demonstrate that the use of phase-related and wavelet-based
features provides a substantial input into the efficiency of the resulting STC
systems. In our research we also focused on the comparison of the linear (SVM)
and nonlinear (DBN) classifiers.
</summary>
    <author>
      <name>Sergey Novoselov</name>
    </author>
    <author>
      <name>Alexandr Kozlov</name>
    </author>
    <author>
      <name>Galina Lavrentyeva</name>
    </author>
    <author>
      <name>Konstantin Simonchik</name>
    </author>
    <author>
      <name>Vadim Shchemelinin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 8 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.08074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.00354v1</id>
    <updated>2015-08-03T09:28:22Z</updated>
    <published>2015-08-03T09:28:22Z</published>
    <title>Significance of Maximum Spectral Amplitude in Sub-bands for Spectral
  Envelope Estimation and Its Application to Statistical Parametric Speech
  Synthesis</title>
    <summary>  In this paper we propose a technique for spectral envelope estimation using
maximum values in the sub-bands of Fourier magnitude spectrum (MSASB). Most
other methods in the literature parametrize spectral envelope in cepstral
domain such as Mel-generalized cepstrum etc. Such cepstral domain
representations, although compact, are not readily interpretable. This
difficulty is overcome by our method which parametrizes in the spectral domain
itself. In our experiments, spectral envelope estimated using MSASB method was
incorporated in the STRAIGHT vocoder. Both objective and subjective results of
analysis-by-synthesis indicate that the proposed method is comparable to
STRAIGHT. We also evaluate the effectiveness of the proposed parametrization in
a statistical parametric speech synthesis framework using deep neural networks.
</summary>
    <author>
      <name>Sivanand Achanta</name>
    </author>
    <author>
      <name>Anandaswarup Vadapalli</name>
    </author>
    <author>
      <name>Sai Krishna R.</name>
    </author>
    <author>
      <name>Suryakanth V. Gangashetty</name>
    </author>
    <link href="http://arxiv.org/abs/1508.00354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.00354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01746v2</id>
    <updated>2016-01-19T16:27:49Z</updated>
    <published>2015-08-07T16:20:52Z</published>
    <title>Using Deep Learning for Detecting Spoofing Attacks on Speech Signals</title>
    <summary>  It is well known that speaker verification systems are subject to spoofing
attacks. The Automatic Speaker Verification Spoofing and Countermeasures
Challenge -- ASVSpoof2015 -- provides a standard spoofing database, containing
attacks based on synthetic speech, along with a protocol for experiments. This
paper describes CPqD's systems submitted to the ASVSpoof2015 Challenge, based
on deep neural networks, working both as a classifier and as a feature
extraction module for a GMM and a SVM classifier. Results show the validity of
this approach, achieving less than 0.5\% EER for known attacks.
</summary>
    <author>
      <name>Alan Godoy</name>
    </author>
    <author>
      <name>Flávio Simões</name>
    </author>
    <author>
      <name>José Augusto Stuchi</name>
    </author>
    <author>
      <name>Marcus de Assis Angeloni</name>
    </author>
    <author>
      <name>Mário Uliani</name>
    </author>
    <author>
      <name>Ricardo Violato</name>
    </author>
    <link href="http://arxiv.org/abs/1508.01746v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.01746v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.04909v1</id>
    <updated>2015-08-20T08:07:10Z</updated>
    <published>2015-08-20T08:07:10Z</published>
    <title>Histogram of gradients of Time-Frequency Representations for Audio scene
  detection</title>
    <summary>  This paper addresses the problem of audio scenes classification and
contributes to the state of the art by proposing a novel feature. We build this
feature by considering histogram of gradients (HOG) of time-frequency
representation of an audio scene. Contrarily to classical audio features like
MFCC, we make the hypothesis that histogram of gradients are able to encode
some relevant informations in a time-frequency {representation:} namely, the
local direction of variation (in time and frequency) of the signal spectral
power. In addition, in order to gain more invariance and robustness, histogram
of gradients are locally pooled. We have evaluated the relevance of {the novel
feature} by comparing its performances with state-of-the-art competitors, on
several datasets, including a novel one that we provide, as part of our
contribution. This dataset, that we make publicly available, involves $19$
classes and contains about $900$ minutes of audio scene recording. We thus
believe that it may be the next standard dataset for evaluating audio scene
classification algorithms. Our comparison results clearly show that our
HOG-based features outperform its competitors
</summary>
    <author>
      <name>Alain Rakotomamonjy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <author>
      <name>Gilles Gasso</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITIS</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1508.04909v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.04909v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.02380v2</id>
    <updated>2016-04-06T15:03:41Z</updated>
    <published>2015-09-08T14:19:22Z</published>
    <title>Source localization and denoising: a perspective from the TDOA space</title>
    <summary>  In this manuscript, we formulate the problem of denoising Time Differences of
Arrival (TDOAs) in the TDOA space, i.e. the Euclidean space spanned by TDOA
measurements. The method consists of pre-processing the TDOAs with the purpose
of reducing the measurement noise. The complete set of TDOAs (i.e., TDOAs
computed at all microphone pairs) is known to form a redundant set, which lies
on a linear subspace in the TDOA space. Noise, however, prevents TDOAs from
lying exactly on this subspace. We therefore show that TDOA denoising can be
seen as a projection operation that suppresses the component of the noise that
is orthogonal to that linear subspace. We then generalize the projection
operator also to the cases where the set of TDOAs is incomplete. We
analytically show that this operator improves the localization accuracy, and we
further confirm that via simulation.
</summary>
    <author>
      <name>Marco Compagnoni</name>
    </author>
    <author>
      <name>Antonio Canclini</name>
    </author>
    <author>
      <name>Paolo Bestagini</name>
    </author>
    <author>
      <name>Fabio Antonacci</name>
    </author>
    <author>
      <name>Augusto Sarti</name>
    </author>
    <author>
      <name>Stefano Tubaro</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11045-016-0400-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11045-016-0400-9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Multidimensional Systems and Signal Processing 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1509.02380v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.02380v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.04934v1</id>
    <updated>2015-09-16T14:28:49Z</updated>
    <published>2015-09-16T14:28:49Z</published>
    <title>Background-tracking Acoustic Features for Genre Identification of
  Broadcast Shows</title>
    <summary>  This paper presents a novel method for extracting acoustic features that
characterise the background environment in audio recordings. These features are
based on the output of an alignment that fits multiple parallel
background--based Constrained Maximum Likelihood Linear Regression
transformations asynchronously to the input audio signal. With this setup, the
resulting features can track changes in the audio background like appearance
and disappearance of music, applause or laughter, independently of the speakers
in the foreground of the audio. The ability to provide this type of acoustic
description in audiovisual data has many potential applications, including
automatic classification of broadcast archives or improving automatic
transcription and subtitling. In this paper, the performance of these features
in a genre identification task in a set of 332 BBC shows is explored. The
proposed background--tracking features outperform short--term Perceptual Linear
Prediction features in this task using Gaussian Mixture Model classifiers (62%
vs 72% accuracy). The use of more complex classifiers, Hidden Markov Models and
Support Vector Machines, increases the performance of the system with the novel
background--tracking features to 79% and 81% in accuracy respectively.
</summary>
    <author>
      <name>Oscar Saz</name>
    </author>
    <author>
      <name>Mortaza Doulaty</name>
    </author>
    <author>
      <name>Thomas Hain</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SLT.2014.7078560</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SLT.2014.7078560" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Spoken Language Technology Workshop (SLT 2014), pp118-123,
  7-10 Dec 2014, Lake Tahoe, NV, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1509.04934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.04934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.04956v1</id>
    <updated>2015-09-16T15:56:22Z</updated>
    <published>2015-09-16T15:56:22Z</published>
    <title>Melodic Contour and Mid-Level Global Features Applied to the Analysis of
  Flamenco Cantes</title>
    <summary>  This work focuses on the topic of melodic characterization and similarity in
a specific musical repertoire: a cappella flamenco singing, more specifically
in debla and martinete styles. We propose the combination of manual and
automatic description. First, we use a state-of-the-art automatic transcription
method to account for general melodic similarity from music recordings. Second,
we define a specific set of representative mid-level melodic features, which
are manually labeled by flamenco experts. Both approaches are then contrasted
and combined into a global similarity measure. This similarity measure is
assessed by inspecting the clusters obtained through phylogenetic algorithms
algorithms and by relating similarity to categorization in terms of style.
Finally, we discuss the advantage of combining automatic and expert annotations
as well as the need to include repertoire-specific descriptions for meaningful
melodic characterization in traditional music collections.
</summary>
    <author>
      <name>Francisco Gómez</name>
    </author>
    <author>
      <name>Joaquín Mora</name>
    </author>
    <author>
      <name>Emilia Gómez</name>
    </author>
    <author>
      <name>José Miguel Díaz-Báñez</name>
    </author>
    <link href="http://arxiv.org/abs/1509.04956v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.04956v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06103v1</id>
    <updated>2015-09-21T03:37:11Z</updated>
    <published>2015-09-21T03:37:11Z</published>
    <title>Noise Robust IOA/CAS Speech Separation and Recognition System For The
  Third 'CHIME' Challenge</title>
    <summary>  This paper presents the contribution to the third 'CHiME' speech separation
and recognition challenge including both front-end signal processing and
back-end speech recognition. In the front-end, Multi-channel Wiener filter
(MWF) is designed to achieve background noise reduction. Different from
traditional MWF, optimized parameter for the tradeoff between noise reduction
and target signal distortion is built according to the desired noise reduction
level. In the back-end, several techniques are taken advantage to improve the
noisy Automatic Speech Recognition (ASR) performance including Deep Neural
Network (DNN), Convolutional Neural Network (CNN) and Long short-term memory
(LSTM) using medium vocabulary, Lattice rescoring with a big vocabulary
language model finite state transducer, and ROVER scheme. Experimental results
show the proposed system combining front-end and back-end is effective to
improve the ASR performance.
</summary>
    <author>
      <name>Xiaofei Wang</name>
    </author>
    <author>
      <name>Chao Wu</name>
    </author>
    <author>
      <name>Pengyuan Zhang</name>
    </author>
    <author>
      <name>Ziteng Wang</name>
    </author>
    <author>
      <name>Yong Liu</name>
    </author>
    <author>
      <name>Xu Li</name>
    </author>
    <author>
      <name>Qiang Fu</name>
    </author>
    <author>
      <name>Yonghong Yan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.06103v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06103v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06882v1</id>
    <updated>2015-09-23T08:34:18Z</updated>
    <published>2015-09-23T08:34:18Z</published>
    <title>Robust coherence-based spectral enhancement for distant speech
  recognition</title>
    <summary>  In this contribution to the 3rd CHiME Speech Separation and Recognition
Challenge (CHiME-3) we extend the acoustic front-end of the CHiME-3 baseline
speech recognition system by a coherence-based Wiener filter which is applied
to the output signal of the baseline beamformer. To compute the time- and
frequency-dependent postfilter gains the ratio between direct and diffuse
signal components at the output of the baseline beamformer is estimated and
used as approximation of the short-time signal-to-noise ratio. The proposed
spectral enhancement technique is evaluated with respect to word error rates of
the CHiME-3 challenge baseline speech recognition system using real speech
recorded in public environments. Results confirm the effectiveness of the
coherence-based postfilter when integrated into the front-end signal
enhancement.
</summary>
    <author>
      <name>Hendrik Barfuss</name>
    </author>
    <author>
      <name>Christian Huemmer</name>
    </author>
    <author>
      <name>Andreas Schwarz</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <link href="http://arxiv.org/abs/1509.06882v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06882v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.07211v1</id>
    <updated>2015-09-24T02:16:11Z</updated>
    <published>2015-09-24T02:16:11Z</published>
    <title>Noise-Robust ASR for the third 'CHiME' Challenge Exploiting
  Time-Frequency Masking based Multi-Channel Speech Enhancement and Recurrent
  Neural Network</title>
    <summary>  In this paper, the Lingban entry to the third 'CHiME' speech separation and
recognition challenge is presented. A time-frequency masking based speech
enhancement front-end is proposed to suppress the environmental noise utilizing
multi-channel coherence and spatial cues. The state-of-the-art speech
recognition techniques, namely recurrent neural network based acoustic and
language modeling, state space minimum Bayes risk based discriminative acoustic
modeling, and i-vector based acoustic condition modeling, are carefully
integrated into the speech recognition back-end. To further improve the system
performance by fully exploiting the advantages of different technologies, the
final recognition results are obtained by lattice combination and rescoring.
Evaluations carried out on the official dataset prove the effectiveness of the
proposed systems. Comparing with the best baseline result, the proposed system
obtains consistent improvements with over 57% relative word error rate
reduction on the real-data test set.
</summary>
    <author>
      <name>Zaihu Pang</name>
    </author>
    <author>
      <name>Fengyun Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 3rd 'CHiME' Speech Separation and Recognition Challenge, 5 pages,
  1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.07211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.07211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.09113v1</id>
    <updated>2015-09-30T10:33:41Z</updated>
    <published>2015-09-30T10:33:41Z</published>
    <title>Processing of acoustical signals via a wavelet-based analysis</title>
    <summary>  In the present paper, details are given on the implementation of a
wavelet-based analysis tailored to the processing of acoustical signals. The
family of the suitable wavelets (`Reimann wavelets') are obtained in the time
domain from a Fourier transform, extracted in Ref.~\cite{r1} after invoking
theoretical principles and time-frequency localisation constraints. A scheme is
set forth to determine the optimal values of the parameters of this type of
wavelet on the basis of the goodness of the reproduction of a $30$-s audio file
containing harmonic signals corresponding to six successive $A$ notes of the
chromatic musical scale, from $A_2$ to $A_7$. The quality of the reproduction
over about six and a half octaves is investigated. Finally, details are given
on the incorporation of the re-assignment method in the analysis framework, as
the means a) to determine the important contributions of the wavelet transforms
and b) to suppress noise present in the signal.
</summary>
    <author>
      <name>Evangelos Matsinos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 8 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.09113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.09113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00268v1</id>
    <updated>2015-10-01T14:57:08Z</updated>
    <published>2015-10-01T14:57:08Z</published>
    <title>The ICSTM+TUM+UP Approach to the 3rd CHIME Challenge: Single-Channel
  LSTM Speech Enhancement with Multi-Channel Correlation Shaping
  Dereverberation and LSTM Language Models</title>
    <summary>  This paper presents our contribution to the 3rd CHiME Speech Separation and
Recognition Challenge. Our system uses Bidirectional Long Short-Term Memory
(BLSTM) Recurrent Neural Networks (RNNs) for Single-channel Speech Enhancement
(SSE). Networks are trained to predict clean speech as well as noise features
from noisy speech features. In addition, the system applies two methods of
dereverberation on the 6-channel recordings of the challenge. The first is the
Phase-Error based Filtering (PEF) that uses time-varying phase-error filters
based on estimated time-difference of arrival of the speech source and the
phases of the microphone signals. The second is the Correlation Shaping (CS)
that applies a reduction of the long-term correlation energy in reverberant
speech. The Linear Prediction (LP) residual is processed to suppress the
long-term correlation. Furthermore, the system employs a LSTM Language Model
(LM) to perform N-best rescoring of recognition hypotheses. Using the proposed
methods, an improved Word Error Rate (WER) of 24.38% is achieved over the real
eval test set. This is around 25% relative improvement over the challenge
baseline.
</summary>
    <author>
      <name>Amr El-Desoky Mousa</name>
    </author>
    <author>
      <name>Erik Marchi</name>
    </author>
    <author>
      <name>Björn Schuller</name>
    </author>
    <link href="http://arxiv.org/abs/1510.00268v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00268v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.01443v1</id>
    <updated>2015-10-06T06:12:31Z</updated>
    <published>2015-10-06T06:12:31Z</published>
    <title>A Waveform Representation Framework for High-quality Statistical
  Parametric Speech Synthesis</title>
    <summary>  State-of-the-art statistical parametric speech synthesis (SPSS) generally
uses a vocoder to represent speech signals and parameterize them into features
for subsequent modeling. Magnitude spectrum has been a dominant feature over
the years. Although perceptual studies have shown that phase spectrum is
essential to the quality of synthesized speech, it is often ignored by using a
minimum phase filter during synthesis and the speech quality suffers. To bypass
this bottleneck in vocoded speech, this paper proposes a phase-embedded
waveform representation framework and establishes a magnitude-phase joint
modeling platform for high-quality SPSS. Our experiments on waveform
reconstruction show that the performance is better than that of the widely-used
STRAIGHT. Furthermore, the proposed modeling and synthesis platform outperforms
a leading-edge, vocoded, deep bidirectional long short-term memory recurrent
neural network (DBLSTM-RNN)-based baseline system in various objective
evaluation metrics conducted.
</summary>
    <author>
      <name>Bo Fan</name>
    </author>
    <author>
      <name>Siu Wa Lee</name>
    </author>
    <author>
      <name>Xiaohai Tian</name>
    </author>
    <author>
      <name>Lei Xie</name>
    </author>
    <author>
      <name>Minghui Dong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted and will appear in APSIPA2015; keywords: speech synthesis,
  LSTM-RNN, vocoder, phase, waveform, modeling</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.01443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.01443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.01806v3</id>
    <updated>2017-01-12T01:45:08Z</updated>
    <published>2015-10-07T03:00:52Z</published>
    <title>Music Viewed by its Entropy Content: A Novel Window for Comparative
  Analysis</title>
    <summary>  Polyphonic music files were analyzed using the set of symbols that produced
the Minimal Entropy Description which we call the Fundamental Scale. This
allowed us to create a novel space to represent music pieces by developing: a)
a method to adjust a description from its original scale of observation to a
general scale, b) the concept of higher order entropy as the entropy associated
to the deviations of a frequency ranked symbol profile from a perfect Zipf
profile. We called this diversity index the "2nd Order Entropy". Applying these
methods to a variety of musical pieces showed how the space of "symbolic
specific diversity-entropy" and that of "2nd order entropy" captures
characteristics that are unique to each music type, style, composer and genre.
Some clustering of these properties around each musical category is shown. This
method allows to visualize a historic trajectory of academic music across this
space, from medieval to contemporary academic music. We show that description
of musical structures using entropy and symbolic diversity allows to
characterize traditional and popular expressions of music. These classification
techniques promise to be useful in other disciplines for pattern recognition
and machine learning, for example.
</summary>
    <author>
      <name>Gerardo Febres</name>
    </author>
    <author>
      <name>Klaus Jaffe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">41 pages, 15 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.01806v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.01806v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.03602v1</id>
    <updated>2015-10-13T09:51:23Z</updated>
    <published>2015-10-13T09:51:23Z</published>
    <title>A language model based approach towards large scale and lightweight
  language identification systems</title>
    <summary>  Multilingual spoken dialogue systems have gained prominence in the recent
past necessitating the requirement for a front-end Language Identification
(LID) system. Most of the existing LID systems rely on modeling the language
discriminative information from low-level acoustic features. Due to the
variabilities of speech (speaker and emotional variabilities, etc.),
large-scale LID systems developed using low-level acoustic features suffer from
a degradation in the performance. In this approach, we have attempted to model
the higher level language discriminative phonotactic information for developing
an LID system. In this paper, the input speech signal is tokenized to phone
sequences by using a language independent phone recognizer. The language
discriminative phonotactic information in the obtained phone sequences are
modeled using statistical and recurrent neural network based language modeling
approaches. As this approach, relies on higher level phonotactical information
it is more robust to variabilities of speech. Proposed approach is
computationally light weight, highly scalable and it can be used in complement
with the existing LID systems.
</summary>
    <author>
      <name>Brij Mohan Lal Srivastava</name>
    </author>
    <author>
      <name>Hari Krishna Vydana</name>
    </author>
    <author>
      <name>Anil Kumar Vuppala</name>
    </author>
    <author>
      <name>Manish Shrivastava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review at ICASSP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.03602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.03602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04029v1</id>
    <updated>2015-10-14T10:14:29Z</updated>
    <published>2015-10-14T10:14:29Z</published>
    <title>Corpus COFLA: A research corpus for the Computational study of Flamenco
  Music</title>
    <summary>  Flamenco is a music tradition from Southern Spain which attracts a growing
community of enthusiasts around the world. Its unique melodic and rhythmic
elements, the typically spontaneous and improvised interpretation and its
diversity regarding styles make this still largely undocumented art form a
particularly interesting material for musicological studies. In prior works it
has already been demonstrated that research on computational analysis of
flamenco music, despite it being a relatively new field, can provide powerful
tools for the discovery and diffusion of this genre. In this paper we present
corpusCOFLA, a data framework for the development of such computational tools.
The proposed collection of audio recordings and meta-data serves as a pool for
creating annotated subsets which can be used in development and evaluation of
algorithms for specific music information retrieval tasks. First, we describe
the design criteria for the corpus creation and then provide various examples
of subsets drawn from the corpus. We showcase possible research applications in
the context of computational study of flamenco music and give perspectives
regarding further development of the corpus.
</summary>
    <author>
      <name>Nadine Kroher</name>
    </author>
    <author>
      <name>José-Miguel Díaz-Báñez</name>
    </author>
    <author>
      <name>Joaquin Mora</name>
    </author>
    <author>
      <name>Emilia Gómez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, submitted to the ACM Journal of Computing and Cultural
  Heritage</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04039v1</id>
    <updated>2015-10-14T10:53:00Z</updated>
    <published>2015-10-14T10:53:00Z</published>
    <title>Automatic Transcription of Flamenco Singing from Polyphonic Music
  Recordings</title>
    <summary>  Automatic note-level transcription is considered one of the most challenging
tasks in music information retrieval. The specific case of flamenco singing
transcription poses a particular challenge due to its complex melodic
progressions, intonation inaccuracies, the use of a high degree of
ornamentation and the presence of guitar accompaniment. In this study, we
explore the limitations of existing state of the art transcription systems for
the case of flamenco singing and propose a specific solution for this genre: We
first extract the predominant melody and apply a novel contour filtering
process to eliminate segments of the pitch contour which originate from the
guitar accompaniment. We formulate a set of onset detection functions based on
volume and pitch characteristics to segment the resulting vocal pitch contour
into discrete note events. A quantised pitch label is assigned to each note
event by combining global pitch class probabilities with local pitch contour
statistics. The proposed system outperforms state of the art singing
transcription systems with respect to voicing accuracy, onset detection and
overall performance when evaluated on flamenco singing datasets.
</summary>
    <author>
      <name>Nadine Kroher</name>
    </author>
    <author>
      <name>Emilia Gómez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2016.2531284</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2016.2531284" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the IEEE Transactions on Audio, Speech and Language
  Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04205v1</id>
    <updated>2015-10-14T17:17:33Z</updated>
    <published>2015-10-14T17:17:33Z</published>
    <title>Reducing one-to-many problem in Voice Conversion by equalizing the
  formant locations using dynamic frequency warping</title>
    <summary>  In this study, we investigate a solution to reduce the effect of one-to-many
problem in voice conversion. One-to-many problem in VC happens when two very
similar speech segments in source speaker have corresponding speech segments in
target speaker that are not similar to each other. As a result, the mapper
function usually over-smoothes the generated features in order to be similar to
both target speech segments. In this study, we propose to equalize the formant
location of source-target frame pairs using dynamic frequency warping in order
to reduce the complexity. After the conversion, another dynamic frequency
warping is further applied to reverse the effect of formant location
equalization during the training. The subjective experiments showed that the
proposed approach improves the speech quality significantly.
</summary>
    <author>
      <name>Seyed Hamidreza Mohammadi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04205v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04205v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04595v3</id>
    <updated>2016-04-15T10:10:30Z</updated>
    <published>2015-10-15T15:48:19Z</published>
    <title>A Variational EM Algorithm for the Separation of Time-Varying
  Convolutive Audio Mixtures</title>
    <summary>  This paper addresses the problem of separating audio sources from
time-varying convolutive mixtures. We propose a probabilistic framework based
on the local complex-Gaussian model combined with non-negative matrix
factorization. The time-varying mixing filters are modeled by a continuous
temporal stochastic process. We present a variational expectation-maximization
(VEM) algorithm that employs a Kalman smoother to estimate the time-varying
mixing matrix, and that jointly estimate the source parameters. The sound
sources are then separated by Wiener filters constructed with the estimators
provided by the VEM algorithm. Extensive experiments on simulated data show
that the proposed method outperforms a block-wise version of a state-of-the-art
baseline method.
</summary>
    <author>
      <name>Dionyssos Kounades-Bastian</name>
    </author>
    <author>
      <name>Laurent Girin</name>
    </author>
    <author>
      <name>Xavier Alameda-Pineda</name>
    </author>
    <author>
      <name>Sharon Gannot</name>
    </author>
    <author>
      <name>Radu Horaud</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2016.2554286</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2016.2554286" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE/ACM Transactions on Audio, Speech and Language Processing,
  24(8), 1408-1423, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1510.04595v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04595v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04616v1</id>
    <updated>2015-10-15T16:42:16Z</updated>
    <published>2015-10-15T16:42:16Z</published>
    <title>Evaluating the Non-Intrusive Room Acoustics Algorithm with the ACE
  Challenge</title>
    <summary>  We present a single channel data driven method for non-intrusive estimation
of full-band reverberation time and full-band direct-to-reverberant ratio. The
method extracts a number of features from reverberant speech and builds a model
using a recurrent neural network to estimate the reverberant acoustic
parameters. We explore three configurations by including different data and
also by combining the recurrent neural network estimates using a support vector
machine. Our best method to estimate DRR provides a Root Mean Square Deviation
(RMSD) of 3.84 dB and a RMSD of 43.19 % for T60 estimation.
</summary>
    <author>
      <name>Pablo Peso Parada</name>
    </author>
    <author>
      <name>Dushyant Sharma</name>
    </author>
    <author>
      <name>Toon van Waterschoot</name>
    </author>
    <author>
      <name>Patrick A. Naylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383)</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04616v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04616v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04620v1</id>
    <updated>2015-10-15T16:48:48Z</updated>
    <published>2015-10-15T16:48:48Z</published>
    <title>Joint Estimation of Reverberation Time and Direct-to-Reverberation Ratio
  from Speech using Auditory-Inspired Features</title>
    <summary>  Blind estimation of acoustic room parameters such as the reverberation time
$T_\mathrm{60}$ and the direct-to-reverberation ratio ($\mathrm{DRR}$) is still
a challenging task, especially in case of blind estimation from reverberant
speech signals. In this work, a novel approach is proposed for joint estimation
of $T_\mathrm{60}$ and $\mathrm{DRR}$ from wideband speech in noisy conditions.
2D Gabor filters arranged in a filterbank are exploited for extracting
features, which are then used as input to a multi-layer perceptron (MLP). The
MLP output neurons correspond to specific pairs of $(T_\mathrm{60},
\mathrm{DRR})$ estimates; the output is integrated over time, and a simple
decision rule results in our estimate. The approach is applied to
single-microphone fullband speech signals provided by the Acoustic
Characterization of Environments (ACE) Challenge. Our approach outperforms the
baseline systems with median errors of close-to-zero and -1.5 dB for the
$T_\mathrm{60}$ and $\mathrm{DRR}$ estimates, respectively, while the
calculation of estimates is 5.8 times faster compared to the baseline.
</summary>
    <author>
      <name>Feifei Xiong</name>
    </author>
    <author>
      <name>Stefan Goetze</name>
    </author>
    <author>
      <name>Bernd T. Meyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383)</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04620v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04620v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04707v1</id>
    <updated>2015-10-15T20:18:29Z</updated>
    <published>2015-10-15T20:18:29Z</published>
    <title>SRMR variants for improved blind room acoustics characterization</title>
    <summary>  Reverberation, especially in large rooms, severely degrades speech
recognition performance and speech intelligibility. Since direct measurement of
room characteristics is usually not possible, blind estimation of
reverberation-related metrics such as the reverberation time (RT) and the
direct-to-reverberant energy ratio (DRR) can be valuable information to speech
recognition and enhancement algorithms operating in enclosed environments. The
objective of this work is to evaluate the performance of five variants of blind
RT and DRR estimators based on a modulation spectrum representation of
reverberant speech with single- and multi-channel speech data. These models are
all based on variants of the so-called Speech-to-Reverberation Modulation
Energy Ratio (SRMR). We show that these measures outperform a state-of-the-art
baseline based on maximum-likelihood estimation of sound decay rates in terms
of root-mean square error (RMSE), as well as Pearson correlation. Compared to
the baseline, the best proposed measure, called NSRMR_k , achieves a 23%
relative improvement in terms of RMSE and allows for relative correlation
improvements ranging from 13% to 47% for RT prediction.
</summary>
    <author>
      <name>M. Senoussaoui</name>
    </author>
    <author>
      <name>J. F. Santos</name>
    </author>
    <author>
      <name>T. H. Falk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACE Chal- lenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383)</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.04880v1</id>
    <updated>2015-10-15T18:42:04Z</updated>
    <published>2015-10-15T18:42:04Z</published>
    <title>Harmonic and Timbre Analysis of Tabla Strokes</title>
    <summary>  Indian twin drums mainly bayan and dayan (tabla) are the most important
percussion instruments in India popularly used for keeping rhythm. It is a twin
percussion/drum instrument of which the right hand drum is called dayan and the
left hand drum is called bayan. Tabla strokes are commonly called as `bol',
constitutes a series of syllables. In this study we have studied the timbre
characteristics of nine strokes from each of five different tablas. Timbre
parameters were calculated from LTAS of each stroke signals. Study of timbre
characteristics is one of the most important deterministic approach for
analyzing tabla and its stroke characteristics. Statistical correlations among
timbre parameters were measured and also through factor analysis we get to know
about the parameters of timbre analysis which are closely related. Tabla
strokes have unique harmonic and timbral characteristics at mid frequency range
and have no uniqueness at low frequency ranges.
</summary>
    <author>
      <name>Anirban Patranabis</name>
    </author>
    <author>
      <name>Kaushik Banerjee</name>
    </author>
    <author>
      <name>Vishal Midya</name>
    </author>
    <author>
      <name>Sneha Chakraborty</name>
    </author>
    <author>
      <name>Shankha Sanyal</name>
    </author>
    <author>
      <name>Archi Banerjee</name>
    </author>
    <author>
      <name>Ranjan Sengupta</name>
    </author>
    <author>
      <name>Dipak Ghosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.04880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.04880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.class-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.05937v2</id>
    <updated>2016-03-31T05:33:49Z</updated>
    <published>2015-10-20T15:49:59Z</published>
    <title>Binary Speaker Embedding</title>
    <summary>  The popular i-vector model represents speakers as low-dimensional continuous
vectors (i-vectors), and hence it is a way of continuous speaker embedding. In
this paper, we investigate binary speaker embedding, which transforms i-vectors
to binary vectors (codes) by a hash function. We start from locality sensitive
hashing (LSH), a simple binarization approach where binary codes are derived
from a set of random hash functions. A potential problem of LSH is that the
randomly sampled hash functions might be suboptimal. We therefore propose an
improved Hamming distance learning approach, where the hash function is learned
by a variable-sized block training that projects each dimension of the original
i-vectors to variable-sized binary codes independently. Our experiments show
that binary speaker embedding can deliver competitive or even better results on
both speaker verification and identification tasks, while the memory usage and
the computation cost are significantly reduced.
</summary>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Chao Xing</name>
    </author>
    <author>
      <name>Kaimin Yu</name>
    </author>
    <author>
      <name>Thomas Fang Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1510.05937v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.05937v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.05940v2</id>
    <updated>2016-03-31T05:27:17Z</updated>
    <published>2015-10-20T16:01:05Z</published>
    <title>Max-margin Metric Learning for Speaker Recognition</title>
    <summary>  Probabilistic linear discriminant analysis (PLDA) is a popular normalization
approach for the i-vector model, and has delivered state-of-the-art performance
in speaker recognition. A potential problem of the PLDA model, however, is that
it essentially assumes Gaussian distributions over speaker vectors, which is
not always true in practice. Additionally, the objective function is not
directly related to the goal of the task, e.g., discriminating true speakers
and imposters. In this paper, we propose a max-margin metric learning approach
to solve the problems. It learns a linear transform with a criterion that the
margin between target and imposter trials are maximized. Experiments conducted
on the SRE08 core test show that compared to PLDA, the new approach can obtain
comparable or even better performance, though the scoring is simply a cosine
computation.
</summary>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Chao Xing</name>
    </author>
    <author>
      <name>Thomas Fang Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1510.05940v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.05940v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.07315v1</id>
    <updated>2015-10-25T22:24:37Z</updated>
    <published>2015-10-25T22:24:37Z</published>
    <title>A Hybrid Approach for Speech Enhancement Using MoG Model and Neural
  Network Phoneme Classifier</title>
    <summary>  In this paper we present a single-microphone speech enhancement algorithm. A
hybrid approach is proposed merging the generative mixture of Gaussians (MoG)
model and the discriminative neural network (NN). The proposed algorithm is
executed in two phases, the training phase, which does not recur, and the test
phase. First, the noise-free speech power spectral density (PSD) is modeled as
a MoG, representing the phoneme based diversity in the speech signal. An NN is
then trained with phoneme labeled database for phoneme classification with
mel-frequency cepstral coefficients (MFCC) as the input features. Given the
phoneme classification results, a speech presence probability (SPP) is obtained
using both the generative and discriminative models. Soft spectral subtraction
is then executed while simultaneously, the noise estimation is updated. The
discriminative NN maintain the continuity of the speech and the generative
phoneme-based MoG preserves the speech spectral structure. Extensive
experimental study using real speech and noise signals is provided. We also
compare the proposed algorithm with alternative speech enhancement algorithms.
We show that we obtain a significant improvement over previous methods in terms
of both speech quality measures and speech recognition results.
</summary>
    <author>
      <name>Shlomo E. Chazan</name>
    </author>
    <author>
      <name>Jacob Goldberger</name>
    </author>
    <author>
      <name>Sharon Gannot</name>
    </author>
    <link href="http://arxiv.org/abs/1510.07315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.07315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.07546v1</id>
    <updated>2015-10-26T16:50:26Z</updated>
    <published>2015-10-26T16:50:26Z</published>
    <title>Direct-to-Reverberant Ratio Estimation on the ACE Corpus Using a
  Two-channel Beamformer</title>
    <summary>  Direct-to-Reverberant Ratio (DRR) is an important measure for characterizing
the properties of a room. The recently proposed DRR Estimation using a
Null-Steered Beamformer (DENBE) algorithm was originally tested on simulated
data where noise was artificially added to the speech after convolution with
impulse responses simulated using the image-source method. This paper evaluates
the performance of this algorithm on speech convolved with measured impulse
responses and noise using the Acoustic Characterization of Environments (ACE)
Evaluation corpus. The fullband DRR estimation performance of the DENBE
algorithm exceeds that of the baselines in all Signal-to-Noise Ratios (SNRs)
and noise types. In addition, estimation of the DRR in one third-octave ISO
frequency bands is demonstrated.
</summary>
    <author>
      <name>James Eaton</name>
    </author>
    <author>
      <name>Patrick A. Naylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383). arXiv admin note: text overlap with
  arXiv:1510.01193</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.07546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.07546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.07774v1</id>
    <updated>2015-10-27T05:25:11Z</updated>
    <published>2015-10-27T05:25:11Z</published>
    <title>A dictionary learning and source recovery based approach to classify
  diverse audio sources</title>
    <summary>  A dictionary learning based audio source classification algorithm is proposed
to classify a sample audio signal as one amongst a finite set of different
audio sources. Cosine similarity measure is used to select the atoms during
dictionary learning. Based on three objective measures proposed, namely, signal
to distortion ratio (SDR), the number of non-zero weights and the sum of
weights, a frame-wise source classification accuracy of 98.2% is obtained for
twelve different sources. Cent percent accuracy has been obtained using moving
SDR accumulated over six successive frames for ten of the audio sources tested,
while the two other sources require accumulation of 10 and 14 frames.
</summary>
    <author>
      <name>K V Vijay Girish</name>
    </author>
    <author>
      <name>T V Ananthapadmanabha</name>
    </author>
    <author>
      <name>A G Ramakrishnan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.07774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.07774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.08950v1</id>
    <updated>2015-10-30T01:52:31Z</updated>
    <published>2015-10-30T01:52:31Z</published>
    <title>Estimation of the direct-to-reverberant Energy Ratio using a spherical
  microphone array</title>
    <summary>  This paper proposes a practical approach to estimate the
direct-to-reverberant energy ratio (DRR) using a spherical microphone array
without having knowledge of the source signal. We base our estimation on a
theoretical relationship between the DRR and the coherence estimation function
between coincident pressure and particle velocity. We discuss the proposed
method's ability to estimate the DRR in a wide variety of room sizes,
reverberation times and source receiver distances with appropriate examples.
Test results show that the method can estimate the room DRR for frequencies
between 199 - 2511 Hz, with $\pm$ 3 dB accuracy.
</summary>
    <author>
      <name>Hanchi Chen</name>
    </author>
    <author>
      <name>Prasanga N. Samarasinghe</name>
    </author>
    <author>
      <name>Thushara D. Abhayapala</name>
    </author>
    <author>
      <name>Wen Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383)</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.08950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.08950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.00067v1</id>
    <updated>2015-10-31T03:48:31Z</updated>
    <published>2015-10-31T03:48:31Z</published>
    <title>Sparsity-based Algorithm for Detecting Faults in Rotating Machines</title>
    <summary>  This paper addresses the detection of periodic transients in vibration
signals for detecting faults in rotating machines. For this purpose, we present
a method to estimate periodic-group-sparse signals in noise. The method is
based on the formulation of a convex optimization problem. A fast iterative
algorithm is given for its solution. A simulated signal is formulated to verify
the performance of the proposed approach for periodic feature extraction. The
detection performance of comparative methods is compared with that of the
proposed approach via RMSE values and receiver operating characteristic (ROC)
curves. Finally, the proposed approach is applied to compound faults diagnosis
of motor bearings. The non-stationary vibration data were acquired from a
SpectraQuest's machinery fault simulator. The processed results show the
proposed approach can effectively detect and extract the useful features of
bearing outer race and inner race defect.
</summary>
    <author>
      <name>Wangpeng He</name>
    </author>
    <author>
      <name>Yin Ding</name>
    </author>
    <author>
      <name>Yanyang Zi</name>
    </author>
    <author>
      <name>Ivan W. Selesnick</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ymssp.2015.11.027</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ymssp.2015.11.027" rel="related"/>
    <link href="http://arxiv.org/abs/1511.00067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.00067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.00393v2</id>
    <updated>2016-07-30T21:13:12Z</updated>
    <published>2015-11-02T06:08:24Z</published>
    <title>Detection of Faults in Rotating Machinery Using Periodic Time-Frequency
  Sparsity</title>
    <summary>  This paper addresses the problem of extracting periodic oscillatory features
in vibration sig- nals for detecting faults in rotating machinery. To extract
the feature, we propose an approach in the short-time Fourier transform (STFT)
domain where the periodic oscillatory feature man- ifests itself as a
relatively sparse grid. To estimate the sparse grid, we formulate an
optimization problem using customized binary weights in the regularizer, where
the weights are formulated to promote periodicity. In order to solve the
proposed optimization problem, we develop an algorithm called augmented
Lagrangian majorization-minimization algorithm, which combines the split
augmented Lagrangian shrinkage algorithm (SALSA) with majorization-minimization
(MM), and is guaranteed to converge for both convex and non-convex formulation.
As examples, the proposed approach is applied to simulated data, and used as a
tool for diagnosing faults in bearings and gearboxes for real data, and
compared to some state-of-the-art methods. The results show the proposed
approach can effectively detect and extract the periodical oscillatory
features.
</summary>
    <author>
      <name>Yin Ding</name>
    </author>
    <author>
      <name>Wangpeng He</name>
    </author>
    <author>
      <name>Binqiang Chen</name>
    </author>
    <author>
      <name>Yanyang Zi</name>
    </author>
    <author>
      <name>Ivan W. Selesnick</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jsv.2016.07.004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jsv.2016.07.004" rel="related"/>
    <link href="http://arxiv.org/abs/1511.00393v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.00393v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.03174v3</id>
    <updated>2016-12-12T13:19:16Z</updated>
    <published>2015-11-10T16:39:01Z</published>
    <title>Fault Diagnosis of Rolling Element Bearings with a Spectrum Searching
  Method</title>
    <summary>  Rolling element bearing faults in rotating systems are observed as impulses
in the vibration signals, which are usually buried in noises. In order to
effectively detect the fault of bearings, a novel spectrum searching method is
proposed. The structural information of spectrum (SIOS) on a predefined basis
is constructed through a searching algorithm, such that the harmonics of
impulses generated by faults can be clearly identified and analyzed. Local
peaks of the spectrum are located on a certain bin of the basis, and then the
SIOS can interpret the spectrum via the number and energy of harmonics related
to frequency bins of the basis. Finally bearings can be diagnosed based on the
SIOS by identifying its dominant components. Mathematical formulation is
developed to guarantee the correct construction of the SISO through searching.
The effectiveness of the proposed method is verified with a simulation signal
and a benchmark study of bearings.
</summary>
    <author>
      <name>Wei Li</name>
    </author>
    <author>
      <name>Mingquan Qiu</name>
    </author>
    <author>
      <name>Zhencai Zhu</name>
    </author>
    <author>
      <name>Fan Jiang</name>
    </author>
    <author>
      <name>Gongbo Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1511.03174v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.03174v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04063v1</id>
    <updated>2015-11-12T20:41:40Z</updated>
    <published>2015-11-12T20:41:40Z</published>
    <title>Single-Channel Maximum-Likelihood T60 Estimation Exploiting Subband
  Information</title>
    <summary>  This contribution presents four algorithms developed by the authors for
single-channel fullband and subband T60 estimation within the ACE challenge.
The blind estimation of the fullband reverberation time (RT) by
maximum-likelihood (ML) estimation based on [15] is considered as baseline
approach. An improvement of this algorithm is devised where an energy-weighted
averaging of the upper subband RT estimates is performed using either a DCT or
1/3-octave filter-bank. The evaluation results show that this approach leads to
a lower variance for the estimation error in comparison to the baseline
approach at the price of an increased computational complexity. Moreover, a new
algorithm to estimate the subband RT is devised, where the RT estimates for the
lower octave subbands are extrapolated from the RT estimates of the upper
subbands by means of a simple model for the frequency-dependency of the subband
RT. The evaluation results of the ACE challenge reveal that this approach
allows to estimate the subband RT with an estimation error which is in a
similar range as for the presented fullband RT estimators.
</summary>
    <author>
      <name>Heinrich Loellmann</name>
    </author>
    <author>
      <name>Andreas Brendel</name>
    </author>
    <author>
      <name>Peter Vary</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACE Challenge Workshop - a satellite event of
  IEEE-WASPAA 2015 (arXiv:1510.00383)</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.04063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.04063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04867v2</id>
    <updated>2015-11-23T09:48:17Z</updated>
    <published>2015-11-16T08:53:37Z</published>
    <title>Quality assessment of voice converted speech using articulatory features</title>
    <summary>  We propose a novel application based on acoustic-to-articulatory inversion
towards quality assessment of voice converted speech. The ability of humans to
speak effortlessly requires coordinated movements of various articulators,
muscles, etc. This effortless movement contributes towards naturalness,
intelligibility and speakers identity which is partially present in voice
converted speech. Hence, during voice conversion, the information related to
speech production is lost. In this paper, this loss is quantified for male
voice, by showing increase in RMSE error for voice converted speech followed by
showing decrease in mutual information. Similar results are obtained in case of
female voice. This observation is extended by showing that articulatory
features can be used as an objective measure. The effectiveness of proposed
measure over MCD is illustrated by comparing their correlation with Mean
Opinion Score.
</summary>
    <author>
      <name>Avni Rajpal</name>
    </author>
    <author>
      <name>Nirmesh J. Shah</name>
    </author>
    <author>
      <name>Mohammadi Zaki</name>
    </author>
    <author>
      <name>Hemant A. Patil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper is withdrawn from the arxiv. Author doesnot want
  circulation of unpublished unverified results</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.04867v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.04867v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.07008v1</id>
    <updated>2015-11-22T13:05:05Z</updated>
    <published>2015-11-22T13:05:05Z</published>
    <title>Real Time Vowel Tremolo Detection Using Low Level Audio Descriptors</title>
    <summary>  This paper resumes the results of a research conducted in a music production
situation Therefore, it is more a final lab report, a prospective methodology
then a scientific experience. The methodology we are presenting was developed
as an answer to a musical problem raised by the Italian composer Marta
Gentilucci. The problem was "how to extract a temporal structure from a vowel
tremolo, on a tenuto (steady state) pitch." The musical goal was to apply, in a
compositional context the vowel tremolo time structure on a tenuto pitch chord,
as a transposition control.In this context we decide to follow, to explore the
potential of low-level MPEG7 audio descriptors to build event detection
functions. One of the main problems using low-level audio descriptors in audio
analysis is the redundancy of information among them. We describe an "ad hoc"
interactive methodology, based on side effect use of dimensionality reduction
by PCA, to choose a feature from a set of low-level audio descriptors, to be
used to detect a vowel tremolo rhythm. This methodology is supposed to be
interactive and easy enough to be used in a live creative context.
</summary>
    <author>
      <name>Mikhail Malt</name>
    </author>
    <author>
      <name>Marta Gentilucci</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures, 2 tables, lab report</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.07008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.07008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02560v1</id>
    <updated>2015-12-08T17:34:49Z</updated>
    <published>2015-12-08T17:34:49Z</published>
    <title>Deep Learning for Single and Multi-Session i-Vector Speaker Recognition</title>
    <summary>  The promising performance of Deep Learning (DL) in speech recognition has
motivated the use of DL in other speech technology applications such as speaker
recognition. Given i-vectors as inputs, the authors proposed an impostor
selection algorithm and a universal model adaptation process in a hybrid system
based on Deep Belief Networks (DBN) and Deep Neural Networks (DNN) to
discriminatively model each target speaker. In order to have more insight into
the behavior of DL techniques in both single and multi-session speaker
enrollment tasks, some experiments have been carried out in this paper in both
scenarios. Additionally, the parameters of the global model, referred to as
universal DBN (UDBN), are normalized before adaptation. UDBN normalization
facilitates training DNNs specifically with more than one hidden layer.
Experiments are performed on the NIST SRE 2006 corpus. It is shown that the
proposed impostor selection algorithm and UDBN adaptation process enhance the
performance of conventional DNNs 8-20 % and 16-20 % in terms of EER for the
single and multi-session tasks, respectively. In both scenarios, the proposed
architectures outperform the baseline systems obtaining up to 17 % reduction in
EER.
</summary>
    <author>
      <name>Omid Ghahabi</name>
    </author>
    <author>
      <name>Javier Hernando</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2017.2661705</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2017.2661705" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE/ACM Transactions on Audio, Speech, and Language Processing,
  Volume: 25, Issue: 4, April 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1512.02560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.05811v1</id>
    <updated>2015-12-17T22:06:51Z</updated>
    <published>2015-12-17T22:06:51Z</published>
    <title>Spectral Study of the Vocal Tract in Vowel Synthesis: A Comparison
  between 1D and 3D Acoustic Analysis</title>
    <summary>  A state-of-the-art 1D acoustic synthesizer has been previously developed, and
coupled to speaker-specific biomechanical models of oropharynx in ArtiSynth. As
expected, the formant frequencies of the synthesized vowel sounds were shown to
be different from those of the recorded audio. Such discrepancy was
hypothesized to be due to the simplified geometry of the vocal tract model as
well as the one dimensional implementation of Navier-Stokes equations. In this
paper, we calculate Helmholtz resonances of our vocal tract geometries using 3D
finite element method (FEM), and compare them with the formant frequencies
obtained from the 1D method and audio. We hope such comparison helps with
clarifying the limitations of our current models and/or speech synthesizer.
</summary>
    <author>
      <name>Negar M. Harandi</name>
    </author>
    <author>
      <name>Daniel Aalto</name>
    </author>
    <author>
      <name>Antti Hannukainen</name>
    </author>
    <author>
      <name>Jarmo Malinen</name>
    </author>
    <author>
      <name>Sidney Fels</name>
    </author>
    <link href="http://arxiv.org/abs/1512.05811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.05811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.06222v1</id>
    <updated>2015-12-19T10:27:59Z</updated>
    <published>2015-12-19T10:27:59Z</published>
    <title>A new robust adaptive algorithm for underwater acoustic channel
  equalization</title>
    <summary>  We introduce a novel family of adaptive robust equalizers for highly
challenging underwater acoustic (UWA) channel equalization. Since the
underwater environment is highly non-stationary and subjected to impulsive
noise, we use adaptive filtering techniques based on a relative logarithmic
cost function inspired by the competitive methods from the online learning
literature. To improve the convergence performance of the conventional linear
equalization methods, while mitigating the stability issues, we intrinsically
combine different norms of the error in the cost function, using logarithmic
functions. Hence, we achieve a comparable convergence performance to least mean
fourth (LMF) equalizer, while significantly enhancing the stability performance
in such an adverse communication medium. We demonstrate the performance of our
algorithms through highly realistic experiments performed on accurately
simulated underwater acoustic channels.
</summary>
    <author>
      <name>Dariush Kari</name>
    </author>
    <author>
      <name>Muhammed Omer Sayin</name>
    </author>
    <author>
      <name>Suleyman Serdar Kozat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.06222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.06222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.07370v1</id>
    <updated>2015-12-23T07:09:13Z</updated>
    <published>2015-12-23T07:09:13Z</published>
    <title>Musical instrument sound classification with deep convolutional neural
  network using feature fusion approach</title>
    <summary>  A new musical instrument classification method using convolutional neural
networks (CNNs) is presented in this paper. Unlike the traditional methods, we
investigated a scheme for classifying musical instruments using the learned
features from CNNs. To create the learned features from CNNs, we not only used
a conventional spectrogram image, but also proposed multiresolution recurrence
plots (MRPs) that contain the phase information of a raw input signal.
Consequently, we fed the characteristic timbre of the particular instrument
into a neural network, which cannot be extracted using a phase-blinded
representations such as a spectrogram. By combining our proposed MRPs and
spectrogram images with a multi-column network, the performance of our proposed
classifier system improves over a system that uses only a spectrogram.
Furthermore, the proposed classifier also outperforms the baseline result from
traditional handcrafted features and classifiers.
</summary>
    <author>
      <name>Taejin Park</name>
    </author>
    <author>
      <name>Taejin Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 5 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.07370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.07370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.08075v1</id>
    <updated>2015-12-26T05:25:00Z</updated>
    <published>2015-12-26T05:25:00Z</published>
    <title>Multichannel audio signal source separation based on an Interchannel
  Loudness Vector Sum</title>
    <summary>  In this paper, a Blind Source Separation (BSS) algorithm for multichannel
audio contents is proposed. Unlike common BSS algorithms targeting stereo audio
contents or microphone array signals, our technique is targeted at multichannel
audio such as 5.1 and 7.1ch audio. Since most multichannel audio object sources
are panned using the Inter-channel Loudness Difference (ILD), we employ the
ILVS (Inter-channel Loudness Vector Sum) concept to cluster common signals
(such as background music) from each channel. After separating the common
signals from each channel, we employ an Expectation Maximization (EM) algorithm
with a von-Mises distribution to successfully classify the clustering of sound
source objects and separate the audio signals from the original mixture. Our
proposed method can therefore separate common audio signals and object source
signals from multiple channels with reasonable quality. Our multichannel audio
content separation technique can be applied to an upmix system or a cinema
audio system requiring multichannel audio source separation.
</summary>
    <author>
      <name>Taejin Park</name>
    </author>
    <author>
      <name>Taejin Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures and 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.08075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.08075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.00287v1</id>
    <updated>2016-01-03T12:30:38Z</updated>
    <published>2016-01-03T12:30:38Z</published>
    <title>Wavelet Scattering on the Pitch Spiral</title>
    <summary>  We present a new representation of harmonic sounds that linearizes the
dynamics of pitch and spectral envelope, while remaining stable to deformations
in the time-frequency plane. It is an instance of the scattering transform, a
generic operator which cascades wavelet convolutions and modulus
nonlinearities. It is derived from the pitch spiral, in that convolutions are
successively performed in time, log-frequency, and octave index. We give a
closed-form approximation of spiral scattering coefficients for a nonstationary
generalization of the harmonic source-filter model.
</summary>
    <author>
      <name>Vincent Lostanlen</name>
    </author>
    <author>
      <name>Stéphane Mallat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 18th International Conference on Digital Audio
  Effects (DAFx-15), Trondheim, Norway, Nov 30 - Dec 3, 2015, pp. 429--432. 4
  pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 18th International Conference on Digital Audio
  Effects (DAFx-15), Trondheim, Norway, Nov 30 - Dec 3, 2015, pp. 429--432</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.00287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.00287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65T60" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.00833v1</id>
    <updated>2016-01-05T14:13:28Z</updated>
    <published>2016-01-05T14:13:28Z</published>
    <title>An Analysis of Rhythmic Staccato-Vocalization Based on Frequency
  Demodulation for Laughter Detection in Conversational Meetings</title>
    <summary>  Human laugh is able to convey various kinds of meanings in human
communications. There exists various kinds of human laugh signal, for example:
vocalized laugh and non vocalized laugh. Following the theories of psychology,
among all the vocalized laugh type, rhythmic staccato-vocalization
significantly evokes the positive responses in the interactions. In this paper
we attempt to exploit this observation to detect human laugh occurrences, i.e.,
the laughter, in multiparty conversations from the AMI meeting corpus. First,
we separate the high energy frames from speech, leaving out the low energy
frames through power spectral density estimation. We borrow the algorithm of
rhythm detection from the area of music analysis to use that on the high energy
frames. Finally, we detect rhythmic laugh frames, analyzing the candidate
rhythmic frames using statistics. This novel approach for detection of
`positive' rhythmic human laughter performs better than the standard laughter
classification baseline.
</summary>
    <author>
      <name>Sucheta Ghosh</name>
    </author>
    <author>
      <name>Milos Cernak</name>
    </author>
    <author>
      <name>Sarbani Palit</name>
    </author>
    <author>
      <name>B. B. Chaudhuri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, conference paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.00833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.00833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01577v1</id>
    <updated>2016-01-07T15:57:45Z</updated>
    <published>2016-01-07T15:57:45Z</published>
    <title>Gender Identification using MFCC for Telephone Applications - A
  Comparative Study</title>
    <summary>  Gender recognition is an essential component of automatic speech recognition
and interactive voice response systems. Determining gender of the speaker
reduces the computational burden of such systems for any further processing.
Typical methods for gender recognition from speech largely depend on features
extraction and classification processes. The purpose of this study is to
evaluate the performance of various state-of-the-art classification methods
along with tuning their parameters for helping selection of the optimal
classification methods for gender recognition tasks. Five classification
schemes including k-nearest neighbor, na\"ive Bayes, multilayer perceptron,
random forest, and support vector machine are comprehensively evaluated for
determination of gender from telephonic speech using the Mel-frequency cepstral
coefficients. Different experiments were performed to determine the effects of
training data sizes, length of the speech streams, and parameter tuning on
classification performance. Results suggest that SVM is the best classifier
among all the five schemes for gender recognition.
</summary>
    <author>
      <name>Jamil Ahmad</name>
    </author>
    <author>
      <name>Mustansar Fiaz</name>
    </author>
    <author>
      <name>Soon-il Kwon</name>
    </author>
    <author>
      <name>Maleerat Sodanil</name>
    </author>
    <author>
      <name>Bay Vo</name>
    </author>
    <author>
      <name>Sung Wook Baik</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Electronics
  Engineering 3.5 (2015): 351-355</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.01577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.02069v1</id>
    <updated>2016-01-09T03:06:18Z</updated>
    <published>2016-01-09T03:06:18Z</published>
    <title>Dynamic Transposition of Melodic Sequences on Digital Devices</title>
    <summary>  A method is proposed which enables one to produce musical compositions by
using transposition in place of harmonic progression. A transposition scale is
introduced to provide a set of intervals commensurate with the musical scale,
such as chromatic or just intonation scales. A sequence of intervals selected
from the transposition scale is used to shift instrument frequency at
predefined times during the composition which serves as a harmonic sequence of
a composition. A transposition sequence constructed in such a way can be
extended to a hierarchy of sequences. The fundamental sound frequency of an
instrument is obtained as a product of the base frequency, instrument key
factor, and a cumulative product of respective factors from all the harmonic
sequences. The multiplication factors are selected from subsets of rational
numbers, which form instrument scales and transposition scales of different
levels. Each harmonic sequence can be related to its own transposition scale,
or a single scale can be used for all levels. When composing for an orchestra
of instruments, harmonic sequences and instrument scales can be assigned
independently to each musical instrument. The method solves the problem of
using just intonation scale across multiple octaves as well as simplifies
writing of instrument scores.
</summary>
    <author>
      <name>Andrei V Smirnov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures, 3 music samples</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.02069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.02069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="11N25, 70J40, 11K70, 11J70, 42A45" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.02339v2</id>
    <updated>2016-08-06T20:12:40Z</updated>
    <published>2016-01-11T06:45:58Z</published>
    <title>Repetitive Transients Extraction Algorithm for Detecting Bearing Faults</title>
    <summary>  This paper addresses the problem of noise reduction with simultaneous
components extrac- tion in vibration signals for faults diagnosis of bearing.
The observed vibration signal is modeled as a summation of two components
contaminated by noise, and each component composes of repetitive transients. To
extract the two components simultaneously, an approach by solving an
optimization problem is proposed in this paper. The problem adopts convex
sparsity-based regularization scheme for decomposition, and non-convex
regularization is used to further promote the sparsity but preserving the
global convexity. A synthetic example is presented to illustrate the
performance of the proposed approach for repetitive feature extraction. The
performance and effectiveness of the proposed method are further demonstrated
by applying to compound faults and single fault diagnosis of a locomotive
bearing. The results show the proposed approach can effectively extract the
features of outer and inner race defects.
</summary>
    <author>
      <name>Wangpeng He</name>
    </author>
    <author>
      <name>Yin Ding</name>
    </author>
    <author>
      <name>Yanyang Zi</name>
    </author>
    <author>
      <name>Ivan W. Selesnick</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ymssp.2016.06.035</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ymssp.2016.06.035" rel="related"/>
    <link href="http://arxiv.org/abs/1601.02339v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.02339v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.02489v1</id>
    <updated>2016-01-03T06:24:56Z</updated>
    <published>2016-01-03T06:24:56Z</published>
    <title>Categorization of Tablas by Wavelet Analysis</title>
    <summary>  Tabla, a percussion instrument, mainly used to accompany vocalists,
instrumentalists and dancers in every style of music from classical to light in
India, mainly used for keeping rhythm. This percussion instrument consists of
two drums played by two hands, structurally different and produces different
harmonic sounds. Earlier work has done labeling tabla strokes from real time
performances by testing neural networks and tree based classification methods.
The current work extends previous work by C. V. Raman and S. Kumar in 1920 on
spectrum modeling of tabla strokes. In this paper we have studied spectral
characteristics (by wavelet analysis by sub band coding method and using
torrence wavelet tool) of nine strokes from each of five tablas using Wavelet
transform. Wavelet analysis is now a common tool for analyzing localized
variations of power within a time series and to find the frequency distribution
in time frequency space. Statistically, we will look into the patterns depicted
by harmonics of different sub bands and the tablas. Distribution of dominant
frequencies at different sub-band of stroke signals, distribution of power and
behavior of harmonics are the important features, leads to categorization of
tabla.
</summary>
    <author>
      <name>Anirban Patranabis</name>
    </author>
    <author>
      <name>Kaushik Banerjee</name>
    </author>
    <author>
      <name>Vishal Midya</name>
    </author>
    <author>
      <name>Shankha Sanyal</name>
    </author>
    <author>
      <name>Archi Banerjee</name>
    </author>
    <author>
      <name>Ranjan Sengupta</name>
    </author>
    <author>
      <name>Dipak Ghosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.02489v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.02489v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.06008v1</id>
    <updated>2016-01-22T14:13:53Z</updated>
    <published>2016-01-22T14:13:53Z</published>
    <title>A Robust Frame-based Nonlinear Prediction System for Automatic Speech
  Coding</title>
    <summary>  In this paper, we propose a neural-based coding scheme in which an artificial
neural network is exploited to automatically compress and decompress speech
signals by a trainable approach. Having a two-stage training phase, the system
can be fully specified to each speech frame and have robust performance across
different speakers and wide range of spoken utterances. Indeed, Frame-based
nonlinear predictive coding (FNPC) would code a frame in the procedure of
training to predict the frame samples. The motivating objective is to analyze
the system behavior in regenerating not only the envelope of spectra, but also
the spectra phase. This scheme has been evaluated in time and discrete cosine
transform (DCT) domains and the output of predicted phonemes show the
potentiality of the FNPC to reconstruct complicated signals. The experiments
were conducted on three voiced plosive phonemes, b/d/g/ in time and DCT domains
versus the number of neurons in the hidden layer. Experiments approve the FNPC
capability as an automatic coding system by which /b/d/g/ phonemes have been
reproduced with a good accuracy. Evaluations revealed that the performance of
FNPC system, trained to predict DCT coefficients is more desirable,
particularly for frames with the wider distribution of energy, compared to time
samples.
</summary>
    <author>
      <name>Mahmood Yousefi-Azar</name>
    </author>
    <author>
      <name>Farbod Razzazi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.06008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.06008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.06652v1</id>
    <updated>2016-01-25T16:13:37Z</updated>
    <published>2016-01-25T16:13:37Z</published>
    <title>A Perceptually Motivated Filter Bank with Perfect Reconstruction for
  Audio Signal Processing</title>
    <summary>  Many audio applications rely on filter banks (FBs) to analyze, process, and
re-synthesize sounds. To approximate the auditory frequency resolution in the
signal chain, some applications rely on perceptually motivated FBs, the
gammatone FB being a popular example. However, most perceptually motivated FBs
only allow partial signal reconstruction at high redundancies and/or do not
have good resistance to sub-channel processing. This paper introduces an
oversampled perceptually motivated FB enabling perfect reconstruction,
efficient FB design, and adaptable redundancy. The filters are directly
constructed in the frequency domain and linearly distributed on a perceptual
frequency scale (e.g. ERB, Bark, or Mel scale). The proposed design allows for
various filter shapes, uniform or non-uniform FB setting, and large
down-sampling factors. For redundancies $\geq$ 3 perfect reconstruction is
achieved by computing the canonical dual FB analytically. For lower
redundancies perfect reconstruction is achieved using an iterative method.
Experiments show performance improvements of the proposed approach when
compared to the gammatone FB in terms of reconstruction error and resistance to
sub-channel processing, especially at low redundancies.
</summary>
    <author>
      <name>Thibaud Necciari</name>
    </author>
    <author>
      <name>Nicki Holighaus</name>
    </author>
    <author>
      <name>Peter Balazs</name>
    </author>
    <author>
      <name>Zdenek Prusa</name>
    </author>
    <link href="http://arxiv.org/abs/1601.06652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.06652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.00739v1</id>
    <updated>2016-02-01T23:00:45Z</updated>
    <published>2016-02-01T23:00:45Z</published>
    <title>Towards a topological fingerprint of music</title>
    <summary>  Can music be represented as a meaningful geometric and topological object? In
this paper, we propose a strategy to describe some music features as a
polyhedral surface obtained by a simplicial interpretation of the
\textit{Tonnetz}. The \textit{Tonnetz} is a graph largely used in computational
musicology to describe the harmonic relationships of notes in equal tuning. In
particular, we use persistent homology in order to describe the
\textit{persistent} properties of music encoded in the aforementioned model.
Both the relevance and the characteristics of this approach are discussed by
analyzing some paradigmatic compositional styles. Eventually, the task of
automatic music style classification is addressed by computing the hierarchical
clustering of the topological fingerprints associated with some collections of
compositions.
</summary>
    <author>
      <name>Mattia G. Bergomi</name>
    </author>
    <author>
      <name>Adriano Baraté</name>
    </author>
    <author>
      <name>Barbara Di Fabio</name>
    </author>
    <link href="http://arxiv.org/abs/1602.00739v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.00739v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.02656v1</id>
    <updated>2016-02-08T17:25:22Z</updated>
    <published>2016-02-08T17:25:22Z</published>
    <title>LSTM Deep Neural Networks Postfiltering for Improving the Quality of
  Synthetic Voices</title>
    <summary>  Recent developments in speech synthesis have produced systems capable of
outcome intelligible speech, but now researchers strive to create models that
more accurately mimic human voices. One such development is the incorporation
of multiple linguistic styles in various languages and accents.
  HMM-based Speech Synthesis is of great interest to many researchers, due to
its ability to produce sophisticated features with small footprint. Despite
such progress, its quality has not yet reached the level of the predominant
unit-selection approaches that choose and concatenate recordings of real
speech. Recent efforts have been made in the direction of improving these
systems.
  In this paper we present the application of Long-Short Term Memory Deep
Neural Networks as a Postfiltering step of HMM-based speech synthesis, in order
to obtain closer spectral characteristics to those of natural speech. The
results show how HMM-voices could be improved using this approach.
</summary>
    <author>
      <name>Marvin Coto-Jiménez</name>
    </author>
    <author>
      <name>John Goddard-Close</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.02656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.02656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.05526v1</id>
    <updated>2016-02-17T18:41:16Z</updated>
    <published>2016-02-17T18:41:16Z</published>
    <title>A High-Quality Speech and Audio Codec With Less Than 10 ms Delay</title>
    <summary>  With increasing quality requirements for multimedia communications, audio
codecs must maintain both high quality and low delay. Typically, audio codecs
offer either low delay or high quality, but rarely both. We propose a codec
that simultaneously addresses both these requirements, with a delay of only 8.7
ms at 44.1 kHz. It uses gain-shape algebraic vector quantisation in the
frequency domain with time-domain pitch prediction. We demonstrate that the
proposed codec operating at 48 kbit/s and 64 kbit/s out-performs both G.722.1C
and MP3 and has quality comparable to AAC-LD, despite having less than one
fourth of the algorithmic delay of these codecs.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Timothy B. Terriberry</name>
    </author>
    <author>
      <name>Christopher Montgomery</name>
    </author>
    <author>
      <name>Gregory Maxwell</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASL.2009.2023186</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASL.2009.2023186" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Audio, Speech and Language Processing, Vol.
  18, No. 1, pp. 58-67, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.05526v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05526v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.05682v2</id>
    <updated>2016-04-27T02:32:38Z</updated>
    <published>2016-02-18T05:49:37Z</published>
    <title>Audio Recording Device Identification Based on Deep Learning</title>
    <summary>  In this paper we present a research on identification of audio recording
devices from background noise, thus providing a method for forensics. The audio
signal is the sum of speech signal and noise signal. Usually, people pay more
attention to speech signal, because it carries the information to deliver. So a
great amount of researches have been dedicated to getting higher
Signal-Noise-Ratio (SNR). There are many speech enhancement algorithms to
improve the quality of the speech, which can be seen as reducing the noise.
However, noises can be regarded as the intrinsic fingerprint traces of an audio
recording device. These digital traces can be characterized and identified by
new machine learning techniques. Therefore, in our research, we use the noise
as the intrinsic features. As for the identification, multiple classifiers of
deep learning methods are used and compared. The identification result shows
that the method of getting feature vector from the noise of each device and
identifying them with deep learning techniques is viable, and well-preformed.
</summary>
    <author>
      <name>Simeng Qi</name>
    </author>
    <author>
      <name>Zheng Huang</name>
    </author>
    <author>
      <name>Yan Li</name>
    </author>
    <author>
      <name>Shaopei Shi</name>
    </author>
    <link href="http://arxiv.org/abs/1602.05682v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05682v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.05900v1</id>
    <updated>2016-02-17T18:15:36Z</updated>
    <published>2016-02-17T18:15:36Z</published>
    <title>An Iterative Linearised Solution to the Sinusoidal Parameter Estimation
  Problem</title>
    <summary>  Signal processing applications use sinusoidal modelling for speech synthesis,
speech coding, and audio coding. Estimation of the model parameters involves
non-linear optimisation methods, which can be very costly for real-time
applications. We propose a low-complexity iterative method that starts from
initial frequency estimates and converges rapidly. We show that for N sinusoids
in a frame of length L, the proposed method has a complexity of O(LN), which is
significantly less than the matching pursuits method. Furthermore, the proposed
method is shown to be more accurate than the matching pursuits and
time-frequency reassignment methods in our experiments.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Daniel V. Smith</name>
    </author>
    <author>
      <name>Christopher Montgomery</name>
    </author>
    <author>
      <name>Timothy B. Terriberry</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.compeleceng.2008.11.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.compeleceng.2008.11.005" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computers and Electrical Engineering (Elsevier), Vol. 36, No. 4,
  pp. 603-616, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.05900v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05900v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.06727v3</id>
    <updated>2016-04-05T11:31:02Z</updated>
    <published>2016-02-22T11:11:04Z</published>
    <title>Improving Trajectory Modelling for DNN-based Speech Synthesis by using
  Stacked Bottleneck Features and Minimum Generation Error Training</title>
    <summary>  We propose two novel techniques --- stacking bottleneck features and minimum
generation error training criterion --- to improve the performance of deep
neural network (DNN)-based speech synthesis. The techniques address the related
issues of frame-by-frame independence and ignorance of the relationship between
static and dynamic features, within current typical DNN-based synthesis
frameworks. Stacking bottleneck features, which are an acoustically--informed
linguistic representation, provides an efficient way to include more detailed
linguistic context at the input. The minimum generation error training
criterion minimises overall output trajectory error across an utterance, rather
than minimising the error per frame independently, and thus takes into account
the interaction between static and dynamic features. The two techniques can be
easily combined to further improve performance. We present both objective and
subjective results that demonstrate the effectiveness of the proposed
techniques. The subjective results show that combining the two techniques leads
to significantly more natural synthetic speech than from conventional DNN or
long short-term memory (LSTM) recurrent neural network (RNN) systems.
</summary>
    <author>
      <name>Zhizheng Wu</name>
    </author>
    <author>
      <name>Simon King</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2016.2551865</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2016.2551865" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to IEEE/ACM Transactions on Audio, Speech and Language
  Processing 2016 (AQ)</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.06727v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.06727v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08045v1</id>
    <updated>2016-02-25T19:18:06Z</updated>
    <published>2016-02-25T19:18:06Z</published>
    <title>PCA/LDA Approach for Text-Independent Speaker Recognition</title>
    <summary>  Various algorithms for text-independent speaker recognition have been
developed through the decades, aiming to improve both accuracy and efficiency.
This paper presents a novel PCA/LDA-based approach that is faster than
traditional statistical model-based methods and achieves competitive results.
First, the performance based on only PCA and only LDA is measured; then a mixed
model, taking advantages of both methods, is introduced. A subset of the TIMIT
corpus composed of 200 male speakers, is used for enrollment, validation and
testing. The best results achieve 100%; 96% and 95% classification rate at
population level 50; 100 and 200, using 39-dimensional MFCC features with delta
and double delta. These results are based on 12-second text-independent speech
for training and 4-second data for test. These are comparable to the
conventional MFCC-GMM methods, but require significantly less time to train and
operate.
</summary>
    <author>
      <name>Zhenhao Ge</name>
    </author>
    <author>
      <name>Sudhendu R. Sharma</name>
    </author>
    <author>
      <name>Mark J. T. Smith</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.919235</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.919235" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Society of Photo-Optical Instrumentation Engineers (SPIE) Conference
  Series</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.08045v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08045v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08128v1</id>
    <updated>2016-02-25T21:48:56Z</updated>
    <published>2016-02-25T21:48:56Z</published>
    <title>PCA Method for Automated Detection of Mispronounced Words</title>
    <summary>  This paper presents a method for detecting mispronunciations with the aim of
improving Computer Assisted Language Learning (CALL) tools used by foreign
language learners. The algorithm is based on Principle Component Analysis
(PCA). It is hierarchical with each successive step refining the estimate to
classify the test word as being either mispronounced or correct. Preprocessing
before detection, like normalization and time-scale modification, is
implemented to guarantee uniformity of the feature vectors input to the
detection system. The performance using various features including spectrograms
and Mel-Frequency Cepstral Coefficients (MFCCs) are compared and evaluated.
Best results were obtained using MFCCs, achieving up to 99% accuracy in word
verification and 93% in native/non-native classification. Compared with Hidden
Markov Models (HMMs) which are used pervasively in recognition application,
this particular approach is computational efficient and effective when training
data is limited.
</summary>
    <author>
      <name>Zhenhao Ge</name>
    </author>
    <author>
      <name>Sudhendu R. Sharma</name>
    </author>
    <author>
      <name>Mark J. T. Smith</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.884155</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.884155" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SPIE Defense, Security, and Sensing</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.08128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08132v1</id>
    <updated>2016-02-25T22:17:31Z</updated>
    <published>2016-02-25T22:17:31Z</published>
    <title>Adaptive Frequency Cepstral Coefficients for Word Mispronunciation
  Detection</title>
    <summary>  Systems based on automatic speech recognition (ASR) technology can provide
important functionality in computer assisted language learning applications.
This is a young but growing area of research motivated by the large number of
students studying foreign languages. Here we propose a Hidden Markov Model
(HMM)-based method to detect mispronunciations. Exploiting the specific dialog
scripting employed in language learning software, HMMs are trained for
different pronunciations. New adaptive features have been developed and
obtained through an adaptive warping of the frequency scale prior to computing
the cepstral coefficients. The optimization criterion used for the warping
function is to maximize separation of two major groups of pronunciations
(native and non-native) in terms of classification rate. Experimental results
show that the adaptive frequency scale yields a better coefficient
representation leading to higher classification rates in comparison with
conventional HMMs using Mel-frequency cepstral coefficients.
</summary>
    <author>
      <name>Zhenhao Ge</name>
    </author>
    <author>
      <name>Sudhendu R. Sharma</name>
    </author>
    <author>
      <name>Mark J. T. Smith</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CISP.2011.6100685</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CISP.2011.6100685" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4th International Congress on Image and Signal Processing (CISP) 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.08132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08507v1</id>
    <updated>2016-02-24T04:52:01Z</updated>
    <published>2016-02-24T04:52:01Z</published>
    <title>Occupancy Estimation in Smart Buildings using Audio-Processing
  Techniques</title>
    <summary>  In the past few years, several case studies have illustrated that the use of
occupancy information in buildings leads to energy-efficient and low-cost HVAC
operation. The widely presented techniques for occupancy estimation include
temperature, humidity, CO2 concentration, image camera, motion sensor and
passive infrared (PIR) sensor. So far little studies have been reported in
literature to utilize audio and speech processing as indoor occupancy
prediction technique. With rapid advances of audio and speech processing
technologies, nowadays it is more feasible and attractive to integrate
audio-based signal processing component into smart buildings. In this work, we
propose to utilize audio processing techniques (i.e., speaker recognition and
background audio energy estimation) to estimate room occupancy (i.e., the
number of people inside a room). Theoretical analysis and simulation results
demonstrate the accuracy and effectiveness of this proposed occupancy
estimation technique. Based on the occupancy estimation, smart buildings will
adjust the thermostat setups and HVAC operations, thus, achieving greater
quality of service and drastic cost savings.
</summary>
    <author>
      <name>Qian Huang</name>
    </author>
    <author>
      <name>Zhenhao Ge</name>
    </author>
    <author>
      <name>Chao Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Computing in Civil and Building
  Engineering (ICCCBE) 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.08507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.08609v1</id>
    <updated>2016-02-27T16:17:06Z</updated>
    <published>2016-02-27T16:17:06Z</published>
    <title>A New Robust Frequency Domain Echo Canceller With Closed-Loop Learning
  Rate Adaptation</title>
    <summary>  One of the main difficulties in echo cancellation is the fact that the
learning rate needs to vary according to conditions such as double-talk and
echo path change. Several methods have been proposed to vary the learning. In
this paper we propose a new closed-loop method where the learning rate is
proportional to a misalignment parameter, which is in turn estimated based on a
gradient adaptive approach. The method is presented in the context of a
multidelay block frequency domain (MDF) echo canceller. We demonstrate that the
proposed algorithm outperforms current popular double-talk detection techniques
by up to 6 dB.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Iain B. Collings</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2007.366624</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2007.366624" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Proc. International Conference on Acoustics, Speech, and
  Signal Processing (ICASSP), 2007</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.08609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.08609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.01770v1</id>
    <updated>2016-03-06T00:06:09Z</updated>
    <published>2016-03-06T00:06:09Z</published>
    <title>An Argument-based Creative Assistant for Harmonic Blending</title>
    <summary>  Conceptual blending is a powerful tool for computational creativity where,
for example, the properties of two harmonic spaces may be combined in a
consistent manner to produce a novel harmonic space. However, deciding about
the importance of property features in the input spaces and evaluating the
results of conceptual blending is a nontrivial task. In the specific case of
musical harmony, defining the salient features of chord transitions and
evaluating invented harmonic spaces requires deep musicological background
knowledge. In this paper, we propose a creative tool that helps musicologists
to evaluate and to enhance harmonic innovation. This tool allows a music expert
to specify arguments over given transition properties. These arguments are then
considered by the system when defining combinations of features in an
idiom-blending process. A music expert can assess whether the new harmonic
idiom makes musicological sense and re-adjust the arguments (selection of
features) to explore alternative blends that can potentially produce better
harmonic spaces. We conclude with a discussion of future work that would
further automate the harmonisation process.
</summary>
    <author>
      <name>Maximos Kaliakatsos-Papakostas</name>
    </author>
    <author>
      <name>Roberto Confalonieri</name>
    </author>
    <author>
      <name>Joseph Corneli</name>
    </author>
    <author>
      <name>Asterios Zacharakis</name>
    </author>
    <author>
      <name>Emilios Cambouropoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pp; submitted to 7th International Conference on Computational
  Creativity</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.01770v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.01770v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.01824v1</id>
    <updated>2016-03-06T13:17:27Z</updated>
    <published>2016-03-06T13:17:27Z</published>
    <title>Low-Complexity Iterative Sinusoidal Parameter Estimation</title>
    <summary>  Sinusoidal parameter estimation is a computationally-intensive task, which
can pose problems for real-time implementations. In this paper, we propose a
low-complexity iterative method for estimating sinusoidal parameters that is
based on the linearisation of the model around an initial frequency estimate.
We show that for N sinusoids in a frame of length L, the proposed method has a
complexity of O(LN), which is significantly less than the matching pursuits
method. Furthermore, the proposed method is shown to be more accurate than the
matching pursuits and time frequency reassignment methods in our experiments.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Daniel V. Smith</name>
    </author>
    <author>
      <name>Christopher Montgomery</name>
    </author>
    <author>
      <name>Timothy B. Terriberry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. arXiv admin note: substantial text overlap with
  arXiv:1602.05900</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of International Conference on Signal Processing and
  Communication Systems (ICSPCS), pp. 276-283, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.01824v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.01824v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.01863v1</id>
    <updated>2016-03-06T19:19:22Z</updated>
    <published>2016-03-06T19:19:22Z</published>
    <title>Improved Noise Weighting in CELP Coding of Speech - Applying the Vorbis
  Psychoacoustic Model To Speex</title>
    <summary>  One key aspect of the CELP algorithm is that it shapes the coding noise using
a simple, yet effective, weighting filter. In this paper, we improve the noise
shaping of CELP using a more modern psychoacoustic model. This has the
significant advantage of improving the quality of an existing codec without the
need to change the bit-stream. More specifically, we improve the Speex CELP
codec by using the psychoacoustic model used in the Vorbis audio codec. The
results show a significant increase in quality, especially at high bit-rates,
where the improvement is equivalent to a 20% reduction in bit-rate. The
technique itself is not specific to Speex and could be applied to other CELP
codecs.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Christopher Montgomery</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, Proceedings of the 120th Audio Engineering Society (AES)
  Convention, Paris, 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.01863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.01863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03215v1</id>
    <updated>2016-03-10T10:42:47Z</updated>
    <published>2016-03-10T10:42:47Z</published>
    <title>Microphone array post-filter for separation of simultaneous
  non-stationary sources</title>
    <summary>  Microphone array post-filters have demonstrated their ability to greatly
reduce noise at the output of a beamformer. However, current techniques only
consider a single source of interest, most of the time assuming stationary
background noise. We propose a microphone array post-filter that enhances the
signals produced by the separation of simultaneous sources using common source
separation algorithms. Our method is based on a loudness-domain optimal
spectral estimator and on the assumption that the noise can be described as the
sum of a stationary component and of a transient component that is due to
leakage between the channels of the initial source separation algorithm. The
system is evaluated in the context of mobile robotics and is shown to produce
better results than current post-filtering techniques, greatly reducing
interference while causing little distortion to the signal of interest, even at
very low SNR.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Jean Rouat</name>
    </author>
    <author>
      <name>François Michaud</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2004.1325962</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2004.1325962" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages. arXiv admin note: substantial text overlap with
  arXiv:1603.02341</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of IEEE International Conference on Acoustics, Speech
  and Signal Processing (ICASSP), pp. 221-224, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.03215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04179v3</id>
    <updated>2017-05-22T10:43:16Z</updated>
    <published>2016-03-14T09:42:28Z</published>
    <title>Performance Analysis of Source Image Estimators in Blind Source
  Separation</title>
    <summary>  Blind methods often separate or identify signals or signal subspaces up to an
unknown scaling factor. Sometimes it is necessary to cope with the scaling
ambiguity, which can be done through reconstructing signals as they are
received by sensors, because scales of the sensor responses (images) have known
physical interpretations. In this paper, we analyze two approaches that are
widely used for computing the sensor responses, especially, in Frequency-Domain
Independent Component Analysis. One approach is the least-squares projection,
while the other one assumes a regular mixing matrix and computes its inverse.
Both estimators are invariant to the unknown scaling. Although frequently used,
their differences were not studied yet. A goal of this work is to fill this
gap. The estimators are compared through a theoretical study, perturbation
analysis and simulations. We point to the fact that the estimators are
equivalent when the separated signal subspaces are orthogonal, and vice versa.
Two applications are shown, one of which demonstrates a case where the
estimators yield substantially different results.
</summary>
    <author>
      <name>Zbyněk Koldovský</name>
    </author>
    <author>
      <name>Francesco Nesta</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2017.2709269</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2017.2709269" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.04179v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04179v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="15-04" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.04264v1</id>
    <updated>2016-03-14T13:49:18Z</updated>
    <published>2016-03-14T13:49:18Z</published>
    <title>Novel Speech Features for Improved Detection of Spoofing Attacks</title>
    <summary>  Now-a-days, speech-based biometric systems such as automatic speaker
verification (ASV) are highly prone to spoofing attacks by an imposture. With
recent development in various voice conversion (VC) and speech synthesis (SS)
algorithms, these spoofing attacks can pose a serious potential threat to the
current state-of-the-art ASV systems. To impede such attacks and enhance the
security of the ASV systems, the development of efficient anti-spoofing
algorithms is essential that can differentiate synthetic or converted speech
from natural or human speech. In this paper, we propose a set of novel speech
features for detecting spoofing attacks. The proposed features are computed
using alternative frequency-warping technique and formant-specific block
transformation of filter bank log energies. We have evaluated existing and
proposed features against several kinds of synthetic speech data from ASVspoof
2015 corpora. The results show that the proposed techniques outperform existing
approaches for various spoofing attack detection task. The techniques
investigated in this paper can also accurately classify natural and synthetic
speech as equal error rates (EERs) of 0% have been achieved.
</summary>
    <author>
      <name>Dipjyoti Paul</name>
    </author>
    <author>
      <name>Monisankha Pal</name>
    </author>
    <author>
      <name>Goutam Saha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/INDICON.2015.7443805</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/INDICON.2015.7443805" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in IEEE 2015 Annual IEEE India Conference (INDICON)</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.04264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.04264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.05435v1</id>
    <updated>2016-03-17T11:35:09Z</updated>
    <published>2016-03-17T11:35:09Z</published>
    <title>Modified Group Delay Based MultiPitch Estimation in Co-Channel Speech</title>
    <summary>  Phase processing has been replaced by group delay processing for the
extraction of source and system parameters from speech. Group delay functions
are ill-behaved when the transfer function has zeros that are close to unit
circle in the z-domain. The modified group delay function addresses this
problem and has been successfully used for formant and monopitch estimation. In
this paper, modified group delay functions are used for multipitch estimation
in concurrent speech. The power spectrum of the speech is first flattened in
order to annihilate the system characteristics, while retaining the source
characteristics. Group delay analysis on this flattened spectrum picks the
predominant pitch in the first pass and a comb filter is used to filter out the
estimated pitch along with its harmonics. The residual spectrum is again
analyzed for the next candidate pitch estimate in the second pass. The final
pitch trajectories of the constituent speech utterances are formed using pitch
grouping and post processing techniques. The performance of the proposed
algorithm was evaluated on standard datasets using two metrics; pitch accuracy
and standard deviation of fine pitch error. Our results show that the proposed
algorithm is a promising pitch detection method in multipitch environment for
real speech recordings.
</summary>
    <author>
      <name>Rajeev Rajan</name>
    </author>
    <author>
      <name>Hema A. Murthy</name>
    </author>
    <link href="http://arxiv.org/abs/1603.05435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.05435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06065v1</id>
    <updated>2016-03-19T08:45:21Z</updated>
    <published>2016-03-19T08:45:21Z</published>
    <title>A pairwise approach to simultaneous onset/offset detection for singing
  voice using correntropy</title>
    <summary>  In this paper, we propose a novelmethod to search for precise locations of
paired note onset and offset in a singing voice signal. In comparison with the
existing onset detection algorithms,our approach differs in two key respects.
First, we employ Correntropy, a generalized correlation function inspired from
Reyni's entropy, as a detection function to capture the instantaneous flux
while preserving insensitiveness to outliers. Next, a novel peak picking
algorithm is specially designed for this detection function. By calculating the
fitness of a pre-defined inverse hyperbolic kernel to a detection function, it
is possible to find an onset and its corresponding offset simultaneously.
Experimental results show that the proposed method achieves performance
significantly better than or comparable to other state-of-the-art techniques
for onset detection in singing voice.
</summary>
    <author>
      <name>Sungkyun Chang</name>
    </author>
    <author>
      <name>Kyogu Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2014.6853672</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2014.6853672" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2014 IEEE International Conference on Acoustics, Speech and Signal
  Processing, 5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07236v2</id>
    <updated>2016-04-26T16:32:24Z</updated>
    <published>2016-03-23T15:29:39Z</published>
    <title>Individual identity in songbirds: signal representations and metric
  learning for locating the information in complex corvid calls</title>
    <summary>  Bird calls range from simple tones to rich dynamic multi-harmonic structures.
The more complex calls are very poorly understood at present, such as those of
the scientifically important corvid family (jackdaws, crows, ravens, etc.).
Individual birds can recognise familiar individuals from calls, but where in
the signal is this identity encoded? We studied the question by applying a
combination of feature representations to a dataset of jackdaw calls, including
linear predictive coding (LPC) and adaptive discrete Fourier transform (aDFT).
We demonstrate through a classification paradigm that we can strongly
outperform a standard spectrogram representation for identifying individuals,
and we apply metric learning to determine which time-frequency regions
contribute most strongly to robust individual identification. Computational
methods can help to direct our search for understanding of these complex
biological signals.
</summary>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Veronica Morfi</name>
    </author>
    <author>
      <name>Lisa F. Gill</name>
    </author>
    <link href="http://arxiv.org/abs/1603.07236v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07236v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00192v1</id>
    <updated>2016-04-01T10:28:51Z</updated>
    <published>2016-04-01T10:28:51Z</published>
    <title>Singing Voice Separation and Vocal F0 Estimation based on Mutual
  Combination of Robust Principal Component Analysis and Subharmonic Summation</title>
    <summary>  This paper presents a new method of singing voice analysis that performs
mutually-dependent singing voice separation and vocal fundamental frequency
(F0) estimation. Vocal F0 estimation is considered to become easier if singing
voices can be separated from a music audio signal, and vocal F0 contours are
useful for singing voice separation. This calls for an approach that improves
the performance of each of these tasks by using the results of the other. The
proposed method first performs robust principal component analysis (RPCA) for
roughly extracting singing voices from a target music audio signal. The F0
contour of the main melody is then estimated from the separated singing voices
by finding the optimal temporal path over an F0 saliency spectrogram. Finally,
the singing voices are separated again more accurately by combining a
conventional time-frequency mask given by RPCA with another mask that passes
only the harmonic structures of the estimated F0s. Experimental results showed
that the proposed method significantly improved the performances of both
singing voice separation and vocal F0 estimation. The proposed method also
outperformed all the other methods of singing voice separation submitted to an
international music analysis competition called MIREX 2014.
</summary>
    <author>
      <name>Yukara Ikemiya</name>
    </author>
    <author>
      <name>Katsutoshi Itoyama</name>
    </author>
    <author>
      <name>Kazuyoshi Yoshii</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2016.2577879</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2016.2577879" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00861v1</id>
    <updated>2016-04-04T13:54:09Z</updated>
    <published>2016-04-04T13:54:09Z</published>
    <title>Recurrent Neural Networks for Polyphonic Sound Event Detection in Real
  Life Recordings</title>
    <summary>  In this paper we present an approach to polyphonic sound event detection in
real life recordings based on bi-directional long short term memory (BLSTM)
recurrent neural networks (RNNs). A single multilabel BLSTM RNN is trained to
map acoustic features of a mixture signal consisting of sounds from multiple
classes, to binary activity indicators of each event class. Our method is
tested on a large database of real-life recordings, with 61 classes (e.g.
music, car, speech) from 10 different everyday contexts. The proposed method
outperforms previous approaches by a large margin, and the results are further
improved using data augmentation techniques. Overall, our system reports an
average F1-score of 65.5% on 1 second blocks and 64.7% on single frames, a
relative improvement over previous state-of-the-art approach of 6.8% and 15.1%
respectively.
</summary>
    <author>
      <name>Giambattista Parascandolo</name>
    </author>
    <author>
      <name>Heikki Huttunen</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2016.7472917</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2016.7472917" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appean in Proceedings of IEEE ICASSP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00861v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00861v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03276v1</id>
    <updated>2016-04-12T07:52:27Z</updated>
    <published>2016-04-12T07:52:27Z</published>
    <title>Noise Robust Speech Recognition Using Multi-Channel Based Channel
  Selection And ChannelWeighting</title>
    <summary>  In this paper, we study several microphone channel selection and weighting
methods for robust automatic speech recognition (ASR) in noisy conditions. For
channel selection, we investigate two methods based on the maximum likelihood
(ML) criterion and minimum autoencoder reconstruction criterion, respectively.
For channel weighting, we produce enhanced log Mel filterbank coefficients as a
weighted sum of the coefficients of all channels. The weights of the channels
are estimated by using the ML criterion with constraints. We evaluate the
proposed methods on the CHiME-3 noisy ASR task. Experiments show that channel
weighting significantly outperforms channel selection due to its higher
flexibility. Furthermore, on real test data in which different channels have
different gains of the target signal, the channel weighting method performs
equally well or better than the MVDR beamforming, despite the fact that the
channel weighting does not make use of the phase delay information which is
normally used in beamforming.
</summary>
    <author>
      <name>Zhaofeng Zhang</name>
    </author>
    <author>
      <name>Xiong Xiao</name>
    </author>
    <author>
      <name>Longbiao Wang</name>
    </author>
    <author>
      <name>EngSiong Chng</name>
    </author>
    <author>
      <name>Haizhou Li</name>
    </author>
    <link href="http://arxiv.org/abs/1604.03276v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03276v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07160v2</id>
    <updated>2016-12-08T04:28:16Z</updated>
    <published>2016-04-25T08:25:03Z</published>
    <title>Deep Convolutional Neural Networks and Data Augmentation for Acoustic
  Event Detection</title>
    <summary>  We propose a novel method for Acoustic Event Detection (AED). In contrast to
speech, sounds coming from acoustic events may be produced by a wide variety of
sources. Furthermore, distinguishing them often requires analyzing an extended
time period due to the lack of a clear sub-word unit. In order to incorporate
the long-time frequency structure for AED, we introduce a convolutional neural
network (CNN) with a large input field. In contrast to previous works, this
enables to train audio event detection end-to-end. Our architecture is inspired
by the success of VGGNet and uses small, 3x3 convolutions, but more depth than
previous methods in AED. In order to prevent over-fitting and to take full
advantage of the modeling capabilities of our network, we further propose a
novel data augmentation method to introduce data variation. Experimental
results show that our CNN significantly outperforms state of the art methods
including Bag of Audio Words (BoAW) and classical CNNs, achieving a 16%
absolute improvement.
</summary>
    <author>
      <name>Naoya Takahashi</name>
    </author>
    <author>
      <name>Michael Gygli</name>
    </author>
    <author>
      <name>Beat Pfister</name>
    </author>
    <author>
      <name>Luc Van Gool</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in INTERSPEECH 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07160v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07160v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08716v1</id>
    <updated>2016-04-29T07:46:59Z</updated>
    <published>2016-04-29T07:46:59Z</published>
    <title>Learning Compact Structural Representations for Audio Events Using
  Regressor Banks</title>
    <summary>  We introduce a new learned descriptor for audio signals which is efficient
for event representation. The entries of the descriptor are produced by
evaluating a set of regressors on the input signal. The regressors are
class-specific and trained using the random regression forests framework. Given
an input signal, each regressor estimates the onset and offset positions of the
target event. The estimation confidence scores output by a regressor are then
used to quantify how the target event aligns with the temporal structure of the
corresponding category. Our proposed descriptor has two advantages. First, it
is compact, i.e. the dimensionality of the descriptor is equal to the number of
event classes. Second, we show that even simple linear classification models,
trained on our descriptor, yield better accuracies on audio event
classification task than not only the nonlinear baselines but also the
state-of-the-art results.
</summary>
    <author>
      <name>Huy Phan</name>
    </author>
    <author>
      <name>Marco Maass</name>
    </author>
    <author>
      <name>Lars Hertel</name>
    </author>
    <author>
      <name>Radoslaw Mazur</name>
    </author>
    <author>
      <name>Ian McLoughlin</name>
    </author>
    <author>
      <name>Alfred Mertins</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2016.7471667</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2016.7471667" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceedings of IEEE ICASSP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08723v1</id>
    <updated>2016-04-29T08:03:00Z</updated>
    <published>2016-04-29T08:03:00Z</published>
    <title>Music transcription modelling and composition using deep learning</title>
    <summary>  We apply deep learning methods, specifically long short-term memory (LSTM)
networks, to music transcription modelling and composition. We build and train
LSTM networks using approximately 23,000 music transcriptions expressed with a
high-level vocabulary (ABC notation), and use them to generate new
transcriptions. Our practical aim is to create music transcription models
useful in particular contexts of music composition. We present results from
three perspectives: 1) at the population level, comparing descriptive
statistics of the set of training transcriptions and generated transcriptions;
2) at the individual level, examining how a generated transcription reflects
the conventions of a music practice in the training transcriptions (Celtic
folk); 3) at the application level, using the system for idea generation in
music composition. We make our datasets, software and sound examples open and
available: \url{https://github.com/IraKorshunova/folk-rnn}.
</summary>
    <author>
      <name>Bob L. Sturm</name>
    </author>
    <author>
      <name>João Felipe Santos</name>
    </author>
    <author>
      <name>Oded Ben-Tal</name>
    </author>
    <author>
      <name>Iryna Korshunova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 4 figures, contribution to 1st Conference on Computer
  Simulation of Musical Creativity</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.08723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01329v1</id>
    <updated>2016-05-04T16:16:12Z</updated>
    <published>2016-05-04T16:16:12Z</published>
    <title>Single Channel Speech Enhancement Using Outlier Detection</title>
    <summary>  Distortion of the underlying speech is a common problem for single-channel
speech enhancement algorithms, and hinders such methods from being used more
extensively. A dictionary based speech enhancement method that emphasizes
preserving the underlying speech is proposed. Spectral patches of clean speech
are sampled and clustered to train a dictionary. Given a noisy speech spectral
patch, the best matching dictionary entry is selected and used to estimate the
noise power at each time-frequency bin. The noise estimation step is formulated
as an outlier detection problem, where the noise at each bin is assumed present
only if it is an outlier to the corresponding bin of the best matching
dictionary entry. This framework assigns higher priority in removing spectral
elements that strongly deviate from a typical spoken unit stored in the trained
dictionary. Even without the aid of a separate noise model, this method can
achieve significant noise reduction for various non-stationary noises, while
effectively preserving the underlying speech in more challenging noisy
environments.
</summary>
    <author>
      <name>Eunjoon Cho</name>
    </author>
    <author>
      <name>Bowon Lee</name>
    </author>
    <author>
      <name>Ronald Schafer</name>
    </author>
    <author>
      <name>Bernard Widrow</name>
    </author>
    <link href="http://arxiv.org/abs/1605.01329v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01329v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02401v3</id>
    <updated>2016-07-06T05:46:56Z</updated>
    <published>2016-05-09T02:17:12Z</published>
    <title>Audio Event Detection using Weakly Labeled Data</title>
    <summary>  Acoustic event detection is essential for content analysis and description of
multimedia recordings. The majority of current literature on the topic learns
the detectors through fully-supervised techniques employing strongly labeled
data. However, the labels available for majority of multimedia data are
generally weak and do not provide sufficient detail for such methods to be
employed. In this paper we propose a framework for learning acoustic event
detectors using only weakly labeled data. We first show that audio event
detection using weak labels can be formulated as an Multiple Instance Learning
problem. We then suggest two frameworks for solving multiple-instance learning,
one based on support vector machines, and the other on neural networks. The
proposed methods can help in removing the time consuming and expensive process
of manually annotating data to facilitate fully supervised learning. Moreover,
it can not only detect events in a recording but can also provide temporal
locations of events in the recording. This helps in obtaining a complete
description of the recording and is notable since temporal information was
never known in the first place in weakly labeled data.
</summary>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2964284.2964310</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2964284.2964310" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Multimedia 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02401v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02401v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06644v3</id>
    <updated>2017-01-10T14:29:13Z</updated>
    <published>2016-05-21T13:49:33Z</published>
    <title>Deep convolutional networks on the pitch spiral for musical instrument
  recognition</title>
    <summary>  Musical performance combines a wide range of pitches, nuances, and expressive
techniques. Audio-based classification of musical instruments thus requires to
build signal representations that are invariant to such transformations. This
article investigates the construction of learned convolutional architectures
for instrument recognition, given a limited amount of annotated training data.
In this context, we benchmark three different weight sharing strategies for
deep convolutional networks in the time-frequency domain: temporal kernels;
time-frequency kernels; and a linear combination of time-frequency kernels
which are one octave apart, akin to a Shepard pitch spiral. We provide an
acoustical interpretation of these strategies within the source-filter
framework of quasi-harmonic sounds with a fixed spectral envelope, which are
archetypal of musical notes. The best classification accuracy is obtained by
hybridizing all three convolutional layers into a single deep learning
architecture.
</summary>
    <author>
      <name>Vincent Lostanlen</name>
    </author>
    <author>
      <name>Carmine-Emanuele Cella</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures. Accepted at the International Society for Music
  Information Retrieval Conference (ISMIR) conference in New York City, NY,
  USA, August 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06644v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06644v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07008v1</id>
    <updated>2016-05-23T13:29:09Z</updated>
    <published>2016-05-23T13:29:09Z</published>
    <title>madmom: a new Python Audio and Music Signal Processing Library</title>
    <summary>  In this paper, we present madmom, an open-source audio processing and music
information retrieval (MIR) library written in Python. madmom features a
concise, NumPy-compatible, object oriented design with simple calling
conventions and sensible default values for all parameters, which facilitates
fast prototyping of MIR applications. Prototypes can be seamlessly converted
into callable processing pipelines through madmom's concept of Processors,
callable objects that run transparently on multiple cores. Processors can also
be serialised, saved, and re-run to allow results to be easily reproduced
anywhere. Apart from low-level audio processing, madmom puts emphasis on
musically meaningful high-level features. Many of these incorporate machine
learning techniques and madmom provides a module that implements some in MIR
commonly used methods such as hidden Markov models and neural networks.
Additionally, madmom comes with several state-of-the-art MIR algorithms for
onset detection, beat, downbeat and meter tracking, tempo estimation, and piano
transcription. These can easily be incorporated into bigger MIR systems or run
as stand-alone programs.
</summary>
    <author>
      <name>Sebastian Böck</name>
    </author>
    <author>
      <name>Filip Korzeniowski</name>
    </author>
    <author>
      <name>Jan Schlüter</name>
    </author>
    <author>
      <name>Florian Krebs</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <link href="http://arxiv.org/abs/1605.07008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07466v1</id>
    <updated>2016-05-24T14:04:48Z</updated>
    <published>2016-05-24T14:04:48Z</published>
    <title>Complex NMF under phase constraints based on signal modeling:
  application to audio source separation</title>
    <summary>  Nonnegative Matrix Factorization (NMF) is a powerful tool for decomposing
mixtures of audio signals in the Time-Frequency (TF) domain. In the source
separation framework, the phase recovery for each extracted component is
necessary for synthesizing time-domain signals. The Complex NMF (CNMF) model
aims to jointly estimate the spectrogram and the phase of the sources, but
requires to constrain the phase in order to produce satisfactory sounding
results. We propose to incorporate phase constraints based on signal models
within the CNMF framework: a \textit{phase unwrapping} constraint that enforces
a form of temporal coherence, and a constraint based on the \textit{repetition}
of audio events, which models the phases of the sources within onset frames. We
also provide an algorithm for estimating the model parameters. The experimental
results highlight the interest of including such constraints in the CNMF
framework for separating overlapping components in complex audio mixtures.
</summary>
    <author>
      <name>Paul Magron</name>
    </author>
    <author>
      <name>Roland Badeau</name>
    </author>
    <author>
      <name>Bertrand David</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2016.7471634</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2016.7471634" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP) 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07466v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07466v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07467v1</id>
    <updated>2016-05-24T14:05:14Z</updated>
    <published>2016-05-24T14:05:14Z</published>
    <title>Phase reconstruction of spectrograms with linear unwrapping: application
  to audio signal restoration</title>
    <summary>  This paper introduces a novel technique for reconstructing the phase of
modified spectrograms of audio signals. From the analysis of mixtures of
sinusoids we obtain relationships between phases of successive time frames in
the Time-Frequency (TF) domain. To obtain similar relationships over
frequencies, in particular within onset frames, we study an impulse model.
Instantaneous frequencies and attack times are estimated locally to encompass
the class of non-stationary signals such as vibratos. These techniques ensure
both the vertical coherence of partials (over frequencies) and the horizontal
coherence (over time). The method is tested on a variety of data and
demonstrates better performance than traditional consistency-based approaches.
We also introduce an audio restoration framework and observe that our technique
outperforms traditional methods.
</summary>
    <author>
      <name>Paul Magron</name>
    </author>
    <author>
      <name>Roland Badeau</name>
    </author>
    <author>
      <name>Bertrand David</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">European Signal Processing Conference (EUSIPCO) 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07468v1</id>
    <updated>2016-05-24T14:05:35Z</updated>
    <published>2016-05-24T14:05:35Z</published>
    <title>Phase reconstruction of spectrograms based on a model of repeated audio
  events</title>
    <summary>  Phase recovery of modified spectrograms is a major issue in audio signal
processing applications, such as source separation. This paper introduces a
novel technique for estimating the phases of components in complex mixtures
within onset frames in the Time-Frequency (TF) domain. We propose to exploit
the phase repetitions from one onset frame to another. We introduce a reference
phase which characterizes a component independently of its activation times.
The onset phases of a component are then modeled as the sum of this reference
and an offset which is linearly dependent on the frequency. We derive a complex
mixture model within onset frames and we provide two algorithms for the
estimation of the model phase parameters. The model is estimated on
experimental data and this technique is integrated into an audio source
separation framework. The results demonstrate that this model is a promising
tool for exploiting phase repetitions, and point out its potential for
separating overlapping components in complex mixtures.
</summary>
    <author>
      <name>Paul Magron</name>
    </author>
    <author>
      <name>Roland Badeau</name>
    </author>
    <author>
      <name>Bertrand David</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/WASPAA.2015.7336935</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/WASPAA.2015.7336935" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Workshop on Applications of Signal Processing to Audio and
  Acoustics (WASPAA) 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07469v1</id>
    <updated>2016-05-24T14:05:51Z</updated>
    <published>2016-05-24T14:05:51Z</published>
    <title>Phase recovery in NMF for audio source separation: an insightful
  benchmark</title>
    <summary>  Nonnegative Matrix Factorization (NMF) is a powerful tool for decomposing
mixtures of audio signals in the Time-Frequency (TF) domain. In applications
such as source separation, the phase recovery for each extracted component is a
major issue since it often leads to audible artifacts. In this paper, we
present a methodology for evaluating various NMF-based source separation
techniques involving phase reconstruction. For each model considered, a
comparison between two approaches (blind separation without prior information
and oracle separation with supervised model learning) is performed, in order to
inquire about the room for improvement for the estimation methods. Experimental
results show that the High Resolution NMF (HRNMF) model is particularly
promising, because it is able to take phases and correlations over time into
account with a great expressive power.
</summary>
    <author>
      <name>Paul Magron</name>
    </author>
    <author>
      <name>Roland Badeau</name>
    </author>
    <author>
      <name>Bertrand David</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2015.7177936</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2015.7177936" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP) 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07469v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07469v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07809v2</id>
    <updated>2016-07-22T20:56:20Z</updated>
    <published>2016-05-25T10:20:07Z</published>
    <title>Using instantaneous frequency and aperiodicity detection to estimate F0
  for high-quality speech synthesis</title>
    <summary>  This paper introduces a general and flexible framework for F0 and
aperiodicity (additive non periodic component) analysis, specifically intended
for high-quality speech synthesis and modification applications. The proposed
framework consists of three subsystems: instantaneous frequency estimator and
initial aperiodicity detector, F0 trajectory tracker, and F0 refinement and
aperiodicity extractor. A preliminary implementation of the proposed framework
substantially outperformed (by a factor of 10 in terms of RMS F0 estimation
error) existing F0 extractors in tracking ability of temporally varying F0
trajectories. The front end aperiodicity detector consists of a complex-valued
wavelet analysis filter with a highly selective temporal and spectral envelope.
This front end aperiodicity detector uses a new measure that quantifies the
deviation from periodicity. The measure is less sensitive to slow FM and AM and
closely correlates with the signal to noise ratio.
</summary>
    <author>
      <name>Hideki Kawahara</name>
    </author>
    <author>
      <name>Yannis Agiomyrgiannakis</name>
    </author>
    <author>
      <name>Heiga Zen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.21437/SSW.2016-36</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.21437/SSW.2016-36" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for presentation in ISCA workshop SSW9</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">9th ISCA Speech Synthesis Workshop, 2016, pp.221-228</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1605.07809v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07809v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08396v1</id>
    <updated>2016-05-26T18:27:56Z</updated>
    <published>2016-05-26T18:27:56Z</published>
    <title>Robust Downbeat Tracking Using an Ensemble of Convolutional Networks</title>
    <summary>  In this paper, we present a novel state of the art system for automatic
downbeat tracking from music signals. The audio signal is first segmented in
frames which are synchronized at the tatum level of the music. We then extract
different kind of features based on harmony, melody, rhythm and bass content to
feed convolutional neural networks that are adapted to take advantage of each
feature characteristics. This ensemble of neural networks is combined to obtain
one downbeat likelihood per tatum. The downbeat sequence is finally decoded
with a flexible and efficient temporal model which takes advantage of the
metrical continuity of a song. We then perform an evaluation of our system on a
large base of 9 datasets, compare its performance to 4 other published
algorithms and obtain a significant increase of 16.8 percent points compared to
the second best system, for altogether a moderate cost in test and training.
The influence of each step of the method is studied to show its strengths and
shortcomings.
</summary>
    <author>
      <name>S. Durand</name>
    </author>
    <author>
      <name>J. P. Bello</name>
    </author>
    <author>
      <name>B. David</name>
    </author>
    <author>
      <name>G. Richard</name>
    </author>
    <link href="http://arxiv.org/abs/1605.08396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00298v1</id>
    <updated>2016-06-01T14:18:08Z</updated>
    <published>2016-06-01T14:18:08Z</published>
    <title>Automatic tagging using deep convolutional neural networks</title>
    <summary>  We present a content-based automatic music tagging algorithm using fully
convolutional neural networks (FCNs). We evaluate different architectures
consisting of 2D convolutional layers and subsampling layers only. In the
experiments, we measure the AUC-ROC scores of the architectures with different
complexities and input types using the MagnaTagATune dataset, where a 4-layer
architecture shows state-of-the-art performance with mel-spectrogram input.
Furthermore, we evaluated the performances of the architectures with varying
the number of layers on a larger dataset (Million Song Dataset), and found that
deeper models outperformed the 4-layer architecture. The experiments show that
mel-spectrogram is an effective time-frequency representation for automatic
tagging and that more complex models benefit from more training data.
</summary>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>George Fazekas</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ISMIR (International Society of Music Information
  Retrieval) Conference 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.00298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.01368v1</id>
    <updated>2016-06-04T10:51:24Z</updated>
    <published>2016-06-04T10:51:24Z</published>
    <title>Modelling Symbolic Music: Beyond the Piano Roll</title>
    <summary>  In this paper, we consider the problem of probabilistically modelling
symbolic music data. We introduce a representation which reduces polyphonic
music to a univariate categorical sequence. In this way, we are able to apply
state of the art natural language processing techniques, namely the long
short-term memory sequence model. The representation we employ permits
arbitrary rhythmic structure, which we assume to be given. We show that our
model is effective on four out of four piano roll based benchmark datasets. We
further improve our model by augmenting our training data set with
transpositions of the original pieces through all musical keys, thereby
convincingly advancing the state of the art on these benchmark problems. We
also fit models to music which is unconstrained in its rhythmic structure,
discuss the properties of this model, and provide musical samples which are
more sophisticated than previously possible with this class of recurrent neural
network sequence models. We also provide our newly preprocessed data set of non
piano-roll music data.
</summary>
    <author>
      <name>Christian Walder</name>
    </author>
    <link href="http://arxiv.org/abs/1606.01368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.01368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05844v1</id>
    <updated>2016-06-19T08:38:26Z</updated>
    <published>2016-06-19T08:38:26Z</published>
    <title>Statistical Parametric Speech Synthesis Using Bottleneck Representation
  From Sequence Auto-encoder</title>
    <summary>  In this paper, we describe a statistical parametric speech synthesis approach
with unit-level acoustic representation. In conventional deep neural network
based speech synthesis, the input text features are repeated for the entire
duration of phoneme for mapping text and speech parameters. This mapping is
learnt at the frame-level which is the de-facto acoustic representation.
However much of this computational requirement can be drastically reduced if
every unit can be represented with a fixed-dimensional representation. Using
recurrent neural network based auto-encoder, we show that it is indeed possible
to map units of varying duration to a single vector. We then use this acoustic
representation at unit-level to synthesize speech using deep neural network
based statistical parametric speech synthesis technique. Results show that the
proposed approach is able to synthesize at the same quality as the conventional
frame based approach at a highly reduced computational cost.
</summary>
    <author>
      <name>Sivanand Achanta</name>
    </author>
    <author>
      <name>KNRK Raju Alluri</name>
    </author>
    <author>
      <name>Suryakanth V Gangashetty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages (with references)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05844v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05844v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06061v2</id>
    <updated>2016-06-22T15:11:30Z</updated>
    <published>2016-06-20T10:54:51Z</published>
    <title>Fast, Compact, and High Quality LSTM-RNN Based Statistical Parametric
  Speech Synthesizers for Mobile Devices</title>
    <summary>  Acoustic models based on long short-term memory recurrent neural networks
(LSTM-RNNs) were applied to statistical parametric speech synthesis (SPSS) and
showed significant improvements in naturalness and latency over those based on
hidden Markov models (HMMs). This paper describes further optimizations of
LSTM-RNN-based SPSS for deployment on mobile devices; weight quantization,
multi-frame inference, and robust inference using an {\epsilon}-contaminated
Gaussian loss function. Experimental results in subjective listening tests show
that these optimizations can make LSTM-RNN-based SPSS comparable to HMM-based
SPSS in runtime speed while maintaining naturalness. Evaluations between
LSTM-RNN- based SPSS and HMM-driven unit selection speech synthesis are also
presented.
</summary>
    <author>
      <name>Heiga Zen</name>
    </author>
    <author>
      <name>Yannis Agiomyrgiannakis</name>
    </author>
    <author>
      <name>Niels Egberts</name>
    </author>
    <author>
      <name>Fergus Henderson</name>
    </author>
    <author>
      <name>Przemysław Szczepaniak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures, Interspeech 2016 (accepted)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06061v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06061v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06197v2</id>
    <updated>2016-06-21T11:43:11Z</updated>
    <published>2016-06-20T16:27:12Z</published>
    <title>Polymetric Rhythmic Feel for a Cognitive Drum Computer</title>
    <summary>  This paper addresses a question about music cognition: how do we derive
polymetric structures. A preference rule system is presented which is
implemented into a drum computer. The preference rule system allows inferring
local polymetric structures, like two-over-three and three-over-two. By
analyzing the micro-timing of West African percussion music a timing pattern
consisting of six pulses was discovered. It integrates binary and ternary
rhythmic feels. The presented drum computer integrates the discovered
superimposed polymetric swing (timing and velocity) appropriate to the rhythmic
sequence the user inputs. For binary sequences, the amount of binary swing is
increased and for ternary sequences, the ternary swing is increased.
</summary>
    <author>
      <name>Oliver Weede</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 14th Int Conf on Culture and Computer Science, Berlin,
  Germany, May 26-27, 2016, pp. 281-295</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.06197v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06197v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06258v1</id>
    <updated>2016-06-20T19:21:46Z</updated>
    <published>2016-06-20T19:21:46Z</published>
    <title>Uncalibrated 3D Room Reconstruction from Sound</title>
    <summary>  This paper presents a method to reconstruct the 3D structure of generic
convex rooms from sound signals. Differently from most of the previous
approaches, the method is fully uncalibrated in the sense that no knowledge
about the microphones and sources position is needed. Moreover, we demonstrate
that it is possible to bypass the well known echo labeling problem, allowing to
reconstruct the room shape in a reasonable computation time without the need of
additional hypotheses on the echoes order of arrival. Finally, the method is
intrinsically robust to outliers and missing data in the echoes detection,
allowing to work also in low SNR conditions. The proposed pipeline formalises
the problem in different steps such as time of arrival estimation, microphones
and sources localization and walls estimation. After providing a solution to
these different problems we present a global optimization approach that links
together all the problems in a single optimization function. The accuracy and
robustness of the method is assessed on a wide set of simulated setups and in a
challenging real scenario. Moreover we make freely available for a challenging
dataset for 3D room reconstruction with accurate ground truth in a real
scenario.
</summary>
    <author>
      <name>Marco Crocco</name>
    </author>
    <author>
      <name>Andrea Trucco</name>
    </author>
    <author>
      <name>Alessio Del Bue</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The present work has been submitted to IEEE/ACM Transactions on Audio
  Speech and Language Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.06258v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06258v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07598v1</id>
    <updated>2016-06-24T08:28:41Z</updated>
    <published>2016-06-24T08:28:41Z</published>
    <title>An Active Machine Hearing System for Auditory Stream Segregation</title>
    <summary>  This study describes a binaural machine hearing system that is capable of
performing auditory stream segregation in scenarios where multiple sound
sources are present. The process of stream segregation refers to the capability
of human listeners to group acoustic signals into sets of distinct auditory
streams, corresponding to individual sound sources. The proposed computational
framework mimics this ability via a probabilistic clustering scheme for joint
localization and segregation. This scheme is based on mixtures of von Mises
distributions to model the angular positions of the sound sources surrounding
the listener. The distribution parameters are estimated using block-wise
processing of auditory cues extracted from binaural signals. Additionally, the
proposed system can conduct rotational head movements to improve localization
and stream segregation performance. Evaluation of the system is conducted in
scenarios containing multiple simultaneously active speech and non-speech
sounds placed at different positions relative to the listener.
</summary>
    <author>
      <name>Christopher Schymura</name>
    </author>
    <author>
      <name>Thomas Walther</name>
    </author>
    <author>
      <name>Dorothea Kolossa</name>
    </author>
    <link href="http://arxiv.org/abs/1606.07598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00211v1</id>
    <updated>2016-07-01T11:49:40Z</updated>
    <published>2016-07-01T11:49:40Z</published>
    <title>Spherical Harmonic Signal Covariance and Sound Field Diffuseness</title>
    <summary>  Characterizing sound field diffuseness has many practical applications, from
room acoustics analysis to speech enhancement and sound field reproduction. In
this paper we investigate how spherical microphone arrays (SMAs) can be used to
characterize diffuseness. Due to their specific geometry, SMAs are particularly
well suited for analyzing the spatial properties of sound fields. In
particular, the signals recorded by an SMA can be analyzed in the spherical
harmonic (SH) domain, which has special and desirable mathematical properties
when it comes to analyzing diffuse sound fields. We present a new measure of
diffuseness, the COMEDIE diffuseness estimate, which is based on the analysis
of the SH signal covariance matrix. This algorithm is suited for the estimation
of diffuseness arising either from the presence of multiple sources distributed
around the SMA or from the presence of a diffuse noise background. As well, we
introduce the concept of a diffuseness profile, which consists in measuring the
diffuseness for several SH orders simultaneously. Experimental results indicate
that diffuseness profiles better describe the properties of the sound field
than a single diffuseness measurement.
</summary>
    <author>
      <name>Nicolas Epain</name>
    </author>
    <author>
      <name>Craig T. Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE/ACM Transactions on Audio, Speech, and Language
  Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.00211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02306v2</id>
    <updated>2016-08-15T18:02:09Z</updated>
    <published>2016-07-08T10:42:43Z</published>
    <title>CaR-FOREST: Joint Classification-Regression Decision Forests for
  Overlapping Audio Event Detection</title>
    <summary>  This report describes our submissions to Task2 and Task3 of the DCASE 2016
challenge. The systems aim at dealing with the detection of overlapping audio
events in continuous streams, where the detectors are based on random decision
forests. The proposed forests are jointly trained for classification and
regression simultaneously. Initially, the training is classification-oriented
to encourage the trees to select discriminative features from overlapping
mixtures to separate positive audio segments from the negative ones. The
regression phase is then carried out to let the positive audio segments vote
for the event onsets and offsets, and therefore model the temporal structure of
audio events. One random decision forest is specifically trained for each event
category of interest. Experimental results on the development data show that
our systems significantly outperform the baseline on the Task2 evaluation while
they are inferior to the baseline in the Task3 evaluation.
</summary>
    <author>
      <name>Huy Phan</name>
    </author>
    <author>
      <name>Lars Hertel</name>
    </author>
    <author>
      <name>Marco Maass</name>
    </author>
    <author>
      <name>Philipp Koch</name>
    </author>
    <author>
      <name>Alfred Mertins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Task2 and Task3 technical report for the DCASE2016 challenge</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.02306v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02306v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03682v3</id>
    <updated>2016-08-13T10:37:53Z</updated>
    <published>2016-07-13T11:31:25Z</published>
    <title>Hierarchical learning for DNN-based acoustic scene classification</title>
    <summary>  In this paper, we present a deep neural network (DNN)-based acoustic scene
classification framework. Two hierarchical learning methods are proposed to
improve the DNN baseline performance by incorporating the hierarchical taxonomy
information of environmental sounds. Firstly, the parameters of the DNN are
initialized by the proposed hierarchical pre-training. Multi-level objective
function is then adopted to add more constraint on the cross-entropy based loss
function. A series of experiments were conducted on the Task1 of the Detection
and Classification of Acoustic Scenes and Events (DCASE) 2016 challenge. The
final DNN-based system achieved a 22.9% relative improvement on average scene
classification error as compared with the Gaussian Mixture Model (GMM)-based
benchmark system across four standard folds.
</summary>
    <author>
      <name>Yong Xu</name>
    </author>
    <author>
      <name>Qiang Huang</name>
    </author>
    <author>
      <name>Wenwu Wang</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, DCASE 2016 challenge workshop paper, poster</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.03682v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03682v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04589v1</id>
    <updated>2016-07-15T17:29:26Z</updated>
    <published>2016-07-15T17:29:26Z</published>
    <title>Automatic Environmental Sound Recognition: Performance versus
  Computational Cost</title>
    <summary>  In the context of the Internet of Things (IoT), sound sensing applications
are required to run on embedded platforms where notions of product pricing and
form factor impose hard constraints on the available computing power. Whereas
Automatic Environmental Sound Recognition (AESR) algorithms are most often
developed with limited consideration for computational cost, this article seeks
which AESR algorithm can make the most of a limited amount of computing power
by comparing the sound classification performance em as a function of its
computational cost. Results suggest that Deep Neural Networks yield the best
ratio of sound classification accuracy across a range of computational costs,
while Gaussian Mixture Models offer a reasonable accuracy at a consistently
small cost, and Support Vector Machines stand between both in terms of
compromise between accuracy and computational cost.
</summary>
    <author>
      <name>Siddharth Sigtia</name>
    </author>
    <author>
      <name>Adam M. Stark</name>
    </author>
    <author>
      <name>Sacha Krstulovic</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2016.2592698</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2016.2592698" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE/ACM Transactions on Audio, Speech and Language Processing
  24(11): 2096-2107, Nov 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.04589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06667v4</id>
    <updated>2018-02-23T10:15:47Z</updated>
    <published>2016-07-22T13:12:33Z</published>
    <title>Inpainting of long audio segments with similarity graphs</title>
    <summary>  We present a novel method for the compensation of long duration data loss in
audio signals, in particular music. The concealment of such signal defects is
based on a graph that encodes signal structure in terms of time-persistent
spectral similarity. A suitable candidate segment for the substitution of the
lost content is proposed by an intuitive optimization scheme and smoothly
inserted into the gap, i.e. the lost or distorted signal region. Extensive
listening tests show that the proposed algorithm provides highly promising
results when applied to a variety of real-world music signals.
</summary>
    <author>
      <name>Nathanael Perraudin</name>
    </author>
    <author>
      <name>Nicki Holighaus</name>
    </author>
    <author>
      <name>Piotr Majdak</name>
    </author>
    <author>
      <name>Peter Balazs</name>
    </author>
    <link href="http://arxiv.org/abs/1607.06667v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06667v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01953v4</id>
    <updated>2018-02-27T09:09:03Z</updated>
    <published>2016-08-05T17:45:05Z</published>
    <title>Model-based STFT phase recovery for audio source separation</title>
    <summary>  For audio source separation applications, it is common to estimate the
magnitude of the short-time Fourier transform (STFT) of each source. In order
to further synthesizing time-domain signals, it is necessary to recover the
phase of the corresponding complex-valued STFT. Most authors in this field
choose a Wiener-like filtering approach which boils down to using the phase of
the original mixture. In this paper, a different standpoint is adopted. Many
music events are partially composed of slowly varying sinusoids and the STFT
phase increment over time of those frequency components takes a specific form.
This allows phase recovery by an unwrapping technique once a short-term
frequency estimate has been obtained. Herein, a novel iterative source
separation procedure is proposed which builds upon these results. It consists
in minimizing the mixing error by means of the auxiliary function method. This
procedure is initialized by exploiting the unwrapping technique in order to
generate estimates that benefit from a temporal continuity property.
Experiments conducted on realistic music pieces show that, given accurate
magnitude estimates, this procedure outperforms the state-of-the-art consistent
Wiener filter.
</summary>
    <author>
      <name>Paul Magron</name>
    </author>
    <author>
      <name>Roland Badeau</name>
    </author>
    <author>
      <name>Bertrand David</name>
    </author>
    <link href="http://arxiv.org/abs/1608.01953v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01953v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02272v1</id>
    <updated>2016-08-07T21:43:08Z</updated>
    <published>2016-08-07T21:43:08Z</published>
    <title>Incorporation of Speech Duration Information in Score Fusion of Speaker
  Recognition Systems</title>
    <summary>  In recent years identity-vector (i-vector) based speaker verification (SV)
systems have become very successful. Nevertheless, environmental noise and
speech duration variability still have a significant effect on degrading the
performance of these systems. In many real-life applications, duration of
recordings are very short; as a result, extracted i-vectors cannot reliably
represent the attributes of the speaker. Here, we investigate the effect of
speech duration on the performance of three state-of-the-art speaker
recognition systems. In addition, using a variety of available score fusion
methods, we investigate the effect of score fusion for those speaker
verification techniques to benefit from the performance difference of different
methods under different enrollment and test speech duration conditions. This
technique performed significantly better than the baseline score fusion
methods.
</summary>
    <author>
      <name>Ali Khodabakhsh</name>
    </author>
    <author>
      <name>Seyyed Saeed Sarfjoo</name>
    </author>
    <author>
      <name>Umut Uludag</name>
    </author>
    <author>
      <name>Osman Soyyigit</name>
    </author>
    <author>
      <name>Cenk Demiroglu</name>
    </author>
    <link href="http://arxiv.org/abs/1608.02272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03720v1</id>
    <updated>2016-08-12T08:52:23Z</updated>
    <published>2016-08-12T08:52:23Z</published>
    <title>Speech Signal Analysis for the Estimation of Heart Rates Under Different
  Emotional States</title>
    <summary>  A non-invasive method for the monitoring of heart activity can help to reduce
the deaths caused by heart disorders such as stroke, arrhythmia and heart
attack. The human voice can be considered as a biometric data that can be used
for estimation of heart rate. In this paper, we propose a method for estimating
the heart rate from human speech dynamically using voice signal analysis and by
the development of an empirical linear predictor model. The correlation between
the voice signal and heart rate are established by classifiers and prediction
of the heart rates with or without emotions are done using linear models. The
prediction accuracy was tested using the data collected from 15 subjects, it is
about 4050 samples of speech signals and corresponding electrocardiogram
samples. The proposed approach can use for early non-invasive detection of
heart rate changes that can be correlated to an emotional state of the
individual and also can be used as a tool for diagnosis of heart conditions in
real-time situations.
</summary>
    <author>
      <name>Aibek Ryskaliyev</name>
    </author>
    <author>
      <name>Sanzhar Askaruly</name>
    </author>
    <author>
      <name>Alex Pappachen James</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in Proceedings of International Conference on Advances in
  Computing, Communications and Informatics, IEEE, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04069v1</id>
    <updated>2016-08-14T07:39:51Z</updated>
    <published>2016-08-14T07:39:51Z</published>
    <title>Design of Variable Bandpass Filters Using First Order Allpass
  Transformation And Coefficient Decimation</title>
    <summary>  In this paper, the design of a computationally efficient variable bandpass
digital filter is presented. The center frequency and bandwidth of this filter
can be changed online without updating the filter coefficients. The warped
filters, obtained by replacing each unit delay of a digital filter with an
allpass filter, are widely used for various audio processing applications.
However, warped filters fail to provide variable bandwidth bandpass responses
for a given center frequency using first order allpass transformation. To
overcome this drawback, our design is accomplished by combining warped filter
with the coefficient decimation technique. The design example shows that the
proposed variable digital filter is simple to design and offers a total gate
count reduction of 36% and 65% over the warped filters compared to the designs
presented in [3] and [1] respectively
</summary>
    <author>
      <name>S. J. Darak</name>
    </author>
    <author>
      <name>A. P. Vinod</name>
    </author>
    <author>
      <name>E. M-K. Lai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18th Electronics New Zealand Conference (ENZCON)</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05179v2</id>
    <updated>2016-08-29T14:31:34Z</updated>
    <published>2016-08-18T05:16:38Z</published>
    <title>Improving the Efficiency of DAMAS for Sound Source Localization via
  Wavelet Compression Computational Grid</title>
    <summary>  Phased microphone arrays are used widely in the applications for acoustic
source localization. Deconvolution approaches such as DAMAS successfully
overcome the spatial resolution limit of the conventional delay-and-sum (DAS)
beamforming method. However deconvolution approaches require high computational
effort compared to conventional DAS beamforming method. This paper presents a
novel method that serves to improve the efficiency of DAMAS via wavelet
compression computational grid rather than via optimizing DAMAS algorithm. In
this method, the efficiency of DAMAS increases with compression ratio. This
method can thus save lots of run time in industrial applications for sound
source localization, particularly when sound sources are just located in a
small extent compared with scanning plane and a band of angular frequency needs
to be calculated. In addition, this method largely retains the spatial
resolution of DAMAS on original computational grid, although with a minor
deficiency that the occurrence probability of aliasing increasing slightly for
complicated sound source.
</summary>
    <author>
      <name>Wei Ma</name>
    </author>
    <author>
      <name>Xun Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.jsv.2017.02.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.jsv.2017.02.005" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 6 figures, 2 tables, 23 conferences</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.05179v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05179v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00291v1</id>
    <updated>2016-09-01T15:52:58Z</updated>
    <published>2016-09-01T15:52:58Z</published>
    <title>A Non-iterative Method for (Re)Construction of Phase from STFT Magnitude</title>
    <summary>  A non-iterative method for the construction of the Short-Time Fourier
Transform (STFT) phase from the magnitude is presented. The method is based on
the direct relationship between the partial derivatives of the phase and the
logarithm of the magnitude of the un-sampled STFT with respect to the Gaussian
window. Although the theory holds in the continuous setting only, the
experiments show that the algorithm performs well even in the discretized
setting (Discrete Gabor transform) with low redundancy using the sampled
Gaussian window, the truncated Gaussian window and even other compactly
supported windows like the Hann window.
  Due to the non-iterative nature, the algorithm is very fast and it is
suitable for long audio signals. Moreover, solutions of iterative phase
reconstruction algorithms can be improved considerably by initializing them
with the phase estimate provided by the present algorithm.
  We present an extensive comparison with the state-of-the-art algorithms in a
reproducible manner.
</summary>
    <author>
      <name>Zdeněk Průša</name>
    </author>
    <author>
      <name>Peter Balazs</name>
    </author>
    <author>
      <name>Peter L. Søndergaard</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2017.2678166</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2017.2678166" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Audio, Speech and Language Processing, Vol.
  25 (5), pp. 1154 - 1164 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.00291v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00291v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01678v2</id>
    <updated>2016-12-20T12:13:56Z</updated>
    <published>2016-09-06T18:03:33Z</published>
    <title>Discriminative Enhancement for Single Channel Audio Source Separation
  using Deep Neural Networks</title>
    <summary>  The sources separated by most single channel audio source separation
techniques are usually distorted and each separated source contains residual
signals from the other sources. To tackle this problem, we propose to enhance
the separated sources to decrease the distortion and interference between the
separated sources using deep neural networks (DNNs). Two different DNNs are
used in this work. The first DNN is used to separate the sources from the mixed
signal. The second DNN is used to enhance the separated signals. To consider
the interactions between the separated sources, we propose to use a single DNN
to enhance all the separated sources together. To reduce the residual signals
of one source from the other separated sources (interference), we train the DNN
for enhancement discriminatively to maximize the dissimilarity between the
predicted sources. The experimental results show that using discriminative
enhancement decreases the distortion and interference between the separated
sources.
</summary>
    <author>
      <name>Emad M. Grais</name>
    </author>
    <author>
      <name>Gerard Roma</name>
    </author>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13th International Conference on Latent Variable Analysis and Signal
  Separation (LVA/ICA 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.01678v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01678v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03213v1</id>
    <updated>2016-09-11T20:36:16Z</updated>
    <published>2016-09-11T20:36:16Z</published>
    <title>Relaxed Binaural LCMV Beamforming</title>
    <summary>  In this paper we propose a new binaural beamforming technique which can be
seen as a relaxation of the linearly constrained minimum variance (LCMV)
framework. The proposed method can achieve simultaneous noise reduction and
exact binaural cue preservation of the target source, similar to the binaural
minimum variance distortionless response (BMVDR) method. However, unlike BMVDR,
the proposed method is also able to preserve the binaural cues of multiple
interferers to a certain predefined accuracy. Specifically, it is able to
control the trade-off between noise reduction and binaural cue preservation of
the interferers by using a separate trade-off parameter per interferer.
Moreover, we provide a robust way of selecting these trade-off parameters in
such a way that the preservation accuracy for the binaural cues of the
interferers is always better than the corresponding ones of the BMVDR. The
relaxation of the constraints in the proposed method achieves approximate
binaural cue preservation of more interferers than other previously presented
LCMV-based binaural beamforming methods that use strict equality constraints.
</summary>
    <author>
      <name>Andreas I. Koutrouvelis</name>
    </author>
    <author>
      <name>Richard C. Hendriks</name>
    </author>
    <author>
      <name>Richard Heusdens</name>
    </author>
    <author>
      <name>Jesper Jensen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2016.2628642</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2016.2628642" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE/ACM Transactions on Audio, Speech, and Language Processing,
  25(1), 137-152, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.03213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.03499v2</id>
    <updated>2016-09-19T18:04:35Z</updated>
    <published>2016-09-12T17:29:40Z</published>
    <title>WaveNet: A Generative Model for Raw Audio</title>
    <summary>  This paper introduces WaveNet, a deep neural network for generating raw audio
waveforms. The model is fully probabilistic and autoregressive, with the
predictive distribution for each audio sample conditioned on all previous ones;
nonetheless we show that it can be efficiently trained on data with tens of
thousands of samples per second of audio. When applied to text-to-speech, it
yields state-of-the-art performance, with human listeners rating it as
significantly more natural sounding than the best parametric and concatenative
systems for both English and Mandarin. A single WaveNet can capture the
characteristics of many different speakers with equal fidelity, and can switch
between them by conditioning on the speaker identity. When trained to model
music, we find that it generates novel and often highly realistic musical
fragments. We also show that it can be employed as a discriminative model,
returning promising results for phoneme recognition.
</summary>
    <author>
      <name>Aaron van den Oord</name>
    </author>
    <author>
      <name>Sander Dieleman</name>
    </author>
    <author>
      <name>Heiga Zen</name>
    </author>
    <author>
      <name>Karen Simonyan</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Nal Kalchbrenner</name>
    </author>
    <author>
      <name>Andrew Senior</name>
    </author>
    <author>
      <name>Koray Kavukcuoglu</name>
    </author>
    <link href="http://arxiv.org/abs/1609.03499v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.03499v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.05104v2</id>
    <updated>2016-12-10T10:20:01Z</updated>
    <published>2016-09-16T15:20:32Z</published>
    <title>Intrinsic normalization and extrinsic denormalization of formant data of
  vowels</title>
    <summary>  Using a known speaker-intrinsic normalization procedure, formant data are
scaled by the reciprocal of the geometric mean of the first three formant
frequencies. This reduces the influence of the talker but results in a
distorted vowel space. The proposed speaker-extrinsic procedure re-scales the
normalized values by the mean formant values of vowels. When tested on the
formant data of vowels published by Peterson and Barney, the combined approach
leads to well separated clusters by reducing the spread due to talkers. The
proposed procedure performs better than two top-ranked normalization procedures
based on the accuracy of vowel classification as the objective measure.
</summary>
    <author>
      <name>T. V. Ananthapadmanabha</name>
    </author>
    <author>
      <name>A. G. Ramakrishnan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 8 figures. Title has been revised. Appendix has been added
  to include more figures and to clarify 'hypothesize-test' procedure, JASA-EL,
  2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.05104v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.05104v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.06026v3</id>
    <updated>2017-06-27T17:09:59Z</updated>
    <published>2016-09-20T05:52:06Z</published>
    <title>An Approach for Self-Training Audio Event Detectors Using Web Data</title>
    <summary>  Audio Event Detection (AED) aims to recognize sounds within audio and video
recordings. AED employs machine learning algorithms commonly trained and tested
on annotated datasets. However, available datasets are limited in number of
samples and hence it is difficult to model acoustic diversity. Therefore, we
propose combining labeled audio from a dataset and unlabeled audio from the web
to improve the sound models. The audio event detectors are trained on the
labeled audio and ran on the unlabeled audio downloaded from YouTube. Whenever
the detectors recognized any of the known sounds with high confidence, the
unlabeled audio was use to re-train the detectors. The performance of the
re-trained detectors is compared to the one from the original detectors using
the annotated test set. Results showed an improvement of the AED, and uncovered
challenges of using web audio from videos.
</summary>
    <author>
      <name>Benjamin Elizalde</name>
    </author>
    <author>
      <name>Ankit Shah</name>
    </author>
    <author>
      <name>Siddharth Dalmia</name>
    </author>
    <author>
      <name>Min Hun Lee</name>
    </author>
    <author>
      <name>Rohan Badlani</name>
    </author>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <author>
      <name>Ian Lane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.06026v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.06026v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.06210v2</id>
    <updated>2017-02-08T13:00:52Z</updated>
    <published>2016-09-20T15:09:36Z</published>
    <title>Interference Reduction in Music Recordings Combining Kernel Additive
  Modelling and Non-Negative Matrix Factorization</title>
    <summary>  In live and studio recordings unexpected sound events often lead to
interferences in the signal. For non-stationary interferences, sound source
separation techniques can be used to reduce the interference level in the
recording. In this context, we present a novel approach combining the strengths
of two algorithmic families: NMF and KAM. The recent KAM approach applies
robust statistics on frames selected by a source-specific kernel to perform
source separation. Based on semi-supervised NMF, we extend this approach in two
ways. First, we locate the interference in the recording based on detected NMF
activity. Second, we improve the kernel-based frame selection by incorporating
an NMF-based estimate of the clean music signal. Further, we introduce a
temporal context in the kernel, taking some musical structure into account. Our
experiments show improved separation quality for our proposed method over a
state-of-the-art approach for interference reduction.
</summary>
    <author>
      <name>Delia Fano Yela</name>
    </author>
    <author>
      <name>Sebastian Ewert</name>
    </author>
    <author>
      <name>Derry FitzGerald</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Acoustics, Speech and Signal Processing
  (ICASSP)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the IEEE International Conference on Acoustics,
  Speech, and Signal Processing (ICASSP), New Orleans, USA, pp. 51-55, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.06210v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.06210v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.06404v1</id>
    <updated>2016-09-21T02:14:23Z</updated>
    <published>2016-09-21T02:14:23Z</published>
    <title>KU-ISPL Language Recognition System for NIST 2015 i-Vector Machine
  Learning Challenge</title>
    <summary>  In language recognition, the task of rejecting/differentiating closely spaced
versus acoustically far spaced languages remains a major challenge. For
confusable closely spaced languages, the system needs longer input test
duration material to obtain sufficient information to distinguish between
languages. Alternatively, if languages are distinct and not
acoustically/linguistically similar to others, duration is not a sufficient
remedy. The solution proposed here is to explore duration distribution analysis
for near/far languages based on the Language Recognition i-Vector Machine
Learning Challenge 2015 (LRiMLC15) database. Using this knowledge, we propose a
likelihood ratio based fusion approach that leveraged both score and duration
information. The experimental results show that the use of duration and score
fusion improves language recognition performance by 5% relative in LRiMLC15
cost.
</summary>
    <author>
      <name>Suwon Shon</name>
    </author>
    <author>
      <name>Seongkyu Mun</name>
    </author>
    <author>
      <name>John H. L. Hansen</name>
    </author>
    <author>
      <name>Hanseok Ko</name>
    </author>
    <link href="http://arxiv.org/abs/1609.06404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.06404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07245v2</id>
    <updated>2016-12-21T08:57:51Z</updated>
    <published>2016-09-23T07:03:32Z</published>
    <title>A New Statistic Feature of the Short-Time Amplitude Spectrum Values for
  Human's Unvoiced Pronunciation</title>
    <summary>  In this paper, a new statistic feature of the discrete short-time amplitude
spectrum is discovered by experiments for the signals of unvoiced
pronunciation. For the random-varying short-time spectrum, this feature reveals
the relationship between the amplitude's average and its standard for every
frequency component. On the other hand, the association between the amplitude
distributions for different frequency components is also studied. A new model
representing such association is inspired by the normalized histogram of
amplitude. By mathematical analysis, the new statistic feature discovered is
proved to be necessary evidence which supports the proposed model, and also can
be direct evidence for the widely used hypothesis of "identical distribution of
amplitude for all frequencies".
</summary>
    <author>
      <name>Xiaodong Zhuang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures, original work</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">WSEAS Transactions on Signal Processing, Volume 12, pp. 265-269,
  2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1609.07245v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07245v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07384v2</id>
    <updated>2017-02-13T01:09:27Z</updated>
    <published>2016-09-23T14:35:17Z</published>
    <title>Discovering Sound Concepts and Acoustic Relations In Text</title>
    <summary>  In this paper we describe approaches for discovering acoustic concepts and
relations in text. The first major goal is to be able to identify text phrases
which contain a notion of audibility and can be termed as a sound or an
acoustic concept. We also propose a method to define an acoustic scene through
a set of sound concepts. We use pattern matching and parts of speech tags to
generate sound concepts from large scale text corpora. We use dependency
parsing and LSTM recurrent neural network to predict a set of sound concepts
for a given acoustic scene. These methods are not only helpful in creating an
acoustic knowledge base but in the future can also directly help acoustic event
and scene detection research.
</summary>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <author>
      <name>Ndapandula Nakashole</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICASSP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.07384v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07384v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07498v1</id>
    <updated>2016-09-23T20:03:14Z</updated>
    <published>2016-09-23T20:03:14Z</published>
    <title>Speaker Recognition for Children's Speech</title>
    <summary>  This paper presents results on Speaker Recognition (SR) for children's
speech, using the OGI Kids corpus and GMM-UBM and GMM-SVM SR systems. Regions
of the spectrum containing important speaker information for children are
identified by conducting SR experiments over 21 frequency bands. As for adults,
the spectrum can be split into four regions, with the first (containing primary
vocal tract resonance information) and third (corresponding to high frequency
speech sounds) being most useful for SR. However, the frequencies at which
these regions occur are from 11% to 38% higher for children. It is also noted
that subband SR rates are lower for younger children. Finally results are
presented of SR experiments to identify a child in a class (30 children,
similar age) and school (288 children, varying ages). Class performance depends
on age, with accuracy varying from 90% for young children to 99% for older
children. The identification rate achieved for a child in a school is 81%.
</summary>
    <author>
      <name>Saeid Safavi</name>
    </author>
    <author>
      <name>Maryam Najafian</name>
    </author>
    <author>
      <name>Abualsoud Hanani</name>
    </author>
    <author>
      <name>Martin J Russell</name>
    </author>
    <author>
      <name>Peter Jancovic</name>
    </author>
    <author>
      <name>Michael J Carey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">INTERSPEECH 2012, Pages 1836-1839</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.07498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08408v1</id>
    <updated>2016-09-25T15:56:06Z</updated>
    <published>2016-09-25T15:56:06Z</published>
    <title>Deep learning for detection of bird vocalisations</title>
    <summary>  This work focuses on reliable detection of bird sound emissions as recorded
in the open field. Acoustic detection of avian sounds can be used for the
automatized monitoring of multiple bird taxa and querying in long-term
recordings for species of interest for researchers, conservation practitioners,
and decision makers. Recordings in the wild can be very noisy due to the
exposure of the microphones to a large number of audio sources originating from
all distances and directions, the number and identity of which cannot be known
a-priori. The co-existence of the target vocalizations with abiotic
interferences in an unconstrained environment is inefficiently treated by
current approaches of audio signal enhancement. A technique that would spot
only bird vocalization while ignoring other audio sources is of prime
importance. These difficulties are tackled in this work, presenting a deep
autoencoder that maps the audio spectrogram of bird vocalizations to its
corresponding binary mask that encircles the spectral blobs of vocalizations
while suppressing other audio sources. The procedure requires minimum human
attendance, it is very fast during execution, thus suitable to scan massive
volumes of data, in order to analyze them, evaluate insights and hypotheses,
identify patterns of bird activity that, hopefully, finally lead to design
policies on biodiversity issues.
</summary>
    <author>
      <name>Ilyas Potamitis</name>
    </author>
    <link href="http://arxiv.org/abs/1609.08408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08419v1</id>
    <updated>2016-09-27T13:29:12Z</updated>
    <published>2016-09-27T13:29:12Z</published>
    <title>Decision Making Based on Cohort Scores for Speaker Verification</title>
    <summary>  Decision making is an important component in a speaker verification system.
For the conventional GMM-UBM architecture, the decision is usually conducted
based on the log likelihood ratio of the test utterance against the GMM of the
claimed speaker and the UBM. This single-score decision is simple but tends to
be sensitive to the complex variations in speech signals (e.g. text content,
channel, speaking style, etc.). In this paper, we propose a decision making
approach based on multiple scores derived from a set of cohort GMMs (cohort
scores). Importantly, these cohort scores are not simply averaged as in
conventional cohort methods; instead, we employ a powerful discriminative model
as the decision maker. Experimental results show that the proposed method
delivers substantial performance improvement over the baseline system,
especially when a deep neural network (DNN) is used as the decision maker, and
the DNN input involves some statistical features derived from the cohort
scores.
</summary>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Renyu Wang</name>
    </author>
    <author>
      <name>Gang Wang</name>
    </author>
    <author>
      <name>Caixia Wang</name>
    </author>
    <author>
      <name>Thomas Fang Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">APSIPA ASC 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.08419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08433v1</id>
    <updated>2016-09-27T13:37:13Z</updated>
    <published>2016-09-27T13:37:13Z</published>
    <title>Local Training for PLDA in Speaker Verification</title>
    <summary>  PLDA is a popular normalization approach for the i-vector model, and it has
delivered state-of-the-art performance in speaker verification. However, PLDA
training requires a large amount of labeled development data, which is highly
expensive in most cases. A possible approach to mitigate the problem is various
unsupervised adaptation methods, which use unlabeled data to adapt the PLDA
scattering matrices to the target domain.
  In this paper, we present a new `local training' approach that utilizes
inaccurate but much cheaper local labels to train the PLDA model. These local
labels discriminate speakers within a single conversion only, and so are much
easier to obtain compared to the normal `global labels'. Our experiments show
that the proposed approach can deliver significant performance improvement,
particularly with limited globally-labeled data.
</summary>
    <author>
      <name>Chenghui Zhao</name>
    </author>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>April Pu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">O-COCOSDA 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.08433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09231v1</id>
    <updated>2016-09-29T07:24:27Z</updated>
    <published>2016-09-29T07:24:27Z</published>
    <title>Low Rank and Sparsity Analysis Applied to Speech Enhancement via Online
  Estimated Dictionary</title>
    <summary>  We propose an online estimated dictionary based single channel speech
enhancement algorithm, which focuses on low rank and sparse matrix
decomposition. In this proposed algorithm, a noisy speech spectral matrix is
considered as the summation of low rank background noise components and an
activation of the online speech dictionary, on which both low rank and sparsity
constraints are imposed. This decomposition takes the advantage of local
estimated dictionary high expressiveness on speech components. The local
dictionary can be obtained through estimating the speech presence probability
by applying Expectation Maximal algorithm, in which a generalized Gamma prior
for speech magnitude spectrum is used. The evaluation results show that the
proposed algorithm achieves significant improvements when compared to four
other speech enhancement algorithms.
</summary>
    <author>
      <name>Pengfei Sun</name>
    </author>
    <author>
      <name>Jun Qin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2016.2627029</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2016.2627029" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures, journal in IEEE signal processing letter 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09390v1</id>
    <updated>2016-09-29T15:33:09Z</updated>
    <published>2016-09-29T15:33:09Z</published>
    <title>Measurement of Sound Fields Using Moving Microphones</title>
    <summary>  The sampling of sound fields involves the measurement of spatially dependent
room impulse responses, where the Nyquist-Shannon sampling theorem applies in
both the temporal and spatial domain. Therefore, sampling inside a volume of
interest requires a huge number of sampling points in space, which comes along
with further difficulties such as exact microphone positioning and calibration
of multiple microphones. In this paper, we present a method for measuring sound
fields using moving microphones whose trajectories are known to the algorithm.
At that, the number of microphones is customizable by trading measurement
effort against sampling time. Through spatial interpolation of the dynamic
measurements, a system of linear equations is set up which allows for the
reconstruction of the entire sound field inside the volume of interest.
</summary>
    <author>
      <name>Fabrice Katzberg</name>
    </author>
    <author>
      <name>Radoslaw Mazur</name>
    </author>
    <author>
      <name>Marco Maass</name>
    </author>
    <author>
      <name>Philipp Koch</name>
    </author>
    <author>
      <name>Alfred Mertins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to ICASSP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09443v2</id>
    <updated>2017-02-23T05:20:54Z</updated>
    <published>2016-09-29T17:54:51Z</published>
    <title>Semi-supervised Speech Enhancement in Envelop and Details Subspaces</title>
    <summary>  In this study, we propose a modulation decoupling based single channel speech
enhancement subspace framework, in which the spectrogram of noisy speech is
decoupled as the product of a spectral envelop subspace and a spectral details
subspace. This decoupling approach provides a method to specifically work on
elimination of those noises that greatly affect the intelligibility. Two
supervised low-rank and sparse decomposition schemes are developed in the
spectral envelop subspace to obtain a robust recovery of speech components. A
Bayesian formulation of non-negative factorization is used to learn the speech
dictionary from the spectral envelop subspace of clean speech samples. In the
spectral details subspace, a standard robust principal component analysis is
implemented to extract the speech components. The validation results show that
compared with four speech enhancement algorithms, including MMSE-SPP, NMF-RPCA,
RPCA, and LARC, the proposed MS based algorithms achieve satisfactory
performance on improving perceptual quality, and especially speech
intelligibility.
</summary>
    <author>
      <name>Pengfei Sun</name>
    </author>
    <author>
      <name>Jun Qin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09443v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09443v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09743v1</id>
    <updated>2016-09-30T14:15:46Z</updated>
    <published>2016-09-30T14:15:46Z</published>
    <title>Rectified binaural ratio: A complex T-distributed feature for robust
  sound localization</title>
    <summary>  Most existing methods in binaural sound source localization rely on some kind
of aggregation of phase-and level-difference cues in the time-frequency plane.
While different ag-gregation schemes exist, they are often heuristic and suffer
in adverse noise conditions. In this paper, we introduce the rectified binaural
ratio as a new feature for sound source local-ization. We show that for
Gaussian-process point source signals corrupted by stationary Gaussian noise,
this ratio follows a complex t-distribution with explicit parameters. This new
formulation provides a principled and statistically sound way to aggregate
binaural features in the presence of noise. We subsequently derive two simple
and efficient methods for robust relative transfer function and time-delay
estimation. Experiments on heavily corrupted simulated and speech signals
demonstrate the robustness of the proposed scheme.
</summary>
    <author>
      <name>Antoine Deleforge</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA</arxiv:affiliation>
    </author>
    <author>
      <name>Florence Forbes</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MISTIS</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">European Signal Processing Conference, Aug 2016, Budapest, Hungary.
  Proceedings of the 24th European Signal Processing Conference (EUSIPCO),
  2016, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09744v2</id>
    <updated>2017-03-20T13:36:08Z</updated>
    <published>2016-09-30T14:17:32Z</published>
    <title>Phase Unmixing : Multichannel Source Separation with Magnitude
  Constraints</title>
    <summary>  We consider the problem of estimating the phases of K mixed complex signals
from a multichannel observation, when the mixing matrix and signal magnitudes
are known. This problem can be cast as a non-convex quadratically constrained
quadratic program which is known to be NP-hard in general. We propose three
approaches to tackle it: a heuristic method, an alternate minimization method,
and a convex relaxation into a semi-definite program. The last two approaches
are showed to outperform the oracle multichannel Wiener filter in
under-determined informed source separation tasks, using simulated and speech
signals. The convex relaxation approach yields best results, including the
potential for exact source separation in under-determined settings.
</summary>
    <author>
      <name>Antoine Deleforge</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA</arxiv:affiliation>
    </author>
    <author>
      <name>Yann Traonmilin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2017 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), Mar 2017, New Orleans, United States</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09744v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09744v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.09747v2</id>
    <updated>2017-03-20T13:39:49Z</updated>
    <published>2016-09-30T14:20:56Z</published>
    <title>Hearing in a shoe-box : binaural source position and wall absorption
  estimation using virtually supervised learning</title>
    <summary>  This paper introduces a new framework for supervised sound source
localization referred to as virtually-supervised learning. An acoustic shoe-box
room simulator is used to generate a large number of binaural single-source
audio scenes. These scenes are used to build a dataset of spatial binaural
features annotated with acoustic properties such as the 3D source position and
the walls' absorption coefficients. A probabilistic high- to low-dimensional
regression framework is used to learn a mapping from these features to the
acoustic properties. Results indicate that this mapping successfully estimates
the azimuth and elevation of new sources, but also their range and even the
walls' absorption coefficients solely based on binaural signals. Results also
reveal that incorporating random-diffusion effects in the data significantly
improves the estimation of all parameters.
</summary>
    <author>
      <name>Saurabh Kataria</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IIT Kanpur, Panama</arxiv:affiliation>
    </author>
    <author>
      <name>Clément Gaultier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Panama</arxiv:affiliation>
    </author>
    <author>
      <name>Antoine Deleforge</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Panama</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2017 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), Mar 2017, New-Orleans, United States</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.09747v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.09747v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.00468v1</id>
    <updated>2016-10-03T09:56:43Z</updated>
    <published>2016-10-03T09:56:43Z</published>
    <title>On the Modeling of Musical Solos as Complex Networks</title>
    <summary>  Notes in a musical piece are building blocks employed in non-random ways to
create melodies. It is the "interaction" among a limited amount of notes that
allows constructing the variety of musical compositions that have been written
in centuries and within different cultures. Networks are a modeling tool that
is commonly employed to represent a set of entities interacting in some way.
Thus, notes composing a melody can be seen as nodes of a network that are
connected whenever these are played in sequence. The outcome of such a process
results in a directed graph. By using complex network theory, some main metrics
of musical graphs can be measured, which characterize the related musical
pieces. In this paper, we define a framework to represent melodies as networks.
Then, we provide an analysis on a set of guitar solos performed by main
musicians. Results of this study indicate that the presented model can have an
impact on audio and multimedia applications such as music classification,
identification, e-learning, automatic music generation, multimedia
entertainment.
</summary>
    <author>
      <name>Stefano Ferretti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ins.2016.10.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ins.2016.10.007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in Information Science, Elsevier. Please cite the paper
  including such information. arXiv admin note: text overlap with
  arXiv:1603.04979</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.00468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.00468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.00644v2</id>
    <updated>2016-10-04T02:16:08Z</updated>
    <published>2016-10-03T17:39:01Z</published>
    <title>Speech Enhancement via Two-Stage Dual Tree Complex Wavelet Packet
  Transform with a Speech Presence Probability Estimator</title>
    <summary>  In this paper, a two-stage dual tree complex wavelet packet transform
(DTCWPT) based speech enhancement algorithm has been proposed, in which a
speech presence probability (SPP) estimator and a generalized minimum mean
squared error (MMSE) estimator are developed. To overcome the drawback of
signal distortions caused by down sampling of WPT, a two-stage analytic
decomposition concatenating undecimated WPT (UWPT) and decimated WPT is
employed. An SPP estimator in the DTCWPT domain is derived based on a
generalized Gamma distribution of speech, and Gaussian noise assumption. The
validation results show that the proposed algorithm can obtain enhanced
perceptual evaluation of speech quality (PESQ), and segmental signal-to-noise
ratio (SegSNR) at low SNR nonstationary noise, compared with other four
state-of-the-art speech enhancement algorithms, including optimally modified
LSA (OM-LSA), soft masking using a posteriori SNR uncertainty (SMPO), a
posteriori SPP based MMSE estimation (MMSE-SPP), and adaptive Bayesian wavelet
thresholding (BWT).
</summary>
    <author>
      <name>Pengfei Sun</name>
    </author>
    <author>
      <name>Jun Qin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1121/1.4976049</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1121/1.4976049" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, Journal of the Acoustical Society of America 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.00644v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.00644v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01382v1</id>
    <updated>2016-10-05T12:16:35Z</updated>
    <published>2016-10-05T12:16:35Z</published>
    <title>Divide-and-Conquer based Ensemble to Spot Emotions in Speech using MFCC
  and Random Forest</title>
    <summary>  Besides spoken words, speech signals also carry information about speaker
gender, age, and emotional state which can be used in a variety of speech
analysis applications. In this paper, a divide and conquer strategy for
ensemble classification has been proposed to recognize emotions in speech.
Intrinsic hierarchy in emotions has been utilized to construct an emotions
tree, which assisted in breaking down the emotion recognition task into smaller
sub tasks. The proposed framework generates predictions in three phases.
Firstly, emotions are detected in the input speech signal by classifying it as
neutral or emotional. If the speech is classified as emotional, then in the
second phase, it is further classified into positive and negative classes.
Finally, individual positive or negative emotions are identified based on the
outcomes of the previous stages. Several experiments have been performed on a
widely used benchmark dataset. The proposed method was able to achieve improved
recognition rates as compared to several other approaches.
</summary>
    <author>
      <name>Abdul Malik Badshah</name>
    </author>
    <author>
      <name>Jamil Ahmad</name>
    </author>
    <author>
      <name>Mi Young Lee</name>
    </author>
    <author>
      <name>Sung Wook Baik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, conference paper, The 2nd International Integrated
  Conference &amp; Concert on Convergence (2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.01382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01797v1</id>
    <updated>2016-10-06T09:51:12Z</updated>
    <published>2016-10-06T09:51:12Z</published>
    <title>A Joint Detection-Classification Model for Audio Tagging of Weakly
  Labelled Data</title>
    <summary>  Audio tagging aims to assign one or several tags to an audio clip. Most of
the datasets are weakly labelled, which means only the tags of the clip are
known, without knowing the occurrence time of the tags. The labeling of an
audio clip is often based on the audio events in the clip and no event level
label is provided to the user. Previous works have used the bag of frames model
assume the tags occur all the time, which is not the case in practice. We
propose a joint detection-classification (JDC) model to detect and classify the
audio clip simultaneously. The JDC model has the ability to attend to
informative and ignore uninformative sounds. Then only informative regions are
used for classification. Experimental results on the "CHiME Home" dataset show
that the JDC model reduces the equal error rate (EER) from 19.0% to 16.9%. More
interestingly, the audio event detector is trained successfully without needing
the event level label.
</summary>
    <author>
      <name>Qiuqiang Kong</name>
    </author>
    <author>
      <name>Yong Xu</name>
    </author>
    <author>
      <name>Wenwu Wang</name>
    </author>
    <author>
      <name>Mark Plumbley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ICASSP 2017</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), 2018, pp. 641-645</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.01797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02392v1</id>
    <updated>2016-10-07T19:57:41Z</updated>
    <published>2016-10-07T19:57:41Z</published>
    <title>An Automatic System for Acoustic Microphone Geometry Calibration based
  on Minimal Solvers</title>
    <summary>  In this paper, robust detection, tracking and geometry estimation methods are
developed and combined into a system for estimating time-difference estimates,
microphone localization and sound source movement. No assumptions on the 3D
locations of the microphones and sound sources are made. The system is capable
of tracking continuously moving sound sources in an reverberant environment.
The multi-path components are explicitly tracked and used in the geometry
estimation parts. The system is based on matching between pairs of channels
using GCC-PHAT. Instead of taking a single maximum at each time instant from
each such pair, we select the four strongest local maxima. This produce a set
of hypothesis to work with in the subsequent steps, where consistency
constraints between the channels and time-continuity constraints are exploited.
In the paper it demonstrated how such detections can be used to estimate
microphone positions, sound source movement and room geometry. The methods are
tested and verified using real data from several reverberant environments. The
evaluation demonstrated accuracy in the order of few millimeters.
</summary>
    <author>
      <name>Simayijiang Zhayida</name>
    </author>
    <author>
      <name>Simon Segerblom Rex</name>
    </author>
    <author>
      <name>Yubin Kuang</name>
    </author>
    <author>
      <name>Fredrik Andersson</name>
    </author>
    <author>
      <name>Kalle Åström</name>
    </author>
    <link href="http://arxiv.org/abs/1610.02392v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02392v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02831v2</id>
    <updated>2016-10-11T05:10:07Z</updated>
    <published>2016-10-10T10:09:49Z</published>
    <title>Domain adaptation based Speaker Recognition on Short Utterances</title>
    <summary>  This paper explores how the in- and out-domain probabilistic linear
discriminant analysis (PLDA) speaker verification behave when enrolment and
verification lengths are reduced. Experiment studies have found that when
full-length utterance is used for evaluation, in-domain PLDA approach shows
more than 28% improvement in EER and DCF values over out-domain PLDA approach
and when short utterances are used for evaluation, the performance gain of
in-domain speaker verification reduces at an increasing rate. Novel modified
inter dataset variability (IDV) compensation is used to compensate the mismatch
between in- and out-domain data and IDV-compensated out-domain PLDA shows
respectively 26% and 14% improvement over out-domain PLDA speaker verification
when SWB and NIST data are respectively used for S normalization. When the
evaluation utterance length is reduced, the performance gain by IDV also
reduces as short utterance evaluation data i-vectors have more variations due
to phonetic variations when compared to the dataset mismatch between in- and
out-domain data.
</summary>
    <author>
      <name>Ahilan Kanagasundaram</name>
    </author>
    <author>
      <name>David Dean</name>
    </author>
    <author>
      <name>Sridha Sridharan</name>
    </author>
    <author>
      <name>Clinton Fookes</name>
    </author>
    <link href="http://arxiv.org/abs/1610.02831v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02831v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03009v1</id>
    <updated>2016-10-10T18:03:29Z</updated>
    <published>2016-10-10T18:03:29Z</published>
    <title>Investigation of Synthetic Speech Detection Using Frame- and
  Segment-Specific Importance Weighting</title>
    <summary>  Speaker verification systems are vulnerable to spoofing attacks which
presents a major problem in their real-life deployment. To date, most of the
proposed synthetic speech detectors (SSDs) have weighted the importance of
different segments of speech equally. However, different attack methods have
different strengths and weaknesses and the traces that they leave may be short
or long term acoustic artifacts. Moreover, those may occur for only particular
phonemes or sounds. Here, we propose three algorithms that weigh
likelihood-ratio scores of individual frames, phonemes, and sound-classes
depending on their importance for the SSD. Significant improvement over the
baseline system has been obtained for known attack methods that were used in
training the SSDs. However, improvement with unknown attack types was not
substantial. Thus, the type of distortions that were caused by the unknown
systems were different and could not be captured better with the proposed SSD
compared to the baseline SSD.
</summary>
    <author>
      <name>Ali Khodabakhsh</name>
    </author>
    <author>
      <name>Cenk Demiroglu</name>
    </author>
    <link href="http://arxiv.org/abs/1610.03009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03190v1</id>
    <updated>2016-10-11T05:04:25Z</updated>
    <published>2016-10-11T05:04:25Z</published>
    <title>DNN based Speaker Recognition on Short Utterances</title>
    <summary>  This paper investigates the effects of limited speech data in the context of
speaker verification using deep neural network (DNN) approach. Being able to
reduce the length of required speech data is important to the development of
speaker verification system in real world applications. The experimental
studies have found that DNN-senone-based Gaussian probabilistic linear
discriminant analysis (GPLDA) system respectively achieves above 50% and 18%
improvements in EER values over GMM-UBM GPLDA system on NIST 2010
coreext-coreext and truncated 15sec-15sec evaluation conditions. Further when
GPLDA model is trained on short-length utterances (30sec) rather than
full-length utterances (2min), DNN-senone GPLDA system achieves above 7%
improvement in EER values on truncated 15sec-15sec condition. This is because
short length development i-vectors have speaker, session and phonetic variation
and GPLDA is able to robustly model those variations. For several real world
applications, longer utterances (2min) can be used for enrollment and shorter
utterances (15sec) are required for verification, and in those conditions,
DNN-senone GPLDA system achieves above 26% improvement in EER values over
GMM-UBM GPLDA systems.
</summary>
    <author>
      <name>Ahilan Kanagasundaram</name>
    </author>
    <author>
      <name>David Dean</name>
    </author>
    <author>
      <name>Sridha Sridharan</name>
    </author>
    <author>
      <name>Clinton Fookes</name>
    </author>
    <link href="http://arxiv.org/abs/1610.03190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.03772v1</id>
    <updated>2016-10-12T16:24:54Z</updated>
    <published>2016-10-12T16:24:54Z</published>
    <title>RAVEN X High Performance Data Mining Toolbox for Bioacoustic Data
  Analysis</title>
    <summary>  Objective of this work is to integrate high performance computing (HPC)
technologies and bioacoustics data-mining capabilities by offering a
MATLAB-based toolbox called Raven-X. Raven-X will provide a
hardware-independent solution, for processing large acoustic datasets - the
toolkit will be available to the community at no cost. This goal will be
achieved by leveraging prior work done which successfully deployed MATLAB based
HPC tools within Cornell University's Bioacoustics Research Program (BRP).
These tools enabled commonly available multi-core computers to process data at
accelerated rates to detect and classify whale sounds in large multi-channel
sound archives. Through this collaboration, we will expand on this effort which
was featured through Mathworks research and industry forums incorporate new
cutting-edge detectors and classifiers, and disseminate Raven-X to the broader
bioacoustics community.
</summary>
    <author>
      <name>Peter J. Dugan</name>
    </author>
    <author>
      <name>Holger Klinck</name>
    </author>
    <author>
      <name>Marie A. Roch</name>
    </author>
    <author>
      <name>Tyler A. Helble</name>
    </author>
    <link href="http://arxiv.org/abs/1610.03772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.03772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04695v1</id>
    <updated>2016-10-15T06:49:15Z</updated>
    <published>2016-10-15T06:49:15Z</published>
    <title>Non-negative matrix factorization-based subband decomposition for
  acoustic source localization</title>
    <summary>  A novel non-negative matrix factorization (NMF) based subband decomposition
in frequency spatial domain for acoustic source localization using a microphone
array is introduced. The proposed method decomposes source and noise subband
and emphasises source dominant frequency bins for more accurate source
representation. By employing NMF, delay basis vectors and their subband
information in frequency spatial domain for each frame is extracted. The
proposed algorithm is evaluated in both simulated noise and real noise with a
speech corpus database. Experimental results clearly indicate that the
algorithm performs more accurately than other conventional algorithms under
both reverberant and noisy acoustic environments.
</summary>
    <author>
      <name>Suwon Shon</name>
    </author>
    <author>
      <name>Seongkyu Mun</name>
    </author>
    <author>
      <name>David Han</name>
    </author>
    <author>
      <name>Hanseok Ko</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1049/el.2015.2665</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1049/el.2015.2665" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on IET Electronics Letters: 31 July 2015; Published
  E-first: 9 October 2015</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronics Letters 51 (2015) 1723-1724</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.04695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04770v1</id>
    <updated>2016-10-15T18:14:57Z</updated>
    <published>2016-10-15T18:14:57Z</published>
    <title>Semi-Supervised Source Localization on Multiple-Manifolds with
  Distributed Microphones</title>
    <summary>  The problem of source localization with ad hoc microphone networks in noisy
and reverberant enclosures, given a training set of prerecorded measurements,
is addressed in this paper. The training set is assumed to consist of a limited
number of labelled measurements, attached with corresponding positions, and a
larger amount of unlabelled measurements from unknown locations. However,
microphone calibration is not required. We use a Bayesian inference approach
for estimating a function that maps measurement-based feature vectors to the
corresponding positions. The central issue is how to combine the information
provided by the different microphones in a unified statistical framework. To
address this challenge, we model this function using a Gaussian process with a
covariance function that encapsulates both the connections between pairs of
microphones and the relations among the samples in the training set. The
parameters of the process are estimated by optimizing a maximum likelihood (ML)
criterion. In addition, a recursive adaptation mechanism is derived where the
new streaming measurements are used to update the model. Performance is
demonstrated for 2-D localization of both simulated data and real-life
recordings in a variety of reverberation and noise levels.
</summary>
    <author>
      <name>Bracha Laufer-Goldshtein</name>
    </author>
    <author>
      <name>Ronen Talmon</name>
    </author>
    <author>
      <name>Sharon Gannot</name>
    </author>
    <link href="http://arxiv.org/abs/1610.04770v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04770v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04965v1</id>
    <updated>2016-10-17T03:36:42Z</updated>
    <published>2016-10-17T03:36:42Z</published>
    <title>Improving Short Utterance PLDA Speaker Verification using SUV Modelling
  and Utterance Partitioning Approach</title>
    <summary>  This paper analyses the short utterance probabilistic linear discriminant
analysis (PLDA) speaker verification with utterance partitioning and short
utterance variance (SUV) modelling approaches. Experimental studies have found
that instead of using single long-utterance as enrolment data, if long enrolled
utterance is partitioned into multiple short utterances and average of short
utterance i-vectors is used as enrolled data, that improves the Gaussian PLDA
(GPLDA) speaker verification. This is because short utterance i-vectors have
speaker, session and utterance variations, and utterance-partitioning approach
compensates the utterance variation. Subsequently, SUV-PLDA is also studied
with utterance partitioning approach, and utterance partitioning-based
SUV-GPLDA system shows relative improvement of 9% and 16% in EER for NIST 2008
and NIST 2010 truncated 10sec-10sec evaluation condition as utterance
partitioning approach compensates the utterance variation and SUV modelling
approach compensates the mismatch between full-length development data and
short-length evaluation data.
</summary>
    <author>
      <name>Ahilan Kanagasundaram</name>
    </author>
    <author>
      <name>David Dean</name>
    </author>
    <author>
      <name>Sridha Sridharan</name>
    </author>
    <author>
      <name>Clinton Fookes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1610.02831</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.04965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.05948v1</id>
    <updated>2016-10-19T10:16:46Z</updated>
    <published>2016-10-19T10:16:46Z</published>
    <title>A Bayesian Approach to Estimation of Speaker Normalization Parameters</title>
    <summary>  In this work, a Bayesian approach to speaker normalization is proposed to
compensate for the degradation in performance of a speaker independent speech
recognition system. The speaker normalization method proposed herein uses the
technique of vocal tract length normalization (VTLN). The VTLN parameters are
estimated using a novel Bayesian approach which utilizes the Gibbs sampler, a
special type of Markov Chain Monte Carlo method. Additionally the
hyperparameters are estimated using maximum likelihood approach. This model is
used assuming that human vocal tract can be modeled as a tube of uniform cross
section. It captures the variation in length of the vocal tract of different
speakers more effectively, than the linear model used in literature. The work
has also investigated different methods like minimization of Mean Square Error
(MSE) and Mean Absolute Error (MAE) for the estimation of VTLN parameters. Both
single pass and two pass approaches are then used to build a VTLN based speech
recognizer. Experimental results on recognition of vowels and Hindi phrases
from a medium vocabulary indicate that the Bayesian method improves the
performance by a considerable margin.
</summary>
    <author>
      <name>Dhananjay Ram</name>
    </author>
    <author>
      <name>Debasis Kundu</name>
    </author>
    <author>
      <name>Rajesh M. Hegde</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 Pages, 9 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.05948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.05948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00326v3</id>
    <updated>2017-04-20T18:43:29Z</updated>
    <published>2016-11-01T18:38:12Z</published>
    <title>Enhanced Factored Three-Way Restricted Boltzmann Machines for Speech
  Detection</title>
    <summary>  In this letter, we propose enhanced factored three way restricted Boltzmann
machines (EFTW-RBMs) for speech detection. The proposed model incorporates
conditional feature learning by multiplying the dynamical state of the third
unit, which allows a modulation over the visible-hidden node pairs. Instead of
stacking previous frames of speech as the third unit in a recursive manner, the
correlation related weighting coefficients are assigned to the contextual
neighboring frames. Specifically, a threshold function is designed to capture
the long-term features and blend the globally stored speech structure. A
factored low rank approximation is introduced to reduce the parameters of the
three-dimensional interaction tensor, on which non-negative constraint is
imposed to address the sparsity characteristic. The validations through the
area-under-ROC-curve (AUC) and signal distortion ratio (SDR) show that our
approach outperforms several existing 1D and 2D (i.e., time and time-frequency
domain) speech detection algorithms in various noisy environments.
</summary>
    <author>
      <name>Pengfei Sun</name>
    </author>
    <author>
      <name>Jun Qin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, Pattern Recognition Letter 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.00326v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00326v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00966v1</id>
    <updated>2016-11-03T12:08:25Z</updated>
    <published>2016-11-03T12:08:25Z</published>
    <title>Frame Theory for Signal Processing in Psychoacoustics</title>
    <summary>  This review chapter aims to strengthen the link between frame theory and
signal processing tasks in psychoacoustics. On the one side, the basic concepts
of frame theory are presented and some proofs are provided to explain those
concepts in some detail. The goal is to reveal to hearing scientists how this
mathematical theory could be relevant for their research. In particular, we
focus on frame theory in a filter bank approach, which is probably the most
relevant view-point for audio signal processing. On the other side, basic
psychoacoustic concepts are presented to stimulate mathematicians to apply
their knowledge in this field.
</summary>
    <author>
      <name>Peter Balazs</name>
    </author>
    <author>
      <name>Nicki Holighaus</name>
    </author>
    <author>
      <name>Thibaud Necciari</name>
    </author>
    <author>
      <name>Diana Stoeva</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-54711-4_10</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-54711-4_10" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In: Balan R., Benedetto J., Czaja W., Dellatorre M., Okoudjou K.
  (eds) Excursions in Harmonic Analysis, Vol. 5. Applied and Numerical Harmonic
  Analysis. Birkh\"auser, Cham, 2017, 225-268</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1611.00966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.04947v1</id>
    <updated>2016-11-15T17:23:17Z</updated>
    <published>2016-11-15T17:23:17Z</published>
    <title>Detection of north atlantic right whale upcalls using local binary
  patterns in a two-stage strategy</title>
    <summary>  In this paper, we investigate the effectiveness of two-stage classification
strategies in detecting north Atlantic right whale upcalls. Time-frequency
measurements of data from passive acoustic monitoring devices are evaluated as
images. Vocalization spectrograms are preprocessed for noise reduction and tone
removal. First stage of the algorithm eliminates non-upcalls by an energy
detection algorithm. In the second stage, two sets of features are extracted
from the remaining signals using contour-based and texture based methods. The
former is based on extraction of time-frequency features from upcall contours,
and the latter employs a Local Binary Pattern operator to extract
distinguishing texture features of the upcalls. Subsequently evaluation phase
is carried out by using several classifiers to assess the effectiveness of both
the contour-based and texture-based features for upcall detection. Experimental
results with the data set provided by the Cornell University Bioacoustics
Research Program reveal that classifiers show accuracy improvements of 3% to 4%
when using LBP features over time-frequency features. Classifiers such as the
Linear Discriminant Analysis, Support Vector Machine, and TreeBagger achieve
high upcall detection rates with LBP features.
</summary>
    <author>
      <name>Mahdi Esfahanian</name>
    </author>
    <author>
      <name>Hanqi Zhuang</name>
    </author>
    <author>
      <name>Nurgun Erdol</name>
    </author>
    <author>
      <name>Edmund Gerstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 11 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.04947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.04947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.07351v1</id>
    <updated>2016-11-22T15:18:31Z</updated>
    <published>2016-11-22T15:18:31Z</published>
    <title>MOMOS-MT: Mobile Monophonic System for Music Transcription</title>
    <summary>  Music holds a significant cultural role in social identity and in the
encouragement of socialization. Technology, by the destruction of physical and
cultural distance, has lead to many changes in musical themes and the complete
loss of forms. Yet, it also allows for the preservation and distribution of
music from societies without a history of written sheet music. This paper
presents early work on a tool for musicians and ethnomusicologists to
transcribe sheet music from monophonic voiced pieces for preservation and
distribution. Using FFT, the system detects the pitch frequencies, also other
methods detect note durations, tempo, time signatures and generates sheet
music. The final system is able to be used in mobile platforms allowing the
user to take recordings and produce sheet music in situ to a performance.
</summary>
    <author>
      <name>Munir Makhmutov</name>
    </author>
    <author>
      <name>Joseph Alexander Brown</name>
    </author>
    <author>
      <name>Manuel Mazzara</name>
    </author>
    <author>
      <name>Leonard Johard</name>
    </author>
    <link href="http://arxiv.org/abs/1611.07351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.07351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.08749v2</id>
    <updated>2017-01-22T22:28:47Z</updated>
    <published>2016-11-26T22:16:35Z</published>
    <title>Fast Chirplet Transform to Enhance CNN Machine Listening - Validation on
  Animal calls and Speech</title>
    <summary>  The scattering framework offers an optimal hierarchical convolutional
decomposition according to its kernels. Convolutional Neural Net (CNN) can be
seen as an optimal kernel decomposition, nevertheless it requires large amount
of training data to learn its kernels. We propose a trade-off between these two
approaches: a Chirplet kernel as an efficient Q constant bioacoustic
representation to pretrain CNN. First we motivate Chirplet bioinspired auditory
representation. Second we give the first algorithm (and code) of a Fast
Chirplet Transform (FCT). Third, we demonstrate the computation efficiency of
FCT on large environmental data base: months of Orca recordings, and 1000 Birds
species from the LifeClef challenge. Fourth, we validate FCT on the vowels
subset of the Speech TIMIT dataset. The results show that FCT accelerates CNN
when it pretrains low level layers: it reduces training duration by -28\% for
birds classification, and by -26% for vowels classification. Scores are also
enhanced by FCT pretraining, with a relative gain of +7.8% of Mean Average
Precision on birds, and +2.3\% of vowel accuracy against raw audio CNN. We
conclude on perspectives on tonotopic FCT deep machine listening, and
inter-species bioacoustic transfer learning to generalise the representation of
animal communication systems.
</summary>
    <author>
      <name>Herve Glotin</name>
    </author>
    <author>
      <name>Julien Ricard</name>
    </author>
    <author>
      <name>Randall Balestriero</name>
    </author>
    <link href="http://arxiv.org/abs/1611.08749v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.08749v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.08905v1</id>
    <updated>2016-11-27T20:29:53Z</updated>
    <published>2016-11-27T20:29:53Z</published>
    <title>SISO and SIMO Accompaniment Cancellation for Live Solo Recordings Based
  on Short-Time ERB-Band Wiener Filtering and Spectral Subtraction</title>
    <summary>  Research in collaborative music learning is subject to unresolved problems
demanding new technological solutions. One such problem poses the suppression
of the accompaniment in a live recording of a performance during practice,
which can be for the purposes of self-assessment or further machine-aided
analysis. Being able to separate a solo from the accompaniment allows to create
learning agents that may act as personal tutors and help the apprentice improve
his or her technique. First, we start from the classical adaptive noise
cancelling approach, and adjust it to the problem at hand. In a second step, we
compare some adaptive and Wiener filtering approaches and assess their
performances on the task. Our findings underpin that adaptive filtering is
inapt of dealing with music signals and that Wiener filtering in the short-time
Fourier transform domain is a much more effective approach. In addition, it is
very cheap if carried out in the frequency bands of auditory filters. A
double-output extension based on maximal-ratio combining is also proposed.
</summary>
    <author>
      <name>Stanislaw Gorlow</name>
    </author>
    <author>
      <name>Mathieu Ramona</name>
    </author>
    <author>
      <name>François Pachet</name>
    </author>
    <link href="http://arxiv.org/abs/1611.08905v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.08905v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09482v1</id>
    <updated>2016-11-29T04:16:44Z</updated>
    <published>2016-11-29T04:16:44Z</published>
    <title>Fast Wavenet Generation Algorithm</title>
    <summary>  This paper presents an efficient implementation of the Wavenet generation
process called Fast Wavenet. Compared to a naive implementation that has
complexity O(2^L) (L denotes the number of layers in the network), our proposed
approach removes redundant convolution operations by caching previous
calculations, thereby reducing the complexity to O(L) time. Timing experiments
show significant advantages of our fast implementation over a naive one. While
this method is presented for Wavenet, the same scheme can be applied anytime
one wants to perform autoregressive generation or online prediction using a
model with dilated convolution layers. The code for our method is publicly
available.
</summary>
    <author>
      <name>Tom Le Paine</name>
    </author>
    <author>
      <name>Pooya Khorrami</name>
    </author>
    <author>
      <name>Shiyu Chang</name>
    </author>
    <author>
      <name>Yang Zhang</name>
    </author>
    <author>
      <name>Prajit Ramachandran</name>
    </author>
    <author>
      <name>Mark A. Hasegawa-Johnson</name>
    </author>
    <author>
      <name>Thomas S. Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.09482v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09482v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.09526v1</id>
    <updated>2016-11-29T08:46:26Z</updated>
    <published>2016-11-29T08:46:26Z</published>
    <title>Learning Filter Banks Using Deep Learning For Acoustic Signals</title>
    <summary>  Designing appropriate features for acoustic event recognition tasks is an
active field of research. Expressive features should both improve the
performance of the tasks and also be interpret-able. Currently, heuristically
designed features based on the domain knowledge requires tremendous effort in
hand-crafting, while features extracted through deep network are difficult for
human to interpret. In this work, we explore the experience guided learning
method for designing acoustic features. This is a novel hybrid approach
combining both domain knowledge and purely data driven feature designing. Based
on the procedure of log Mel-filter banks, we design a filter bank learning
layer. We concatenate this layer with a convolutional neural network (CNN)
model. After training the network, the weight of the filter bank learning layer
is extracted to facilitate the design of acoustic features. We smooth the
trained weight of the learning layer and re-initialize it in filter bank
learning layer as audio feature extractor. For the environmental sound
recognition task based on the Urban- sound8K dataset, the experience guided
learning leads to a 2% accuracy improvement compared with the fixed feature
extractors (the log Mel-filter bank). The shape of the new filter banks are
visualized and explained to prove the effectiveness of the feature design
process.
</summary>
    <author>
      <name>Shuhui Qu</name>
    </author>
    <author>
      <name>Juncheng Li</name>
    </author>
    <author>
      <name>Wei Dai</name>
    </author>
    <author>
      <name>Samarjit Das</name>
    </author>
    <link href="http://arxiv.org/abs/1611.09526v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.09526v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.00171v1</id>
    <updated>2016-12-01T08:25:45Z</updated>
    <published>2016-12-01T08:25:45Z</published>
    <title>A Non Linear Multifractal Study to Illustrate the Evolution of Tagore
  Songs Over a Century</title>
    <summary>  The works of Rabindranath Tagore have been sung by various artistes over
generations spanning over almost 100 years. there are few songs which were
popular in the early years and have been able to retain their popularity over
the years while some others have faded away. In this study we look to find cues
for the singing style of these songs which have kept them alive for all these
years. For this we took 3 min clip of four Tagore songs which have been sung by
five generation of artistes over 100 years and analyze them with the help of
latest nonlinear techniques Multifractal Detrended Fluctuation Analysis
(MFDFA). The multifractal spectral width is a manifestation of the inherent
complexity of the signal and may prove to be an important parameter to identify
the singing style of particular generation of singers and how this style varies
over different generations. The results are discussed in detail.
</summary>
    <author>
      <name>Shankha Sanyal</name>
    </author>
    <author>
      <name>Archi Banerjee</name>
    </author>
    <author>
      <name>Tarit Guhathakurata</name>
    </author>
    <author>
      <name>Ranjan Sengupta</name>
    </author>
    <author>
      <name>Dipak Ghosh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 PAGES, 5 FIGURES, Presented in International Symposium on Frontiers
  of Research in Speech and Music (FRSM)2016 held in North Orissa University,
  11-12 November 2016. arXiv admin note: text overlap with arXiv:1601.07709</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.00171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.00171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01840v3</id>
    <updated>2017-09-05T18:38:33Z</updated>
    <published>2016-12-06T14:58:59Z</published>
    <title>FMA: A Dataset For Music Analysis</title>
    <summary>  We introduce the Free Music Archive (FMA), an open and easily accessible
dataset suitable for evaluating several tasks in MIR, a field concerned with
browsing, searching, and organizing large music collections. The community's
growing interest in feature and end-to-end learning is however restrained by
the limited availability of large audio datasets. The FMA aims to overcome this
hurdle by providing 917 GiB and 343 days of Creative Commons-licensed audio
from 106,574 tracks from 16,341 artists and 14,854 albums, arranged in a
hierarchical taxonomy of 161 genres. It provides full-length and high-quality
audio, pre-computed features, together with track- and user-level metadata,
tags, and free-form text such as biographies. We here describe the dataset and
how it was created, propose a train/validation/test split and three subsets,
discuss some suitable MIR tasks, and evaluate some baselines for genre
recognition. Code, data, and usage examples are available at
https://github.com/mdeff/fma
</summary>
    <author>
      <name>Michaël Defferrard</name>
    </author>
    <author>
      <name>Kirell Benzi</name>
    </author>
    <author>
      <name>Pierre Vandergheynst</name>
    </author>
    <author>
      <name>Xavier Bresson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISMIR 2017 camera-ready</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.01840v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01840v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01943v1</id>
    <updated>2016-12-06T18:37:30Z</updated>
    <published>2016-12-06T18:37:30Z</published>
    <title>Segmental Convolutional Neural Networks for Detection of Cardiac
  Abnormality With Noisy Heart Sound Recordings</title>
    <summary>  Heart diseases constitute a global health burden, and the problem is
exacerbated by the error-prone nature of listening to and interpreting heart
sounds. This motivates the development of automated classification to screen
for abnormal heart sounds. Existing machine learning-based systems achieve
accurate classification of heart sound recordings but rely on expert features
that have not been thoroughly evaluated on noisy recordings. Here we propose a
segmental convolutional neural network architecture that achieves automatic
feature learning from noisy heart sound recordings. Our experiments show that
our best model, trained on noisy recording segments acquired with an existing
hidden semi-markov model-based approach, attains a classification accuracy of
87.5% on the 2016 PhysioNet/CinC Challenge dataset, compared to the 84.6%
accuracy of the state-of-the-art statistical classifier trained and evaluated
on the same dataset. Our results indicate the potential of using neural
network-based methods to increase the accuracy of automated classification of
heart sound recordings for improved screening of heart diseases.
</summary>
    <author>
      <name>Yuhao Zhang</name>
    </author>
    <author>
      <name>Sandeep Ayyar</name>
    </author>
    <author>
      <name>Long-Huei Chen</name>
    </author>
    <author>
      <name>Ethan J. Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This work was finished in May 2016, and remains unpublished until
  December 2016 due to a request from the data provider</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.01943v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01943v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.02198v2</id>
    <updated>2016-12-13T11:14:30Z</updated>
    <published>2016-12-07T11:18:21Z</published>
    <title>Towards computer-assisted understanding of dynamics in symphonic music</title>
    <summary>  Many people enjoy classical symphonic music. Its diverse instrumentation
makes for a rich listening experience. This diversity adds to the conductor's
expressive freedom to shape the sound according to their imagination. As a
result, the same piece may sound quite differently from one conductor to
another. Differences in interpretation may be noticeable subjectively to
listeners, but they are sometimes hard to pinpoint, presumably because of the
acoustic complexity of the sound. We describe a computational model that
interprets dynamics---expressive loudness variations in performances---in terms
of the musical score, highlighting differences between performances of the same
piece. We demonstrate experimentally that the model has predictive power, and
give examples of conductor ideosyncrasies found by using the model as an
explanatory tool. Although the present model is still in active development, it
may pave the road for a consumer-oriented companion to interactive classical
music understanding.
</summary>
    <author>
      <name>Maarten Grachten</name>
    </author>
    <author>
      <name>Carlos Eduardo Cancino-Chacón</name>
    </author>
    <author>
      <name>Thassilo Gadermaier</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <link href="http://arxiv.org/abs/1612.02198v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.02198v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.03789v1</id>
    <updated>2016-12-12T17:06:19Z</updated>
    <published>2016-12-12T17:06:19Z</published>
    <title>A Unit Selection Methodology for Music Generation Using Deep Neural
  Networks</title>
    <summary>  Several methods exist for a computer to generate music based on data
including Markov chains, recurrent neural networks, recombinancy, and grammars.
We explore the use of unit selection and concatenation as a means of generating
music using a procedure based on ranking, where, we consider a unit to be a
variable length number of measures of music. We first examine whether a unit
selection method, that is restricted to a finite size unit library, can be
sufficient for encompassing a wide spectrum of music. We do this by developing
a deep autoencoder that encodes a musical input and reconstructs the input by
selecting from the library. We then describe a generative model that combines a
deep structured semantic model (DSSM) with an LSTM to predict the next unit,
where units consist of four, two, and one measures of music. We evaluate the
generative model using objective metrics including mean rank and accuracy and
with a subjective listening test in which expert musicians are asked to
complete a forced-choiced ranking task. We compare our model to a note-level
generative baseline that consists of a stacked LSTM trained to predict forward
by one note.
</summary>
    <author>
      <name>Mason Bretan</name>
    </author>
    <author>
      <name>Gil Weinberg</name>
    </author>
    <author>
      <name>Larry Heck</name>
    </author>
    <link href="http://arxiv.org/abs/1612.03789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.03789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04028v3</id>
    <updated>2017-04-29T20:31:18Z</updated>
    <published>2016-12-13T04:56:47Z</published>
    <title>Adaptive DCTNet for Audio Signal Classification</title>
    <summary>  In this paper, we investigate DCTNet for audio signal classification. Its
output feature is related to Cohen's class of time-frequency distributions. We
introduce the use of adaptive DCTNet (A-DCTNet) for audio signals feature
extraction. The A-DCTNet applies the idea of constant-Q transform, with its
center frequencies of filterbanks geometrically spaced. The A-DCTNet is
adaptive to different acoustic scales, and it can better capture low frequency
acoustic information that is sensitive to human audio perception than features
such as Mel-frequency spectral coefficients (MFSC). We use features extracted
by the A-DCTNet as input for classifiers. Experimental results show that the
A-DCTNet and Recurrent Neural Networks (RNN) achieve state-of-the-art
performance in bird song classification rate, and improve artist identification
accuracy in music data. They demonstrate A-DCTNet's applicability to signal
processing problems.
</summary>
    <author>
      <name>Yin Xian</name>
    </author>
    <author>
      <name>Yunchen Pu</name>
    </author>
    <author>
      <name>Zhe Gan</name>
    </author>
    <author>
      <name>Liang Lu</name>
    </author>
    <author>
      <name>Andrew Thompson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1121/1.4970932</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1121/1.4970932" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference of Acoustic and Speech Signal Processing
  (ICASSP). New Orleans, United States, March, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.04028v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04028v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04056v2</id>
    <updated>2017-01-19T15:33:53Z</updated>
    <published>2016-12-13T08:13:03Z</published>
    <title>Joint Bayesian Gaussian discriminant analysis for speaker verification</title>
    <summary>  State-of-the-art i-vector based speaker verification relies on variants of
Probabilistic Linear Discriminant Analysis (PLDA) for discriminant analysis. We
are mainly motivated by the recent work of the joint Bayesian (JB) method,
which is originally proposed for discriminant analysis in face verification. We
apply JB to speaker verification and make three contributions beyond the
original JB. 1) In contrast to the EM iterations with approximated statistics
in the original JB, the EM iterations with exact statistics are employed and
give better performance. 2) We propose to do simultaneous diagonalization (SD)
of the within-class and between-class covariance matrices to achieve efficient
testing, which has broader application scope than the SVD-based efficient
testing method in the original JB. 3) We scrutinize similarities and
differences between various Gaussian PLDAs and JB, complementing the previous
analysis of comparing JB only with Prince-Elder PLDA. Extensive experiments are
conducted on NIST SRE10 core condition 5, empirically validating the
superiority of JB with faster convergence rate and 9-13% EER reduction compared
with state-of-the-art PLDA.
</summary>
    <author>
      <name>Yiyan Wang</name>
    </author>
    <author>
      <name>Haotian Xu</name>
    </author>
    <author>
      <name>Zhijian Ou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by ICASSP2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.04056v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04056v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04742v4</id>
    <updated>2018-04-14T12:43:15Z</updated>
    <published>2016-12-14T17:33:38Z</published>
    <title>Imposing higher-level Structure in Polyphonic Music Generation using
  Convolutional Restricted Boltzmann Machines and Constraints</title>
    <summary>  We introduce a method for imposing higher-level structure on generated,
polyphonic music. A Convolutional Restricted Boltzmann Machine (C-RBM) as a
generative model is combined with gradient descent constraint optimisation to
provide further control over the generation process. Among other things, this
allows for the use of a "template" piece, from which some structural properties
can be extracted, and transferred as constraints to the newly generated
material. The sampling process is guided with Simulated Annealing to avoid
local optima, and to find solutions that both satisfy the constraints, and are
relatively stable with respect to the C-RBM. Results show that with this
approach it is possible to control the higher-level self-similarity structure,
the meter, and the tonal properties of the resulting musical piece, while
preserving its local musical coherence.
</summary>
    <author>
      <name>Stefan Lattner</name>
    </author>
    <author>
      <name>Maarten Grachten</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5920/jcms.2018.01</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5920/jcms.2018.01" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">31 pages, 11 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Creative Music Systems, Volume 2, Issue 1, March 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1612.04742v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04742v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04928v1</id>
    <updated>2016-12-15T05:06:40Z</updated>
    <published>2016-12-15T05:06:40Z</published>
    <title>Music Generation with Deep Learning</title>
    <summary>  The use of deep learning to solve problems in literary arts has been a recent
trend that has gained a lot of attention and automated generation of music has
been an active area. This project deals with the generation of music using raw
audio files in the frequency domain relying on various LSTM architectures.
Fully connected and convolutional layers are used along with LSTM's to capture
rich features in the frequency domain and increase the quality of music
generated. The work is focused on unconstrained music generation and uses no
information about musical structure(notes or chords) to aid learning.The music
generated from various architectures are compared using blind fold tests. Using
the raw audio to train models is the direction to tapping the enormous amount
of mp3 files that exist over the internet without requiring the manual effort
to make structured MIDI files. Moreover, not all audio files can be represented
with MIDI files making the study of these models an interesting prospect to the
future of such models.
</summary>
    <author>
      <name>Vasanth Kalingeri</name>
    </author>
    <author>
      <name>Srikanth Grandhe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.04928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05065v1</id>
    <updated>2016-12-15T14:01:50Z</updated>
    <published>2016-12-15T14:01:50Z</published>
    <title>Feature Learning for Chord Recognition: The Deep Chroma Extractor</title>
    <summary>  We explore frame-level audio feature learning for chord recognition using
artificial neural networks. We present the argument that chroma vectors
potentially hold enough information to model harmonic content of audio for
chord recognition, but that standard chroma extractors compute too noisy
features. This leads us to propose a learned chroma feature extractor based on
artificial neural networks. It is trained to compute chroma features that
encode harmonic information important for chord recognition, while being robust
to irrelevant interferences. We achieve this by feeding the network an audio
spectrum with context instead of a single frame as input. This way, the network
can learn to selectively compensate noise and resolve harmonic ambiguities.
  We compare the resulting features to hand-crafted ones by using a simple
linear frame-wise classifier for chord recognition on various data sets. The
results show that the learned feature extractor produces superior chroma
vectors for chord recognition.
</summary>
    <author>
      <name>Filip Korzeniowski</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 17th International Society for Music
  Information Retrieval Conference (ISMIR), New York, USA, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05153v1</id>
    <updated>2016-12-15T17:32:11Z</updated>
    <published>2016-12-15T17:32:11Z</published>
    <title>On the Potential of Simple Framewise Approaches to Piano Transcription</title>
    <summary>  In an attempt at exploring the limitations of simple approaches to the task
of piano transcription (as usually defined in MIR), we conduct an in-depth
analysis of neural network-based framewise transcription. We systematically
compare different popular input representations for transcription systems to
determine the ones most suitable for use with neural networks. Exploiting
recent advances in training techniques and new regularizers, and taking into
account hyper-parameter tuning, we show that it is possible, by simple
bottom-up frame-wise processing, to obtain a piano transcriber that outperforms
the current published state of the art on the publicly available MAPS dataset
-- without any complex post-processing steps. Thus, we propose this simple
approach as a new baseline for this dataset, for future transcription research
to build on and improve.
</summary>
    <author>
      <name>Rainer Kelz</name>
    </author>
    <author>
      <name>Matthias Dorfer</name>
    </author>
    <author>
      <name>Filip Korzeniowski</name>
    </author>
    <author>
      <name>Sebastian Böck</name>
    </author>
    <author>
      <name>Andreas Arzt</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 17th International Society for Music Information
  Retrieval Conference (ISMIR 2016), New York, NY</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05156v2</id>
    <updated>2017-09-06T11:35:59Z</updated>
    <published>2016-12-15T19:43:54Z</published>
    <title>A Phase Vocoder based on Nonstationary Gabor Frames</title>
    <summary>  We propose a new algorithm for time stretching music signals based on the
theory of nonstationary Gabor frames (NSGFs). The algorithm extends the
techniques of the classical phase vocoder (PV) by incorporating adaptive
time-frequency (TF) representations and adaptive phase locking. The adaptive TF
representations imply good time resolution for the onsets of attack transients
and good frequency resolution for the sinusoidal components. We estimate the
phase values only at peak channels and the remaining phases are then locked to
the values of the peaks in an adaptive manner. During attack transients we keep
the stretch factor equal to one and we propose a new strategy for determining
which channels are relevant for reinitializing the corresponding phase values.
In contrast to previously published algorithms we use a non-uniform NSGF to
obtain a low redundancy of the corresponding TF representation. We show that
with just three times as many TF coefficients as signal samples, artifacts such
as phasiness and transient smearing can be greatly reduced compared to the
classical PV. The proposed algorithm is tested on both synthetic and real world
signals and compared with state of the art algorithms in a reproducible manner.
</summary>
    <author>
      <name>Emil Solsbæk Ottosen</name>
    </author>
    <author>
      <name>Monika Dörfler</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2017.2750767</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2017.2750767" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05156v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05156v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05369v2</id>
    <updated>2017-03-31T20:17:12Z</updated>
    <published>2016-12-16T05:09:14Z</published>
    <title>Neural networks based EEG-Speech Models</title>
    <summary>  In this paper, we propose an end-to-end neural network (NN) based EEG-speech
(NES) modeling framework, in which three network structures are developed to
map imagined EEG signals to phonemes. The proposed NES models incorporate a
language model based EEG feature extraction layer, an acoustic feature mapping
layer, and a restricted Boltzmann machine (RBM) based the feature learning
layer. The NES models can jointly realize the representation of multichannel
EEG signals and the projection of acoustic speech signals. Among three proposed
NES models, two augmented networks utilize spoken EEG signals as either bias or
gate information to strengthen the feature learning and translation of imagined
EEG signals. Experimental results show that all three proposed NES models
outperform the baseline support vector machine (SVM) method on EEG-speech
classification. With respect to binary classification, our approach achieves
comparable results relative to deep believe network approach.
</summary>
    <author>
      <name>Pengfei Sun</name>
    </author>
    <author>
      <name>Jun Qin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05369v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05369v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05432v1</id>
    <updated>2016-12-16T11:05:52Z</updated>
    <published>2016-12-16T11:05:52Z</published>
    <title>Basis-Function Modeling of Loudness Variations in Ensemble Performance</title>
    <summary>  This paper describes a computational model of loudness variations in
expressive ensemble performance. The model predicts and explains the continuous
variation of loudness as a function of information extracted automatically from
the written score. Although such models have been proposed for expressive
performance in solo instruments, this is (to the best of our knowledge) the
first attempt to define a model for expressive performance in ensembles. To
that end, we extend an existing model that was designed to model expressive
piano performances, and describe the additional steps necessary for the model
to deal with scores of arbitrary instrumentation, including orchestral scores.
We test both linear and non-linear variants of the extended model n a data set
of audio recordings of symphonic music, in a leave-one-out setting. The
experiments reveal that the most successful model variant is a recurrent,
non-linear model. Even if the accuracy of the predicted loudness varies from
one recording to another, in several cases the model explains well over 50% of
the variance in loudness.
</summary>
    <author>
      <name>Thassilo Gadermaier</name>
    </author>
    <author>
      <name>Maarten Grachten</name>
    </author>
    <author>
      <name>Carlos Eduardo Cancino Chacón</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 3 figures, 2 tables. Originally in 2nd International
  Conference on New Music Concepts (ICNMC 2016), Treviso, Italy. This version
  may have a different layout</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.05432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05489v1</id>
    <updated>2016-12-16T14:40:43Z</updated>
    <published>2016-12-16T14:40:43Z</published>
    <title>On-bird Sound Recordings: Automatic Acoustic Recognition of Activities
  and Contexts</title>
    <summary>  We introduce a novel approach to studying animal behaviour and the context in
which it occurs, through the use of microphone backpacks carried on the backs
of individual free-flying birds. These sensors are increasingly used by animal
behaviour researchers to study individual vocalisations of freely behaving
animals, even in the field. However such devices may record more than an
animals vocal behaviour, and have the potential to be used for investigating
specific activities (movement) and context (background) within which
vocalisations occur. To facilitate this approach, we investigate the automatic
annotation of such recordings through two different sound scene analysis
paradigms: a scene-classification method using feature learning, and an
event-detection method using probabilistic latent component analysis (PLCA). We
analyse recordings made with Eurasian jackdaws (Corvus monedula) in both
captive and field settings. Results are comparable with the state of the art in
sound scene analysis; we find that the current recognition quality level
enables scalable automatic annotation of audio logger data, given partial
annotation, but also find that individual differences between animals and/or
their backpacks limit the generalisation from one individual to another. we
consider the interrelation of 'scenes' and 'events' in this particular task,
and issues of temporal resolution.
</summary>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Emmanouil Benetos</name>
    </author>
    <author>
      <name>Lisa F. Gill</name>
    </author>
    <link href="http://arxiv.org/abs/1612.05489v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05489v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.06151v3</id>
    <updated>2017-03-09T08:47:38Z</updated>
    <published>2016-12-19T12:19:25Z</published>
    <title>HRTF-based two-dimensional robust least-squares frequency-invariant
  beamformer design for robot audition</title>
    <summary>  In this work, we propose a two-dimensional Head-Related Transfer Function
(HRTF)-based robust beamformer design for robot audition, which allows for
explicit control of the beamformer response for the entire three-dimensional
sound field surrounding a humanoid robot. We evaluate the proposed method by
means of both signal-independent and signal-dependent measures in a robot
audition scenario. Our results confirm the effectiveness of the proposed
two-dimensional HRTF-based beamformer design, compared to our previously
published one-dimensional HRTF-based beamformer design, which was carried out
for a fixed elevation angle only.
</summary>
    <author>
      <name>Hendrik Barfuss</name>
    </author>
    <author>
      <name>Michael Buerger</name>
    </author>
    <author>
      <name>Jasper Podschus</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Joint Workshop on Hands-free Speech Communication and Microphone
  Arrays (HSCMA), March 2017, San Francisco, CA, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.06151v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06151v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.06287v1</id>
    <updated>2016-12-14T15:40:44Z</updated>
    <published>2016-12-14T15:40:44Z</published>
    <title>VAST : The Virtual Acoustic Space Traveler Dataset</title>
    <summary>  This paper introduces a new paradigm for sound source lo-calization referred
to as virtual acoustic space traveling (VAST) and presents a first dataset
designed for this purpose. Existing sound source localization methods are
either based on an approximate physical model (physics-driven) or on a
specific-purpose calibration set (data-driven). With VAST, the idea is to learn
a mapping from audio features to desired audio properties using a massive
dataset of simulated room impulse responses. This virtual dataset is designed
to be maximally representative of the potential audio scenes that the
considered system may be evolving in, while remaining reasonably compact. We
show that virtually-learned mappings on this dataset generalize to real data,
overcoming some intrinsic limitations of traditional binaural sound
localization methods based on time differences of arrival.
</summary>
    <author>
      <name>Clément Gaultier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA</arxiv:affiliation>
    </author>
    <author>
      <name>Saurabh Kataria</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA, IIT Kanpur</arxiv:affiliation>
    </author>
    <author>
      <name>Antoine Deleforge</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PANAMA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Latent Variable Analysis and Signal
  Separation (LVA/ICA), Feb 2017, Grenoble, France. International Conference on
  Latent Variable Analysis and Signal Separation</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.06287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.06642v1</id>
    <updated>2016-12-20T13:04:33Z</updated>
    <published>2016-12-20T13:04:33Z</published>
    <title>Efficient Target Activity Detection based on Recurrent Neural Networks</title>
    <summary>  This paper addresses the problem of Target Activity Detection (TAD) for
binaural listening devices. TAD denotes the problem of robustly detecting the
activity of a target speaker in a harsh acoustic environment, which comprises
interfering speakers and noise (cocktail party scenario). In previous work, it
has been shown that employing a Feed-forward Neural Network (FNN) for detecting
the target speaker activity is a promising approach to combine the advantage of
different TAD features (used as network inputs). In this contribution, we
exploit a larger context window for TAD and compare the performance of FNNs and
Recurrent Neural Networks (RNNs) with an explicit focus on small network
topologies as desirable for embedded acoustic signal processing systems. More
specifically, the investigations include a comparison between three different
types of RNNs, namely plain RNNs, Long Short-Term Memories, and Gated Recurrent
Units. The results indicate that all versions of RNNs outperform FNNs for the
task of TAD.
</summary>
    <author>
      <name>Daniel Gerber</name>
    </author>
    <author>
      <name>Stefan Meier</name>
    </author>
    <author>
      <name>Walter Kellermann</name>
    </author>
    <link href="http://arxiv.org/abs/1612.06642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.07523v1</id>
    <updated>2016-12-22T10:14:59Z</updated>
    <published>2016-12-22T10:14:59Z</published>
    <title>Robustness of Voice Conversion Techniques Under Mismatched Conditions</title>
    <summary>  Most of the existing studies on voice conversion (VC) are conducted in
acoustically matched conditions between source and target signal. However, the
robustness of VC methods in presence of mismatch remains unknown. In this
paper, we report a comparative analysis of different VC techniques under
mismatched conditions. The extensive experiments with five different VC
techniques on CMU ARCTIC corpus suggest that performance of VC methods
substantially degrades in noisy conditions. We have found that bilinear
frequency warping with amplitude scaling (BLFWAS) outperforms other methods in
most of the noisy conditions. We further explore the suitability of different
speech enhancement techniques for robust conversion. The objective evaluation
results indicate that spectral subtraction and log minimum mean square error
(logMMSE) based speech enhancement techniques can be used to improve the
performance in specific noisy conditions.
</summary>
    <author>
      <name>Monisankha Pal</name>
    </author>
    <author>
      <name>Dipjyoti Paul</name>
    </author>
    <author>
      <name>Md Sahidullah</name>
    </author>
    <author>
      <name>Goutam Saha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.07523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.07523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.07837v2</id>
    <updated>2017-02-11T20:04:46Z</updated>
    <published>2016-12-22T23:28:47Z</published>
    <title>SampleRNN: An Unconditional End-to-End Neural Audio Generation Model</title>
    <summary>  In this paper we propose a novel model for unconditional audio generation
based on generating one audio sample at a time. We show that our model, which
profits from combining memory-less modules, namely autoregressive multilayer
perceptrons, and stateful recurrent neural networks in a hierarchical structure
is able to capture underlying sources of variations in the temporal sequences
over very long time spans, on three datasets of different nature. Human
evaluation on the generated samples indicate that our model is preferred over
competing models. We also show how each component of the model contributes to
the exhibited performance.
</summary>
    <author>
      <name>Soroush Mehri</name>
    </author>
    <author>
      <name>Kundan Kumar</name>
    </author>
    <author>
      <name>Ishaan Gulrajani</name>
    </author>
    <author>
      <name>Rithesh Kumar</name>
    </author>
    <author>
      <name>Shubham Jain</name>
    </author>
    <author>
      <name>Jose Sotelo</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.07837v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.07837v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.09150v2</id>
    <updated>2016-12-30T08:46:05Z</updated>
    <published>2016-12-29T14:05:56Z</published>
    <title>Phase-incorporating Speech Enhancement Based on Complex-valued Gaussian
  Process Latent Variable Model</title>
    <summary>  Traditional speech enhancement techniques modify the magnitude of a speech in
time-frequency domain, and use the phase of a noisy speech to resynthesize a
time domain speech. This work proposes a complex-valued Gaussian process latent
variable model (CGPLVM) to enhance directly the complex-valued noisy spectrum,
modifying not only the magnitude but also the phase. The main idea that
underlies the developed method is the modeling of short-time Fourier transform
(STFT) coefficients across the time frames of a speech as a proper complex
Gaussian process (GP) with noise added. The proposed method is based on
projecting the spectrum into a low-dimensional subspace. The likelihood
criterion is used to optimize the hyperparameters of the model. Experiments
were carried out on the CHTTL database, which contains the digits zero to nine
in Mandarin. Several standard measures are used to demonstrate that the
proposed method outperforms baseline methods.
</summary>
    <author>
      <name>Sih-Huei Chen</name>
    </author>
    <author>
      <name>Yuan-Shan Lee</name>
    </author>
    <author>
      <name>Jia-Ching Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">add author name</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.09150v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.09150v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.06078v2</id>
    <updated>2017-01-24T16:25:15Z</updated>
    <published>2017-01-21T20:15:08Z</published>
    <title>Lyrics-to-Audio Alignment by Unsupervised Discovery of Repetitive
  Patterns in Vowel Acoustics</title>
    <summary>  Most of the previous approaches to lyrics-to-audio alignment used a
pre-developed automatic speech recognition (ASR) system that innately suffered
from several difficulties to adapt the speech model to individual singers. A
significant aspect missing in previous works is the self-learnability of
repetitive vowel patterns in the singing voice, where the vowel part used is
more consistent than the consonant part. Based on this, our system first learns
a discriminative subspace of vowel sequences, based on weighted symmetric
non-negative matrix factorization (WS-NMF), by taking the self-similarity of a
standard acoustic feature as an input. Then, we make use of canonical time
warping (CTW), derived from a recent computer vision technique, to find an
optimal spatiotemporal transformation between the text and the acoustic
sequences. Experiments with Korean and English data sets showed that deploying
this method after a pre-developed, unsupervised, singing source separation
achieved more promising results than other state-of-the-art unsupervised
approaches and an existing ASR-based system.
</summary>
    <author>
      <name>Sungkyun Chang</name>
    </author>
    <author>
      <name>Kyogu Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2017.2738558</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCESS.2017.2738558" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Access, Vol. 5, (2017) 16635-16648</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.06078v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.06078v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00025v1</id>
    <updated>2017-01-31T19:21:41Z</updated>
    <published>2017-01-31T19:21:41Z</published>
    <title>An Experimental Analysis of the Entanglement Problem in
  Neural-Network-based Music Transcription Systems</title>
    <summary>  Several recent polyphonic music transcription systems have utilized deep
neural networks to achieve state of the art results on various benchmark
datasets, pushing the envelope on framewise and note-level performance
measures. Unfortunately we can observe a sort of glass ceiling effect. To
investigate this effect, we provide a detailed analysis of the particular kinds
of errors that state of the art deep neural transcription systems make, when
trained and tested on a piano transcription task. We are ultimately forced to
draw a rather disheartening conclusion: the networks seem to learn combinations
of notes, and have a hard time generalizing to unseen combinations of notes.
Furthermore, we speculate on various means to alleviate this situation.
</summary>
    <author>
      <name>Rainer Kelz</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to AES Conference on Semantic Audio, Erlangen, Germany,
  2017 June 22, 24</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.00025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00178v2</id>
    <updated>2017-03-31T11:24:42Z</updated>
    <published>2017-02-01T09:44:44Z</published>
    <title>On the Futility of Learning Complex Frame-Level Language Models for
  Chord Recognition</title>
    <summary>  Chord recognition systems use temporal models to post-process frame-wise
chord preditions from acoustic models. Traditionally, first-order models such
as Hidden Markov Models were used for this task, with recent works suggesting
to apply Recurrent Neural Networks instead. Due to their ability to learn
longer-term dependencies, these models are supposed to learn and to apply
musical knowledge, instead of just smoothing the output of the acoustic model.
In this paper, we argue that learning complex temporal models at the level of
audio frames is futile on principle, and that non-Markovian models do not
perform better than their first-order counterparts. We support our argument
through three experiments on the McGill Billboard dataset. The first two show
1) that when learning complex temporal models at the frame level, improvements
in chord sequence modelling are marginal; and 2) that these improvements do not
translate when applied within a full chord recognition system. The third, still
rather preliminary experiment gives first indications that the use of complex
sequential models for chord prediction at higher temporal levels might be more
promising.
</summary>
    <author>
      <name>Filip Korzeniowski</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.17743/aesconf.2017.978-1-942220-15-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.17743/aesconf.2017.978-1-942220-15-2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at AES Conference on Semantic Audio 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.00178v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00178v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00956v2</id>
    <updated>2017-02-06T03:37:28Z</updated>
    <published>2017-02-03T10:15:29Z</published>
    <title>KU-ISPL Speaker Recognition Systems under Language mismatch condition
  for NIST 2016 Speaker Recognition Evaluation</title>
    <summary>  Korea University Intelligent Signal Processing Lab. (KU-ISPL) developed
speaker recognition system for SRE16 fixed training condition. Data for
evaluation trials are collected from outside North America, spoken in Tagalog
and Cantonese while training data only is spoken English. Thus, main issue for
SRE16 is compensating the discrepancy between different languages. As
development dataset which is spoken in Cebuano and Mandarin, we could prepare
the evaluation trials through preliminary experiments to compensate the
language mismatched condition. Our team developed 4 different approaches to
extract i-vectors and applied state-of-the-art techniques as backend. To
compensate language mismatch, we investigated and endeavored unique method such
as unsupervised language clustering, inter language variability compensation
and gender/language dependent score normalization.
</summary>
    <author>
      <name>Suwon Shon</name>
    </author>
    <author>
      <name>Hanseok Ko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SRE16, NIST SRE 2016 system description</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.00956v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00956v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.01999v1</id>
    <updated>2017-02-07T13:19:08Z</updated>
    <published>2017-02-07T13:19:08Z</published>
    <title>Identification of Voice Utterance with Aging Factor Using the Method of
  MFCC Multichannel</title>
    <summary>  This research was conducted to develop a method to identify voice utterance.
For voice utterance that encounters change caused by aging factor, with the
interval of 10 to 25 years. The change of voice utterance influenced by aging
factor might be extracted by MFCC (Mel Frequency Cepstrum Coefficient).
However, the level of the compatibility of the feature may be dropped down to
55%. While the ones which do not encounter it may reach 95%. To improve the
compatibility of the changing voice feature influenced by aging factor, then
the method of the more specific feature extraction is developed: which is by
separating the voice into several channels, suggested as MFCC multichannel,
consisting of multichannel 5 filterbank (M5FB), multichannel 2 filterbank
(M2FB) and multichannel 1 filterbank (M1FB). The result of the test shows that
for model M5FB and M2FB have the highest score in the level of compatibility
with 85% and 82% with 25 years interval. While model M5FB gets the highest
score of 86% for 10 years time interval.
</summary>
    <author>
      <name>Roy Rudolf Huizen</name>
    </author>
    <author>
      <name>Jazi Eko Istiyanto</name>
    </author>
    <author>
      <name>Agfianto Eko Putra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Huizen, R.R., Istiyanto, J.E. and Putra, A.E., 2017, International
  Journal of Advanced Studies in Computer Science and Engineering (IJASCSE),
  Volume 6 Issue 01</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1702.01999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02130v2</id>
    <updated>2017-04-11T12:23:37Z</updated>
    <published>2017-02-07T18:41:31Z</published>
    <title>On the Importance of Temporal Context in Proximity Kernels: A Vocal
  Separation Case Study</title>
    <summary>  Musical source separation methods exploit source-specific spectral
characteristics to facilitate the decomposition process. Kernel Additive
Modelling (KAM) models a source applying robust statistics to time-frequency
bins as specified by a source-specific kernel, a function defining similarity
between bins. Kernels in existing approaches are typically defined using
metrics between single time frames. In the presence of noise and other sound
sources information from a single-frame, however, turns out to be unreliable
and often incorrect frames are selected as similar. In this paper, we
incorporate a temporal context into the kernel to provide additional
information stabilizing the similarity search. Evaluated in the context of
vocal separation, our simple extension led to a considerable improvement in
separation quality compared to previous kernels.
</summary>
    <author>
      <name>Delia Fano Yela</name>
    </author>
    <author>
      <name>Sebastian Ewert</name>
    </author>
    <author>
      <name>Derry FitzGerald</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2017 AES International Conference on Semantic Audio</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the AES International Conference on Semantic Audio,
  Erlangen, Germany, pp. 13-20, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1702.02130v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02130v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03791v1</id>
    <updated>2017-02-13T14:44:17Z</updated>
    <published>2017-02-13T14:44:17Z</published>
    <title>DNN Filter Bank Cepstral Coefficients for Spoofing Detection</title>
    <summary>  With the development of speech synthesis techniques, automatic speaker
verification systems face the serious challenge of spoofing attack. In order to
improve the reliability of speaker verification systems, we develop a new
filter bank based cepstral feature, deep neural network filter bank cepstral
coefficients (DNN-FBCC), to distinguish between natural and spoofed speech. The
deep neural network filter bank is automatically generated by training a filter
bank neural network (FBNN) using natural and synthetic speech. By adding
restrictions on the training rules, the learned weight matrix of FBNN is
band-limited and sorted by frequency, similar to the normal filter bank. Unlike
the manually designed filter bank, the learned filter bank has different filter
shapes in different channels, which can capture the differences between natural
and synthetic speech more effectively. The experimental results on the ASVspoof
{2015} database show that the Gaussian mixture model maximum-likelihood
(GMM-ML) classifier trained by the new feature performs better than the
state-of-the-art linear frequency cepstral coefficients (LFCC) based
classifier, especially on detecting unknown attacks.
</summary>
    <author>
      <name>Hong Yu</name>
    </author>
    <author>
      <name>Zheng-Hua Tan</name>
    </author>
    <author>
      <name>Zhanyu Ma</name>
    </author>
    <author>
      <name>Jun Guo</name>
    </author>
    <link href="http://arxiv.org/abs/1702.03791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00384v1</id>
    <updated>2017-02-28T11:00:19Z</updated>
    <published>2017-02-28T11:00:19Z</published>
    <title>Nonlinear Volterra model of a loudspeaker behavior based on Laser
  Doppler Vibrometry</title>
    <summary>  We demonstrate the capabilities of nonlinear Volterra models to simulate the
behavior of an audio system and compare them to linear filters. In this paper a
nonlinear model of an audio system based on Volterra series is presented and
Normalized Least Mean Square algorithm is used to determine the Volterra series
to third order. Training data for the models were collected measuring a
physical speaker using a laser interferometer. We explore several training
signals and filter's parameters. Results indicate a decrease in Mean Squared
Error compared to the linear model with a dependency on the particular test
signal, the order and the parameters of the model.
</summary>
    <author>
      <name>Alessandro Loriga</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Intranet Standard GmbH, Munich, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Parvin Moyassari</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Intranet Standard GmbH, Munich, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Daniele Bernardini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Intranet Standard GmbH, Munich, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Gregorio Landi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dipartimento di Fisica, Universita di Firenze, Italy</arxiv:affiliation>
    </author>
    <author>
      <name>Francesca Venturini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute of Applied Mathematics and Physics, Zurich University of Applied Sciences, Winterthur, Switzerland</arxiv:affiliation>
    </author>
    <author>
      <name>Elisabeth Dumont</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Institute of Applied Mathematics and Physics, Zurich University of Applied Sciences, Winterthur, Switzerland</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1703.00384v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00384v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; I.2.6; I.6.3; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01789v2</id>
    <updated>2017-05-22T04:46:36Z</updated>
    <published>2017-03-06T09:49:48Z</published>
    <title>Sample-level Deep Convolutional Neural Networks for Music Auto-tagging
  Using Raw Waveforms</title>
    <summary>  Recently, the end-to-end approach that learns hierarchical representations
from raw data using deep convolutional neural networks has been successfully
explored in the image, text and speech domains. This approach was applied to
musical signals as well but has been not fully explored yet. To this end, we
propose sample-level deep convolutional neural networks which learn
representations from very small grains of waveforms (e.g. 2 or 3 samples)
beyond typical frame-level input representations. Our experiments show how deep
architectures with sample-level filters improve the accuracy in music
auto-tagging and they provide results comparable to previous state-of-the-art
performances for the Magnatagatune dataset and Million Song Dataset. In
addition, we visualize filters learned in a sample-level DCNN in each layer to
identify hierarchically learned features and show that they are sensitive to
log-scaled frequency along layer, such as mel-frequency spectrogram that is
widely used in music classification systems.
</summary>
    <author>
      <name>Jongpil Lee</name>
    </author>
    <author>
      <name>Jiyoung Park</name>
    </author>
    <author>
      <name>Keunhyoung Luke Kim</name>
    </author>
    <author>
      <name>Juhan Nam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, Sound and Music Computing Conference (SMC), 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.01789v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01789v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.02318v1</id>
    <updated>2017-03-07T10:36:50Z</updated>
    <published>2017-03-07T10:36:50Z</published>
    <title>Linear and Circular Microphone Array for Remote Surveillance: Simulated
  Performance Analysis</title>
    <summary>  Acoustic beamforming with a microphone array represents an adequate
technology for remote acoustic surveillance, as the system has no mechanical
parts and it has moderate size. However, in order to accomplish real
implementation, several challenges need to be addressed, such as array
geometry, microphone characteristics, and the digital beamforming algorithms.
This paper presents a simulated analysis on the effect of the array geometry in
the beamforming response. Two geometries are considered, namely, the linear and
the circular geometry. The analysis is performed with computer simulations to
mimic reality. The future steps comprise the construction of the physical
microphone array, and the software implementation on a multichannel digital
signal processing (DSP) system.
</summary>
    <author>
      <name>Abdulla AlShehhi</name>
    </author>
    <author>
      <name>M. Luai Hammadih</name>
    </author>
    <author>
      <name>M. Sami Zitouni</name>
    </author>
    <author>
      <name>Saif AlKindi</name>
    </author>
    <author>
      <name>Nazar Ali</name>
    </author>
    <author>
      <name>Luis Weruaga</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BCS International IT Conference 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.02318v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.02318v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04770v2</id>
    <updated>2017-06-05T12:41:28Z</updated>
    <published>2017-03-14T22:17:49Z</published>
    <title>Audio Scene Classification with Deep Recurrent Neural Networks</title>
    <summary>  We introduce in this work an efficient approach for audio scene
classification using deep recurrent neural networks. An audio scene is firstly
transformed into a sequence of high-level label tree embedding feature vectors.
The vector sequence is then divided into multiple subsequences on which a deep
GRU-based recurrent neural network is trained for sequence-to-label
classification. The global predicted label for the entire sequence is finally
obtained via aggregation of subsequence classification outputs. We will show
that our approach obtains an F1-score of 97.7% on the LITIS Rouen dataset,
which is the largest dataset publicly available for the task. Compared to the
best previously reported result on the dataset, our approach is able to reduce
the relative classification error by 35.3%.
</summary>
    <author>
      <name>Huy Phan</name>
    </author>
    <author>
      <name>Philipp Koch</name>
    </author>
    <author>
      <name>Fabrice Katzberg</name>
    </author>
    <author>
      <name>Marco Maass</name>
    </author>
    <author>
      <name>Radoslaw Mazur</name>
    </author>
    <author>
      <name>Alfred Mertins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for Interspeech 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.04770v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04770v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04783v1</id>
    <updated>2017-03-14T22:28:51Z</updated>
    <published>2017-03-14T22:28:51Z</published>
    <title>Multichannel End-to-end Speech Recognition</title>
    <summary>  The field of speech recognition is in the midst of a paradigm shift:
end-to-end neural networks are challenging the dominance of hidden Markov
models as a core technology. Using an attention mechanism in a recurrent
encoder-decoder architecture solves the dynamic time alignment problem,
allowing joint end-to-end training of the acoustic and language modeling
components. In this paper we extend the end-to-end framework to encompass
microphone array signal processing for noise suppression and speech enhancement
within the acoustic encoding network. This allows the beamforming components to
be optimized jointly within the recognition architecture to improve the
end-to-end speech recognition objective. Experiments on the noisy speech
benchmarks (CHiME-4 and AMI) show that our multichannel end-to-end system
outperformed the attention-based baseline with input from a conventional
adaptive beamformer.
</summary>
    <author>
      <name>Tsubasa Ochiai</name>
    </author>
    <author>
      <name>Shinji Watanabe</name>
    </author>
    <author>
      <name>Takaaki Hori</name>
    </author>
    <author>
      <name>John R. Hershey</name>
    </author>
    <link href="http://arxiv.org/abs/1703.04783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05003v2</id>
    <updated>2018-01-16T11:16:40Z</updated>
    <published>2017-03-15T08:33:38Z</published>
    <title>On the Importance of Super-Gaussian Speech Priors for Machine-Learning
  Based Speech Enhancement</title>
    <summary>  For enhancing noisy signals, machine-learning based single-channel speech
enhancement schemes exploit prior knowledge about typical speech spectral
structures. To ensure a good generalization and to meet requirements in terms
of computational complexity and memory consumption, certain methods restrict
themselves to learning speech spectral envelopes. We refer to these approaches
as machine-learning spectral envelope (MLSE)-based approaches.
  In this paper we show by means of theoretical and experimental analyses that
for MLSE-based approaches, super-Gaussian priors allow for a reduction of noise
between speech spectral harmonics which is not achievable using Gaussian
estimators such as the Wiener filter. For the evaluation, we use a deep neural
network (DNN)-based phoneme classifier and a low-rank nonnegative matrix
factorization (NMF) framework as examples of MLSE-based approaches. A listening
experiment and instrumental measures confirm that while super-Gaussian priors
yield only moderate improvements for classic enhancement schemes, for
MLSE-based approaches super-Gaussian priors clearly make an important
difference and significantly outperform Gaussian priors.
</summary>
    <author>
      <name>Robert Rehr</name>
    </author>
    <author>
      <name>Timo Gerkmann</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2017.2778151</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2017.2778151" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE/ACM Transactions on Audio, Speech, and Language Processing,
  vol. 26, no. 2, pp. 357-366, Feb. 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1703.05003v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05003v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05344v1</id>
    <updated>2017-03-15T18:41:37Z</updated>
    <published>2017-03-15T18:41:37Z</published>
    <title>Deducing the severity of psychiatric symptoms from the human voice</title>
    <summary>  Psychiatric illnesses are often associated with multiple symptoms, whose
severity must be graded for accurate diagnosis and treatment. This grading is
usually done by trained clinicians based on human observations and judgments
made within doctor-patient sessions. Current research provides sufficient
reason to expect that the human voice may carry biomarkers or signatures of
many, if not all, these symptoms. Based on this conjecture, we explore the
possibility of objectively and automatically grading the symptoms of
psychiatric illnesses with reference to various standard psychiatric rating
scales. Using acoustic data from several clinician-patient interviews within
hospital settings, we use non-parametric models to learn and predict the
relations between symptom-ratings and voice. In the process, we show that
different articulatory-phonetic units of speech are able to capture the effects
of different symptoms differently, and use this to establish a plausible
methodology that could be employed for automatically grading psychiatric
symptoms for clinical purposes.
</summary>
    <author>
      <name>Rita Singh</name>
    </author>
    <author>
      <name>Justin Baker</name>
    </author>
    <author>
      <name>Luciana Pennant</name>
    </author>
    <author>
      <name>Louis-Philippe Morency</name>
    </author>
    <link href="http://arxiv.org/abs/1703.05344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06052v1</id>
    <updated>2017-03-17T15:31:58Z</updated>
    <published>2017-03-17T15:31:58Z</published>
    <title>Attention and Localization based on a Deep Convolutional Recurrent Model
  for Weakly Supervised Audio Tagging</title>
    <summary>  Audio tagging aims to perform multi-label classification on audio chunks and
it is a newly proposed task in the Detection and Classification of Acoustic
Scenes and Events 2016 (DCASE 2016) challenge. This task encourages research
efforts to better analyze and understand the content of the huge amounts of
audio data on the web. The difficulty in audio tagging is that it only has a
chunk-level label without a frame-level label. This paper presents a weakly
supervised method to not only predict the tags but also indicate the temporal
locations of the occurred acoustic events. The attention scheme is found to be
effective in identifying the important frames while ignoring the unrelated
frames. The proposed framework is a deep convolutional recurrent model with two
auxiliary modules: an attention module and a localization module. The proposed
algorithm was evaluated on the Task 4 of DCASE 2016 challenge. State-of-the-art
performance was achieved on the evaluation set with equal error rate (EER)
reduced from 0.13 to 0.11, compared with the convolutional recurrent baseline
system.
</summary>
    <author>
      <name>Yong Xu</name>
    </author>
    <author>
      <name>Qiuqiang Kong</name>
    </author>
    <author>
      <name>Qiang Huang</name>
    </author>
    <author>
      <name>Wenwu Wang</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, submitted to interspeech2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.06052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06697v2</id>
    <updated>2017-06-02T11:10:47Z</updated>
    <published>2017-03-20T12:00:04Z</published>
    <title>Timbre Analysis of Music Audio Signals with Convolutional Neural
  Networks</title>
    <summary>  The focus of this work is to study how to efficiently tailor Convolutional
Neural Networks (CNNs) towards learning timbre representations from log-mel
magnitude spectrograms. We first review the trends when designing CNN
architectures. Through this literature overview we discuss which are the
crucial points to consider for efficiently learning timbre representations
using CNNs. From this discussion we propose a design strategy meant to capture
the relevant time-frequency contexts for learning timbre, which permits using
domain knowledge for designing architectures. In addition, one of our main
goals is to design efficient CNN architectures -- what reduces the risk of
these models to over-fit, since CNNs' number of parameters is minimized.
Several architectures based on the design principles we propose are
successfully assessed for different research tasks related to timbre: singing
voice phoneme classification, musical instrument recognition and music
auto-tagging.
</summary>
    <author>
      <name>Jordi Pons</name>
    </author>
    <author>
      <name>Olga Slizovskaia</name>
    </author>
    <author>
      <name>Rong Gong</name>
    </author>
    <author>
      <name>Emilia Gómez</name>
    </author>
    <author>
      <name>Xavier Serra</name>
    </author>
    <link href="http://arxiv.org/abs/1703.06697v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06697v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06812v3</id>
    <updated>2018-07-04T11:30:51Z</updated>
    <published>2017-03-20T15:58:02Z</published>
    <title>A heuristic approach to obtain signal envelope with a simple software
  implementation</title>
    <summary>  Signal amplitude envelope allows to obtain information of the signal features
for different applications. It is widely used to pre-process sound and other
signals of physiological origin in human or animal studies. In order to obtain
signal envelope, a fast and simple algorithm is proposed based on peak
detection. The procedure presented here is quite straightforward and can be
used in different applications of time series analysis. It can be applied in
signals with different origin and frequency content. This algorithm presented
is implemented based on python libraries. An open source code is also provided.
Aspects on the parameter selection are discussed to adapt the same method for
different applications. Also traditional methods are revisited and compared
with the one proposed here.
</summary>
    <author>
      <name>Cecilia Jarne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2018 Anales AFA,
  https://anales.fisica.org.ar/journal/index.php/analesafa/article/view/2173/2174</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1703.06812v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06812v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07065v1</id>
    <updated>2017-03-21T06:09:32Z</updated>
    <published>2017-03-21T06:09:32Z</published>
    <title>Adaptive Multi-Class Audio Classification in Noisy In-Vehicle
  Environment</title>
    <summary>  With ever-increasing number of car-mounted electric devices and their
complexity, audio classification is increasingly important for the automotive
industry as a fundamental tool for human-device interactions. Existing
approaches for audio classification, however, fall short as the unique and
dynamic audio characteristics of in-vehicle environments are not appropriately
taken into account. In this paper, we develop an audio classification system
that classifies an audio stream into music, speech, speech+music, and noise,
adaptably depending on driving environments including highway, local road,
crowded city, and stopped vehicle. More than 420 minutes of audio data
including various genres of music, speech, speech+music, and noise are
collected from diverse driving environments. The results demonstrate that the
proposed approach improves the average classification accuracy up to 166%, and
64% for speech, and speech+music, respectively, compared with a non-adaptive
approach in our experimental settings.
</summary>
    <author>
      <name>Myounggyu Won</name>
    </author>
    <author>
      <name>Haitham Alsaadan</name>
    </author>
    <author>
      <name>Yongsoon Eun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3123266.3123397</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3123266.3123397" rel="related"/>
    <link href="http://arxiv.org/abs/1703.07065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07172v1</id>
    <updated>2017-03-21T12:35:21Z</updated>
    <published>2017-03-21T12:35:21Z</published>
    <title>Multi-Objective Learning and Mask-Based Post-Processing for Deep Neural
  Network Based Speech Enhancement</title>
    <summary>  We propose a multi-objective framework to learn both secondary targets not
directly related to the intended task of speech enhancement (SE) and the
primary target of the clean log-power spectra (LPS) features to be used
directly for constructing the enhanced speech signals. In deep neural network
(DNN) based SE we introduce an auxiliary structure to learn secondary
continuous features, such as mel-frequency cepstral coefficients (MFCCs), and
categorical information, such as the ideal binary mask (IBM), and integrate it
into the original DNN architecture for joint optimization of all the
parameters. This joint estimation scheme imposes additional constraints not
available in the direct prediction of LPS, and potentially improves the
learning of the primary target. Furthermore, the learned secondary information
as a byproduct can be used for other purposes, e.g., the IBM-based
post-processing in this work. A series of experiments show that joint LPS and
MFCC learning improves the SE performance, and IBM-based post-processing
further enhances listening quality of the reconstructed speech.
</summary>
    <author>
      <name>Yong Xu</name>
    </author>
    <author>
      <name>Jun Du</name>
    </author>
    <author>
      <name>Zhen Huang</name>
    </author>
    <author>
      <name>Li-Rong Dai</name>
    </author>
    <author>
      <name>Chin-Hui Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">interspeech2015 paper, Germany</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.07172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08019v2</id>
    <updated>2017-10-13T16:00:02Z</updated>
    <published>2017-03-23T11:45:10Z</published>
    <title>Single Channel Audio Source Separation using Convolutional Denoising
  Autoencoders</title>
    <summary>  Deep learning techniques have been used recently to tackle the audio source
separation problem. In this work, we propose to use deep fully convolutional
denoising autoencoders (CDAEs) for monaural audio source separation. We use as
many CDAEs as the number of sources to be separated from the mixed signal. Each
CDAE is trained to separate one source and treats the other sources as
background noise. The main idea is to allow each CDAE to learn suitable
spectral-temporal filters and features to its corresponding source. Our
experimental results show that CDAEs perform source separation slightly better
than the deep feedforward neural networks (FNNs) even with fewer parameters
than FNNs.
</summary>
    <author>
      <name>Emad M. Grais</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at GlobalSIP 2017 and the final version is available at
  http://epubs.surrey.ac.uk/841860/</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.08019v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08019v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; I.5; I.2.6; I.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.09302v1</id>
    <updated>2017-03-27T20:37:33Z</updated>
    <published>2017-03-27T20:37:33Z</published>
    <title>Speech Enhancement using a Deep Mixture of Experts</title>
    <summary>  In this study we present a Deep Mixture of Experts (DMoE) neural-network
architecture for single microphone speech enhancement. By contrast to most
speech enhancement algorithms that overlook the speech variability mainly
caused by phoneme structure, our framework comprises a set of deep neural
networks (DNNs), each one of which is an 'expert' in enhancing a given speech
type corresponding to a phoneme. A gating DNN determines which expert is
assigned to a given speech segment. A speech presence probability (SPP) is then
obtained as a weighted average of the expert SPP decisions, with the weights
determined by the gating DNN. A soft spectral attenuation, based on the SPP, is
then applied to enhance the noisy speech signal. The experts and the gating
components of the DMoE network are trained jointly. As part of the training,
speech clustering into different subsets is performed in an unsupervised
manner. Therefore, unlike previous methods, a phoneme-labeled database is not
required for the training procedure. A series of experiments with different
noise types verified the applicability of the new algorithm to the task of
speech enhancement. The proposed scheme outperforms other schemes that either
do not consider phoneme structure or use a simpler training methodology.
</summary>
    <author>
      <name>Shlomo E. Chazan</name>
    </author>
    <author>
      <name>Jacob Goldberger</name>
    </author>
    <author>
      <name>Sharon Gannot</name>
    </author>
    <link href="http://arxiv.org/abs/1703.09302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.01985v4</id>
    <updated>2017-06-19T10:57:38Z</updated>
    <published>2017-03-22T08:39:32Z</published>
    <title>Recognizing Multi-talker Speech with Permutation Invariant Training</title>
    <summary>  In this paper, we propose a novel technique for direct recognition of
multiple speech streams given the single channel of mixed speech, without first
separating them. Our technique is based on permutation invariant training (PIT)
for automatic speech recognition (ASR). In PIT-ASR, we compute the average
cross entropy (CE) over all frames in the whole utterance for each possible
output-target assignment, pick the one with the minimum CE, and optimize for
that assignment. PIT-ASR forces all the frames of the same speaker to be
aligned with the same output layer. This strategy elegantly solves the label
permutation problem and speaker tracing problem in one shot. Our experiments on
artificially mixed AMI data showed that the proposed approach is very
promising.
</summary>
    <author>
      <name>Dong Yu</name>
    </author>
    <author>
      <name>Xuankai Chang</name>
    </author>
    <author>
      <name>Yanmin Qian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 figures, InterSpeech2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.01985v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.01985v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02216v2</id>
    <updated>2017-10-27T19:36:55Z</updated>
    <published>2017-04-07T13:15:15Z</published>
    <title>OBTAIN: Real-Time Beat Tracking in Audio Signals</title>
    <summary>  In this paper, we design a system in order to perform the real-time beat
tracking for an audio signal. We use Onset Strength Signal (OSS) to detect the
onsets and estimate the tempos. Then, we form Cumulative Beat Strength Signal
(CBSS) by taking advantage of OSS and estimated tempos. Next, we perform peak
detection by extracting the periodic sequence of beats among all CBSS peaks. In
simulations, we can see that our proposed algorithm, Online Beat TrAckINg
(OBTAIN), outperforms state-of-art results in terms of prediction accuracy
while maintaining comparable and practical computational complexity. The
real-time performance is tractable visually as illustrated in the simulations.
</summary>
    <author>
      <name>Ali Mottaghi</name>
    </author>
    <author>
      <name>Kayhan Behdin</name>
    </author>
    <author>
      <name>Ashkan Esmaeili</name>
    </author>
    <author>
      <name>Mohammadreza Heydari</name>
    </author>
    <author>
      <name>Farokh Marvasti</name>
    </author>
    <link href="http://arxiv.org/abs/1704.02216v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02216v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03626v1</id>
    <updated>2017-04-12T05:46:44Z</updated>
    <published>2017-04-12T05:46:44Z</published>
    <title>Sampling-based speech parameter generation using moment-matching
  networks</title>
    <summary>  This paper presents sampling-based speech parameter generation using
moment-matching networks for Deep Neural Network (DNN)-based speech synthesis.
Although people never produce exactly the same speech even if we try to express
the same linguistic and para-linguistic information, typical statistical speech
synthesis produces completely the same speech, i.e., there is no
inter-utterance variation in synthetic speech. To give synthetic speech natural
inter-utterance variation, this paper builds DNN acoustic models that make it
possible to randomly sample speech parameters. The DNNs are trained so that
they make the moments of generated speech parameters close to those of natural
speech parameters. Since the variation of speech parameters is compressed into
a low-dimensional simple prior noise vector, our algorithm has lower
computation cost than direct sampling of speech parameters. As the first step
towards generating synthetic speech that has natural inter-utterance variation,
this paper investigates whether or not the proposed sampling-based generation
deteriorates synthetic speech quality. In evaluation, we compare speech quality
of conventional maximum likelihood-based generation and proposed sampling-based
generation. The result demonstrates the proposed generation causes no
degradation in speech quality.
</summary>
    <author>
      <name>Shinnosuke Takamichi</name>
    </author>
    <author>
      <name>Tomoki Koriyama</name>
    </author>
    <author>
      <name>Hiroshi Saruwatari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to INTERSPEECH 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.03626v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03626v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03809v3</id>
    <updated>2017-08-17T12:20:01Z</updated>
    <published>2017-04-12T15:57:08Z</published>
    <title>A Neural Parametric Singing Synthesizer</title>
    <summary>  We present a new model for singing synthesis based on a modified version of
the WaveNet architecture. Instead of modeling raw waveform, we model features
produced by a parametric vocoder that separates the influence of pitch and
timbre. This allows conveniently modifying pitch to match any target melody,
facilitates training on more modest dataset sizes, and significantly reduces
training and generation times. Our model makes frame-wise predictions using
mixture density outputs rather than categorical outputs in order to reduce the
required parameter count. As we found overfitting to be an issue with the
relatively small datasets used in our experiments, we propose a method to
regularize the model and make the autoregressive generation process more robust
to prediction errors. Using a simple multi-stream architecture, harmonic,
aperiodic and voiced/unvoiced components can all be predicted in a coherent
manner. We compare our method to existing parametric statistical and
state-of-the-art concatenative methods using quantitative metrics and a
listening test. While naive implementations of the autoregressive generation
algorithm tend to be inefficient, using a smart algorithm we can greatly speed
up the process and obtain a system that's competitive in both speed and
quality.
</summary>
    <author>
      <name>Merlijn Blaauw</name>
    </author>
    <author>
      <name>Jordi Bonada</name>
    </author>
    <link href="http://arxiv.org/abs/1704.03809v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03809v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03934v1</id>
    <updated>2017-04-12T21:12:34Z</updated>
    <published>2017-04-12T21:12:34Z</published>
    <title>i Vector used in Speaker Identification by Dimension Compactness</title>
    <summary>  The automatic speaker identification procedure is used to extract features
that help to identify the components of the acoustic signal by discarding all
the other stuff like background noise, emotion, hesitation, etc. The acoustic
signal is generated by a human that is filtered by the shape of the vocal
tract, including tongue, teeth, etc. The shape of the vocal tract determines
and produced, what signal comes out in real time. The analytically develops
shape of the vocal tract, which exhibits envelop for the short time power
spectrum. The ASR needs efficient way of extracting features from the acoustic
signal that is used effectively to makes the shape of the individual vocal
tract. To identify any acoustic signal in the large collection of acoustic
signal i.e. corpora, it needs dimension compactness of total variability space
by using the GMM mean super vector. This work presents the efficient way to
implement dimension compactness in total variability space and using cosine
distance scoring to predict a fast output score for small size utterance.
</summary>
    <author>
      <name>Soumen Kanrar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.03934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03939v1</id>
    <updated>2017-04-12T21:43:20Z</updated>
    <published>2017-04-12T21:43:20Z</published>
    <title>Speaker Identification by GMM based i Vector</title>
    <summary>  Speaker Identification process is to identify a particular vocal cord from a
set of existing speakers. In the speaker identification processes, unknown
speaker voice sample targets each of the existing speakers present in the
system and gives a predication. The predication may be more than one existing
known speaker voice and is very close to the unknown speaker voice. The model
is a Gaussian mixture model built by the extracted acoustic feature vectors
from voice. The i-vector based dimension compression mapping function of the
channel depended speaker, and super vector give better predicted scores
according to cosine distance scoring associated with the order pair of
speakers. In the order pair, the first coordinate is the unknown speaker i.e.
test speaker, and the second coordinates is the existing known speaker i.e.
target speaker. This paper presents the enhancement of the prediction based on
i- vector in compare to the normalized set of predicted score. In the
simulation, known speaker voices are collected through different channels and
in different languages. In the testing, the GMM voice models, and GMM based
i-Vector speaker voice models of the known speakers are used among the numbers
of clusters in the test data set.
</summary>
    <author>
      <name>Soumen Kanrar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.03939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.06008v1</id>
    <updated>2017-04-20T04:38:29Z</updated>
    <published>2017-04-20T04:38:29Z</published>
    <title>Effects of virtual acoustics on dynamic auditory distance perception</title>
    <summary>  Sound propagation encompasses various acoustic phenomena including
reverberation. Current virtual acoustic methods, ranging from parametric
filters to physically-accurate solvers, can simulate reverberation with varying
degrees of fidelity. We investigate the effects of reverberant sounds generated
using different propagation algorithms on acoustic distance perception, i.e.,
how faraway humans perceive a sound source. In particular, we evaluate two
classes of methods for real-time sound propagation in dynamic scenes based on
parametric filters and ray tracing. Our study shows that the more accurate
method shows less distance compression as compared to the approximate,
filter-based method. This suggests that accurate reverberation in VR results in
a better reproduction of acoustic distances. We also quantify the levels of
distance compression introduced by different propagation methods in a virtual
environment.
</summary>
    <author>
      <name>Atul Rungta</name>
    </author>
    <author>
      <name>Nicholas Rewkowski</name>
    </author>
    <author>
      <name>Roberta Klatzky</name>
    </author>
    <author>
      <name>Ming Lin</name>
    </author>
    <author>
      <name>Dinesh Manocha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1121/1.4981234</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1121/1.4981234" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.06008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.06008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08953v3</id>
    <updated>2017-08-01T13:27:34Z</updated>
    <published>2017-04-28T14:30:20Z</published>
    <title>Design of robust two-dimensional polynomial beamformers as a convex
  optimization problem with application to robot audition</title>
    <summary>  We propose a robust two-dimensional polynomial beamformer design method,
formulated as a convex optimization problem, which allows for flexible steering
of a previously proposed data-independent robust beamformer in both azimuth and
elevation direction.~As an exemplary application, the proposed two-dimensional
polynomial beamformer design is applied to a twelve-element microphone array,
integrated into the head of a humanoid robot. To account for the effects of the
robot's head on the sound field, measured head-related transfer functions are
integrated into the optimization problem as steering vectors. The
two-dimensional polynomial beamformer design is evaluated using
signal-independent and signal-dependent measures. The results confirm that the
proposed polynomial beamformer design approximates the original fixed
beamformer design very accurately, which makes it an attractive approach for
robust real-time data-independent beamforming.
</summary>
    <author>
      <name>Hendrik Barfuss</name>
    </author>
    <author>
      <name>Markus Bachmann</name>
    </author>
    <author>
      <name>Michael Buerger</name>
    </author>
    <author>
      <name>Martin Schneider</name>
    </author>
    <author>
      <name>Walter Kellerman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to IEEE Workshop on Applications of Signal Processing to
  Audio and Acoustics (WASPAA) 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08953v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08953v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.00919v2</id>
    <updated>2017-12-12T12:57:36Z</updated>
    <published>2017-05-02T11:31:43Z</published>
    <title>Broadband DOA estimation using Convolutional neural networks trained
  with noise signals</title>
    <summary>  A convolution neural network (CNN) based classification method for broadband
DOA estimation is proposed, where the phase component of the short-time Fourier
transform coefficients of the received microphone signals are directly fed into
the CNN and the features required for DOA estimation are learnt during
training. Since only the phase component of the input is used, the CNN can be
trained with synthesized noise signals, thereby making the preparation of the
training data set easier compared to using speech signals. Through experimental
evaluation, the ability of the proposed noise trained CNN framework to
generalize to speech sources is demonstrated. In addition, the robustness of
the system to noise, small perturbations in microphone positions, as well as
its ability to adapt to different acoustic conditions is investigated using
experiments with simulated and real data.
</summary>
    <author>
      <name>Soumitro Chakrabarty</name>
    </author>
    <author>
      <name>Emanuël. A. P. Habets</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/WASPAA.2017.8170010</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/WASPAA.2017.8170010" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Proceedings of IEEE Workshop on Applications of Signal
  Processing to Audio and Acoustics (WASPAA) 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.00919v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00919v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.02514v2</id>
    <updated>2017-10-31T01:58:50Z</updated>
    <published>2017-05-06T18:27:09Z</published>
    <title>End-to-end Source Separation with Adaptive Front-Ends</title>
    <summary>  Source separation and other audio applications have traditionally relied on
the use of short-time Fourier transforms as a front-end frequency domain
representation step. The unavailability of a neural network equivalent to
forward and inverse transforms hinders the implementation of end-to-end
learning systems for these applications. We present an auto-encoder neural
network that can act as an equivalent to short-time front-end transforms. We
demonstrate the ability of the network to learn optimal, real-valued basis
functions directly from the raw waveform of a signal and further show how it
can be used as an adaptive front-end for supervised source separation. In terms
of separation performance, these transforms significantly outperform their
Fourier counterparts. Finally, we also propose a novel source to distortion
ratio based cost function for end-to-end source separation.
</summary>
    <author>
      <name>Shrikant Venkataramani</name>
    </author>
    <author>
      <name>Jonah Casebeer</name>
    </author>
    <author>
      <name>Paris Smaragdis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 figures, 4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.02514v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.02514v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03670v1</id>
    <updated>2017-05-10T09:30:42Z</updated>
    <published>2017-05-10T09:30:42Z</published>
    <title>Deep Speaker Feature Learning for Text-independent Speaker Verification</title>
    <summary>  Recently deep neural networks (DNNs) have been used to learn speaker
features. However, the quality of the learned features is not sufficiently
good, so a complex back-end model, either neural or probabilistic, has to be
used to address the residual uncertainty when applied to speaker verification,
just as with raw features. This paper presents a convolutional time-delay deep
neural network structure (CT-DNN) for speaker feature learning. Our
experimental results on the Fisher database demonstrated that this CT-DNN can
produce high-quality speaker features: even with a single feature (0.3 seconds
including the context), the EER can be as low as 7.68%. This effectively
confirmed that the speaker trait is largely a deterministic short-time property
rather than a long-time distributional pattern, and therefore can be extracted
from just dozens of frames.
</summary>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Yixiang Chen</name>
    </author>
    <author>
      <name>Ying Shi</name>
    </author>
    <author>
      <name>Zhiyuan Tang</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">deep neural networks, speaker verification, speaker feature</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.03670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03877v2</id>
    <updated>2017-05-15T20:49:42Z</updated>
    <published>2017-05-10T17:58:53Z</published>
    <title>Frequency Domain Singular Value Decomposition for Efficient Spatial
  Audio Coding</title>
    <summary>  Advances in virtual reality have generated substantial interest in accurately
reproducing and storing spatial audio in the higher order ambisonics (HOA)
representation, given its rendering flexibility. Recent standardization for HOA
compression adopted a framework wherein HOA data are decomposed into principal
components that are then encoded by standard audio coding, i.e., frequency
domain quantization and entropy coding to exploit psychoacoustic redundancy. A
noted shortcoming of this approach is the occasional mismatch in principal
components across blocks, and the resulting suboptimal transitions in the data
fed to the audio coder. Instead, we propose a framework where singular value
decomposition (SVD) is performed after transformation to the frequency domain
via the modified discrete cosine transform (MDCT). This framework not only
ensures smooth transition across blocks, but also enables frequency dependent
SVD for better energy compaction. Moreover, we introduce a novel noise
substitution technique to compensate for suppressed ambient energy in discarded
higher order ambisonics channels, which significantly enhances the perceptual
quality of the reconstructed HOA signal. Objective and subjective evaluation
results provide evidence for the effectiveness of the proposed framework in
terms of both higher compression gains and better perceptual quality, compared
to existing methods.
</summary>
    <author>
      <name>Sina Zamani</name>
    </author>
    <author>
      <name>Tejaswi Nanjundaswamy</name>
    </author>
    <author>
      <name>Kenneth Rose</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.03877v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03877v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.04792v1</id>
    <updated>2017-05-13T06:43:46Z</updated>
    <published>2017-05-13T06:43:46Z</published>
    <title>Riddim: A Rhythm Analysis and Decomposition Tool Based On Independent
  Subspace Analysis</title>
    <summary>  The goal of this thesis was to implement a tool that, given a digital audio
input, can extract and represent rhythm and musical time. The purpose of the
tool is to help develop better models of rhythm for real-time computer based
performance and composition. This analysis tool, Riddim, uses Independent
Subspace Analysis (ISA) and a robust onset detection scheme to separate and
detect salient rhythmic and timing information from different sonic sources
within the input. This information is then represented in a format that can be
used by a variety of algorithms that interpret timing information to infer
rhythmic and musical structure. A secondary objective of this work is a "proof
of concept" as a non-real-time rhythm analysis system based on ISA. This is a
necessary step since ultimately it is desirable to incorporate this
functionality in a real-time plug-in for live performance and improvisation.
</summary>
    <author>
      <name>Iroro Orife</name>
    </author>
    <link href="http://arxiv.org/abs/1705.04792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.04792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05472v1</id>
    <updated>2017-05-15T22:20:22Z</updated>
    <published>2017-05-15T22:20:22Z</published>
    <title>A Biomimetic Vocalisation System for MiRo</title>
    <summary>  There is increasing interest in the use of animal-like robots in applications
such as companionship and pet therapy. However, in the majority of cases it is
only the robot's physical appearance that mimics a given animal. In contrast,
MiRo is the first commercial biomimetic robot to be based on a hardware and
software architecture that is modelled on the biological brain. This paper
describes how MiRo's vocalisation system was designed, not using pre-recorded
animal sounds, but based on the implementation of a real-time parametric
general-purpose mammalian vocal synthesiser tailored to the specific physical
characteristics of the robot. The novel outcome has been the creation of an
'appropriate' voice for MiRo that is perfectly aligned to the physical and
behavioural affordances of the robot, thereby avoiding the 'uncanny valley'
effect and contributing strongly to the effectiveness of MiRo as an interactive
device.
</summary>
    <author>
      <name>Roger K. Moore</name>
    </author>
    <author>
      <name>Ben Mitchinson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages; accepted for publication at Living Machines 2017, Stanford,
  USA; 25-28 July 2017; http://livingmachinesconference.eu/2017/</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.05472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05874v1</id>
    <updated>2017-05-16T18:45:41Z</updated>
    <published>2017-05-16T18:45:41Z</published>
    <title>Time-frequency or time-scale representation fission and fusion rules</title>
    <summary>  Time-frequency representations are important for the analysis of time series.
We have developed an online time-series analysis system and equipped it to
reliably handle re-alignment in the time-frequency plane. The system can deal
with issues like invalid regions in time-frequency representations and
discontinuities in data transmissions, making it suitable for on-line
processing in real-world situations. In retrospect the whole problem can be
considered to be a generalization of ideas present in overlap-and-add
filtering, but then for time-frequency representations and including the
calculation of non-causal features. Here we present our design for
time-frequency representation fission and fusion rules. We present these rules
in the context of two typical use cases, which facilitate understanding of the
underlying choices.
</summary>
    <author>
      <name>Coen Jonker</name>
    </author>
    <author>
      <name>Arryon D. Tijsma</name>
    </author>
    <author>
      <name>Ronald A. J. van Elburg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accompanying the release of libSoundAnnotator. The
  whole implementation is open sourced through GitHub:
  https://github.com/soundappraisal/libsoundannotator under the Apache License,
  Version 2.0. Both UseCases described are available as documented code through
  GitHub https://github.com/soundappraisal/soundannotatordemo under the Apache
  License, Version 2.0. 11 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.05874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08660v1</id>
    <updated>2017-05-24T08:39:04Z</updated>
    <published>2017-05-24T08:39:04Z</published>
    <title>Matrix of Polynomials Model based Polynomial Dictionary Learning Method
  for Acoustic Impulse Response Modeling</title>
    <summary>  We study the problem of dictionary learning for signals that can be
represented as polynomials or polynomial matrices, such as convolutive signals
with time delays or acoustic impulse responses. Recently, we developed a method
for polynomial dictionary learning based on the fact that a polynomial matrix
can be expressed as a polynomial with matrix coefficients, where the
coefficient of the polynomial at each time lag is a scalar matrix. However, a
polynomial matrix can be also equally represented as a matrix with polynomial
elements. In this paper, we develop an alternative method for learning a
polynomial dictionary and a sparse representation method for polynomial signal
reconstruction based on this model. The proposed methods can be used directly
to operate on the polynomial matrix without having to access its coefficients
matrices. We demonstrate the performance of the proposed method for acoustic
impulse response modeling.
</summary>
    <author>
      <name>Jian Guan</name>
    </author>
    <author>
      <name>Xuan Wang</name>
    </author>
    <author>
      <name>Pengming Feng</name>
    </author>
    <author>
      <name>Jing Dong</name>
    </author>
    <author>
      <name>Wenwu Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.08660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08858v1</id>
    <updated>2017-05-24T16:48:03Z</updated>
    <published>2017-05-24T16:48:03Z</published>
    <title>Audio-replay attack detection countermeasures</title>
    <summary>  This paper presents the Speech Technology Center (STC) replay attack
detection systems proposed for Automatic Speaker Verification Spoofing and
Countermeasures Challenge 2017. In this study we focused on comparison of
different spoofing detection approaches. These were GMM based methods, high
level features extraction with simple classifier and deep learning frameworks.
Experiments performed on the development and evaluation parts of the challenge
dataset demonstrated stable efficiency of deep learning approaches in case of
changing acoustic conditions. At the same time SVM classifier with high level
features provided a substantial input in the efficiency of the resulting STC
systems according to the fusion systems results.
</summary>
    <author>
      <name>Galina Lavrentyeva</name>
    </author>
    <author>
      <name>Sergey Novoselov</name>
    </author>
    <author>
      <name>Egor Malykh</name>
    </author>
    <author>
      <name>Alexander Kozlov</name>
    </author>
    <author>
      <name>Oleg Kudashev</name>
    </author>
    <author>
      <name>Vadim Shchemelinin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures, accepted for Specom 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.08858v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08858v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08865v1</id>
    <updated>2017-05-24T16:58:03Z</updated>
    <published>2017-05-24T16:58:03Z</published>
    <title>Anti-spoofing Methods for Automatic SpeakerVerification System</title>
    <summary>  Growing interest in automatic speaker verification (ASV)systems has lead to
significant quality improvement of spoofing attackson them. Many research works
confirm that despite the low equal er-ror rate (EER) ASV systems are still
vulnerable to spoofing attacks. Inthis work we overview different acoustic
feature spaces and classifiersto determine reliable and robust countermeasures
against spoofing at-tacks. We compared several spoofing detection systems,
presented so far,on the development and evaluation datasets of the Automatic
SpeakerVerification Spoofing and Countermeasures (ASVspoof) Challenge
2015.Experimental results presented in this paper demonstrate that the useof
magnitude and phase information combination provides a substantialinput into
the efficiency of the spoofing detection systems. Also wavelet-based features
show impressive results in terms of equal error rate. Inour overview we compare
spoofing performance for systems based on dif-ferent classifiers. Comparison
results demonstrate that the linear SVMclassifier outperforms the conventional
GMM approach. However, manyresearchers inspired by the great success of deep
neural networks (DNN)approaches in the automatic speech recognition, applied
DNN in thespoofing detection task and obtained quite low EER for known and
un-known type of spoofing attacks.
</summary>
    <author>
      <name>Galina Lavrentyeva</name>
    </author>
    <author>
      <name>Sergey Novoselov</name>
    </author>
    <author>
      <name>Konstantin Simonchik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 0 figures, published in Springer Communications in Computer
  and Information Science (CCIS) vol. 661</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.08865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.09185v1</id>
    <updated>2017-05-25T13:59:18Z</updated>
    <published>2017-05-25T13:59:18Z</published>
    <title>Investigation of Using VAE for i-Vector Speaker Verification</title>
    <summary>  New system for i-vector speaker recognition based on variational autoencoder
(VAE) is investigated. VAE is a promising approach for developing accurate deep
nonlinear generative models of complex data. Experiments show that VAE provides
speaker embedding and can be effectively trained in an unsupervised manner. LLR
estimate for VAE is developed. Experiments on NIST SRE 2010 data demonstrate
its correctness. Additionally, we show that the performance of VAE-based system
in the i-vectors space is close to that of the diagonal PLDA. Several
interesting results are also observed in the experiments with $\beta$-VAE. In
particular, we found that for $\beta\ll 1$, VAE can be trained to capture the
features of complex input data distributions in an effective way, which is hard
to obtain in the standard VAE ($\beta=1$).
</summary>
    <author>
      <name>Timur Pekhovsky</name>
    </author>
    <author>
      <name>Maxim Korenevsky</name>
    </author>
    <link href="http://arxiv.org/abs/1705.09185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.09289v1</id>
    <updated>2017-05-25T19:08:08Z</updated>
    <published>2017-05-25T19:08:08Z</published>
    <title>Improved I-vector-based Speaker Recognition for Utterances with Speaker
  Generated Non-speech sounds</title>
    <summary>  Conversational speech not only contains several variants of neutral speech
but is also prominently interlaced with several speaker generated non-speech
sounds such as laughter and breath. A robust speaker recognition system should
be capable of recognizing a speaker irrespective of these variations in his
speech. An understanding of whether the speaker-specific information
represented by these variations is similar or not helps build a good speaker
recognition system. In this paper, speaker variations captured by neutral
speech of a speaker is analyzed by considering speech-laugh (a variant of
neutral speech) and laughter (non-speech) sounds of the speaker. We study an
i-vector-based speaker recognition system trained only on neutral speech and
evaluate its performance on speech-laugh and laughter. Further, we analyze the
effect of including laughter sounds during training of an i-vector-basedspeaker
recognition system. Our experimental results show that the inclusion of
laughter sounds during training seem to provide complementary speaker-specific
information which results in an overall improved performance of the speaker
recognition system, especially on the utterances with speech-laugh segments.
</summary>
    <author>
      <name>Sri Harsha Dumpala</name>
    </author>
    <author>
      <name>Ashish Panda</name>
    </author>
    <author>
      <name>Sunil Kumar Kopparapu</name>
    </author>
    <link href="http://arxiv.org/abs/1705.09289v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.09289v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.10368v1</id>
    <updated>2017-05-29T19:24:14Z</updated>
    <published>2017-05-29T19:24:14Z</published>
    <title>DNN-based uncertainty estimation for weighted DNN-HMM ASR</title>
    <summary>  In this paper, the uncertainty is defined as the mean square error between a
given enhanced noisy observation vector and the corresponding clean one. Then,
a DNN is trained by using enhanced noisy observation vectors as input and the
uncertainty as output with a training database. In testing, the DNN receives an
enhanced noisy observation vector and delivers the estimated uncertainty. This
uncertainty in employed in combination with a weighted DNN-HMM based speech
recognition system and compared with an existing estimation of the noise
cancelling uncertainty variance based on an additive noise model. Experiments
were carried out with Aurora-4 task. Results with clean, multi-noise and
multi-condition training are presented.
</summary>
    <author>
      <name>José Novoa</name>
    </author>
    <author>
      <name>Josué Fredes</name>
    </author>
    <author>
      <name>Néstor Becerra Yoma</name>
    </author>
    <link href="http://arxiv.org/abs/1705.10368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.10368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.10874v3</id>
    <updated>2018-09-21T09:05:57Z</updated>
    <published>2017-05-30T21:31:25Z</published>
    <title>Deep Learning for Environmentally Robust Speech Recognition: An Overview
  of Recent Developments</title>
    <summary>  Eliminating the negative effect of non-stationary environmental noise is a
long-standing research topic for automatic speech recognition that stills
remains an important challenge. Data-driven supervised approaches, including
ones based on deep neural networks, have recently emerged as potential
alternatives to traditional unsupervised approaches and with sufficient
training, can alleviate the shortcomings of the unsupervised methods in various
real-life acoustic environments. In this light, we review recently developed,
representative deep learning approaches for tackling non-stationary additive
and convolutional degradation of speech with the aim of providing guidelines
for those involved in the development of environmentally robust speech
recognition systems. We separately discuss single- and multi-channel techniques
developed for the front-end and back-end of speech recognition systems, as well
as joint front-end and back-end training frameworks.
</summary>
    <author>
      <name>Zixing Zhang</name>
    </author>
    <author>
      <name>Jürgen Geiger</name>
    </author>
    <author>
      <name>Jouni Pohjalainen</name>
    </author>
    <author>
      <name>Amr El-Desoky Mousa</name>
    </author>
    <author>
      <name>Wenyu Jin</name>
    </author>
    <author>
      <name>Björn Schuller</name>
    </author>
    <link href="http://arxiv.org/abs/1705.10874v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.10874v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00114v1</id>
    <updated>2017-05-31T22:27:37Z</updated>
    <published>2017-05-31T22:27:37Z</published>
    <title>Mixed penalization in convolutive nonnegative matrix factorization for
  blind speech dereverberation</title>
    <summary>  When a signal is recorded in an enclosed room, it typically gets affected by
reverberation. This degradation represents a problem when dealing with audio
signals, particularly in the field of speech signal processing, such as
automatic speech recognition. Although there are some approaches to deal with
this issue that are quite satisfactory under certain conditions, constructing a
method that works well in a general context still poses a significant
challenge. In this article, we propose a method based on convolutive
nonnegative matrix factorization that mixes two penalizers in order to impose
certain characteristics over the time-frequency components of the restored
signal and the reverberant components. An algorithm for implementing the method
is described and tested. Comparisons of the results against those obtained with
state of the art methods are presented, showing significant improvement.
</summary>
    <author>
      <name>Francisco J. Ibarrola</name>
    </author>
    <author>
      <name>Leandro E. Di Persia</name>
    </author>
    <author>
      <name>Ruben D. Spies</name>
    </author>
    <link href="http://arxiv.org/abs/1706.00114v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00114v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01777v2</id>
    <updated>2017-06-25T10:10:35Z</updated>
    <published>2017-06-05T15:02:39Z</published>
    <title>Deep Factorization for Speech Signal</title>
    <summary>  Speech signals are complex intermingling of various informative factors, and
this information blending makes decoding any of the individual factors
extremely difficult. A natural idea is to factorize each speech frame into
independent factors, though it turns out to be even more difficult than
decoding each individual factor. A major encumbrance is that the speaker trait,
a major factor in speech signals, has been suspected to be a long-term
distributional pattern and so not identifiable at the frame level. In this
paper, we demonstrated that the speaker factor is also a short-time spectral
pattern and can be largely identified with just a few frames using a simple
deep neural network (DNN). This discovery motivated a cascade deep
factorization (CDF) framework that infers speech factors in a sequential way,
and factors previously inferred are used as conditional variables when
inferring other factors. Our experiment on an automatic emotion recognition
(AER) task demonstrated that this approach can effectively factorize speech
signals, and using these factors, the original speech spectrum can be recovered
with high accuracy. This factorization and reconstruction approach provides a
novel tool for many speech processing tasks.
</summary>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Ying Shi</name>
    </author>
    <author>
      <name>Yixiang Chen</name>
    </author>
    <author>
      <name>Zhiyuan Tang</name>
    </author>
    <link href="http://arxiv.org/abs/1706.01777v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01777v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02101v1</id>
    <updated>2017-06-07T09:31:00Z</updated>
    <published>2017-06-07T09:31:00Z</published>
    <title>A Study on Replay Attack and Anti-Spoofing for Automatic Speaker
  Verification</title>
    <summary>  For practical automatic speaker verification (ASV) systems, replay attack
poses a true risk. By replaying a pre-recorded speech signal of the genuine
speaker, ASV systems tend to be easily fooled. An effective replay detection
method is therefore highly desirable. In this study, we investigate a major
difficulty in replay detection: the over-fitting problem caused by variability
factors in speech signal. An F-ratio probing tool is proposed and three
variability factors are investigated using this tool: speaker identity, speech
content and playback &amp; recording device. The analysis shows that device is the
most influential factor that contributes the highest over-fitting risk. A
frequency warping approach is studied to alleviate the over-fitting problem, as
verified on the ASV-spoof 2017 database.
</summary>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Yixiang Chen</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Thomas Fang Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1706.02101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02291v1</id>
    <updated>2017-06-07T06:01:48Z</updated>
    <published>2017-06-07T06:01:48Z</published>
    <title>Sound Event Detection Using Spatial Features and Convolutional Recurrent
  Neural Network</title>
    <summary>  This paper proposes to use low-level spatial features extracted from
multichannel audio for sound event detection. We extend the convolutional
recurrent neural network to handle more than one type of these multichannel
features by learning from each of them separately in the initial stages. We
show that instead of concatenating the features of each channel into a single
feature vector the network learns sound events in multichannel audio better
when they are presented as separate layers of a volume. Using the proposed
spatial features over monaural features on the same network gives an absolute
F-score improvement of 6.1% on the publicly available TUT-SED 2016 dataset and
2.7% on the TUT-SED 2009 dataset that is fifteen times larger.
</summary>
    <author>
      <name>Sharath Adavanne</name>
    </author>
    <author>
      <name>Pasi Pertilä</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.02291v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02291v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02293v1</id>
    <updated>2017-06-07T06:11:32Z</updated>
    <published>2017-06-07T06:11:32Z</published>
    <title>Sound Event Detection in Multichannel Audio Using Spatial and Harmonic
  Features</title>
    <summary>  In this paper, we propose the use of spatial and harmonic features in
combination with long short term memory (LSTM) recurrent neural network (RNN)
for automatic sound event detection (SED) task. Real life sound recordings
typically have many overlapping sound events, making it hard to recognize with
just mono channel audio. Human listeners have been successfully recognizing the
mixture of overlapping sound events using pitch cues and exploiting the stereo
(multichannel) audio signal available at their ears to spatially localize these
events. Traditionally SED systems have only been using mono channel audio,
motivated by the human listener we propose to extend them to use multichannel
audio. The proposed SED system is compared against the state of the art mono
channel method on the development subset of TUT sound events detection 2016
database. The usage of spatial and harmonic features are shown to improve the
performance of SED.
</summary>
    <author>
      <name>Sharath Adavanne</name>
    </author>
    <author>
      <name>Giambattista Parascandolo</name>
    </author>
    <author>
      <name>Pasi Pertilä</name>
    </author>
    <author>
      <name>Toni Heittola</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <link href="http://arxiv.org/abs/1706.02293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.04924v1</id>
    <updated>2017-06-15T15:24:44Z</updated>
    <published>2017-06-15T15:24:44Z</published>
    <title>Investigating the Potential of Pseudo Quadrature Mirror Filter-Banks in
  Music Source Separation Tasks</title>
    <summary>  Estimating audio and musical signals from single channel mixtures often, if
not always, involves a transformation of the mixture signal to the
time-frequency (T-F) domain in which a masking operation takes place. Masking
is realized as an element-wise multiplication of the mixture signal's T-F
representation with a ratio of computed sources' spectrogram. Studies have
shown that the performance of the overall source estimation scheme is subject
to the sparsity and disjointness properties of a given T-F representation. In
this work we investigate the potential of an optimized pseudo quadrature mirror
filter-bank (PQMF), as a T-F representation for music source separation tasks.
Experimental results, suggest that the PQMF maintains the aforementioned
desirable properties and can be regarded as an alternative for representing
mixtures of musical signals.
</summary>
    <author>
      <name>Stylianos Ioannis Mimilakis</name>
    </author>
    <author>
      <name>Gerald Schuller</name>
    </author>
    <link href="http://arxiv.org/abs/1706.04924v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04924v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07162v3</id>
    <updated>2018-01-31T09:38:44Z</updated>
    <published>2017-06-22T04:13:43Z</published>
    <title>A Wavenet for Speech Denoising</title>
    <summary>  Currently, most speech processing techniques use magnitude spectrograms as
front-end and are therefore by default discarding part of the signal: the
phase. In order to overcome this limitation, we propose an end-to-end learning
method for speech denoising based on Wavenet. The proposed model adaptation
retains Wavenet's powerful acoustic modeling capabilities, while significantly
reducing its time-complexity by eliminating its autoregressive nature.
Specifically, the model makes use of non-causal, dilated convolutions and
predicts target fields instead of a single target sample. The discriminative
adaptation of the model we propose, learns in a supervised fashion via
minimizing a regression loss. These modifications make the model highly
parallelizable during both training and inference. Both computational and
perceptual evaluations indicate that the proposed method is preferred to Wiener
filtering, a common method based on processing the magnitude spectrogram.
</summary>
    <author>
      <name>Dario Rethage</name>
    </author>
    <author>
      <name>Jordi Pons</name>
    </author>
    <author>
      <name>Xavier Serra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In proceedings of the 43rd IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP2018). Code:
  https://github.com/drethage/speech-denoising-wavenet - Audio examples:
  http://jordipons.me/apps/speech-denoising-wavenet/</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07162v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07162v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07326v2</id>
    <updated>2017-11-28T15:52:27Z</updated>
    <published>2017-06-22T14:00:35Z</published>
    <title>A universal negative group delay filter for the prediction of
  band-limited signals</title>
    <summary>  A filter for universal real-time prediction of band-limited signals is
presented. The filter consists of multiple time-delayed feedback terms in order
to accomplish anticipatory coupling, which again leads to a negative group
delay for frequencies in the baseband. The universality of the filter arises
from its property that it does not rely on a specific model of the signal.
Specifically, as long as the signal to be predicted is band-limited with a
known cutoff frequency, the filter order, the only parameter of the filter,
follows and the filter predicts the signal in real time up to a prediction
horizon that depends on the cutoff frequency, too. It is worked out in detail
how signal prediction arises from the negative group delay of the filter. Its
properties, including stability, are investigated theoretically, by numerical
simulations, and by application to a physiological signal. Possible control and
signal processing applications of this filter are discussed.
</summary>
    <author>
      <name>Henning U. Voss</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be re-submitted to IEEE-TSP</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07326v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07326v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07859v1</id>
    <updated>2017-06-22T04:33:59Z</updated>
    <published>2017-06-22T04:33:59Z</published>
    <title>Deep Speaker Verification: Do We Need End to End?</title>
    <summary>  End-to-end learning treats the entire system as a whole adaptable black box,
which, if sufficient data are available, may learn a system that works very
well for the target task. This principle has recently been applied to several
prototype research on speaker verification (SV), where the feature learning and
classifier are learned together with an objective function that is consistent
with the evaluation metric. An opposite approach to end-to-end is feature
learning, which firstly trains a feature learning model, and then constructs a
back-end classifier separately to perform SV. Recently, both approaches
achieved significant performance gains on SV, mainly attributed to the smart
utilization of deep neural networks. However, the two approaches have not been
carefully compared, and their respective advantages have not been well
discussed. In this paper, we compare the end-to-end and feature learning
approaches on a text-independent SV task. Our experiments on a dataset sampled
from the Fisher database and involving 5,000 speakers demonstrated that the
feature learning approach outperformed the end-to-end approach. This is a
strong support for the feature learning approach, at least with data and
computation resources similar to ours.
</summary>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Zhiyuan Tang</name>
    </author>
    <author>
      <name>Thomas Fang Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1706.07859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07860v1</id>
    <updated>2017-06-22T04:26:39Z</updated>
    <published>2017-06-22T04:26:39Z</published>
    <title>Speaker Recognition with Cough, Laugh and "Wei"</title>
    <summary>  This paper proposes a speaker recognition (SRE) task with trivial speech
events, such as cough and laugh. These trivial events are ubiquitous in
conversations and less subjected to intentional change, therefore offering
valuable particularities to discover the genuine speaker from disguised speech.
However, trivial events are often short and idiocratic in spectral patterns,
making SRE extremely difficult. Fortunately, we found a very powerful deep
feature learning structure that can extract highly speaker-sensitive features.
By employing this tool, we studied the SRE performance on three types of
trivial events: cough, laugh and "Wei" (a short Chinese "Hello"). The results
show that there is rich speaker information within these trivial events, even
for cough that is intuitively less speaker distinguishable. With the deep
feature approach, the EER can reach 10%-14% with the three trivial events,
despite their extremely short durations (0.2-1.0 seconds).
</summary>
    <author>
      <name>Miao Zhang</name>
    </author>
    <author>
      <name>Yixiang Chen</name>
    </author>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1706.07860v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07860v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07861v1</id>
    <updated>2017-06-22T04:32:40Z</updated>
    <published>2017-06-22T04:32:40Z</published>
    <title>Cross-lingual Speaker Verification with Deep Feature Learning</title>
    <summary>  Existing speaker verification (SV) systems often suffer from performance
degradation if there is any language mismatch between model training, speaker
enrollment, and test. A major cause of this degradation is that most existing
SV methods rely on a probabilistic model to infer the speaker factor, so any
significant change on the distribution of the speech signal will impact the
inference. Recently, we proposed a deep learning model that can learn how to
extract the speaker factor by a deep neural network (DNN). By this feature
learning, an SV system can be constructed with a very simple back-end model. In
this paper, we investigate the robustness of the feature-based SV system in
situations with language mismatch. Our experiments were conducted on a complex
cross-lingual scenario, where the model training was in English, and the
enrollment and test were in Chinese or Uyghur. The experiments demonstrated
that the feature-based system outperformed the i-vector system with a large
margin, particularly with language mismatch between enrollment and test.
</summary>
    <author>
      <name>Lantian Li</name>
    </author>
    <author>
      <name>Dong Wang</name>
    </author>
    <author>
      <name>Askar Rozi</name>
    </author>
    <author>
      <name>Thomas Fang Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1706.07861v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07861v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07927v1</id>
    <updated>2017-06-24T08:50:20Z</updated>
    <published>2017-06-24T08:50:20Z</published>
    <title>A Variational EM Method for Pole-Zero Modeling of Speech with Mixed
  Block Sparse and Gaussian Excitation</title>
    <summary>  The modeling of speech can be used for speech synthesis and speech
recognition. We present a speech analysis method based on pole-zero modeling of
speech with mixed block sparse and Gaussian excitation. By using a pole-zero
model, instead of the all-pole model, a better spectral fitting can be
expected. Moreover, motivated by the block sparse glottal flow excitation
during voiced speech and the white noise excitation for unvoiced speech, we
model the excitation sequence as a combination of block sparse signals and
white noise. A variational EM (VEM) method is proposed for estimating the
posterior PDFs of the block sparse residuals and point estimates of mod- elling
parameters within a sparse Bayesian learning framework. Compared to
conventional pole-zero and all-pole based methods, experimental results show
that the proposed method has lower spectral distortion and good performance in
reconstructing of the block sparse excitation.
</summary>
    <author>
      <name>Liming Shi</name>
    </author>
    <author>
      <name>Jesper Kjær Nielsen</name>
    </author>
    <author>
      <name>Jesper Rindom Jensen</name>
    </author>
    <author>
      <name>Mads Græsbøll Christensen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in the 25th European Signal Processing Conference (EUSIPCO
  2017), published by EUROSIP, scheduled for Aug. 28 - Sep. 2 in Kos island,
  Greece</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08231v1</id>
    <updated>2017-06-26T04:57:06Z</updated>
    <published>2017-06-26T04:57:06Z</published>
    <title>Between Homomorphic Signal Processing and Deep Neural Networks:
  Constructing Deep Algorithms for Polyphonic Music Transcription</title>
    <summary>  This paper presents a new approach in understanding how deep neural networks
(DNNs) work by applying homomorphic signal processing techniques. Focusing on
the task of multi-pitch estimation (MPE), this paper demonstrates the
equivalence relation between a generalized cepstrum and a DNN in terms of their
structures and functionality. Such an equivalence relation, together with pitch
perception theories and the recently established
rectified-correlations-on-a-sphere (RECOS) filter analysis, provide an
alternative way in explaining the role of the nonlinear activation function and
the multi-layer structure, both of which exist in a cepstrum and a DNN. To
validate the efficacy of this new approach, a new feature designed in the same
fashion is proposed for pitch salience function. The new feature outperforms
the one-layer spectrum in the MPE task and, as predicted, it addresses the
issue of the missing fundamental effect and also achieves better robustness to
noise.
</summary>
    <author>
      <name>Li Su</name>
    </author>
    <link href="http://arxiv.org/abs/1706.08231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08612v2</id>
    <updated>2018-05-30T06:52:06Z</updated>
    <published>2017-06-26T21:42:27Z</published>
    <title>VoxCeleb: a large-scale speaker identification dataset</title>
    <summary>  Most existing datasets for speaker identification contain samples obtained
under quite constrained conditions, and are usually hand-annotated, hence
limited in size. The goal of this paper is to generate a large scale
text-independent speaker identification dataset collected 'in the wild'. We
make two contributions. First, we propose a fully automated pipeline based on
computer vision techniques to create the dataset from open-source media. Our
pipeline involves obtaining videos from YouTube; performing active speaker
verification using a two-stream synchronization Convolutional Neural Network
(CNN), and confirming the identity of the speaker using CNN based facial
recognition. We use this pipeline to curate VoxCeleb which contains hundreds of
thousands of 'real world' utterances for over 1,000 celebrities. Our second
contribution is to apply and compare various state of the art speaker
identification techniques on our dataset to establish baseline performance. We
show that a CNN based architecture obtains the best performance for both
identification and verification.
</summary>
    <author>
      <name>Arsha Nagrani</name>
    </author>
    <author>
      <name>Joon Son Chung</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.21437/Interspeech.2017-950</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.21437/Interspeech.2017-950" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The dataset can be downloaded from
  http://www.robots.ox.ac.uk/~vgg/data/voxceleb . 1706.08612v2: minor fixes; 6
  pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.08612v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08612v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08759v1</id>
    <updated>2017-06-27T10:03:52Z</updated>
    <published>2017-06-27T10:03:52Z</published>
    <title>Impulsive Sound Detection by a Novel Energy Formula and its Usage for
  Gunshot Recognition</title>
    <summary>  There are many methods proposed for the detection of impulsive sounds in
literature. Most of them are complex and require adaptation to ambient noise.
In this paper we propose a very simple and efficient method to detect impulsive
sounds. Although we use energy like most of the others to determine impulsive
sounds, the way we calculate the energy is quite different. Also our
calculation is immune to ambient noise and does not require any limit or
adaptation. We could detect impulsive sounds embedded in various kinds of
noises by using this formula.
  As our ultimate aim is to detect gunshots, next phase of impulsive sound
detection is gunshot recognition phase. Detected impulsive sounds are fed into
recognition phase in which we can decide on gunshots with high success rate.
</summary>
    <author>
      <name>Yüksel Arslan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.08759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08818v4</id>
    <updated>2019-10-01T12:48:14Z</updated>
    <published>2017-06-27T12:39:10Z</published>
    <title>Gabor frames and deep scattering networks in audio processing</title>
    <summary>  This paper introduces Gabor scattering, a feature extractor based on Gabor
frames and Mallat's scattering transform. By using a simple signal model for
audio signals specific properties of Gabor scattering are studied. It is shown
that for each layer, specific invariances to certain signal characteristics
occur. Furthermore, deformation stability of the coefficient vector generated
by the feature extractor is derived by using a decoupling technique which
exploits the contractivity of general scattering networks. Deformations are
introduced as changes in spectral shape and frequency modulation. The
theoretical results are illustrated by numerical examples and experiments.
Numerical evidence is given by evaluation on a synthetic and a "real" data set,
that the invariances encoded by the Gabor scattering transform lead to higher
performance in comparison with just using Gabor transform, especially when few
training samples are available.
</summary>
    <author>
      <name>Roswitha Bammer</name>
    </author>
    <author>
      <name>Monika Dörfler</name>
    </author>
    <author>
      <name>Pavol Harar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/axioms8040106</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/axioms8040106" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 8 figures, 4 tables. Repository for reproducibility:
  https://gitlab.com/hararticles/gs-gt . Keywords: machine learning; scattering
  transform; Gabor transform; deep learning; time-frequency analysis; CNN.
  Accepted and published after peer revision</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Axioms 2019, 8(4), 106</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.08818v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08818v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09055v1</id>
    <updated>2017-06-27T21:28:31Z</updated>
    <published>2017-06-27T21:28:31Z</published>
    <title>Acoustic Modeling Using a Shallow CNN-HTSVM Architecture</title>
    <summary>  High-accuracy speech recognition is especially challenging when large
datasets are not available. It is possible to bridge this gap with careful and
knowledge-driven parsing combined with the biologically inspired CNN and the
learning guarantees of the Vapnik Chervonenkis (VC) theory. This work presents
a Shallow-CNN-HTSVM (Hierarchical Tree Support Vector Machine classifier)
architecture which uses a predefined knowledge-based set of rules with
statistical machine learning techniques. Here we show that gross errors present
even in state-of-the-art systems can be avoided and that an accurate acoustic
model can be built in a hierarchical fashion. The CNN-HTSVM acoustic model
outperforms traditional GMM-HMM models and the HTSVM structure outperforms a
MLP multi-class classifier. More importantly we isolate the performance of the
acoustic model and provide results on both the frame and phoneme level
considering the true robustness of the model. We show that even with a small
amount of data accurate and robust recognition rates can be obtained.
</summary>
    <author>
      <name>Christopher Dane Shulby</name>
    </author>
    <author>
      <name>Martha Dais Ferreira</name>
    </author>
    <author>
      <name>Rodrigo F. de Mello</name>
    </author>
    <author>
      <name>Sandra Maria Aluisio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pre-review version of Bracis 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.09055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09088v2</id>
    <updated>2017-06-29T02:33:06Z</updated>
    <published>2017-06-28T00:46:50Z</published>
    <title>Modeling Musical Context with Word2vec</title>
    <summary>  We present a semantic vector space model for capturing complex polyphonic
musical context. A word2vec model based on a skip-gram representation with
negative sampling was used to model slices of music from a dataset of
Beethoven's piano sonatas. A visualization of the reduced vector space using
t-distributed stochastic neighbor embedding shows that the resulting embedded
vector space captures tonal relationships, even without any explicit
information about the musical contents of the slices. Secondly, an excerpt of
the Moonlight Sonata from Beethoven was altered by replacing slices based on
context similarity. The resulting music shows that the selected slice based on
similar word2vec context also has a relatively short tonal distance from the
original slice.
</summary>
    <author>
      <name>Dorien Herremans</name>
    </author>
    <author>
      <name>Ching-Hua Chuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Conference on Deep Learning
  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Workshop on Deep Learning
  and Music joint with IJCNN. Anchorage, US. 1(1). pp 11-18 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.09088v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09088v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09386v3</id>
    <updated>2018-07-03T14:31:29Z</updated>
    <published>2017-06-22T08:05:11Z</published>
    <title>On a Novel Speech Representation Using Multitapered Modified Group Delay
  Function</title>
    <summary>  In this paper, a novel multitaper modified group delay function-based
representation for speech signals is proposed. With a set of phoneme-based
experiments, it is shown that the proposed method performs better that an
existing multitaper magnitude (MT-MAG) estimation technique, in terms of
variance and MSE, both in spectral- and cepstral-domains. In particular, the
performance of MT-MOGDF is found to be the best with the Thomson tapers.
Additionally, the utility of the MT-MOGDF technique is highlighted in a speaker
recognition experimental setup, where an improvement of around $20\%$ compared
to the next-best technique is obtained. Moreover, the computational
requirements of the proposed technique is comparable to that of MT-MAG. The
proposed feature can be used in for many speech-related applications; in
particular, it is best suited among those that require information of speaker
and speech.
</summary>
    <author>
      <name>K. C. Narendra</name>
    </author>
    <author>
      <name>R. Kumaraswamy</name>
    </author>
    <author>
      <name>S. Gurugopinath</name>
    </author>
    <link href="http://arxiv.org/abs/1706.09386v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09386v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09551v1</id>
    <updated>2017-06-29T02:33:56Z</updated>
    <published>2017-06-29T02:33:56Z</published>
    <title>Toward Inverse Control of Physics-Based Sound Synthesis</title>
    <summary>  Long Short-Term Memory networks (LSTMs) can be trained to realize inverse
control of physics-based sound synthesizers. Physics-based sound synthesizers
simulate the laws of physics to produce output sound according to input gesture
signals. When a user's gestures are measured in real time, she or he can use
them to control physics-based sound synthesizers, thereby creating simulated
virtual instruments. An intriguing question is how to program a computer to
learn to play such physics-based models. This work demonstrates that LSTMs can
be trained to accomplish this inverse control task with four physics-based
sound synthesizers.
</summary>
    <author>
      <name>A. Pfalz</name>
    </author>
    <author>
      <name>E. Berdahl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Conference on Deep Learning
  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Workshop on Deep Learning
  and Music joint with IJCNN, 1(1). pp 56-61 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.09551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09552v1</id>
    <updated>2017-06-29T02:38:02Z</updated>
    <published>2017-06-29T02:38:02Z</published>
    <title>Chord Label Personalization through Deep Learning of Integrated Harmonic
  Interval-based Representations</title>
    <summary>  The increasing accuracy of automatic chord estimation systems, the
availability of vast amounts of heterogeneous reference annotations, and
insights from annotator subjectivity research make chord label personalization
increasingly important. Nevertheless, automatic chord estimation systems are
historically exclusively trained and evaluated on a single reference
annotation. We introduce a first approach to automatic chord label
personalization by modeling subjectivity through deep learning of a harmonic
interval-based chord label representation. After integrating these
representations from multiple annotators, we can accurately personalize chord
labels for individual annotators from a single model and the annotators' chord
label vocabulary. Furthermore, we show that chord personalization using
multiple reference annotations outperforms using a single reference annotation.
</summary>
    <author>
      <name>H. V. Koops</name>
    </author>
    <author>
      <name>W. B. de Haas</name>
    </author>
    <author>
      <name>J. Bransen</name>
    </author>
    <author>
      <name>A. Volk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Conference on Deep Learning
  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of the Int. Workshop on Deep Learning and Music. Anchorage,
  US. 1(1). pp19-25 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.09552v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09552v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09553v1</id>
    <updated>2017-06-29T02:39:00Z</updated>
    <published>2017-06-29T02:39:00Z</published>
    <title>Transforming Musical Signals through a Genre Classifying Convolutional
  Neural Network</title>
    <summary>  Convolutional neural networks (CNNs) have been successfully applied on both
discriminative and generative modeling for music-related tasks. For a
particular task, the trained CNN contains information representing the decision
making or the abstracting process. One can hope to manipulate existing music
based on this 'informed' network and create music with new features
corresponding to the knowledge obtained by the network. In this paper, we
propose a method to utilize the stored information from a CNN trained on
musical genre classification task. The network was composed of three
convolutional layers, and was trained to classify five-second song clips into
five different genres. After training, randomly selected clips were modified by
maximizing the sum of outputs from the network layers. In addition to the
potential of such CNNs to produce interesting audio transformation, more
information about the network and the original music could be obtained from the
analysis of the generated features since these features indicate how the
network 'understands' the music.
</summary>
    <author>
      <name>S. Geng</name>
    </author>
    <author>
      <name>G. Ren</name>
    </author>
    <author>
      <name>M. Ogihara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Conference on Deep Learning
  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of the First Int. Workshop on Deep Learning and Music joint
  with IJCNN. Anchorage, US. 1(1). pp 48-49 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.09553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09555v1</id>
    <updated>2017-06-29T02:41:30Z</updated>
    <published>2017-06-29T02:41:30Z</published>
    <title>Music Signal Processing Using Vector Product Neural Networks</title>
    <summary>  We propose a novel neural network model for music signal processing using
vector product neurons and dimensionality transformations. Here, the inputs are
first mapped from real values into three-dimensional vectors then fed into a
three-dimensional vector product neural network where the inputs, outputs, and
weights are all three-dimensional values. Next, the final outputs are mapped
back to the reals. Two methods for dimensionality transformation are proposed,
one via context windows and the other via spectral coloring. Experimental
results on the iKala dataset for blind singing voice separation confirm the
efficacy of our model.
</summary>
    <author>
      <name>Z. C. Fan</name>
    </author>
    <author>
      <name>T. S. Chan</name>
    </author>
    <author>
      <name>Y. H. Yang</name>
    </author>
    <author>
      <name>J. S. R. Jang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Conference on Deep Learning
  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of the First Int. Workshop on Deep Learning and Music joint
  with IJCNN. Anchorage, US. 1(1). pp 36-30 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.09555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09558v1</id>
    <updated>2017-06-29T03:03:35Z</updated>
    <published>2017-06-29T03:03:35Z</published>
    <title>Talking Drums: Generating drum grooves with neural networks</title>
    <summary>  Presented is a method of generating a full drum kit part for a provided
kick-drum sequence. A sequence to sequence neural network model used in natural
language translation was adopted to encode multiple musical styles and an
online survey was developed to test different techniques for sampling the
output of the softmax function. The strongest results were found using a
sampling technique that drew from the three most probable outputs at each
subdivision of the drum pattern but the consistency of output was found to be
heavily dependent on style.
</summary>
    <author>
      <name>P. Hutchings</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Conference on Deep Learning
  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Workshop on Deep Learning
  and Music joint with IJCNN. Anchorage, US. 1(1). pp 43-47 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.09558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09559v1</id>
    <updated>2017-06-29T03:04:06Z</updated>
    <published>2017-06-29T03:04:06Z</published>
    <title>Audio Spectrogram Representations for Processing with Convolutional
  Neural Networks</title>
    <summary>  One of the decisions that arise when designing a neural network for any
application is how the data should be represented in order to be presented to,
and possibly generated by, a neural network. For audio, the choice is less
obvious than it seems to be for visual images, and a variety of representations
have been used for different applications including the raw digitized sample
stream, hand-crafted features, machine discovered features, MFCCs and variants
that include deltas, and a variety of spectral representations. This paper
reviews some of these representations and issues that arise, focusing
particularly on spectrograms for generating audio using neural networks for
style transfer.
</summary>
    <author>
      <name>L. Wyse</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Conference on Deep Learning
  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Workshop on Deep Learning
  and Music joint with IJCNN. Anchorage, US. 1(1). pp 37-41 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.09559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09588v1</id>
    <updated>2017-06-29T05:56:06Z</updated>
    <published>2017-06-29T05:56:06Z</published>
    <title>Multi-scale Multi-band DenseNets for Audio Source Separation</title>
    <summary>  This paper deals with the problem of audio source separation. To handle the
complex and ill-posed nature of the problems of audio source separation, the
current state-of-the-art approaches employ deep neural networks to obtain
instrumental spectra from a mixture. In this study, we propose a novel network
architecture that extends the recently developed densely connected
convolutional network (DenseNet), which has shown excellent results on image
classification tasks. To deal with the specific problem of audio source
separation, an up-sampling layer, block skip connection and band-dedicated
dense blocks are incorporated on top of DenseNet. The proposed approach takes
advantage of long contextual information and outperforms state-of-the-art
results on SiSEC 2016 competition by a large margin in terms of
signal-to-distortion ratio. Moreover, the proposed architecture requires
significantly fewer parameters and considerably less training time compared
with other methods.
</summary>
    <author>
      <name>Naoya Takahashi</name>
    </author>
    <author>
      <name>Yuki Mitsufuji</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear at WASPAA 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.09588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09716v1</id>
    <updated>2017-06-29T12:33:27Z</updated>
    <published>2017-06-29T12:33:27Z</published>
    <title>Enhancing speaker identification performance under the shouted talking
  condition using second-order circular hidden Markov models</title>
    <summary>  It is known that the performance of speaker identification systems is high
under the neutral talking condition; however, the performance deteriorates
under the shouted talking condition. In this paper, second-order circular
hidden Markov models (CHMM2s) have been proposed and implemented to enhance the
performance of isolated-word text-dependent speaker identification systems
under the shouted talking condition. Our results show that CHMM2s significantly
improve speaker identification performance under such a condition compared to
the first-order left-to-right hidden Markov models (LTRHMM1s), second-order
left-to-right hidden Markov models (LTRHMM2s), and the first-order circular
hidden Markov models (CHMM1s). Under the shouted talking condition, our results
show that the average speaker identification performance is 23% based on
LTRHMM1s, 59% based on LTRHMM2s, and 60% based on CHMM1s. On the other hand,
the average speaker identification performance under the same talking condition
based on CHMM2s is 72%.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.specom.2006.01.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.specom.2006.01.005" rel="related"/>
    <link href="http://arxiv.org/abs/1706.09716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09722v1</id>
    <updated>2017-06-29T12:45:35Z</updated>
    <published>2017-06-29T12:45:35Z</published>
    <title>Employing Second-Order Circular Suprasegmental Hidden Markov Models to
  Enhance Speaker Identification Performance in Shouted Talking Environments</title>
    <summary>  Speaker identification performance is almost perfect in neutral talking
environments; however, the performance is deteriorated significantly in shouted
talking environments. This work is devoted to proposing, implementing and
evaluating new models called Second-Order Circular Suprasegmental Hidden Markov
Models (CSPHMM2s) to alleviate the deteriorated performance in the shouted
talking environments. These proposed models possess the characteristics of both
Circular Suprasegmental Hidden Markov Models (CSPHMMs) and Second-Order
Suprasegmental Hidden Markov Models (SPHMM2s). The results of this work show
that CSPHMM2s outperform each of: First-Order Left-to-Right Suprasegmental
Hidden Markov Models (LTRSPHMM1s), Second-Order Left-to-Right Suprasegmental
Hidden Markov Models (LTRSPHMM2s) and First-Order Circular Suprasegmental
Hidden Markov Models (CSPHMM1s) in the shouted talking environments. In such
talking environments and using our collected speech database, average speaker
identification performance based on LTRSPHMM1s, LTRSPHMM2s, CSPHMM1s and
CSPHMM2s is 74.6%, 78.4%, 78.7% and 83.4%, respectively. Speaker identification
performance obtained based on CSPHMM2s is close to that obtained based on
subjective assessment by human listeners.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1155/2010/862138</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1155/2010/862138" rel="related"/>
    <link href="http://arxiv.org/abs/1706.09722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09729v1</id>
    <updated>2017-06-29T12:54:31Z</updated>
    <published>2017-06-29T12:54:31Z</published>
    <title>Talking Condition Recognition in Stressful and Emotional Talking
  Environments Based on CSPHMM2s</title>
    <summary>  This work is aimed at exploiting Second-Order Circular Suprasegmental Hidden
Markov Models (CSPHMM2s) as classifiers to enhance talking condition
recognition in stressful and emotional talking environments (completely two
separate environments). The stressful talking environment that has been used in
this work uses Speech Under Simulated and Actual Stress (SUSAS) database, while
the emotional talking environment uses Emotional Prosody Speech and Transcripts
(EPST) database. The achieved results of this work using Mel-Frequency Cepstral
Coefficients (MFCCs) demonstrate that CSPHMM2s outperform each of Hidden Markov
Models (HMMs), Second-Order Circular Hidden Markov Models (CHMM2s), and
Suprasegmental Hidden Markov Models (SPHMMs) in enhancing talking condition
recognition in the stressful and emotional talking environments. The results
also show that the performance of talking condition recognition in stressful
talking environments leads that in emotional talking environments by 3.67%
based on CSPHMM2s. Our results obtained in subjective evaluation by human
judges fall within 2.14% and 3.08% of those obtained, respectively, in
stressful and emotional talking environments based on CSPHMM2s.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <author>
      <name>Mohammed Nasser Ba-Hutair</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10772-014-9251-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10772-014-9251-7" rel="related"/>
    <link href="http://arxiv.org/abs/1706.09729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09760v1</id>
    <updated>2017-06-29T13:57:12Z</updated>
    <published>2017-06-29T13:57:12Z</published>
    <title>Employing both Gender and Emotion Cues to Enhance Speaker Identification
  Performance in Emotional Talking Environments</title>
    <summary>  Speaker recognition performance in emotional talking environments is not as
high as it is in neutral talking environments. This work focuses on proposing,
implementing, and evaluating a new approach to enhance the performance in
emotional talking environments. The new proposed approach is based on
identifying the unknown speaker using both his/her gender and emotion cues.
Both Hidden Markov Models (HMMs) and Suprasegmental Hidden Markov Models
(SPHMMs) have been used as classifiers in this work. This approach has been
tested on our collected emotional speech database which is composed of six
emotions. The results of this work show that speaker identification performance
based on using both gender and emotion cues is higher than that based on using
gender cues only, emotion cues only, and neither gender nor emotion cues by
7.22%, 4.45%, and 19.56%, respectively. This work also shows that the optimum
speaker identification performance takes place when the classifiers are
completely biased towards suprasegmental models and no impact of acoustic
models in the emotional talking environments. The achieved average speaker
identification performance based on the new proposed approach falls within
2.35% of that obtained in subjective evaluation by human judges.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10772-013-9188-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10772-013-9188-2" rel="related"/>
    <link href="http://arxiv.org/abs/1706.09760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.10006v2</id>
    <updated>2017-10-24T11:36:08Z</updated>
    <published>2017-06-30T02:55:55Z</published>
    <title>Automated Audio Captioning with Recurrent Neural Networks</title>
    <summary>  We present the first approach to automated audio captioning. We employ an
encoder-decoder scheme with an alignment model in between. The input to the
encoder is a sequence of log mel-band energies calculated from an audio file,
while the output is a sequence of words, i.e. a caption. The encoder is a
multi-layered, bi-directional gated recurrent unit (GRU) and the decoder a
multi-layered GRU with a classification layer connected to the last GRU of the
decoder. The classification layer and the alignment model are fully connected
layers with shared weights between timesteps. The proposed method is evaluated
using data drawn from a commercial sound effects library, ProSound Effects. The
resulting captions were rated through metrics utilized in machine translation
and image captioning fields. Results from metrics show that the proposed method
can predict words appearing in the original caption, but not always correctly
ordered.
</summary>
    <author>
      <name>Konstantinos Drossos</name>
    </author>
    <author>
      <name>Sharath Adavanne</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 11th IEEE Workshop on Applications of Signal
  Processing to Audio and Acoustics (WASPAA), 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.10006v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.10006v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00160v3</id>
    <updated>2017-07-30T11:13:30Z</updated>
    <published>2017-07-01T14:06:51Z</published>
    <title>An Augmented Lagrangian Method for Piano Transcription using Equal
  Loudness Thresholding and LSTM-based Decoding</title>
    <summary>  A central goal in automatic music transcription is to detect individual note
events in music recordings. An important variant is instrument-dependent music
transcription where methods can use calibration data for the instruments in
use. However, despite the additional information, results rarely exceed an
f-measure of 80%. As a potential explanation, the transcription problem can be
shown to be badly conditioned and thus relies on appropriate regularization. A
recently proposed method employs a mixture of simple, convex regularizers (to
stabilize the parameter estimation process) and more complex terms (to
encourage more meaningful structure). In this paper, we present two extensions
to this method. First, we integrate a computational loudness model to better
differentiate real from spurious note detections. Second, we employ
(Bidirectional) Long Short Term Memory networks to re-weight the likelihood of
detected note constellations. Despite their simplicity, our two extensions lead
to a drop of about 35% in note error rate compared to the state-of-the-art.
</summary>
    <author>
      <name>Sebastian Ewert</name>
    </author>
    <author>
      <name>Mark B. Sandler</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the IEEE Workshop on Applications of Signal
  Processing to Audio and Acoustics (WASPAA), New Paltz, NY, USA, pp. 146-150,
  2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1707.00160v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00160v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00680v1</id>
    <updated>2017-07-01T11:00:36Z</updated>
    <published>2017-07-01T11:00:36Z</published>
    <title>Studying and Enhancing Talking Condition Recognition in Stressful and
  Emotional Talking Environments Based on HMMs, CHMM2s and SPHMMs</title>
    <summary>  The work of this research is devoted to studying and enhancing talking
condition recognition in stressful and emotional talking environments
(completely two separate environments) based on three different and separate
classifiers. The three classifiers are: Hidden Markov Models (HMMs),
Second-Order Circular Hidden Markov Models (CHMM2s) and Suprasegmental Hidden
Markov Models (SPHMMs). The stressful talking environments that have been used
in this work are composed of neutral, shouted, slow, loud, soft and fast
talking conditions, while the emotional talking environments are made up of
neutral, angry, sad, happy, disgust and fear emotions. The achieved results in
the current work show that SPHMMs lead each of HMMs and CHMM2s in improving
talking condition recognition in stressful and emotional talking environments.
The results also demonstrate that talking condition recognition in stressful
talking environments outperforms that in emotional talking environments by
2.7%, 1.8% and 3.3% based on HMMs, CHMM2s and SPHMMs, respectively. Based on
subjective assessment by human judges, the recognition performance of stressful
talking conditions leads that of emotional ones by 5.2%.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s12193-011-0082-4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s12193-011-0082-4" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1706.09729</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal on Multimodal User Interfaces July 2012, Volume 6, Issue
  1-2, pp 59-71</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1707.00680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00972v1</id>
    <updated>2017-07-04T13:31:34Z</updated>
    <published>2017-07-04T13:31:34Z</published>
    <title>Automatic estimation of harmonic tension by distributed representation
  of chords</title>
    <summary>  The buildup and release of a sense of tension is one of the most essential
aspects of the process of listening to music. A veridical computational model
of perceived musical tension would be an important ingredient for many music
informatics applications. The present paper presents a new approach to
modelling harmonic tension based on a distributed representation of chords. The
starting hypothesis is that harmonic tension as perceived by human listeners is
related, among other things, to the expectedness of harmonic units (chords) in
their local harmonic context. We train a word2vec-type neural network to learn
a vector space that captures contextual similarity and expectedness, and define
a quantitative measure of harmonic tension on top of this. To assess the
veridicality of the model, we compare its outputs on a number of well-defined
chord classes and cadential contexts to results from pertinent empirical
studies in music psychology. Statistical analysis shows that the model's
predictions conform very well with empirical evidence obtained from human
listeners.
</summary>
    <author>
      <name>Ali Nikrang</name>
    </author>
    <author>
      <name>David R. W. Sears</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures. To appear in Proceedings of the 13th
  International Symposium on Computer Music Multidisciplinary Research (CMMR),
  Porto, Portugal</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.00972v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00972v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01670v2</id>
    <updated>2017-07-11T04:00:23Z</updated>
    <published>2017-07-06T08:01:24Z</published>
    <title>Statistical Parametric Speech Synthesis Using Generative Adversarial
  Networks Under A Multi-task Learning Framework</title>
    <summary>  In this paper, we aim at improving the performance of synthesized speech in
statistical parametric speech synthesis (SPSS) based on a generative
adversarial network (GAN). In particular, we propose a novel architecture
combining the traditional acoustic loss function and the GAN's discriminative
loss under a multi-task learning (MTL) framework. The mean squared error (MSE)
is usually used to estimate the parameters of deep neural networks, which only
considers the numerical difference between the raw audio and the synthesized
one. To mitigate this problem, we introduce the GAN as a second task to
determine if the input is a natural speech with specific conditions. In this
MTL framework, the MSE optimization improves the stability of GAN, and at the
same time GAN produces samples with a distribution closer to natural speech.
Listening tests show that the multi-task architecture can generate more natural
speech that satisfies human perception than the conventional methods.
</summary>
    <author>
      <name>Shan Yang</name>
    </author>
    <author>
      <name>Lei Xie</name>
    </author>
    <author>
      <name>Xiao Chen</name>
    </author>
    <author>
      <name>Xiaoyan Lou</name>
    </author>
    <author>
      <name>Xuan Zhu</name>
    </author>
    <author>
      <name>Dongyan Huang</name>
    </author>
    <author>
      <name>Haizhou Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Automatic Speech Recognition and Understanding (ASRU)
  2017 Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.01670v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01670v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02651v3</id>
    <updated>2018-01-31T04:25:19Z</updated>
    <published>2017-07-09T22:37:26Z</published>
    <title>Model-Based Speech Enhancement in the Modulation Domain</title>
    <summary>  This paper presents an algorithm for modulation-domain speech enhancement
using a Kalman filter. The proposed estimator jointly models the estimated
dynamics of the spectral amplitudes of speech and noise to obtain an MMSE
estimation of the speech amplitude spectrum with the assumption that the speech
and noise are additive in the complex domain. In order to include the dynamics
of noise amplitudes with those of speech amplitudes, we propose a statistical
"Gaussring" model that comprises a mixture of Gaussians whose centers lie in a
circle on the complex plane. The performance of the proposed algorithm is
evaluated using the perceptual evaluation of speech quality measure, segmental
SNR measure, and short-time objective intelligibility measure. For speech
quality measures, the proposed algorithm is shown to give a consistent
improvement over a wide range of SNRs when compared to competitive algorithms.
Speech recognition experiments also show that the Gaussring-model-based
algorithm performs well for two types of noise.
</summary>
    <author>
      <name>Yu Wang</name>
    </author>
    <author>
      <name>Mike Brookes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2017.2786863</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2017.2786863" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 18 figures; IEEE/ACM Transactions on Audio, Speech and
  Language Processing, Vol. 26, No. 3, Mar. 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.02651v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02651v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02661v1</id>
    <updated>2017-07-10T00:14:32Z</updated>
    <published>2017-07-10T00:14:32Z</published>
    <title>Feature Joint-State Posterior Estimation in Factorial Speech Processing
  Models using Deep Neural Networks</title>
    <summary>  This paper proposes a new method for calculating joint-state posteriors of
mixed-audio features using deep neural networks to be used in factorial speech
processing models. The joint-state posterior information is required in
factorial models to perform joint-decoding. The novelty of this work is its
architecture which enables the network to infer joint-state posteriors from the
pairs of state posteriors of stereo features. This paper defines an objective
function to solve an underdetermined system of equations, which is used by the
network for extracting joint-state posteriors. It develops the required
expressions for fine-tuning the network in a unified way. The experiments
compare the proposed network decoding results to those of the vector Taylor
series method and show 2.3% absolute performance improvement in the monaural
speech separation and recognition challenge. This achievement is substantial
when we consider the simplicity of joint-state posterior extraction provided by
deep neural networks.
</summary>
    <author>
      <name>Mahdi Khademian</name>
    </author>
    <author>
      <name>Mohammad Mehdi Homayounpour</name>
    </author>
    <link href="http://arxiv.org/abs/1707.02661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.03544v1</id>
    <updated>2017-07-12T05:15:23Z</updated>
    <published>2017-07-12T05:15:23Z</published>
    <title>Score-informed syllable segmentation for a cappella singing voice with
  convolutional neural networks</title>
    <summary>  This paper introduces a new score-informed method for the segmentation of
jingju a cappella singing phrase into syllables. The proposed method estimates
the most likely sequence of syllable boundaries given the estimated syllable
onset detection function (ODF) and its score. Throughout the paper, we first
examine the jingju syllables structure and propose a definition of the term
"syllable onset". Then, we identify which are the challenges that jingju a
cappella singing poses. Further, we investigate how to improve the syllable ODF
estimation with convolutional neural networks (CNNs). We propose a novel CNN
architecture that allows to efficiently capture different time-frequency scales
for estimating syllable onsets. In addition, we propose using a score-informed
Viterbi algorithm -instead of thresholding the onset function-, because the
available musical knowledge we have (the score) can be used to inform the
Viterbi algorithm in order to overcome the identified challenges. The proposed
method outperforms the state-of-the-art in syllable segmentation for jingju a
cappella singing. We further provide an analysis of the segmentation errors
which points possible research directions.
</summary>
    <author>
      <name>Jordi Pons</name>
    </author>
    <author>
      <name>Rong Gong</name>
    </author>
    <author>
      <name>Xavier Serra</name>
    </author>
    <link href="http://arxiv.org/abs/1707.03544v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.03544v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.03547v1</id>
    <updated>2017-07-12T05:32:22Z</updated>
    <published>2017-07-12T05:32:22Z</published>
    <title>Audio to score matching by combining phonetic and duration information</title>
    <summary>  We approach the singing phrase audio to score matching problem by using
phonetic and duration information - with a focus on studying the jingju a
cappella singing case. We argue that, due to the existence of a basic melodic
contour for each mode in jingju music, only using melodic information (such as
pitch contour) will result in an ambiguous matching. This leads us to propose a
matching approach based on the use of phonetic and duration information.
Phonetic information is extracted with an acoustic model shaped with our data,
and duration information is considered with the Hidden Markov Models (HMMs)
variants we investigate. We build a model for each lyric path in our scores and
we achieve the matching by ranking the posterior probabilities of the decoded
most likely state sequences. Three acoustic models are investigated: (i)
convolutional neural networks (CNNs), (ii) deep neural networks (DNNs) and
(iii) Gaussian mixture models (GMMs). Also, two duration models are compared:
(i) hidden semi-Markov model (HSMM) and (ii) post-processor duration model.
Results show that CNNs perform better in our (small) audio dataset and also
that HSMM outperforms the post-processor duration model.
</summary>
    <author>
      <name>Rong Gong</name>
    </author>
    <author>
      <name>Jordi Pons</name>
    </author>
    <author>
      <name>Xavier Serra</name>
    </author>
    <link href="http://arxiv.org/abs/1707.03547v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.03547v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.04504v1</id>
    <updated>2017-07-14T13:25:44Z</updated>
    <published>2017-07-14T13:25:44Z</published>
    <title>Localization of Sound Sources in a Room with One Microphone</title>
    <summary>  Estimation of the location of sound sources is usually done using microphone
arrays. Such settings provide an environment where we know the difference
between the received signals among different microphones in the terms of phase
or attenuation, which enables localization of the sound sources. In our
solution we exploit the properties of the room transfer function in order to
localize a sound source inside a room with only one microphone. The shape of
the room and the position of the microphone are assumed to be known. The design
guidelines and limitations of the sensing matrix are given. Implementation is
based on the sparsity in the terms of voxels in a room that are occupied by a
source. What is especially interesting about our solution is that we provide
localization of the sound sources not only in the horizontal plane, but in the
terms of the 3D coordinates inside the room.
</summary>
    <author>
      <name>Helena Peic Tukuljac</name>
    </author>
    <author>
      <name>Herve Lissek</name>
    </author>
    <author>
      <name>Pierre Vandergheynst</name>
    </author>
    <link href="http://arxiv.org/abs/1707.04504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.04504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.04642v2</id>
    <updated>2017-10-19T08:27:01Z</updated>
    <published>2017-07-14T21:17:24Z</published>
    <title>Recognizing Abnormal Heart Sounds Using Deep Learning</title>
    <summary>  The work presented here applies deep learning to the task of automated
cardiac auscultation, i.e. recognizing abnormalities in heart sounds. We
describe an automated heart sound classification algorithm that combines the
use of time-frequency heat map representations with a deep convolutional neural
network (CNN). Given the cost-sensitive nature of misclassification, our CNN
architecture is trained using a modified loss function that directly optimizes
the trade-off between sensitivity and specificity. We evaluated our algorithm
at the 2016 PhysioNet Computing in Cardiology challenge where the objective was
to accurately classify normal and abnormal heart sounds from single, short,
potentially noisy recordings. Our entry to the challenge achieved a final
specificity of 0.95, sensitivity of 0.73 and overall score of 0.84. We achieved
the greatest specificity score out of all challenge entries and, using just a
single CNN, our algorithm differed in overall score by only 0.02 compared to
the top place finisher, which used an ensemble approach.
</summary>
    <author>
      <name>Jonathan Rubin</name>
    </author>
    <author>
      <name>Rui Abreu</name>
    </author>
    <author>
      <name>Anurag Ganguli</name>
    </author>
    <author>
      <name>Saigopal Nelaturi</name>
    </author>
    <author>
      <name>Ion Matei</name>
    </author>
    <author>
      <name>Kumar Sricharan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IJCAI 2017 Knowledge Discovery in Healthcare Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.04642v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.04642v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06163v1</id>
    <updated>2017-07-19T15:39:09Z</updated>
    <published>2017-07-19T15:39:09Z</published>
    <title>Metrical-accent Aware Vocal Onset Detection in Polyphonic Audio</title>
    <summary>  The goal of this study is the automatic detection of onsets of the singing
voice in polyphonic audio recordings. Starting with a hypothesis that the
knowledge of the current position in a metrical cycle (i.e. metrical accent)
can improve the accuracy of vocal note onset detection, we propose a novel
probabilistic model to jointly track beats and vocal note onsets. The proposed
model extends a state of the art model for beat and meter tracking, in which
a-priori probability of a note at a specific metrical accent interacts with the
probability of observing a vocal note onset. We carry out an evaluation on a
varied collection of multi-instrument datasets from two music traditions
(English popular music and Turkish makam) with different types of metrical
cycles and singing styles. Results confirm that the proposed model reasonably
improves vocal note onset detection accuracy compared to a baseline model that
does not take metrical position into account.
</summary>
    <author>
      <name>Georgi Dzhambazov</name>
    </author>
    <author>
      <name>Andre Holzapfel</name>
    </author>
    <author>
      <name>Ajay Srinivasamurthy</name>
    </author>
    <author>
      <name>Xavier Serra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Society for Music Information Retrieval Conferece
  (ISMIR 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.06163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06231v1</id>
    <updated>2017-07-19T16:37:23Z</updated>
    <published>2017-07-19T16:37:23Z</published>
    <title>From Bach to the Beatles: The simulation of human tonal expectation
  using ecologically-trained predictive models</title>
    <summary>  Tonal structure is in part conveyed by statistical regularities between
musical events, and research has shown that computational models reflect tonal
structure in music by capturing these regularities in schematic constructs like
pitch histograms. Of the few studies that model the acquisition of perceptual
learning from musical data, most have employed self-organizing models that
learn a topology of static descriptions of musical contexts. Also, the stimuli
used to train these models are often symbolic rather than acoustically faithful
representations of musical material. In this work we investigate whether
sequential predictive models of musical memory (specifically, recurrent neural
networks), trained on audio from commercial CD recordings, induce tonal
knowledge in a similar manner to listeners (as shown in behavioral studies in
music perception). Our experiments indicate that various types of recurrent
neural networks produce musical expectations that clearly convey tonal
structure. Furthermore, the results imply that although implicit knowledge of
tonal structure is a necessary condition for accurate musical expectation, the
most accurate predictive models also use other cues beyond the tonal structure
of the musical context.
</summary>
    <author>
      <name>Carlos Cancino-Chacón</name>
    </author>
    <author>
      <name>Maarten Grachten</name>
    </author>
    <author>
      <name>Kat Agres</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 18th International Society of Music Information
  Retrieval Conference (ISMIR 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.06231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09917v1</id>
    <updated>2017-07-12T02:00:15Z</updated>
    <published>2017-07-12T02:00:15Z</published>
    <title>A breakthrough in Speech emotion recognition using Deep Retinal
  Convolution Neural Networks</title>
    <summary>  Speech emotion recognition (SER) is to study the formation and change of
speaker's emotional state from the speech signal perspective, so as to make the
interaction between human and computer more intelligent. SER is a challenging
task that has encountered the problem of less training data and low prediction
accuracy. Here we propose a data augmentation algorithm based on the imaging
principle of the retina and convex lens, to acquire the different sizes of
spectrogram and increase the amount of training data by changing the distance
between the spectrogram and the convex lens. Meanwhile, with the help of deep
learning to get the high-level features, we propose the Deep Retinal
Convolution Neural Networks (DRCNNs) for SER and achieve the average accuracy
over 99%. The experimental results indicate that DRCNNs outperforms the
previous studies in terms of both the number of emotions and the accuracy of
recognition. Predictably, our results will dramatically improve human-computer
interaction.
</summary>
    <author>
      <name>Yafeng Niu</name>
    </author>
    <author>
      <name>Dongsheng Zou</name>
    </author>
    <author>
      <name>Yadong Niu</name>
    </author>
    <author>
      <name>Zhongshi He</name>
    </author>
    <author>
      <name>Hua Tan</name>
    </author>
    <link href="http://arxiv.org/abs/1707.09917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00853v1</id>
    <updated>2017-08-02T20:07:39Z</updated>
    <published>2017-08-02T20:07:39Z</published>
    <title>Audio Super Resolution using Neural Networks</title>
    <summary>  We introduce a new audio processing technique that increases the sampling
rate of signals such as speech or music using deep convolutional neural
networks. Our model is trained on pairs of low and high-quality audio examples;
at test-time, it predicts missing samples within a low-resolution signal in an
interpolation process similar to image super-resolution. Our method is simple
and does not involve specialized audio processing techniques; in our
experiments, it outperforms baselines on standard speech and music benchmarks
at upscaling ratios of 2x, 4x, and 6x. The method has practical applications in
telephony, compression, and text-to-speech generation; it demonstrates the
effectiveness of feed-forward convolutional architectures on an audio
generation task.
</summary>
    <author>
      <name>Volodymyr Kuleshov</name>
    </author>
    <author>
      <name>S. Zayd Enam</name>
    </author>
    <author>
      <name>Stefano Ermon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 5th International Conference on Learning
  Representations (ICLR) 2017, Workshop Track, Toulon, France</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.00853v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00853v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.01227v2</id>
    <updated>2017-08-28T14:29:48Z</updated>
    <published>2017-08-03T17:18:58Z</published>
    <title>Autoencoder based Domain Adaptation for Speaker Recognition under
  Insufficient Channel Information</title>
    <summary>  In real-life conditions, mismatch between development and test domain
degrades speaker recognition performance. To solve the issue, many researchers
explored domain adaptation approaches using matched in-domain dataset. However,
adaptation would be not effective if the dataset is insufficient to estimate
channel variability of the domain. In this paper, we explore the problem of
performance degradation under such a situation of insufficient channel
information. In order to exploit limited in-domain dataset effectively, we
propose an unsupervised domain adaptation approach using Autoencoder based
Domain Adaptation (AEDA). The proposed approach combines an autoencoder with a
denoising autoencoder to adapt resource-rich development dataset to test
domain. The proposed technique is evaluated on the Domain Adaptation Challenge
13 experimental protocols that is widely used in speaker recognition for domain
mismatched condition. The results show significant improvements over baselines
and results from other prior studies.
</summary>
    <author>
      <name>Suwon Shon</name>
    </author>
    <author>
      <name>Seongkyu Mun</name>
    </author>
    <author>
      <name>Wooil Kim</name>
    </author>
    <author>
      <name>Hanseok Ko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Interspeech 2017, pp 1014-1018</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.01227v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.01227v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.01232v2</id>
    <updated>2017-08-28T14:30:07Z</updated>
    <published>2017-08-03T17:24:31Z</published>
    <title>Recursive Whitening Transformation for Speaker Recognition on Language
  Mismatched Condition</title>
    <summary>  Recently in speaker recognition, performance degradation due to the channel
domain mismatched condition has been actively addressed. However, the
mismatches arising from language is yet to be sufficiently addressed. This
paper proposes an approach which employs recursive whitening transformation to
mitigate the language mismatched condition. The proposed method is based on the
multiple whitening transformation, which is intended to remove un-whitened
residual components in the dataset associated with i-vector length
normalization. The experiments were conducted on the Speaker Recognition
Evaluation 2016 trials of which the task is non-English speaker recognition
using development dataset consist of both a large scale out-of-domain (English)
dataset and an extremely low-quantity in-domain (non-English) dataset. For
performance comparison, we develop a state-of- the-art system using deep neural
network and bottleneck feature, which is based on a phonetically aware model.
From the experimental results, along with other prior studies, effectiveness of
the proposed method on language mismatched condition is validated.
</summary>
    <author>
      <name>Suwon Shon</name>
    </author>
    <author>
      <name>Seongkyu Mun</name>
    </author>
    <author>
      <name>Hanseok Ko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Interspeech 2017, pp 2869-2873</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.01232v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.01232v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.02171v1</id>
    <updated>2017-08-07T15:42:48Z</updated>
    <published>2017-08-07T15:42:48Z</published>
    <title>Phase-Aware Single-Channel Speech Enhancement with Modulation-Domain
  Kalman Filtering</title>
    <summary>  We present a single-channel phase-sensitive speech enhancement algorithm that
is based on modulation-domain Kalman filtering and on tracking the speech phase
using circular statistics. With Kalman filtering, using that speech and noise
are additive in the complex STFT domain, the algorithm tracks the speech
log-spectrum, the noise log-spectrum and the speech phase. Joint amplitude and
phase estimation of speech is performed. Given the noisy speech signal,
conventional algorithms use the noisy phase for signal reconstruction
approximating the speech phase with the noisy phase. In the proposed Kalman
filtering algorithm, the speech phase posterior is used to create an enhanced
speech phase spectrum for signal reconstruction. The Kalman filter prediction
models the temporal/inter-frame correlation of the speech and noise log-spectra
and of the speech phase, while the Kalman filter update models their nonlinear
relations. With the proposed algorithm, speech is tracked and estimated both in
the log-spectral and spectral phase domains. The algorithm is evaluated in
terms of speech quality and different algorithm configurations, dependent on
the signal model, are compared in different noise types. Experimental results
show that the proposed algorithm outperforms traditional enhancement algorithms
over a range of SNRs for various noise types.
</summary>
    <author>
      <name>Nikolaos Dionelis</name>
    </author>
    <author>
      <name>Mike Brookes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 17 figures, Submitted to IEEE/ACM Transactions on Audio,
  Speech and Language Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.02171v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.02171v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.02840v2</id>
    <updated>2017-09-15T13:49:45Z</updated>
    <published>2017-08-09T13:53:01Z</published>
    <title>Speaker Diarization using Deep Recurrent Convolutional Neural Networks
  for Speaker Embeddings</title>
    <summary>  In this paper we propose a new method of speaker diarization that employs a
deep learning architecture to learn speaker embeddings. In contrast to the
traditional approaches that build their speaker embeddings using manually
hand-crafted spectral features, we propose to train for this purpose a
recurrent convolutional neural network applied directly on magnitude
spectrograms. To compare our approach with the state of the art, we collect and
release for the public an additional dataset of over 6 hours of fully annotated
broadcast material. The results of our evaluation on the new dataset and three
other benchmark datasets show that our proposed method significantly
outperforms the competitors and reduces diarization error rate by a large
margin of over 30% with respect to the baseline.
</summary>
    <author>
      <name>Pawel Cyrta</name>
    </author>
    <author>
      <name>Tomasz Trzciński</name>
    </author>
    <author>
      <name>Wojciech Stokowiec</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-67220-5_10</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-67220-5_10" rel="related"/>
    <link href="http://arxiv.org/abs/1708.02840v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.02840v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03211v2</id>
    <updated>2017-10-18T14:38:07Z</updated>
    <published>2017-08-10T13:44:31Z</published>
    <title>DNN and CNN with Weighted and Multi-task Loss Functions for Audio Event
  Detection</title>
    <summary>  This report presents our audio event detection system submitted for Task 2,
"Detection of rare sound events", of DCASE 2017 challenge. The proposed system
is based on convolutional neural networks (CNNs) and deep neural networks
(DNNs) coupled with novel weighted and multi-task loss functions and
state-of-the-art phase-aware signal enhancement. The loss functions are
tailored for audio event detection in audio streams. The weighted loss is
designed to tackle the common issue of imbalanced data in background/foreground
classification while the multi-task loss enables the networks to simultaneously
model the class distribution and the temporal structures of the target events
for recognition. Our proposed systems significantly outperform the challenge
baseline, improving F-score from 72.7% to 90.0% and reducing detection error
rate from 0.53 to 0.18 on average on the development data. On the evaluation
data, our submission obtains an average F1-score of 88.3% and an error rate of
0.22 which are significantly better than those obtained by the DCASE baseline
(i.e. an F1-score of 64.1% and an error rate of 0.64).
</summary>
    <author>
      <name>Huy Phan</name>
    </author>
    <author>
      <name>Martin Krawczyk-Becker</name>
    </author>
    <author>
      <name>Timo Gerkmann</name>
    </author>
    <author>
      <name>Alfred Mertins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">DCASE 2017 technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.03211v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03211v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03822v2</id>
    <updated>2018-09-28T21:44:01Z</updated>
    <published>2017-08-12T21:08:18Z</published>
    <title>Classical Music Composition Using State Space Models</title>
    <summary>  Algorithmic composition of music has a long history and with the development
of powerful deep learning methods, there has recently been increased interest
in exploring algorithms and models to create art. We explore the utility of
state space models, in particular hidden Markov models (HMMs) and variants, in
composing classical piano pieces from the Romantic era and consider the models'
ability to generate new pieces that sound like they were composed by a human.
We find that the models we explored are fairly successful at generating new
pieces that have largely consonant harmonies, especially when trained on
original pieces with simple harmonic structure. However, we conclude that the
major limitation in using these models to generate music that sounds like it
was composed by a human is the lack of melodic progression in the composed
pieces. We also examine the performance of the models in the context of music
theory.
</summary>
    <author>
      <name>Anna K. Yanchenko</name>
    </author>
    <author>
      <name>Sayan Mukherjee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Added case studies section and increased discussion of interpretation
  of results in terms of music theory</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.03822v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03822v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03986v1</id>
    <updated>2017-08-14T01:43:13Z</updated>
    <published>2017-08-14T01:43:13Z</published>
    <title>Creating an A Cappella Singing Audio Dataset for Automatic Jingju
  Singing Evaluation Research</title>
    <summary>  The data-driven computational research on automatic jingju (also known as
Beijing or Peking opera) singing evaluation lacks a suitable and comprehensive
a cappella singing audio dataset. In this work, we present an a cappella
singing audio dataset which consists of 120 arias, accounting for 1265 melodic
lines. This dataset is also an extension our existing CompMusic jingju corpus.
Both professional and amateur singers were invited to the dataset recording
sessions, and the most common jingju musical elements have been covered. This
dataset is also accompanied by metadata per aria and melodic line annotated for
automatic singing evaluation research purpose. All the gathered data is openly
available online.
</summary>
    <author>
      <name>Rong Gong</name>
    </author>
    <author>
      <name>Rafael Caro Repetto</name>
    </author>
    <author>
      <name>Xavier Serra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4th International Digital Libraries for Musicology workshop (DLfM
  2017), Shanghai, China</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.03986v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03986v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03989v1</id>
    <updated>2017-08-14T01:59:34Z</updated>
    <published>2017-08-14T01:59:34Z</published>
    <title>Convolutive Audio Source Separation using Robust ICA and an intelligent
  evolving permutation ambiguity solution</title>
    <summary>  Audio source separation is the task of isolating sound sources that are
active simultaneously in a room captured by a set of microphones. Convolutive
audio source separation of equal number of sources and microphones has a number
of shortcomings including the complexity of frequency-domain ICA, the
permutation ambiguity and the problem's scalabity with increasing number of
sensors. In this paper, the authors propose a multiple-microphone audio source
separation algorithm based on a previous work of Mitianoudis and Davies (2003).
Complex FastICA is substituted by Robust ICA increasing robustness and
performance. Permutation ambiguity is solved using two methodologies. The first
is using the Likelihood Ration Jump solution, which is now modified to decrease
computational complexity in the case of multiple microphones. The application
of the MuSIC algorithm, as a preprocessing step to the previous solution, forms
a second methodology with promising results.
</summary>
    <author>
      <name>Dimitrios Mallis</name>
    </author>
    <author>
      <name>Thomas Sgouros</name>
    </author>
    <author>
      <name>Nikolaos Mitianoudis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s12530-017-9199-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s12530-017-9199-3" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Evolving Systems, Volume 9, Issue 4, pp 315,329, December 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1708.03989v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03989v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04795v1</id>
    <updated>2017-08-16T07:44:36Z</updated>
    <published>2017-08-16T07:44:36Z</published>
    <title>Independent Low-Rank Matrix Analysis Based on Complex Student's
  $t$-Distribution for Blind Audio Source Separation</title>
    <summary>  In this paper, we generalize a source generative model in a state-of-the-art
blind source separation (BSS), independent low-rank matrix analysis (ILRMA).
ILRMA is a unified method of frequency-domain independent component analysis
and nonnegative matrix factorization and can provide better performance for
audio BSS tasks. To further improve the performance and stability of the
separation, we introduce an isotropic complex Student's $t$-distribution as a
source generative model, which includes the isotropic complex Gaussian
distribution used in conventional ILRMA. Experiments are conducted using both
music and speech BSS tasks, and the results show the validity of the proposed
method.
</summary>
    <author>
      <name>Shinichi Mogami</name>
    </author>
    <author>
      <name>Daichi Kitamura</name>
    </author>
    <author>
      <name>Yoshiki Mitsui</name>
    </author>
    <author>
      <name>Norihiro Takamune</name>
    </author>
    <author>
      <name>Hiroshi Saruwatari</name>
    </author>
    <author>
      <name>Nobutaka Ono</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint manuscript of 2017 IEEE International Workshop on Machine
  Learning for Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.04795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04816v1</id>
    <updated>2017-08-16T08:56:51Z</updated>
    <published>2017-08-16T08:56:51Z</published>
    <title>A Generalised Directional Laplacian Distribution: Estimation, Mixture
  Models and Audio Source Separation</title>
    <summary>  Directional or Circular statistics are pertaining to the analysis and
interpretation of directions or rotations. In this work, a novel probability
distribution is proposed to model multidimensional sparse directional data. The
Generalised Directional Laplacian Distribution (DLD) is a hybrid between the
Laplacian distribution and the von Mises-Fisher distribution. The
distribution's parameters are estimated using Maximum-Likelihood Estimation
over a set of training data points. Mixtures of Directional Laplacian
Distributions (MDLD) are also introduced in order to model multiple
concentrations of sparse directional data. The author explores the application
of the derived DLD mixture model to cluster sound sources that exist in an
underdetermined instantaneous sound mixture. The proposed model can solve the
general K x L (K&lt;L) underdetermined instantaneous source separation problem,
offering a fast and stable solution.
</summary>
    <author>
      <name>Nikolaos Mitianoudis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASL.2012.2203804</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASL.2012.2203804" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Audio, Speech and Language Processing, Vol.
  20, No. 9, pp. 2397- 2408 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1708.04816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06750v3</id>
    <updated>2017-12-01T16:47:22Z</updated>
    <published>2017-08-22T17:55:29Z</published>
    <title>Bitwise Source Separation on Hashed Spectra: An Efficient Posterior
  Estimation Scheme Using Partial Rank Order Metrics</title>
    <summary>  This paper proposes an efficient bitwise solution to the single-channel
source separation task. Most dictionary-based source separation algorithms rely
on iterative update rules during the run time, which becomes computationally
costly especially when we employ an overcomplete dictionary and sparse encoding
that tend to give better separation results. To avoid such cost we propose a
bitwise scheme on hashed spectra that leads to an efficient posterior
probability calculation. For each source, the algorithm uses a partial rank
order metric to extract robust features that form a binarized dictionary of
hashed spectra. Then, for a mixture spectrum, its hash code is compared with
each source's hashed dictionary in one pass. This simple voting-based
dictionary search allows a fast and iteration-free estimation of ratio masking
at each bin of a signal spectrogram. We verify that the proposed BitWise Source
Separation (BWSS) algorithm produces sensible source separation results for the
single-channel speech denoising task, with 6-8 dB mean SDR. To our knowledge,
this is the first dictionary based algorithm for this task that is completely
iteration-free in both training and testing.
</summary>
    <author>
      <name>Lijiang Guo</name>
    </author>
    <author>
      <name>Minje Kim</name>
    </author>
    <link href="http://arxiv.org/abs/1708.06750v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06750v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.07050v1</id>
    <updated>2017-08-23T15:27:00Z</updated>
    <published>2017-08-23T15:27:00Z</published>
    <title>Capturing Long-term Temporal Dependencies with Convolutional Networks
  for Continuous Emotion Recognition</title>
    <summary>  The goal of continuous emotion recognition is to assign an emotion value to
every frame in a sequence of acoustic features. We show that incorporating
long-term temporal dependencies is critical for continuous emotion recognition
tasks. To this end, we first investigate architectures that use dilated
convolutions. We show that even though such architectures outperform previously
reported systems, the output signals produced from such architectures undergo
erratic changes between consecutive time steps. This is inconsistent with the
slow moving ground-truth emotion labels that are obtained from human
annotators. To deal with this problem, we model a downsampled version of the
input signal and then generate the output signal through upsampling. Not only
does the resulting downsampling/upsampling network achieve good performance, it
also generates smooth output trajectories. Our method yields the best known
audio-only performance on the RECOLA dataset.
</summary>
    <author>
      <name>Soheil Khorram</name>
    </author>
    <author>
      <name>Zakaria Aldeneh</name>
    </author>
    <author>
      <name>Dimitrios Dimitriadis</name>
    </author>
    <author>
      <name>Melvin McInnis</name>
    </author>
    <author>
      <name>Emily Mower Provost</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, 2 tables, Interspeech 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.07050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.07050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.08251v1</id>
    <updated>2017-08-28T09:41:41Z</updated>
    <published>2017-08-28T09:41:41Z</published>
    <title>Integrated Speech Enhancement Method Based on Weighted Prediction Error
  and DNN for Dereverberation and Denoising</title>
    <summary>  Both reverberation and additive noises degrade the speech quality and
intelligibility. Weighted prediction error (WPE) method performs well on the
dereverberation but with limitations. First, WPE doesn't consider the influence
of the additive noise which degrades the performance of dereverberation.
Second, it relies on a time-consuming iterative process, and there is no
guarantee or a widely accepted criterion on its convergence. In this paper, we
integrate deep neural network (DNN) into WPE for dereverberation and denoising.
DNN is used to suppress the background noise to meet the noise-free assumption
of WPE. Meanwhile, DNN is applied to directly predict spectral variance of the
target speech to make the WPE work without iteration. The experimental results
show that the proposed method has a significant improvement in speech quality
and runs fast.
</summary>
    <author>
      <name>Hao Li</name>
    </author>
    <author>
      <name>Xueliang Zhang</name>
    </author>
    <author>
      <name>Hui Zhang</name>
    </author>
    <author>
      <name>Guanglai Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1708.08251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.08251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.08740v1</id>
    <updated>2017-08-29T13:38:39Z</updated>
    <published>2017-08-29T13:38:39Z</published>
    <title>Improving Source Separation via Multi-Speaker Representations</title>
    <summary>  Lately there have been novel developments in deep learning towards solving
the cocktail party problem. Initial results are very promising and allow for
more research in the domain. One technique that has not yet been explored in
the neural network approach to this task is speaker adaptation. Intuitively,
information on the speakers that we are trying to separate seems fundamentally
important for the speaker separation task. However, retrieving this speaker
information is challenging since the speaker identities are not known a priori
and multiple speakers are simultaneously active. There is thus some sort of
chicken and egg problem. To tackle this, source signals and i-vectors are
estimated alternately. We show that blind multi-speaker adaptation improves the
results of the network and that (in our case) the network is not capable of
adequately retrieving this useful speaker information itself.
</summary>
    <author>
      <name>Jeroen Zegers</name>
    </author>
    <author>
      <name>Hugo Van hamme</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.21437/Interspeech.2017-754</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.21437/Interspeech.2017-754" rel="related"/>
    <link href="http://arxiv.org/abs/1708.08740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.08740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00375v3</id>
    <updated>2019-04-07T20:11:19Z</updated>
    <published>2017-09-01T16:00:22Z</published>
    <title>2:3:4-Harmony within the Tritave</title>
    <summary>  In the Pythagorean tuning system, the fifth is used to generate a scale of 12
notes per octave. In this paper, we use the octave to generate a scale of 19
notes per tritave; one can play this scale on a traditional piano. In this
system, the octave becomes a proper interval and the 2:3:4 chord a proper
chord. We study harmonic properties obtained from the 2:3:4 chord, in
particular composition elements using dominants, subdominants, higher
dominants, associated minor chords, inversions, and diminished chords. The
Tonnetz (array notation) turns out to be an effective tool to visualize the
harmonic development in a composition based on these elements. 2:3:4-harmony
may sound pure, yet sparse, as we illustrate in a short piece.
</summary>
    <author>
      <name>Markus Schmidmeier</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/17459737.2019.1605626</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/17459737.2019.1605626" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In the new version, some remarks about music perception have been
  added. 29 pages, 22 figures, 7 tables. To appear in the Journal of
  Mathematics and Music</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Mathematics and Music (2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.00375v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00375v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="00A65 (Primary) 11A55, 05E99 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00611v2</id>
    <updated>2018-04-24T09:17:13Z</updated>
    <published>2017-09-02T17:46:55Z</published>
    <title>A Recurrent Encoder-Decoder Approach with Skip-filtering Connections for
  Monaural Singing Voice Separation</title>
    <summary>  The objective of deep learning methods based on encoder-decoder architectures
for music source separation is to approximate either ideal time-frequency masks
or spectral representations of the target music source(s). The spectral
representations are then used to derive time-frequency masks. In this work we
introduce a method to directly learn time-frequency masks from an observed
mixture magnitude spectrum. We employ recurrent neural networks and train them
using prior knowledge only for the magnitude spectrum of the target source. To
assess the performance of the proposed method, we focus on the task of singing
voice separation. The results from an objective evaluation show that our
proposed method provides comparable results to deep learning based methods
which operate over complicated signal representations. Compared to previous
methods that approximate time-frequency masks, our method has increased
performance of signal to distortion ratio by an average of 3.8 dB.
</summary>
    <author>
      <name>Stylianos Ioannis Mimilakis</name>
    </author>
    <author>
      <name>Konstantinos Drossos</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <author>
      <name>Gerald Schuller</name>
    </author>
    <link href="http://arxiv.org/abs/1709.00611v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00611v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00917v1</id>
    <updated>2017-09-04T12:25:18Z</updated>
    <published>2017-09-04T12:25:18Z</published>
    <title>Using Optimal Ratio Mask as Training Target for Supervised Speech
  Separation</title>
    <summary>  Supervised speech separation uses supervised learning algorithms to learn a
mapping from an input noisy signal to an output target. With the fast
development of deep learning, supervised separation has become the most
important direction in speech separation area in recent years. For the
supervised algorithm, training target has a significant impact on the
performance. Ideal ratio mask is a commonly used training target, which can
improve the speech intelligibility and quality of the separated speech.
However, it does not take into account the correlation between noise and clean
speech. In this paper, we use the optimal ratio mask as the training target of
the deep neural network (DNN) for speech separation. The experiments are
carried out under various noise environments and signal to noise ratio (SNR)
conditions. The results show that the optimal ratio mask outperforms other
training targets in general.
</summary>
    <author>
      <name>Shasha Xia</name>
    </author>
    <author>
      <name>Hao Li</name>
    </author>
    <author>
      <name>Xueliang Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1709.00917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01346v1</id>
    <updated>2017-09-05T12:22:01Z</updated>
    <published>2017-09-05T12:22:01Z</published>
    <title>PSD Estimation of Multiple Sound Sources in a Reverberant Room Using a
  Spherical Microphone Array</title>
    <summary>  We propose an efficient method to estimate source power spectral densities
(PSDs) in a multi-source reverberant environment using a spherical microphone
array. The proposed method utilizes the spatial correlation between the
spherical harmonics (SH) coefficients of a sound field to estimate source PSDs.
The use of the spatial cross-correlation of the SH coefficients allows us to
employ the method in an environment with a higher number of sources compared to
conventional methods. Furthermore, the orthogonality property of the SH basis
functions saves the effort of designing specific beampatterns of a conventional
beamformer-based method. We evaluate the performance of the algorithm with
different number of sources in practical reverberant and non-reverberant rooms.
We also demonstrate an application of the method by separating source signals
using a conventional beamformer and a Wiener post-filter designed from the
estimated PSDs.
</summary>
    <author>
      <name>Abdullah Fahim</name>
    </author>
    <author>
      <name>Prasanga N. Samarasinghe</name>
    </author>
    <author>
      <name>Thushara D. Abhayapala</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/WASPAA.2017.8169998</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/WASPAA.2017.8169998" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for WASPAA 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.01346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03629v1</id>
    <updated>2017-09-11T23:56:37Z</updated>
    <published>2017-09-11T23:56:37Z</published>
    <title>What were you expecting? Using Expectancy Features to Predict Expressive
  Performances of Classical Piano Music</title>
    <summary>  In this paper we present preliminary work examining the relationship between
the formation of expectations and the realization of musical performances,
paying particular attention to expressive tempo and dynamics. To compute
features that reflect what a listener is expecting to hear, we employ a
computational model of auditory expectation called the Information Dynamics of
Music model (IDyOM). We then explore how well these expectancy features -- when
combined with score descriptors using the Basis-Function modeling approach --
can predict expressive tempo and dynamics in a dataset of Mozart piano sonata
performances. Our results suggest that using expectancy features significantly
improves the predictions for tempo.
</summary>
    <author>
      <name>Carlos Cancino-Chacón</name>
    </author>
    <author>
      <name>Maarten Grachten</name>
    </author>
    <author>
      <name>David R. W. Sears</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, 10th International Workshop on Machine Learning
  and Music (MML 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.03629v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03629v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06663v1</id>
    <updated>2017-09-19T22:36:16Z</updated>
    <published>2017-09-19T22:36:16Z</published>
    <title>Linear Computer-Music through Sequences over Galois Fields</title>
    <summary>  It is shown how binary sequences can be associated with automatic composition
of monophonic pieces. We are concerned with the composition of e-music from
finite field structures. The information at the input may be either random or
information from a black-and-white, grayscale or color picture. New
e-compositions and music score are made available, including a new piece from
the famous Lenna picture: the score of the e-music &lt;&lt;Between Lenna's eyes in C
major.&gt;&gt; The corresponding stretch of music score are presented. Some
particular structures, including clock arithmetic (mod 12), GF(7), GF(8),
GF(13) and GF(17) are addressed. Further, multilevel block-codes are also used
in a new approach of e-music composition, engendering a particular style as an
e-composer. As an example, Pascal multilevel block codes recently introduced
are handled to generate a new style of electronic music over GF(13).
</summary>
    <author>
      <name>H. M. de Oliveira</name>
    </author>
    <author>
      <name>R. C. de Oliveira</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.06663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U07, 68W05, 11B50, 12E20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; J.5; I.5.4; F.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07124v1</id>
    <updated>2017-09-21T01:46:19Z</updated>
    <published>2017-09-21T01:46:19Z</published>
    <title>Deep Recurrent NMF for Speech Separation by Unfolding Iterative
  Thresholding</title>
    <summary>  In this paper, we propose a novel recurrent neural network architecture for
speech separation. This architecture is constructed by unfolding the iterations
of a sequential iterative soft-thresholding algorithm (ISTA) that solves the
optimization problem for sparse nonnegative matrix factorization (NMF) of
spectrograms. We name this network architecture deep recurrent NMF (DR-NMF).
The proposed DR-NMF network has three distinct advantages. First, DR-NMF
provides better interpretability than other deep architectures, since the
weights correspond to NMF model parameters, even after training. This
interpretability also provides principled initializations that enable faster
training and convergence to better solutions compared to conventional random
initialization. Second, like many deep networks, DR-NMF is an order of
magnitude faster at test time than NMF, since computation of the network output
only requires evaluating a few layers at each time step. Third, when a limited
amount of training data is available, DR-NMF exhibits stronger generalization
and separation performance compared to sparse NMF and state-of-the-art
long-short term memory (LSTM) networks. When a large amount of training data is
available, DR-NMF achieves lower yet competitive separation performance
compared to LSTM networks.
</summary>
    <author>
      <name>Scott Wisdom</name>
    </author>
    <author>
      <name>Thomas Powers</name>
    </author>
    <author>
      <name>James Pitton</name>
    </author>
    <author>
      <name>Les Atlas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be presented at WASPAA 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.07124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07153v2</id>
    <updated>2017-09-22T01:49:31Z</updated>
    <published>2017-09-21T04:28:59Z</published>
    <title>Large Vocabulary Automatic Chord Estimation Using Deep Neural Nets:
  Design Framework, System Variations and Limitations</title>
    <summary>  In this paper, we propose a new system design framework for large vocabulary
automatic chord estimation. Our approach is based on an integration of
traditional sequence segmentation processes and deep learning chord
classification techniques. We systematically explore the design space of the
proposed framework for a range of parameters, namely deep neural nets, network
configurations, input feature representations, segment tiling schemes, and
training data sizes. Experimental results show that among the three proposed
deep neural nets and a baseline model, the recurrent neural network based
system has the best average chord quality accuracy that significantly
outperforms the other considered models. Furthermore, our bias-variance
analysis has identified a glass ceiling as a potential hindrance to future
improvements of large vocabulary automatic chord estimation systems.
</summary>
    <author>
      <name>Junqi Deng</name>
    </author>
    <author>
      <name>Yu-Kwong Kwok</name>
    </author>
    <link href="http://arxiv.org/abs/1709.07153v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07153v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.08344v1</id>
    <updated>2017-09-25T06:52:42Z</updated>
    <published>2017-09-25T06:52:42Z</published>
    <title>Predicting interviewee attitude and body language from speech
  descriptors</title>
    <summary>  This present research investigated the relationship between personal
impressions and the acoustic nonverbal communication conveyed by employees
being interviewed. First, we investigated the extent to which different
conversation topics addressed during the interview induced changes in the
interviewees' acoustic parameters. Next, we attempted to predict the observed
and self-assessed attitudes and body language of the interviewees based on the
acoustic data. The results showed that topicality caused significant deviations
in the acoustic parameters statistics, but the ability to predict the personal
perceptions of the interviewees based on their acoustic non-verbal
communication was relatively independent of topicality, due to the natural
redundancy inherent in acoustic attributes. Our findings suggest that joint
modeling of speech and visual cues may improve the assessment of interviewee
profiles.
</summary>
    <author>
      <name>Yosef Solewicz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Israel National Police</arxiv:affiliation>
    </author>
    <author>
      <name>Chagay Orenshtein</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tel Hai College</arxiv:affiliation>
    </author>
    <author>
      <name>Avital Friedland</name>
    </author>
    <link href="http://arxiv.org/abs/1709.08344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.08344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09708v1</id>
    <updated>2017-09-13T15:04:30Z</updated>
    <published>2017-09-13T15:04:30Z</published>
    <title>On the Complex Network Structure of Musical Pieces: Analysis of Some Use
  Cases from Different Music Genres</title>
    <summary>  This paper focuses on the modeling of musical melodies as networks. Notes of
a melody can be treated as nodes of a network. Connections are created whenever
notes are played in sequence. We analyze some main tracks coming from different
music genres, with melodies played using different musical instruments. We find
out that the considered networks are, in general, scale free networks and
exhibit the small world property. We measure the main metrics and assess
whether these networks can be considered as formed by sub-communities. Outcomes
confirm that peculiar features of the tracks can be extracted from this
analysis methodology. This approach can have an impact in several multimedia
applications such as music didactics, multimedia entertainment, and digital
music generation.
</summary>
    <author>
      <name>Stefano Ferretti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11042-017-5175-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11042-017-5175-y" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to Multimedia Tools and Applications, Springer</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.09708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00082v1</id>
    <updated>2017-09-29T20:33:38Z</updated>
    <published>2017-09-29T20:33:38Z</published>
    <title>Real-Time Wind Noise Detection and Suppression with Neural-Based Signal
  Reconstruction for Mult-Channel, Low-Power Devices</title>
    <summary>  Active wind noise detection and suppression techniques are a new and
essential paradigm for enhancing ASR-based functionality with smart glasses, in
addition to other wearable and smart devices in the broader IoT (Internet of
things). In this paper, we develop two separate algorithms for wind noise
detection and suppression, respectively, operational in a challenging,
low-energy regime. Together, these algorithms comprise a robust wind noise
suppression system. In the first case, we advance a real-time wind detection
algorithm (RTWD) that uses two distinct sets of low-dimensional signal features
to discriminate the presence of wind noise with high accuracy. For wind noise
suppression, we employ an additional algorithm - attentive neural wind
suppression (ANWS) - that utilizes a neural network to reconstruct the wearer
speech signal from wind-corrupted audio in the spectral regions that are most
adversely affected by wind noise. Finally, we test our algorithms through
real-time experiments using low-power, multi-microphone devices with a wind
simulator under challenging detection criteria and a variety of wind
intensities.
</summary>
    <author>
      <name>Anthony D. Rhodes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.00082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00343v1</id>
    <updated>2017-10-01T12:57:45Z</updated>
    <published>2017-10-01T12:57:45Z</published>
    <title>Large-scale weakly supervised audio classification using gated
  convolutional neural network</title>
    <summary>  In this paper, we present a gated convolutional neural network and a temporal
attention-based localization method for audio classification, which won the 1st
place in the large-scale weakly supervised sound event detection task of
Detection and Classification of Acoustic Scenes and Events (DCASE) 2017
challenge. The audio clips in this task, which are extracted from YouTube
videos, are manually labeled with one or a few audio tags but without
timestamps of the audio events, which is called as weakly labeled data. Two
sub-tasks are defined in this challenge including audio tagging and sound event
detection using this weakly labeled data. A convolutional recurrent neural
network (CRNN) with learnable gated linear units (GLUs) non-linearity applied
on the log Mel spectrogram is proposed. In addition, a temporal attention
method is proposed along the frames to predicate the locations of each audio
event in a chunk from the weakly labeled data. We ranked the 1st and the 2nd as
a team in these two sub-tasks of DCASE 2017 challenge with F value 55.6\% and
Equal error 0.73, respectively.
</summary>
    <author>
      <name>Yong Xu</name>
    </author>
    <author>
      <name>Qiuqiang Kong</name>
    </author>
    <author>
      <name>Wenwu Wang</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to ICASSP2018, summary on the 1st place system in DCASE2017
  task4 challenge</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.00343v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00343v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01446v1</id>
    <updated>2017-10-04T03:11:31Z</updated>
    <published>2017-10-04T03:11:31Z</published>
    <title>Improving Compression Based Dissimilarity Measure for Music Score
  Analysis</title>
    <summary>  In this paper, we propose a way to improve the compression based
dissimilarity measure, CDM. We propose to use a modified value of the file
size, where the original CDM uses an unmodified file size. Our application is a
music score analysis. We have chosen piano pieces from five different
composers. We have selected 75 famous pieces (15 pieces for each composer). We
computed the distances among all pieces by using the modified CDM. We use the
K-nearest neighbor method when we estimate the composer of each piece of music.
The modified CDM shows improved accuracy. The difference is statistically
significant.
</summary>
    <author>
      <name>Ayaka Takamoto</name>
    </author>
    <author>
      <name>Mayu Umemura</name>
    </author>
    <author>
      <name>Mitsuo Yoshida</name>
    </author>
    <author>
      <name>Kyoji Umemura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 2016 International Conference On Advanced Informatics: Concepts,
  Theory And Application (ICAICTA2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.01446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01589v1</id>
    <updated>2017-10-04T13:12:44Z</updated>
    <published>2017-10-04T13:12:44Z</published>
    <title>Independent Low-Rank Matrix Analysis Based on Parametric
  Majorization-Equalization Algorithm</title>
    <summary>  In this paper, we propose a new optimization method for independent low-rank
matrix analysis (ILRMA) based on a parametric majorization-equalization
algorithm. ILRMA is an efficient blind source separation technique that
simultaneously estimates a spatial demixing matrix (spatial model) and the
power spectrograms of each estimated source (source model). In ILRMA, since
both models are alternately optimized by iterative update rules, the difference
in the convergence speeds between these models often results in a poor local
solution. To solve this problem, we introduce a new parameter that controls the
convergence speed of the source model and find the best balance between the
optimizations in the spatial and source models for ILRMA.
</summary>
    <author>
      <name>Yoshiki Mitsui</name>
    </author>
    <author>
      <name>Daichi Kitamura</name>
    </author>
    <author>
      <name>Norihiro Takamune</name>
    </author>
    <author>
      <name>Hiroshi Saruwatari</name>
    </author>
    <author>
      <name>Yu Takahashi</name>
    </author>
    <author>
      <name>Kazunobu Kondo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint Manuscript of 2017 IEEE International Workshop on
  Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.01589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02280v1</id>
    <updated>2017-10-06T05:53:20Z</updated>
    <published>2017-10-06T05:53:20Z</published>
    <title>Generating Nontrivial Melodies for Music as a Service</title>
    <summary>  We present a hybrid neural network and rule-based system that generates pop
music. Music produced by pure rule-based systems often sounds mechanical. Music
produced by machine learning sounds better, but still lacks hierarchical
temporal structure. We restore temporal hierarchy by augmenting machine
learning with a temporal production grammar, which generates the music's
overall structure and chord progressions. A compatible melody is then generated
by a conditional variational recurrent autoencoder. The autoencoder is trained
with eight-measure segments from a corpus of 10,000 MIDI files, each of which
has had its melody track and chord progressions identified heuristically. The
autoencoder maps melody into a multi-dimensional feature space, conditioned by
the underlying chord progression. A melody is then generated by feeding a
random sample from that space to the autoencoder's decoder, along with the
chord progression generated by the grammar. The autoencoder can make musically
plausible variations on an existing melody, suitable for recurring motifs. It
can also reharmonize a melody to a new chord progression, keeping the rhythm
and contour. The generated music compares favorably with that generated by
other academic and commercial software designed for the music-as-a-service
industry.
</summary>
    <author>
      <name>Yifei Teng</name>
    </author>
    <author>
      <name>An Zhao</name>
    </author>
    <author>
      <name>Camille Goudeseune</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISMIR 2017 Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.02280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02997v1</id>
    <updated>2017-10-09T09:12:01Z</updated>
    <published>2017-10-09T09:12:01Z</published>
    <title>A report on sound event detection with different binaural features</title>
    <summary>  In this paper, we compare the performance of using binaural audio features in
place of single-channel features for sound event detection. Three different
binaural features are studied and evaluated on the publicly available TUT Sound
Events 2017 dataset of length 70 minutes. Sound event detection is performed
separately with single-channel and binaural features using stacked
convolutional and recurrent neural network and the evaluation is reported using
standard metrics of error rate and F-score. The studied binaural features are
seen to consistently perform equal to or better than the single-channel
features with respect to error rate metric.
</summary>
    <author>
      <name>Sharath Adavanne</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical report for the top performing method in Task 3: Real life
  sound event detection challenge, at Detection and classification of acoustic
  scene and events (DCASE) 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.02997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.04196v1</id>
    <updated>2017-10-11T17:44:41Z</updated>
    <published>2017-10-11T17:44:41Z</published>
    <title>Pyroomacoustics: A Python package for audio room simulations and array
  processing algorithms</title>
    <summary>  We present pyroomacoustics, a software package aimed at the rapid development
and testing of audio array processing algorithms. The content of the package
can be divided into three main components: an intuitive Python object-oriented
interface to quickly construct different simulation scenarios involving
multiple sound sources and microphones in 2D and 3D rooms; a fast C
implementation of the image source model for general polyhedral rooms to
efficiently generate room impulse responses and simulate the propagation
between sources and receivers; and finally, reference implementations of
popular algorithms for beamforming, direction finding, and adaptive filtering.
Together, they form a package with the potential to speed up the time to market
of new algorithms by significantly reducing the implementation overhead in the
performance evaluation step.
</summary>
    <author>
      <name>Robin Scheibler</name>
    </author>
    <author>
      <name>Eric Bezzam</name>
    </author>
    <author>
      <name>Ivan Dokmanić</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2018.8461310</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2018.8461310" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, describes a software package</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.04196v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.04196v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.06648v2</id>
    <updated>2018-06-19T01:58:09Z</updated>
    <published>2017-10-18T09:42:16Z</published>
    <title>Representation Learning of Music Using Artist Labels</title>
    <summary>  In music domain, feature learning has been conducted mainly in two ways:
unsupervised learning based on sparse representations or supervised learning by
semantic labels such as music genre. However, finding discriminative features
in an unsupervised way is challenging and supervised feature learning using
semantic labels may involve noisy or expensive annotation. In this paper, we
present a supervised feature learning approach using artist labels annotated in
every single track as objective meta data. We propose two deep convolutional
neural networks (DCNN) to learn the deep artist features. One is a plain DCNN
trained with the whole artist labels simultaneously, and the other is a Siamese
DCNN trained with a subset of the artist labels based on the artist identity.
We apply the trained models to music classification and retrieval tasks in
transfer learning settings. The results show that our approach is comparable to
previous state-of-the-art methods, indicating that the proposed approach
captures general music audio features as much as the models learned with
semantic labels. Also, we discuss the advantages and disadvantages of the two
models.
</summary>
    <author>
      <name>Jiyoung Park</name>
    </author>
    <author>
      <name>Jongpil Lee</name>
    </author>
    <author>
      <name>Jangyeon Park</name>
    </author>
    <author>
      <name>Jung-Woo Ha</name>
    </author>
    <author>
      <name>Juhan Nam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19th International Society for Music Information Retrieval Conference
  (ISMIR), 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.06648v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.06648v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.07654v3</id>
    <updated>2018-02-22T06:23:45Z</updated>
    <published>2017-10-20T18:17:23Z</published>
    <title>Deep Voice 3: Scaling Text-to-Speech with Convolutional Sequence
  Learning</title>
    <summary>  We present Deep Voice 3, a fully-convolutional attention-based neural
text-to-speech (TTS) system. Deep Voice 3 matches state-of-the-art neural
speech synthesis systems in naturalness while training ten times faster. We
scale Deep Voice 3 to data set sizes unprecedented for TTS, training on more
than eight hundred hours of audio from over two thousand speakers. In addition,
we identify common error modes of attention-based speech synthesis networks,
demonstrate how to mitigate them, and compare several different waveform
synthesis methods. We also describe how to scale inference to ten million
queries per day on one single-GPU server.
</summary>
    <author>
      <name>Wei Ping</name>
    </author>
    <author>
      <name>Kainan Peng</name>
    </author>
    <author>
      <name>Andrew Gibiansky</name>
    </author>
    <author>
      <name>Sercan O. Arik</name>
    </author>
    <author>
      <name>Ajay Kannan</name>
    </author>
    <author>
      <name>Sharan Narang</name>
    </author>
    <author>
      <name>Jonathan Raiman</name>
    </author>
    <author>
      <name>John Miller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2018. (v3 changed paper
  title)</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.07654v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.07654v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.07868v2</id>
    <updated>2017-10-24T14:59:30Z</updated>
    <published>2017-10-22T01:06:23Z</published>
    <title>Deep Triphone Embedding Improves Phoneme Recognition</title>
    <summary>  In this paper, we present a novel Deep Triphone Embedding (DTE)
representation derived from Deep Neural Network (DNN) to encapsulate the
discriminative information present in the adjoining speech frames. DTEs are
generated using a four hidden layer DNN with 3000 nodes in each hidden layer at
the first-stage. This DNN is trained with the tied-triphone classification
accuracy as an optimization criterion. Thereafter, we retain the activation
vectors (3000) of the last hidden layer, for each speech MFCC frame, and
perform dimension reduction to further obtain a 300 dimensional representation,
which we termed as DTE. DTEs along with MFCC features are fed into a
second-stage four hidden layer DNN, which is subsequently trained for the task
of tied-triphone classification. Both DNNs are trained using tri-phone labels
generated from a tied-state triphone HMM-GMM system, by performing a
forced-alignment between the transcriptions and MFCC feature frames. We conduct
the experiments on publicly available TED-LIUM speech corpus. The results show
that the proposed DTE method provides an improvement of absolute 2.11% in
phoneme recognition, when compared with a competitive hybrid tied-state
triphone HMM-DNN system.
</summary>
    <author>
      <name>Mohit Yadav</name>
    </author>
    <author>
      <name>Vivek Tyagi</name>
    </author>
    <link href="http://arxiv.org/abs/1710.07868v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.07868v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.08377v1</id>
    <updated>2017-10-23T16:47:05Z</updated>
    <published>2017-10-23T16:47:05Z</published>
    <title>Listening to the World Improves Speech Command Recognition</title>
    <summary>  We study transfer learning in convolutional network architectures applied to
the task of recognizing audio, such as environmental sound events and speech
commands. Our key finding is that not only is it possible to transfer
representations from an unrelated task like environmental sound classification
to a voice-focused task like speech command recognition, but also that doing so
improves accuracies significantly. We also investigate the effect of increased
model capacity for transfer learning audio, by first validating known results
from the field of Computer Vision of achieving better accuracies with
increasingly deeper networks on two audio datasets: UrbanSound8k and the newly
released Google Speech Commands dataset. Then we propose a simple multiscale
input representation using dilated convolutions and show that it is able to
aggregate larger contexts and increase classification performance. Further, the
models trained using a combination of transfer learning and multiscale input
representations need only 40% of the training data to achieve similar
accuracies as a freshly trained model with 100% of the training data. Finally,
we demonstrate a positive interaction effect for the multiscale input and
transfer learning, making a case for the joint application of the two
techniques.
</summary>
    <author>
      <name>Brian McMahan</name>
    </author>
    <author>
      <name>Delip Rao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.08377v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08377v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.08684v1</id>
    <updated>2017-10-24T09:59:21Z</updated>
    <published>2017-10-24T09:59:21Z</published>
    <title>Inferring Room Semantics Using Acoustic Monitoring</title>
    <summary>  Having knowledge of the environmental context of the user i.e. the knowledge
of the users' indoor location and the semantics of their environment, can
facilitate the development of many of location-aware applications. In this
paper, we propose an acoustic monitoring technique that infers semantic
knowledge about an indoor space \emph{over time,} using audio recordings from
it. Our technique uses the impulse response of these spaces as well as the
ambient sounds produced in them in order to determine a semantic label for
them. As we process more recordings, we update our \emph{confidence} in the
assigned label. We evaluate our technique on a dataset of single-speaker human
speech recordings obtained in different types of rooms at three university
buildings. In our evaluation, the confidence\emph{ }for the true label
generally outstripped the confidence for all other labels and in some cases
converged to 100\% with less than 30 samples.
</summary>
    <author>
      <name>Muhammad A. Shah</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <author>
      <name>Khaled A. Harras</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MLSP.2017.8168153</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MLSP.2017.8168153" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2017 IEEE International Workshop on Machine Learning for Signal
  Processing, Sept.\ 25--28, 2017, Tokyo, Japan</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Workshop on Machine Learning for Signal
  Processing (MLSP) 27 (2017) 1-6</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1710.08684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.08969v2</id>
    <updated>2020-09-30T05:41:53Z</updated>
    <published>2017-10-24T19:56:32Z</published>
    <title>Efficiently Trainable Text-to-Speech System Based on Deep Convolutional
  Networks with Guided Attention</title>
    <summary>  This paper describes a novel text-to-speech (TTS) technique based on deep
convolutional neural networks (CNN), without use of any recurrent units.
Recurrent neural networks (RNN) have become a standard technique to model
sequential data recently, and this technique has been used in some cutting-edge
neural TTS techniques. However, training RNN components often requires a very
powerful computer, or a very long time, typically several days or weeks. Recent
other studies, on the other hand, have shown that CNN-based sequence synthesis
can be much faster than RNN-based techniques, because of high
parallelizability. The objective of this paper is to show that an alternative
neural TTS based only on CNN alleviate these economic costs of training. In our
experiment, the proposed Deep Convolutional TTS was sufficiently trained
overnight (15 hours), using an ordinary gaming PC equipped with two GPUs, while
the quality of the synthesized speech was almost acceptable.
</summary>
    <author>
      <name>Hideyuki Tachibana</name>
    </author>
    <author>
      <name>Katsuya Uenoyama</name>
    </author>
    <author>
      <name>Shunsuke Aihara</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2018.8461829</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2018.8461829" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3figures, IEEE ICASSP 2018</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. ICASSP (2018) 4784-4788</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1710.08969v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.08969v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09064v3</id>
    <updated>2021-07-08T15:43:27Z</updated>
    <published>2017-10-25T03:21:44Z</published>
    <title>End-to-End Optimized Speech Coding with Deep Neural Networks</title>
    <summary>  Modern compression algorithms are often the result of laborious
domain-specific research; industry standards such as MP3, JPEG, and AMR-WB took
years to develop and were largely hand-designed. We present a deep neural
network model which optimizes all the steps of a wideband speech coding
pipeline (compression, quantization, entropy coding, and decompression)
end-to-end directly from raw speech data -- no manual feature engineering
necessary, and it trains in hours. In testing, our DNN-based coder performs on
par with the AMR-WB standard at a variety of bitrates (~9kbps up to ~24kbps).
It also runs in realtime on a 3.8GhZ Intel CPU.
</summary>
    <author>
      <name>Srihari Kankanahalli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted and presented at ICASSP 2018. Samples available here:
  http://srik.tk/speech-coding/</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.09064v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09064v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09091v1</id>
    <updated>2017-10-25T07:02:02Z</updated>
    <published>2017-10-25T07:02:02Z</published>
    <title>Relative Transfer Function Inverse Regression from Low Dimensional
  Manifold</title>
    <summary>  In room acoustic environments, the Relative Transfer Functions (RTFs) are
controlled by few underlying modes of variability. Accordingly, they are
confined to a low-dimensional manifold. In this letter, we investigate a RTF
inverse regression problem, the task of which is to generate the
high-dimensional responses from their low-dimensional representations. The
problem is addressed from a pure data-driven perspective and a supervised Deep
Neural Network (DNN) model is applied to learn a mapping from the
source-receiver poses (positions and orientations) to the frequency domain RTF
vectors. The experiments show promising results: the model achieves lower
prediction error of the RTF than the free field assumption. However, it fails
to compete with the linear interpolation technique in small sampling distances.
</summary>
    <author>
      <name>Ziteng Wang</name>
    </author>
    <author>
      <name>Emmanuel Vincent</name>
    </author>
    <author>
      <name>Yonghong Yan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, in preparation for Signal Processing Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.09091v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09091v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10005v1</id>
    <updated>2017-10-27T06:49:04Z</updated>
    <published>2017-10-27T06:49:04Z</published>
    <title>Separation of Moving Sound Sources Using Multichannel NMF and Acoustic
  Tracking</title>
    <summary>  In this paper we propose a method for separation of moving sound sources. The
method is based on first tracking the sources and then estimation of source
spectrograms using multichannel non-negative matrix factorization (NMF) and
extracting the sources from the mixture by single-channel Wiener filtering. We
propose a novel multichannel NMF model with time-varying mixing of the sources
denoted by spatial covariance matrices (SCM) and provide update equations for
optimizing model parameters minimizing squared Frobenius norm. The SCMs of the
model are obtained based on estimated directions of arrival of tracked sources
at each time frame. The evaluation is based on established objective separation
criteria and using real recordings of two and three simultaneous moving sound
sources. The compared methods include conventional beamforming and ideal ratio
mask separation. The proposed method is shown to exceed the separation quality
of other evaluated blind approaches according to all measured quantities.
Additionally, we evaluate the method's susceptibility towards tracking errors
by comparing the separation quality achieved using annotated ground truth
source trajectories.
</summary>
    <author>
      <name>Joonas Nikunen</name>
    </author>
    <author>
      <name>Aleksandr Diment</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint of manuscript submitted to IEEE/ACM Transactions on Audio
  Speech and Language processing (R1)</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.10005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10059v2</id>
    <updated>2018-08-05T19:54:58Z</updated>
    <published>2017-10-27T10:24:00Z</published>
    <title>Direction of arrival estimation for multiple sound sources using
  convolutional recurrent neural network</title>
    <summary>  This paper proposes a deep neural network for estimating the directions of
arrival (DOA) of multiple sound sources. The proposed stacked convolutional and
recurrent neural network (DOAnet) generates a spatial pseudo-spectrum (SPS)
along with the DOA estimates in both azimuth and elevation. We avoid any
explicit feature extraction step by using the magnitudes and phases of the
spectrograms of all the channels as input to the network. The proposed DOAnet
is evaluated by estimating the DOAs of multiple concurrently present sources in
anechoic, matched and unmatched reverberant conditions. The results show that
the proposed DOAnet is capable of estimating the number of sources and their
respective DOAs with good precision and generate SPS with high signal-to-noise
ratio.
</summary>
    <author>
      <name>Sharath Adavanne</name>
    </author>
    <author>
      <name>Archontis Politis</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EUSIPCO 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.10059v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10059v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10436v4</id>
    <updated>2018-09-02T12:54:23Z</updated>
    <published>2017-10-28T09:46:03Z</published>
    <title>Investigation of Frame Alignments for GMM-based Digit-prompted Speaker
  Verification</title>
    <summary>  Frame alignments can be computed by different methods in GMM-based speaker
verification. By incorporating a phonetic Gaussian mixture model (PGMM), we are
able to compare the performance using alignments extracted from the deep neural
networks (DNN) and the conventional hidden Markov model (HMM) in digit-prompted
speaker verification. Based on the different characteristics of these two
alignments, we present a novel content verification method to improve the
system security without much computational overhead. Our experiments on the
RSR2015 Part-3 digit-prompted task show that, the DNN based alignment performs
on par with the HMM alignment. The results also demonstrate the effectiveness
of the proposed Kullback-Leibler (KL) divergence based scoring to reject speech
with incorrect pass-phrases.
</summary>
    <author>
      <name>Yi Liu</name>
    </author>
    <author>
      <name>Liang He</name>
    </author>
    <author>
      <name>Weiqiang Zhang</name>
    </author>
    <author>
      <name>Jia Liu</name>
    </author>
    <author>
      <name>Michael T. Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by APSIPA ASC 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.10436v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10436v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10451v2</id>
    <updated>2018-02-14T04:39:50Z</updated>
    <published>2017-10-28T11:55:50Z</published>
    <title>Sample-level CNN Architectures for Music Auto-tagging Using Raw
  Waveforms</title>
    <summary>  Recent work has shown that the end-to-end approach using convolutional neural
network (CNN) is effective in various types of machine learning tasks. For
audio signals, the approach takes raw waveforms as input using an 1-D
convolution layer. In this paper, we improve the 1-D CNN architecture for music
auto-tagging by adopting building blocks from state-of-the-art image
classification models, ResNets and SENets, and adding multi-level feature
aggregation to it. We compare different combinations of the modules in building
CNN architectures. The results show that they achieve significant improvements
over previous state-of-the-art models on the MagnaTagATune dataset and
comparable results on Million Song Dataset. Furthermore, we analyze and
visualize our model to show how the 1-D CNN operates.
</summary>
    <author>
      <name>Taejun Kim</name>
    </author>
    <author>
      <name>Jongpil Lee</name>
    </author>
    <author>
      <name>Juhan Nam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at ICASSP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.10451v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10451v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10948v1</id>
    <updated>2017-10-27T01:14:51Z</updated>
    <published>2017-10-27T01:14:51Z</published>
    <title>Sound Source Localization in a Multipath Environment Using Convolutional
  Neural Networks</title>
    <summary>  The propagation of sound in a shallow water environment is characterized by
boundary reflections from the sea surface and sea floor. These reflections
result in multiple (indirect) sound propagation paths, which can degrade the
performance of passive sound source localization methods. This paper proposes
the use of convolutional neural networks (CNNs) for the localization of sources
of broadband acoustic radiated noise (such as motor vessels) in shallow water
multipath environments. It is shown that CNNs operating on cepstrogram and
generalized cross-correlogram inputs are able to more reliably estimate the
instantaneous range and bearing of transiting motor vessels when the source
localization performance of conventional passive ranging methods is degraded.
The ensuing improvement in source localization performance is demonstrated
using real data collected during an at-sea experiment.
</summary>
    <author>
      <name>Eric L. Ferguson</name>
    </author>
    <author>
      <name>Stefan B. Williams</name>
    </author>
    <author>
      <name>Craig T. Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, Final draft of paper submitted to 2018 IEEE
  International Conference on Acoustics, Speech and Signal Processing (ICASSP)
  15-20 April 2018 in Calgary, Alberta, Canada. arXiv admin note: text overlap
  with arXiv:1612.03505</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.10948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10974v3</id>
    <updated>2018-02-15T15:05:30Z</updated>
    <published>2017-10-30T14:32:02Z</published>
    <title>Content-based Representations of audio using Siamese neural networks</title>
    <summary>  In this paper, we focus on the problem of content-based retrieval for audio,
which aims to retrieve all semantically similar audio recordings for a given
audio clip query. This problem is similar to the problem of query by example of
audio, which aims to retrieve media samples from a database, which are similar
to the user-provided example. We propose a novel approach which encodes the
audio into a vector representation using Siamese Neural Networks. The goal is
to obtain an encoding similar for files belonging to the same audio class, thus
allowing retrieval of semantically similar audio. Using simple similarity
measures such as those based on simple euclidean distance and cosine similarity
we show that these representations can be very effectively used for retrieving
recordings similar in audio content.
</summary>
    <author>
      <name>Pranay Manocha</name>
    </author>
    <author>
      <name>Rohan Badlani</name>
    </author>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Ankit Shah</name>
    </author>
    <author>
      <name>Benjamin Elizalde</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <link href="http://arxiv.org/abs/1710.10974v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10974v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.11153v2</id>
    <updated>2018-06-05T04:20:28Z</updated>
    <published>2017-10-30T18:05:49Z</published>
    <title>Onsets and Frames: Dual-Objective Piano Transcription</title>
    <summary>  We advance the state of the art in polyphonic piano music transcription by
using a deep convolutional and recurrent neural network which is trained to
jointly predict onsets and frames. Our model predicts pitch onset events and
then uses those predictions to condition framewise pitch predictions. During
inference, we restrict the predictions from the framewise detector by not
allowing a new note to start unless the onset detector also agrees that an
onset for that pitch is present in the frame. We focus on improving onsets and
offsets together instead of either in isolation as we believe this correlates
better with human musical perception. Our approach results in over a 100%
relative improvement in note F1 score (with offsets) on the MAPS dataset.
Furthermore, we extend the model to predict relative velocities of normalized
audio which results in more natural-sounding transcriptions.
</summary>
    <author>
      <name>Curtis Hawthorne</name>
    </author>
    <author>
      <name>Erich Elsen</name>
    </author>
    <author>
      <name>Jialin Song</name>
    </author>
    <author>
      <name>Adam Roberts</name>
    </author>
    <author>
      <name>Ian Simon</name>
    </author>
    <author>
      <name>Colin Raffel</name>
    </author>
    <author>
      <name>Jesse Engel</name>
    </author>
    <author>
      <name>Sageev Oore</name>
    </author>
    <author>
      <name>Douglas Eck</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Examples available at https://goo.gl/magenta/onsets-frames-examples</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.11153v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.11153v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.11418v2</id>
    <updated>2018-07-02T04:44:03Z</updated>
    <published>2017-10-31T11:57:00Z</published>
    <title>Polyphonic Music Generation with Sequence Generative Adversarial
  Networks</title>
    <summary>  We propose an application of sequence generative adversarial networks
(SeqGAN), which are generative adversarial networks for discrete sequence
generation, for creating polyphonic musical sequences. Instead of a monophonic
melody generation suggested in the original work, we present an efficient
representation of a polyphony MIDI file that simultaneously captures chords and
melodies with dynamic timings. The proposed method condenses duration, octaves,
and keys of both melodies and chords into a single word vector representation,
and recurrent neural networks learn to predict distributions of sequences from
the embedded musical word space. We experiment with the original method and the
least squares method to the discriminator, which is known to stabilize the
training of GANs. The network can create sequences that are musically coherent
and shows an improved quantitative and qualitative measures. We also report
that careful optimization of reinforcement learning signals of the model is
crucial for general application of the model.
</summary>
    <author>
      <name>Sang-gil Lee</name>
    </author>
    <author>
      <name>Uiwon Hwang</name>
    </author>
    <author>
      <name>Seonwoo Min</name>
    </author>
    <author>
      <name>Sungroh Yoon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.11418v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.11418v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.11428v2</id>
    <updated>2017-11-13T13:29:58Z</updated>
    <published>2017-10-31T12:19:23Z</published>
    <title>SVSGAN: Singing Voice Separation via Generative Adversarial Network</title>
    <summary>  Separating two sources from an audio mixture is an important task with many
applications. It is a challenging problem since only one signal channel is
available for analysis. In this paper, we propose a novel framework for singing
voice separation using the generative adversarial network (GAN) with a
time-frequency masking function. The mixture spectra is considered to be a
distribution and is mapped to the clean spectra which is also considered a
distribtution. The approximation of distributions between mixture spectra and
clean spectra is performed during the adversarial training process. In contrast
with current deep learning approaches for source separation, the parameters of
the proposed framework are first initialized in a supervised setting and then
optimized by the training procedure of GAN in an unsupervised setting.
Experimental results on three datasets (MIR-1K, iKala and DSD100) show that
performance can be improved by the proposed framework consisting of
conventional networks.
</summary>
    <author>
      <name>Zhe-Cheng Fan</name>
    </author>
    <author>
      <name>Yen-Lin Lai</name>
    </author>
    <author>
      <name>Jyh-Shing Roger Jang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures, 1 table. Demo website:
  http://mirlab.org/demo/svsgan</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.11428v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.11428v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.11473v1</id>
    <updated>2017-10-28T22:12:08Z</updated>
    <published>2017-10-28T22:12:08Z</published>
    <title>Multi-Resolution Fully Convolutional Neural Networks for Monaural Audio
  Source Separation</title>
    <summary>  In deep neural networks with convolutional layers, each layer typically has
fixed-size/single-resolution receptive field (RF). Convolutional layers with a
large RF capture global information from the input features, while layers with
small RF size capture local details with high resolution from the input
features. In this work, we introduce novel deep multi-resolution fully
convolutional neural networks (MR-FCNN), where each layer has different RF
sizes to extract multi-resolution features that capture the global and local
details information from its input features. The proposed MR-FCNN is applied to
separate a target audio source from a mixture of many audio sources.
Experimental results show that using MR-FCNN improves the performance compared
to feedforward deep neural networks (DNNs) and single resolution deep fully
convolutional neural networks (FCNNs) on the audio source separation problem.
</summary>
    <author>
      <name>Emad M. Grais</name>
    </author>
    <author>
      <name>Hagen Wierstorf</name>
    </author>
    <author>
      <name>Dominic Ward</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1703.08019</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.11473v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.11473v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; I.5; I.2.6; I.4.3; I.4; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00254v1</id>
    <updated>2017-12-01T09:49:21Z</updated>
    <published>2017-12-01T09:49:21Z</published>
    <title>Utilizing Domain Knowledge in End-to-End Audio Processing</title>
    <summary>  End-to-end neural network based approaches to audio modelling are generally
outperformed by models trained on high-level data representations. In this
paper we present preliminary work that shows the feasibility of training the
first layers of a deep convolutional neural network (CNN) model to learn the
commonly-used log-scaled mel-spectrogram transformation. Secondly, we
demonstrate that upon initializing the first layers of an end-to-end CNN
classifier with the learned transformation, convergence and performance on the
ESC-50 environmental sound classification dataset are similar to a CNN-based
model trained on the highly pre-processed log-scaled mel-spectrogram features.
</summary>
    <author>
      <name>Tycho Max Sylvester Tax</name>
    </author>
    <author>
      <name>Jose Luis Diez Antich</name>
    </author>
    <author>
      <name>Hendrik Purwins</name>
    </author>
    <author>
      <name>Lars Maaløe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the ML4Audio workshop at the NIPS 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.00254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00866v1</id>
    <updated>2017-12-04T00:58:58Z</updated>
    <published>2017-12-04T00:58:58Z</published>
    <title>Raw Waveform-based Audio Classification Using Sample-level CNN
  Architectures</title>
    <summary>  Music, speech, and acoustic scene sound are often handled separately in the
audio domain because of their different signal characteristics. However, as the
image domain grows rapidly by versatile image classification models, it is
necessary to study extensible classification models in the audio domain as
well. In this study, we approach this problem using two types of sample-level
deep convolutional neural networks that take raw waveforms as input and uses
filters with small granularity. One is a basic model that consists of
convolution and pooling layers. The other is an improved model that
additionally has residual connections, squeeze-and-excitation modules and
multi-level concatenation. We show that the sample-level models reach
state-of-the-art performance levels for the three different categories of
sound. Also, we visualize the filters along layers and compare the
characteristics of learned filters.
</summary>
    <author>
      <name>Jongpil Lee</name>
    </author>
    <author>
      <name>Taejun Kim</name>
    </author>
    <author>
      <name>Jiyoung Park</name>
    </author>
    <author>
      <name>Juhan Nam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NIPS, Machine Learning for Audio Signal Processing Workshop
  (ML4Audio), 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.00866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00917v1</id>
    <updated>2017-12-04T06:09:02Z</updated>
    <published>2017-12-04T06:09:02Z</published>
    <title>A text-independent speaker verification model: A comparative analysis</title>
    <summary>  The most pressing challenge in the field of voice biometrics is selecting the
most efficient technique of speaker recognition. Every individual's voice is
peculiar, factors like physical differences in vocal organs, accent and
pronunciation contributes to the problem's complexity. In this paper, we
explore the various methods available in each block in the process of speaker
recognition with the objective to identify best of techniques that could be
used to get precise results. We study the results on text independent corpora.
We use MFCC (Melfrequency cepstral coefficient), LPCC (linear predictive
cepstral coefficient) and PLP (perceptual linear prediction) algorithms for
feature extraction, PCA (Principal Component Analysis) and tSNE for
dimensionality reduction and SVM (Support Vector Machine), feed forward,
nearest neighbor and decision tree algorithms for classification block in
speaker recognition system and comparatively analyze each block to determine
the best technique
</summary>
    <author>
      <name>Rishi Charan</name>
    </author>
    <author>
      <name>Manisha. A</name>
    </author>
    <author>
      <name>Karthik. R</name>
    </author>
    <author>
      <name>Rajesh Kumar M</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">presented and accepted by 2017 International Conference on
  Intelligent Computing and Control (I2C2)</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.00917v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00917v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.01011v1</id>
    <updated>2017-12-04T11:18:08Z</updated>
    <published>2017-12-04T11:18:08Z</published>
    <title>Chord Generation from Symbolic Melody Using BLSTM Networks</title>
    <summary>  Generating a chord progression from a monophonic melody is a challenging
problem because a chord progression requires a series of layered notes played
simultaneously. This paper presents a novel method of generating chord
sequences from a symbolic melody using bidirectional long short-term memory
(BLSTM) networks trained on a lead sheet database. To this end, a group of
feature vectors composed of 12 semitones is extracted from the notes in each
bar of monophonic melodies. In order to ensure that the data shares uniform key
and duration characteristics, the key and the time signatures of the vectors
are normalized. The BLSTM networks then learn from the data to incorporate the
temporal dependencies to produce a chord progression. Both quantitative and
qualitative evaluations are conducted by comparing the proposed method with the
conventional HMM and DNN-HMM based approaches. Proposed model achieves 23.8%
and 11.4% performance increase from the other models, respectively. User
studies further confirm that the chord sequences generated by the proposed
method are preferred by listeners.
</summary>
    <author>
      <name>Hyungui Lim</name>
    </author>
    <author>
      <name>Seungyeon Rhyu</name>
    </author>
    <author>
      <name>Kyogu Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18th International Society for Music Information Retrieval Conference
  (ISMIR 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.01011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.01011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.02116v2</id>
    <updated>2019-04-06T22:20:28Z</updated>
    <published>2017-12-06T10:12:46Z</published>
    <title>Enabling Early Audio Event Detection with Neural Networks</title>
    <summary>  This paper presents a methodology for early detection of audio events from
audio streams. Early detection is the ability to infer an ongoing event during
its initial stage. The proposed system consists of a novel inference step
coupled with dual parallel tailored-loss deep neural networks (DNNs). The DNNs
share a similar architecture except for their loss functions, i.e. weighted
loss and multitask loss, which are designed to efficiently cope with issues
common to audio event detection. The inference step is newly introduced to make
use of the network outputs for recognizing ongoing events. The monotonicity of
the detection function is required for reliable early detection, and will also
be proved. Experiments on the ITC-Irst database show that the proposed system
achieves state-of-the-art detection performance. Furthermore, even partial
events are sufficient to achieve good performance similar to that obtained when
an entire event is observed, enabling early event detection.
</summary>
    <author>
      <name>Huy Phan</name>
    </author>
    <author>
      <name>Philipp Koch</name>
    </author>
    <author>
      <name>Ian McLoughlin</name>
    </author>
    <author>
      <name>Alfred Mertins</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2018.8461859</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2018.8461859" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published version available at
  https://ieeexplore.ieee.org/document/8461859</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Proceedings of 43rd IEEE International Conference on
  Acoustics, Speech, and Signal Processing (ICASSP), pp. 141-145, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.02116v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.02116v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03569v1</id>
    <updated>2017-12-10T18:24:23Z</updated>
    <published>2017-12-10T18:24:23Z</published>
    <title>The organization of a three-manual keyboard for 53-tone tempered and
  other tempered systems</title>
    <summary>  The aim is to explore new opportunities of the pitch organization of the
musical scale. Specifically, a numerical comparison of the different musical
temperaments among themselves in the degree of approximation of the Pythagorean
scale is provided, and thus it numerically substantiates the thesis that the
53-tone tempered system is the most advanced among possible others. We present
numerical data on the approximation of overtones from first twenty by steps of
the 53-tone temperament. Here were proposed some schemes of the three-manual
keyboard for the implementation of 53-tone temperament, which are also
implemented at the same time for 12 -, 17 -, 24 -, 29 - and 41-sounding system.
If there are technical means then these schemes can be used to play music in
any temperaments, based on said number of steps.
</summary>
    <author>
      <name>Vladimir P. Burskii</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, in Russian, 10 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.03569v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03569v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94A99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.03579v1</id>
    <updated>2017-12-10T19:52:51Z</updated>
    <published>2017-12-10T19:52:51Z</published>
    <title>Prodorshok I: A Bengali Isolated Speech Dataset for Voice-Based
  Assistive Technologies - A comparative analysis of the effects of data
  augmentation on HMM-GMM and DNN classifiers</title>
    <summary>  Prodorshok I is a Bengali isolated word dataset tailored to help create
speaker-independent, voice-command driven automated speech recognition (ASR)
based assistive technologies to help improve human-computer interaction (HCI).
This paper presents the results of an objective analysis that was undertaken
using a subset of words from Prodorshok I to assess its reliability in ASR
systems that utilize Hidden Markov Models (HMM) with Gaussian emissions and
Deep Neural Networks (DNN). The results show that simple data augmentation
involving a small pitch shift can make surprisingly tangible improvements to
accuracy levels in speech recognition.
</summary>
    <author>
      <name>Mohi Reza</name>
    </author>
    <author>
      <name>Warida Rashid</name>
    </author>
    <author>
      <name>Moin Mostakim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, accepted for oral presentation at the 5th IEEE R10 HTC 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.03579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.03579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05274v2</id>
    <updated>2018-09-05T03:25:40Z</updated>
    <published>2017-12-14T15:11:09Z</published>
    <title>A Hierarchical Recurrent Neural Network for Symbolic Melody Generation</title>
    <summary>  In recent years, neural networks have been used to generate symbolic
melodies. However, the long-term structure in the melody has posed great
difficulty for designing a good model. In this paper, we present a hierarchical
recurrent neural network for melody generation, which consists of three
Long-Short-Term-Memory (LSTM) subnetworks working in a coarse-to-fine manner
along time. Specifically, the three subnetworks generate bar profiles, beat
profiles and notes in turn, and the output of the high-level subnetworks are
fed into the low-level subnetworks, serving as guidance for generating the
finer time-scale melody components in low-level subnetworks. Two human behavior
experiments demonstrate the advantage of this structure over the single-layer
LSTM which attempts to learn all hidden structures in melodies. Compared with
the state-of-the-art models MidiNet and MusicVAE, the hierarchical recurrent
neural network produces better melodies evaluated by humans.
</summary>
    <author>
      <name>Jian Wu</name>
    </author>
    <author>
      <name>Changran Hu</name>
    </author>
    <author>
      <name>Yulong Wang</name>
    </author>
    <author>
      <name>Xiaolin Hu</name>
    </author>
    <author>
      <name>Jun Zhu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.05274v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05274v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.06340v1</id>
    <updated>2017-12-18T11:16:08Z</updated>
    <published>2017-12-18T11:16:08Z</published>
    <title>Language and Noise Transfer in Speech Enhancement Generative Adversarial
  Network</title>
    <summary>  Speech enhancement deep learning systems usually require large amounts of
training data to operate in broad conditions or real applications. This makes
the adaptability of those systems into new, low resource environments an
important topic. In this work, we present the results of adapting a speech
enhancement generative adversarial network by finetuning the generator with
small amounts of data. We investigate the minimum requirements to obtain a
stable behavior in terms of several objective metrics in two very different
languages: Catalan and Korean. We also study the variability of test
performance to unseen noise as a function of the amount of different types of
noise available for training. Results show that adapting a pre-trained English
model with 10 min of data already achieves a comparable performance to having
two orders of magnitude more data. They also demonstrate the relative stability
in test performance with respect to the number of training noise types.
</summary>
    <author>
      <name>Santiago Pascual</name>
    </author>
    <author>
      <name>Maruchan Park</name>
    </author>
    <author>
      <name>Joan Serrà</name>
    </author>
    <author>
      <name>Antonio Bonafonte</name>
    </author>
    <author>
      <name>Kang-Hun Ahn</name>
    </author>
    <link href="http://arxiv.org/abs/1712.06340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.06340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.07065v1</id>
    <updated>2017-12-19T17:30:19Z</updated>
    <published>2017-12-19T17:30:19Z</published>
    <title>Joint model-based recognition and localization of overlapped acoustic
  events using a set of distributed small microphone arrays</title>
    <summary>  In the analysis of acoustic scenes, often the occurring sounds have to be
detected in time, recognized, and localized in space. Usually, each of these
tasks is done separately. In this paper, a model-based approach to jointly
carry them out for the case of multiple simultaneous sources is presented and
tested. The recognized event classes and their respective room positions are
obtained with a single system that maximizes the combination of a large set of
scores, each one resulting from a different acoustic event model and a
different beamformer output signal, which comes from one of several
arbitrarily-located small microphone arrays. By using a two-step method, the
experimental work for a specific scenario consisting of meeting-room acoustic
events, either isolated or overlapped with speech, is reported. Tests carried
out with two datasets show the advantage of the proposed approach with respect
to some usual techniques, and that the inclusion of estimated priors brings a
further performance improvement.
</summary>
    <author>
      <name>Rupayan Chakraborty</name>
    </author>
    <author>
      <name>Climent Nadeu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Computational acoustic scene analysis, microphone array signal
  processing, acoustic event detection</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.07065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.07799v1</id>
    <updated>2017-12-21T05:28:15Z</updated>
    <published>2017-12-21T05:28:15Z</published>
    <title>Towards a Deep Improviser: a prototype deep learning post-tonal free
  music generator</title>
    <summary>  Two modest-sized symbolic corpora of post-tonal and post-metric keyboard
music have been constructed, one algorithmic, the other improvised. Deep
learning models of each have been trained and largely optimised. Our purpose is
to obtain a model with sufficient generalisation capacity that in response to a
small quantity of separate fresh input seed material, it can generate outputs
that are distinctive, rather than recreative of the learned corpora or the seed
material. This objective has been first assessed statistically, and as judged
by k-sample Anderson-Darling and Cramer tests, has been achieved. Music has
been generated using the approach, and informal judgements place it roughly on
a par with algorithmic and composed music in related forms. Future work will
aim to enhance the model such that it can be evaluated in relation to
expression, meaning and utility in real-time performance.
</summary>
    <author>
      <name>Roger T. Dean</name>
    </author>
    <author>
      <name>Jamie Forth</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 1 Figure, 3 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.07799v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07799v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.07814v1</id>
    <updated>2017-12-21T07:26:53Z</updated>
    <published>2017-12-21T07:26:53Z</published>
    <title>Indoor Sound Source Localization with Probabilistic Neural Network</title>
    <summary>  It is known that adverse environments such as high reverberation and low
signal-to-noise ratio (SNR) pose a great challenge to indoor sound source
localization. To address this challenge, in this paper, we propose a sound
source localization algorithm based on probabilistic neural network, namely
Generalized cross correlation Classification Algorithm (GCA). Experimental
results for adverse environments with high reverberation time T60 up to 600ms
and low SNR such as -10dB show that, the average azimuth angle error and
elevation angle error by GCA are only 4.6 degrees and 3.1 degrees respectively.
Compared with three recently published algorithms, GCA has increased the
success rate on direction of arrival estimation significantly with good
robustness to environmental changes. These results show that the proposed GCA
can localize accurately and robustly for diverse indoor applications where the
site acoustic features can be studied prior to the localization stage.
</summary>
    <author>
      <name>Yingxiang Sun</name>
    </author>
    <author>
      <name>Jiajia Chen</name>
    </author>
    <author>
      <name>Chau Yuen</name>
    </author>
    <author>
      <name>Susanto Rahardja</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIE.2017.2786219</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIE.2017.2786219" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, accepted by IEEE Transactions on Industrial Electronics</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Industrial Electronics, vol. 65, no. 8, pp.
  6403-6413, Aug. 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.07814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08363v2</id>
    <updated>2018-03-08T09:17:27Z</updated>
    <published>2017-12-22T09:19:23Z</published>
    <title>On Using Backpropagation for Speech Texture Generation and Voice
  Conversion</title>
    <summary>  Inspired by recent work on neural network image generation which rely on
backpropagation towards the network inputs, we present a proof-of-concept
system for speech texture synthesis and voice conversion based on two
mechanisms: approximate inversion of the representation learned by a speech
recognition neural network, and on matching statistics of neuron activations
between different source and target utterances. Similar to image texture
synthesis and neural style transfer, the system works by optimizing a cost
function with respect to the input waveform samples. To this end we use a
differentiable mel-filterbank feature extraction pipeline and train a
convolutional CTC speech recognition network. Our system is able to extract
speaker characteristics from very limited amounts of target speaker data, as
little as a few seconds, and can be used to generate realistic speech babble or
reconstruct an utterance in a different voice.
</summary>
    <author>
      <name>Jan Chorowski</name>
    </author>
    <author>
      <name>Ron J. Weiss</name>
    </author>
    <author>
      <name>Rif A. Saurous</name>
    </author>
    <author>
      <name>Samy Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICASSP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.08363v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08363v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08708v3</id>
    <updated>2020-07-28T01:35:27Z</updated>
    <published>2017-12-23T03:54:00Z</published>
    <title>Variational Autoencoders for Learning Latent Representations of Speech
  Emotion: A Preliminary Study</title>
    <summary>  Learning the latent representation of data in unsupervised fashion is a very
interesting process that provides relevant features for enhancing the
performance of a classifier. For speech emotion recognition tasks, generating
effective features is crucial. Currently, handcrafted features are mostly used
for speech emotion recognition, however, features learned automatically using
deep learning have shown strong success in many problems, especially in image
processing. In particular, deep generative models such as Variational
Autoencoders (VAEs) have gained enormous success for generating features for
natural images. Inspired by this, we propose VAEs for deriving the latent
representation of speech signals and use this representation to classify
emotions. To the best of our knowledge, we are the first to propose VAEs for
speech emotion classification. Evaluations on the IEMOCAP dataset demonstrate
that features learned by VAEs can produce state-of-the-art results for speech
emotion classification.
</summary>
    <author>
      <name>Siddique Latif</name>
    </author>
    <author>
      <name>Rajib Rana</name>
    </author>
    <author>
      <name>Junaid Qadir</name>
    </author>
    <author>
      <name>Julien Epps</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. Interspeech 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.08708v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08708v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09668v2</id>
    <updated>2018-02-19T13:50:35Z</updated>
    <published>2017-12-27T20:09:05Z</published>
    <title>Eventness: Object Detection on Spectrograms for Temporal Localization of
  Audio Events</title>
    <summary>  In this paper, we introduce the concept of Eventness for audio event
detection, which can, in part, be thought of as an analogue to Objectness from
computer vision. The key observation behind the eventness concept is that audio
events reveal themselves as 2-dimensional time-frequency patterns with specific
textures and geometric structures in spectrograms. These time-frequency
patterns can then be viewed analogously to objects occurring in natural images
(with the exception that scaling and rotation invariance properties do not
apply). With this key observation in mind, we pose the problem of detecting
monophonic or polyphonic audio events as an equivalent visual object(s)
detection problem under partial occlusion and clutter in spectrograms. We adapt
a state-of-the-art visual object detection model to evaluate the audio event
detection task on publicly available datasets. The proposed network has
comparable results with a state-of-the-art baseline and is more robust on
minority events. Provided large-scale datasets, we hope that our proposed
conceptual model of eventness will be beneficial to the audio signal processing
community towards improving performance of audio event detection.
</summary>
    <author>
      <name>Phuong Pham</name>
    </author>
    <author>
      <name>Juncheng Li</name>
    </author>
    <author>
      <name>Joseph Szurley</name>
    </author>
    <author>
      <name>Samarjit Das</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, accepted to ICASSP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.09668v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09668v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.00887v1</id>
    <updated>2018-01-03T02:54:08Z</updated>
    <published>2018-01-03T02:54:08Z</published>
    <title>DeepJ: Style-Specific Music Generation</title>
    <summary>  Recent advances in deep neural networks have enabled algorithms to compose
music that is comparable to music composed by humans. However, few algorithms
allow the user to generate music with tunable parameters. The ability to tune
properties of generated music will yield more practical benefits for aiding
artists, filmmakers, and composers in their creative tasks. In this paper, we
introduce DeepJ - an end-to-end generative model that is capable of composing
music conditioned on a specific mixture of composer styles. Our innovations
include methods to learn musical style and music dynamics. We use our model to
demonstrate a simple technique for controlling the style of generated music as
a proof of concept. Evaluation of our model using human raters shows that we
have improved over the Biaxial LSTM approach.
</summary>
    <author>
      <name>Huanru Henry Mao</name>
    </author>
    <author>
      <name>Taylor Shin</name>
    </author>
    <author>
      <name>Garrison W. Cottrell</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICSC.2018.00077</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICSC.2018.00077" rel="related"/>
    <link href="http://arxiv.org/abs/1801.00887v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.00887v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.01589v1</id>
    <updated>2018-01-04T23:59:07Z</updated>
    <published>2018-01-04T23:59:07Z</published>
    <title>Neural Style Transfer for Audio Spectograms</title>
    <summary>  There has been fascinating work on creating artistic transformations of
images by Gatys. This was revolutionary in how we can in some sense alter the
'style' of an image while generally preserving its 'content'. In our work, we
present a method for creating new sounds using a similar approach, treating it
as a style-transfer problem, starting from a random-noise input signal and
iteratively using back-propagation to optimize the sound to conform to
filter-outputs from a pre-trained neural architecture of interest.
  For demonstration, we investigate two different tasks, resulting in bandwidth
expansion/compression, and timbral transfer from singing voice to musical
instruments. A feature of our method is that a single architecture can generate
these different audio-style-transfer types using the same set of parameters
which otherwise require different complex hand-tuned diverse signal processing
pipelines.
</summary>
    <author>
      <name>Prateek Verma</name>
    </author>
    <author>
      <name>Julius O. Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appeared in 31st Conference on Neural Information Processing Systems
  (NIPS 2017), Long Beach, CA, USA at the workshop for Machine Learning for
  Creativity and Design</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.01589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.01712v1</id>
    <updated>2018-01-05T11:09:31Z</updated>
    <published>2018-01-05T11:09:31Z</published>
    <title>Tree based classification of tabla strokes</title>
    <summary>  The paper attempts to validate the effectiveness of tree classifiers to
classify tabla strokes especially the ones which are overlapping in nature. It
uses decision tree, ID3 and random forest as classifiers. A custom made data
sets of 650 samples of 13 different tabla strokes were used for experimental
purpose. 31 different features with their mean and variances were extracted for
classification. Three data sets consisting of 21361, 18802 and 19543 instances
respectively were used for the purpose. Validation has been done using measures
like ROC curve and accuracy. The experimental results showed that all the
classifiers showing excellent results with random forest outperforming the
other two. The effectiveness of random forest in classifying strokes which are
overlapping in nature is done by comparing the known results of that with
multi-layer perceptron.
</summary>
    <author>
      <name>Subodh Deolekar</name>
    </author>
    <author>
      <name>Siby Abraham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 11 figures, current science</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.01712v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.01712v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.02690v1</id>
    <updated>2018-01-08T21:12:49Z</updated>
    <published>2018-01-08T21:12:49Z</published>
    <title>DCASE 2017 Task 1: Acoustic Scene Classification Using Shift-Invariant
  Kernels and Random Features</title>
    <summary>  Acoustic scene recordings are represented by different types of handcrafted
or Neural Network-derived features. These features, typically of thousands of
dimensions, are classified in state of the art approaches using kernel
machines, such as the Support Vector Machines (SVM). However, the complexity of
training these methods increases with the dimensionality of these input
features and the size of the dataset. A solution is to map the input features
to a randomized lower-dimensional feature space. The resulting random features
can approximate non-linear kernels with faster linear kernel computation. In
this work, we computed a set of 6,553 input features and used them to compute
random features to approximate three types of kernels, Gaussian, Laplacian and
Cauchy. We compared their performance using an SVM in the context of the DCASE
Task 1 - Acoustic Scene Classification. Experiments show that both, input and
random features outperformed the DCASE baseline by an absolute 4%. Moreover,
the random features reduced the dimensionality of the input by more than three
times with minimal loss of performance and by more than six times and still
outperformed the baseline. Hence, random features could be employed by state of
the art approaches to compute low-storage features and perform faster kernel
computations.
</summary>
    <author>
      <name>Abelino Jimenez</name>
    </author>
    <author>
      <name>Benjamin Elizalde</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <link href="http://arxiv.org/abs/1801.02690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.02690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05544v2</id>
    <updated>2023-03-29T19:52:25Z</updated>
    <published>2018-01-17T04:29:12Z</published>
    <title>NELS -- Never-Ending Learner of Sounds</title>
    <summary>  Sounds are essential to how humans perceive and interact with the world and
are captured in recordings and shared on the Internet on a minute-by-minute
basis. These recordings, which are predominantly videos, constitute the largest
archive of sounds we know. However, most of these recordings have undescribed
content making necessary methods for automatic sound analysis, indexing and
retrieval. These methods have to address multiple challenges, such as the
relation between sounds and language, numerous and diverse sound classes, and
large-scale evaluation. We propose a system that continuously learns from the
web relations between sounds and language, improves sound recognition models
over time and evaluates its learning competency in the large-scale without
references. We introduce the Never-Ending Learner of Sounds (NELS), a project
for continuously learning of sounds and their associated knowledge, available
on line in nels.cs.cmu.edu
</summary>
    <author>
      <name>Benjamin Elizalde</name>
    </author>
    <author>
      <name>Rohan Badlani</name>
    </author>
    <author>
      <name>Ankit Shah</name>
    </author>
    <author>
      <name>Anurag Kumar</name>
    </author>
    <author>
      <name>Bhiksha Raj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at Machine Learning for Audio Signal Processing (ML4Audio),
  31st Conference on Neural Information Processing Systems (NIPS 2017), Long
  Beach, CA, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.05544v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05544v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.07054v1</id>
    <updated>2018-01-22T11:33:50Z</updated>
    <published>2018-01-22T11:33:50Z</published>
    <title>Identifying Speakers Using Their Emotion Cues</title>
    <summary>  This paper addresses the formulation of a new speaker identification approach
which employs knowledge of emotional content of speaker information. Our
proposed approach in this work is based on a two-stage recognizer that combines
and integrates both emotion recognizer and speaker recognizer into one
recognizer. The proposed approach employs both Hidden Markov Models (HMMs) and
Suprasegmental Hidden Markov Models (SPHMMs) as classifiers. In the
experiments, six emotions are considered including neutral, angry, sad, happy,
disgust and fear. Our results show that average speaker identification
performance based on the proposed two-stage recognizer is 79.92% with a
significant improvement over a one-stage recognizer with an identification
performance of 71.58%. The results obtained based on the proposed approach are
close to those achieved in subjective evaluation by human listeners.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10772-011-9089-1.</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10772-011-9089-1." rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages. arXiv admin note: text overlap with arXiv:1707.00137</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Identifying speakers using their emotion cues, International
  Journal of Speech Technology, Vol. 14, No. 2, June 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1801.07054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.07054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09522v1</id>
    <updated>2018-01-29T14:24:39Z</updated>
    <published>2018-01-29T14:24:39Z</published>
    <title>Multichannel Sound Event Detection Using 3D Convolutional Neural
  Networks for Learning Inter-channel Features</title>
    <summary>  In this paper, we propose a stacked convolutional and recurrent neural
network (CRNN) with a 3D convolutional neural network (CNN) in the first layer
for the multichannel sound event detection (SED) task. The 3D CNN enables the
network to simultaneously learn the inter- and intra-channel features from the
input multichannel audio. In order to evaluate the proposed method,
multichannel audio datasets with different number of overlapping sound sources
are synthesized. Each of this dataset has a four-channel first-order Ambisonic,
binaural, and single-channel versions, on which the performance of SED using
the proposed method are compared to study the potential of SED using
multichannel audio. A similar study is also done with the binaural and
single-channel versions of the real-life recording TUT-SED 2017 development
dataset. The proposed method learns to recognize overlapping sound events from
multichannel features faster and performs better SED with a fewer number of
training epochs. The results show that on using multichannel Ambisonic audio in
place of single-channel audio we improve the overall F-score by 7.5%, overall
error rate by 10% and recognize 15.6% more sound events in time frames with
four overlapping sound sources.
</summary>
    <author>
      <name>Sharath Adavanne</name>
    </author>
    <author>
      <name>Archontis Politis</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <link href="http://arxiv.org/abs/1801.09522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.09774v1</id>
    <updated>2018-01-29T21:45:05Z</updated>
    <published>2018-01-29T21:45:05Z</published>
    <title>On Psychoacoustically Weighted Cost Functions Towards Resource-Efficient
  Deep Neural Networks for Speech Denoising</title>
    <summary>  We present a psychoacoustically enhanced cost function to balance network
complexity and perceptual performance of deep neural networks for speech
denoising. While training the network, we utilize perceptual weights added to
the ordinary mean-squared error to emphasize contribution from frequency bins
which are most audible while ignoring error from inaudible bins. To generate
the weights, we employ psychoacoustic models to compute the global masking
threshold from the clean speech spectra. We then evaluate the speech denoising
performance of our perceptually guided neural network by using both objective
and perceptual sound quality metrics, testing on various network structures
ranging from shallow and narrow ones to deep and wide ones. The experimental
results showcase our method as a valid approach for infusing perceptual
significance to deep neural network operations. In particular, the more
perceptually sensible enhancement in performance seen by simple neural network
topologies proves that the proposed method can lead to resource-efficient
speech denoising implementations in small devices without degrading the
perceived signal fidelity.
</summary>
    <author>
      <name>Kai Zhen</name>
    </author>
    <author>
      <name>Aswin Sivaraman</name>
    </author>
    <author>
      <name>Jongmo Sung</name>
    </author>
    <author>
      <name>Minje Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.09774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.09774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.10492v3</id>
    <updated>2018-12-19T22:16:26Z</updated>
    <published>2018-01-31T15:30:32Z</published>
    <title>Deep Predictive Models in Interactive Music</title>
    <summary>  Musical performance requires prediction to operate instruments, to perform in
groups and to improvise. In this paper, we investigate how a number of digital
musical instruments (DMIs), including two of our own, have applied predictive
machine learning models that assist users by predicting unknown states of
musical processes. We characterise these predictions as focussed within a
musical instrument, at the level of individual performers, and between members
of an ensemble. These models can connect to existing frameworks for DMI design
and have parallels in the cognitive predictions of human musicians.
  We discuss how recent advances in deep learning highlight the role of
prediction in DMIs, by allowing data-driven predictive models with a long
memory of past states. The systems we review are used to motivate musical
use-cases where prediction is a necessary component, and to highlight a number
of challenges for DMI designers seeking to apply deep predictive models in
interactive music systems of the future.
</summary>
    <author>
      <name>Charles P. Martin</name>
    </author>
    <author>
      <name>Kai Olav Ellefsen</name>
    </author>
    <author>
      <name>Jim Torresen</name>
    </author>
    <link href="http://arxiv.org/abs/1801.10492v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.10492v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00254v1</id>
    <updated>2018-02-01T12:00:45Z</updated>
    <published>2018-02-01T12:00:45Z</published>
    <title>Phonetic and Graphemic Systems for Multi-Genre Broadcast Transcription</title>
    <summary>  State-of-the-art English automatic speech recognition systems typically use
phonetic rather than graphemic lexicons. Graphemic systems are known to perform
less well for English as the mapping from the written form to the spoken form
is complicated. However, in recent years the representational power of
deep-learning based acoustic models has improved, raising interest in graphemic
acoustic models for English, due to the simplicity of generating the lexicon.
In this paper, phonetic and graphemic models are compared for an English
Multi-Genre Broadcast transcription task. A range of acoustic models based on
lattice-free MMI training are constructed using phonetic and graphemic
lexicons. For this task, it is found that having a long-span temporal history
reduces the difference in performance between the two forms of models. In
addition, system combination is examined, using parameter smoothing and
hypothesis combination. As the combination approaches become more complicated
the difference between the phonetic and graphemic systems further decreases.
Finally, for all configurations examined the combination of phonetic and
graphemic systems yields consistent gains.
</summary>
    <author>
      <name>Yu Wang</name>
    </author>
    <author>
      <name>Xie Chen</name>
    </author>
    <author>
      <name>Mark Gales</name>
    </author>
    <author>
      <name>Anton Ragni</name>
    </author>
    <author>
      <name>Jeremy Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 6 tables, to appear in 2018 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.00254v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00254v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00300v1</id>
    <updated>2018-02-01T14:31:36Z</updated>
    <published>2018-02-01T14:31:36Z</published>
    <title>MaD TwinNet: Masker-Denoiser Architecture with Twin Networks for
  Monaural Sound Source Separation</title>
    <summary>  Monaural singing voice separation task focuses on the prediction of the
singing voice from a single channel music mixture signal. Current state of the
art (SOTA) results in monaural singing voice separation are obtained with deep
learning based methods. In this work we present a novel deep learning based
method that learns long-term temporal patterns and structures of a musical
piece. We build upon the recently proposed Masker-Denoiser (MaD) architecture
and we enhance it with the Twin Networks, a technique to regularize a recurrent
generative network using a backward running copy of the network. We evaluate
our method using the Demixing Secret Dataset and we obtain an increment to
signal-to-distortion ratio (SDR) of 0.37 dB and to signal-to-interference ratio
(SIR) of 0.23 dB, compared to previous SOTA results.
</summary>
    <author>
      <name>Konstantinos Drossos</name>
    </author>
    <author>
      <name>Stylianos Ioannis Mimilakis</name>
    </author>
    <author>
      <name>Dmitriy Serdyuk</name>
    </author>
    <author>
      <name>Gerald Schuller</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1802.00300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00380v1</id>
    <updated>2018-02-01T16:41:09Z</updated>
    <published>2018-02-01T16:41:09Z</published>
    <title>Approximate Message Passing for Underdetermined Audio Source Separation</title>
    <summary>  Approximate message passing (AMP) algorithms have shown great promise in
sparse signal reconstruction due to their low computational requirements and
fast convergence to an exact solution. Moreover, they provide a probabilistic
framework that is often more intuitive than alternatives such as convex
optimisation. In this paper, AMP is used for audio source separation from
underdetermined instantaneous mixtures. In the time-frequency domain, it is
typical to assume a priori that the sources are sparse, so we solve the
corresponding sparse linear inverse problem using AMP. We present a block-based
approach that uses AMP to process multiple time-frequency points
simultaneously. Two algorithms known as AMP and vector AMP (VAMP) are evaluated
in particular. Results show that they are promising in terms of artefact
suppression.
</summary>
    <author>
      <name>Turab Iqbal</name>
    </author>
    <author>
      <name>Wenwu Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper accepted for 3rd International Conference on Intelligent Signal
  Processing (ISP 2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.00380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00604v1</id>
    <updated>2018-02-02T09:00:39Z</updated>
    <published>2018-02-02T09:00:39Z</published>
    <title>Monaural Speech Enhancement using Deep Neural Networks by Maximizing a
  Short-Time Objective Intelligibility Measure</title>
    <summary>  In this paper we propose a Deep Neural Network (DNN) based Speech Enhancement
(SE) system that is designed to maximize an approximation of the Short-Time
Objective Intelligibility (STOI) measure. We formalize an approximate-STOI cost
function and derive analytical expressions for the gradients required for DNN
training and show that these gradients have desirable properties when used
together with gradient based optimization techniques. We show through
simulation experiments that the proposed SE system achieves large improvements
in estimated speech intelligibility, when tested on matched and unmatched
natural noise types, at multiple signal-to-noise ratios. Furthermore, we show
that the SE system, when trained using an approximate-STOI cost function
performs on par with a system trained with a mean square error cost applied to
short-time temporal envelopes. Finally, we show that the proposed SE system
performs on par with a traditional DNN based Short-Time Spectral Amplitude
(STSA) SE system in terms of estimated speech intelligibility. These results
are important because they suggest that traditional DNN based STSA SE systems
might be optimal in terms of estimated speech intelligibility.
</summary>
    <author>
      <name>Morten Kolbæk</name>
    </author>
    <author>
      <name>Zheng-Hua Tan</name>
    </author>
    <author>
      <name>Jesper Jensen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ICASSP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.00604v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00604v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03581v1</id>
    <updated>2018-02-10T12:50:34Z</updated>
    <published>2018-02-10T12:50:34Z</published>
    <title>2-gram-based Phonetic Feature Generation for Convolutional Neural
  Network in Assessment of Trademark Similarity</title>
    <summary>  A trademark is a mark used to identify various commodities. If same or
similar trademark is registered for the same or similar commodity, the
purchaser of the goods may be confused. Therefore, in the process of trademark
registration examination, the examiner judges whether the trademark is the same
or similar to the other applied or registered trademarks. The confusion in
trademarks is based on the visual, phonetic or conceptual similarity of the
marks. In this paper, we focus specifically on the phonetic similarity between
trademarks. We propose a method to generate 2D phonetic feature for
convolutional neural network in assessment of trademark similarity. This
proposed algorithm is tested with 12,553 trademark phonetic similar pairs and
34,020 trademark phonetic non-similar pairs from 2010 to 2016. As a result, we
have obtained approximately 92% judgment accuracy.
</summary>
    <author>
      <name>Kyung Pyo Ko</name>
    </author>
    <author>
      <name>Kwang Hee Lee</name>
    </author>
    <author>
      <name>Mi So Jang</name>
    </author>
    <author>
      <name>Gun Hong Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures, 10 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.03581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.04113v1</id>
    <updated>2018-02-12T15:23:35Z</updated>
    <published>2018-02-12T15:23:35Z</published>
    <title>Linear Regression for Speaker Verification</title>
    <summary>  This paper presents a linear regression based back-end for speaker
verification. Linear regression is a simple linear model that minimizes the
mean squared estimation error between the target and its estimate with a closed
form solution, where the target is defined as the ground-truth indicator
vectors of utterances. We use the linear regression model to learn speaker
models from a front-end, and verify the similarity of two speaker models by a
cosine similarity scoring classifier. To evaluate the effectiveness of the
linear regression model, we construct three speaker verification systems that
use the Gaussian mixture model and identity-vector (GMM/i-vector) front-end,
deep neural network and i-vector (DNN/i-vector) front-end, and deep vector
(d-vector) front-end as their front-ends, respectively. Our empirical
comparison results on the NIST speaker recognition evaluation data sets show
that the proposed method outperforms within-class covariance normalization,
linear discriminant analysis, and probabilistic linear discriminant analysis,
given any of the three front-ends.
</summary>
    <author>
      <name>Xiao-Lei Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/1802.04113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.04113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.04208v3</id>
    <updated>2019-02-09T00:51:18Z</updated>
    <published>2018-02-12T17:50:43Z</published>
    <title>Adversarial Audio Synthesis</title>
    <summary>  Audio signals are sampled at high temporal resolutions, and learning to
synthesize audio requires capturing structure across a range of timescales.
Generative adversarial networks (GANs) have seen wide success at generating
images that are both locally and globally coherent, but they have seen little
application to audio generation. In this paper we introduce WaveGAN, a first
attempt at applying GANs to unsupervised synthesis of raw-waveform audio.
WaveGAN is capable of synthesizing one second slices of audio waveforms with
global coherence, suitable for sound effect generation. Our experiments
demonstrate that, without labels, WaveGAN learns to produce intelligible words
when trained on a small-vocabulary speech dataset, and can also synthesize
audio from other domains such as drums, bird vocalizations, and piano. We
compare WaveGAN to a method which applies GANs designed for image generation on
image-like audio feature representations, finding both approaches to be
promising.
</summary>
    <author>
      <name>Chris Donahue</name>
    </author>
    <author>
      <name>Julian McAuley</name>
    </author>
    <author>
      <name>Miller Puckette</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ICLR 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.04208v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.04208v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05162v2</id>
    <updated>2018-02-20T12:45:51Z</updated>
    <published>2018-02-14T15:42:17Z</published>
    <title>BachProp: Learning to Compose Music in Multiple Styles</title>
    <summary>  Hand in hand with deep learning advancements, algorithms of music composition
increase in performance. However, most of the successful models are designed
for specific musical structures. Here, we present BachProp, an algorithmic
composer that can generate music scores in any style given sufficient training
data. To adapt BachProp to a broad range of musical styles, we propose a novel
normalized representation of music and train a deep network to predict the note
transition probabilities of a given music corpus. In this paper, new music
scores sampled by BachProp are compared with the original corpora via
crowdsourcing. This evaluation indicates that the music scores generated by
BachProp are not less preferred than the original music corpus the algorithm
was provided with.
</summary>
    <author>
      <name>Florian Colombo</name>
    </author>
    <author>
      <name>Wulfram Gerstner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary work. Under review by the 2018 International Conference
  on Machine Learning (ICML)</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.05162v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05162v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05429v1</id>
    <updated>2018-02-15T08:01:48Z</updated>
    <published>2018-02-15T08:01:48Z</published>
    <title>Blind Source Separation with Optimal Transport Non-negative Matrix
  Factorization</title>
    <summary>  Optimal transport as a loss for machine learning optimization problems has
recently gained a lot of attention. Building upon recent advances in
computational optimal transport, we develop an optimal transport non-negative
matrix factorization (NMF) algorithm for supervised speech blind source
separation (BSS). Optimal transport allows us to design and leverage a cost
between short-time Fourier transform (STFT) spectrogram frequencies, which
takes into account how humans perceive sound. We give empirical evidence that
using our proposed optimal transport NMF leads to perceptually better results
than Euclidean NMF, for both isolated voice reconstruction and BSS tasks.
Finally, we demonstrate how to use optimal transport for cross domain sound
processing tasks, where frequencies represented in the input spectrograms may
be different from one spectrogram to another.
</summary>
    <author>
      <name>Antoine Rolet</name>
    </author>
    <author>
      <name>Vivien Seguy</name>
    </author>
    <author>
      <name>Mathieu Blondel</name>
    </author>
    <author>
      <name>Hiroshi Sawada</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1186/s13634-018-0576-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1186/s13634-018-0576-2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 7 figures, 2 additional files</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.05429v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05429v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05630v2</id>
    <updated>2018-09-11T18:05:16Z</updated>
    <published>2018-02-15T16:05:02Z</published>
    <title>CNN+LSTM Architecture for Speech Emotion Recognition with Data
  Augmentation</title>
    <summary>  In this work we design a neural network for recognizing emotions in speech,
using the IEMOCAP dataset. Following the latest advances in audio analysis, we
use an architecture involving both convolutional layers, for extracting
high-level features from raw spectrograms, and recurrent ones for aggregating
long-term dependencies. We examine the techniques of data augmentation with
vocal track length perturbation, layer-wise optimizer adjustment, batch
normalization of recurrent layers and obtain highly competitive results of
64.5% for weighted accuracy and 61.7% for unweighted accuracy on four emotions.
</summary>
    <author>
      <name>Caroline Etienne</name>
    </author>
    <author>
      <name>Guillaume Fidanza</name>
    </author>
    <author>
      <name>Andrei Petrovskii</name>
    </author>
    <author>
      <name>Laurence Devillers</name>
    </author>
    <author>
      <name>Benoit Schmauch</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.21437/SMM.2018-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.21437/SMM.2018-5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop on Speech, Music and Mind 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.05630v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05630v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08370v1</id>
    <updated>2018-02-23T02:45:50Z</updated>
    <published>2018-02-23T02:45:50Z</published>
    <title>Do WaveNets Dream of Acoustic Waves?</title>
    <summary>  Various sources have reported the WaveNet deep learning architecture being
able to generate high-quality speech, but to our knowledge there haven't been
studies on the interpretation or visualization of trained WaveNets. This study
investigates the possibility that WaveNet understands speech by unsupervisedly
learning an acoustically meaningful latent representation of the speech signals
in its receptive field; we also attempt to interpret the mechanism by which the
feature extraction is performed. Suggested by singular value decomposition and
linear regression analysis on the activations and known acoustic features (e.g.
F0), the key findings are (1) activations in the higher layers are highly
correlated with spectral features; (2) WaveNet explicitly performs pitch
extraction despite being trained to directly predict the next audio sample and
(3) for the said feature analysis to take place, the latent signal
representation is converted back and forth between baseband and wideband
components.
</summary>
    <author>
      <name>Kanru Hua</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages with 7 figures; submitted to Interspeech 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.08370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.09697v1</id>
    <updated>2018-02-27T03:08:55Z</updated>
    <published>2018-02-27T03:08:55Z</published>
    <title>Convolutional Neural Network Achieves Human-level Accuracy in Music
  Genre Classification</title>
    <summary>  Music genre classification is one example of content-based analysis of music
signals. Traditionally, human-engineered features were used to automatize this
task and 61% accuracy has been achieved in the 10-genre classification.
However, it's still below the 70% accuracy that humans could achieve in the
same task. Here, we propose a new method that combines knowledge of human
perception study in music genre classification and the neurophysiology of the
auditory system. The method works by training a simple convolutional neural
network (CNN) to classify a short segment of the music signal. Then, the genre
of a music is determined by splitting it into short segments and then combining
CNN's predictions from all short segments. After training, this method achieves
human-level (70%) accuracy and the filters learned in the CNN resemble the
spectrotemporal receptive field (STRF) in the auditory system.
</summary>
    <author>
      <name>Mingwen Dong</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">https://ccneuro.org/2018/proceedings/1153.pdf</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.09697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.09697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.10058v1</id>
    <updated>2018-02-27T18:29:14Z</updated>
    <published>2018-02-27T18:29:14Z</published>
    <title>Effect of Transducer Positioning in Active Noise Control</title>
    <summary>  Research in traditional Active Noise Control(ANC) often abstracts acoustic
channels with band-limited filter coefficients. This is a limitation in
exploring structural and positional aspects of ANC. As a solution to this, we
propose the use of room acoustic models in ANC research. As a use case, we
demonstrate anti-noise source position optimization using room acoustics models
in achieving better noise control. Using numerical simulations, we show that
level of cancellation can be improved up to 7.34 dB. All the codes and results
are available in the Github repository https://github.com/cksajil/ancram in the
spirit of reproducible research.
</summary>
    <author>
      <name>C. K. Sajil</name>
    </author>
    <author>
      <name>C. L. Biji</name>
    </author>
    <author>
      <name>S. Nair Achuthsankar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 11 figures. To appear in the Proceedings of the 5th
  International Conference on Signal Processing and Integrated Networks(SPIN
  2018), 22-23 February 2018, Delhi, India</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.10058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.10058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.10162v1</id>
    <updated>2018-02-27T21:03:37Z</updated>
    <published>2018-02-27T21:03:37Z</published>
    <title>Interplay between musical practices and tuning in the marimba de chonta
  music</title>
    <summary>  In the Pacific Coast of Colombia there is a type of marimba called marimba de
chonta, which provides the melodic and harmonic contour for traditional music
with characteristic chants and dances. The tunings of this marimba are based on
the voice of female singers and allows musical practices, as a transposition
that preserves relative distances between bars. Here we show that traditional
tunings are consistent with isotonic scales, and that they have changed in the
last three decades due to the influence of Western music. Specifically, low
octaves have changed into just octaves. Additionally, consonance properties of
this instrument include the occurrence of a broad minimum of dissonance that is
used in the musical practices, while the narrow local peaks of dissonance are
avoided. We found that the main reason for this is the occurrence of
uncertainties in the tunings with respect to the mathematical successions of
isotonic scales. We conclude that in this music the emergence of tunings and
musical practices cannot be considered as separate issues. Consonance, timbre,
and musical practices are entangled.
</summary>
    <author>
      <name>Jorge Useche</name>
    </author>
    <author>
      <name>Rafael Hurtado</name>
    </author>
    <author>
      <name>Federico Demmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Total number of pages: 52, main manuscript: 18 pages, supplemental
  material: 34 pages, the main manuscript contains 6 tables and 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.10162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.10162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91Cxx, 91Dxx, 91Exx, 91Fxx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00187v1</id>
    <updated>2018-03-01T03:12:15Z</updated>
    <published>2018-03-01T03:12:15Z</published>
    <title>Mode Domain Spatial Active Noise Control Using Sparse Signal
  Representation</title>
    <summary>  Active noise control (ANC) over a sizeable space requires a large number of
reference and error microphones to satisfy the spatial Nyquist sampling
criterion, which limits the feasibility of practical realization of such
systems. This paper proposes a mode-domain feedforward ANC method to attenuate
the noise field over a large space while reducing the number of microphones
required. We adopt a sparse reference signal representation to precisely
calculate the reference mode coefficients. The proposed system consists of
circular reference and error microphone arrays, which capture the reference
noise signal and residual error signal, respectively, and a circular
loudspeaker array to drive the anti-noise signal. Experimental results indicate
that above the spatial Nyquist frequency,our proposed method can perform well
compared to a conventional methods. Moreover, the proposed method can even
reduce the number of reference microphones while achieving better noise
attenuation.
</summary>
    <author>
      <name>Yu Maeno</name>
    </author>
    <author>
      <name>Yuki Mitsufuji</name>
    </author>
    <author>
      <name>Thushara D. Abhayapala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear at ICASSP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.00187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00702v1</id>
    <updated>2018-03-02T03:47:47Z</updated>
    <published>2018-03-02T03:47:47Z</published>
    <title>Raw Multi-Channel Audio Source Separation using Multi-Resolution
  Convolutional Auto-Encoders</title>
    <summary>  Supervised multi-channel audio source separation requires extracting useful
spectral, temporal, and spatial features from the mixed signals. The success of
many existing systems is therefore largely dependent on the choice of features
used for training. In this work, we introduce a novel multi-channel,
multi-resolution convolutional auto-encoder neural network that works on raw
time-domain signals to determine appropriate multi-resolution features for
separating the singing-voice from stereo music. Our experimental results show
that the proposed method can achieve multi-channel audio source separation
without the need for hand-crafted features or any pre- or post-processing.
</summary>
    <author>
      <name>Emad M. Grais</name>
    </author>
    <author>
      <name>Dominic Ward</name>
    </author>
    <author>
      <name>Mark D. Plumbley</name>
    </author>
    <link href="http://arxiv.org/abs/1803.00702v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00702v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01, 68T10, 68T45, 62H25" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; I.5; I.2.6; I.4.3; I.4; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.01107v1</id>
    <updated>2018-03-03T05:12:06Z</updated>
    <published>2018-03-03T05:12:06Z</published>
    <title>Audio-only Bird Species Automated Identification Method with Limited
  Training Data Based on Multi-Channel Deep Convolutional Neural Networks</title>
    <summary>  Based on the transfer learning, we design a bird species identification model
that uses the VGG-16 model (pretrained on ImageNet) for feature extraction,
then a classifier consisting of two fully-connected hidden layers and a Softmax
layer is attached. We compare the performance of the proposed model with the
original VGG16 model. The results show that the former has higher train
efficiency, but lower mean average precisions(MAP). To improve the MAP of the
proposed model, we investigate the result fusion mode to form multi-channel
identification model, the best MAP reaches 0.9998. The number of model
parameters is 13110, which is only 0.0082% of the VGG16 model. Also, the size
demand of sample is decreased.
</summary>
    <author>
      <name>Jiang-jian Xie</name>
    </author>
    <author>
      <name>Chang-qing Ding</name>
    </author>
    <author>
      <name>Wen-bin Li</name>
    </author>
    <author>
      <name>Cheng-hao Cai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages,11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.01107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.01107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.04652v1</id>
    <updated>2018-03-13T06:39:49Z</updated>
    <published>2018-03-13T06:39:49Z</published>
    <title>Music Genre Classification Using Spectral Analysis and Sparse
  Representation of the Signals</title>
    <summary>  In this paper, we proposed a robust music genre classification method based
on a sparse FFT based feature extraction method which extracted with
discriminating power of spectral analysis of non-stationary audio signals, and
the capability of sparse representation based classifiers. Feature extraction
method combines two sets of features namely short-term features (extracted from
windowed signals) and long-term features (extracted from combination of
extracted short-time features). Experimental results demonstrate that the
proposed feature extraction method leads to a sparse representation of audio
signals. As a result, a significant reduction in the dimensionality of the
signals is achieved. The extracted features are then fed into a sparse
representation based classifier (SRC). Our experimental results on the GTZAN
database demonstrate that the proposed method outperforms the other state of
the art SRC approaches. Moreover, the computational efficiency of the proposed
method is better than that of the other Compressive Sampling (CS)-based
classifiers.
</summary>
    <author>
      <name>Mehdi Banitalebi-Dehkordi</name>
    </author>
    <author>
      <name>Amin Banitalebi-Dehkordi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Signal Processing Systems, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.04652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.05058v1</id>
    <updated>2018-03-13T21:40:59Z</updated>
    <published>2018-03-13T21:40:59Z</published>
    <title>Investigating the Effect of Music and Lyrics on Spoken-Word Recognition</title>
    <summary>  Background music in social interaction settings can hinder conversation. Yet,
little is known of how specific properties of music impact speech processing.
This paper addresses this knowledge gap by investigating 1) whether the masking
effect of background music with lyrics is larger than that of music without
lyrics, and 2) whether the masking effect is larger for more complex music. To
answer these questions, a word identification experiment was run in which Dutch
participants listened to Dutch CVC words embedded in stretches of background
music in two conditions, with and without lyrics, and at three SNRs. Three
songs were used of different genres and complexities. Music stretches with and
without lyrics were sampled from the same song in order to control for factors
beyond the presence of lyrics. The results showed a clear negative impact of
the presence of lyrics in background music on spoken-word recognition. This
impact is independent of complexity. The results suggest that social spaces
(e.g., restaurants, caf\'es and bars) should make careful choices of music to
promote conversation, and open a path for future work.
</summary>
    <author>
      <name>Odette Scharenborg</name>
    </author>
    <author>
      <name>Martha Larson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary study</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.05058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.05058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.08629v2</id>
    <updated>2018-05-27T17:03:09Z</updated>
    <published>2018-03-23T01:26:39Z</published>
    <title>Generalization Challenges for Neural Architectures in Audio Source
  Separation</title>
    <summary>  Recent work has shown that recurrent neural networks can be trained to
separate individual speakers in a sound mixture with high fidelity. Here we
explore convolutional neural network models as an alternative and show that
they achieve state-of-the-art results with an order of magnitude fewer
parameters. We also characterize and compare the robustness and ability of
these different approaches to generalize under three different test conditions:
longer time sequences, the addition of intermittent noise, and different
datasets not seen during training. For the last condition, we create a new
dataset, RealTalkLibri, to test source separation in real-world environments.
We show that the acoustics of the environment have significant impact on the
structure of the waveform and the overall performance of neural network models,
with the convolutional model showing superior ability to generalize to new
environments. The code for our study is available at
https://github.com/ShariqM/source_separation.
</summary>
    <author>
      <name>Shariq Mobin</name>
    </author>
    <author>
      <name>Brian Cheung</name>
    </author>
    <author>
      <name>Bruno Olshausen</name>
    </author>
    <link href="http://arxiv.org/abs/1803.08629v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.08629v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.09059v1</id>
    <updated>2018-03-24T05:49:07Z</updated>
    <published>2018-03-24T05:49:07Z</published>
    <title>MTGAN: Speaker Verification through Multitasking Triplet Generative
  Adversarial Networks</title>
    <summary>  In this paper, we propose an enhanced triplet method that improves the
encoding process of embeddings by jointly utilizing generative adversarial
mechanism and multitasking optimization. We extend our triplet encoder with
Generative Adversarial Networks (GANs) and softmax loss function. GAN is
introduced for increasing the generality and diversity of samples, while
softmax is for reinforcing features about speakers. For simplification, we term
our method Multitasking Triplet Generative Adversarial Networks (MTGAN).
Experiment on short utterances demonstrates that MTGAN reduces the verification
equal error rate (EER) by 67% (relatively) and 32% (relatively) over
conventional i-vector method and state-of-the-art triplet loss method
respectively. This effectively indicates that MTGAN outperforms triplet methods
in the aspect of expressing the high-level feature of speaker information.
</summary>
    <author>
      <name>Wenhao Ding</name>
    </author>
    <author>
      <name>Liang He</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to Interspeech 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.09059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.09816v1</id>
    <updated>2018-03-26T19:56:21Z</updated>
    <published>2018-03-26T19:56:21Z</published>
    <title>Spectral feature mapping with mimic loss for robust speech recognition</title>
    <summary>  For the task of speech enhancement, local learning objectives are agnostic to
phonetic structures helpful for speech recognition. We propose to add a global
criterion to ensure de-noised speech is useful for downstream tasks like ASR.
We first train a spectral classifier on clean speech to predict senone labels.
Then, the spectral classifier is joined with our speech enhancer as a noisy
speech recognizer. This model is taught to imitate the output of the spectral
classifier alone on clean speech. This \textit{mimic loss} is combined with the
traditional local criterion to train the speech enhancer to produce de-noised
speech. Feeding the de-noised speech to an off-the-shelf Kaldi training recipe
for the CHiME-2 corpus shows significant improvements in WER.
</summary>
    <author>
      <name>Deblin Bagchi</name>
    </author>
    <author>
      <name>Peter Plantinga</name>
    </author>
    <author>
      <name>Adam Stiff</name>
    </author>
    <author>
      <name>Eric Fosler-Lussier</name>
    </author>
    <link href="http://arxiv.org/abs/1803.09816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.10146v3</id>
    <updated>2018-10-25T07:11:54Z</updated>
    <published>2018-03-27T15:39:46Z</published>
    <title>Empirical Evaluation of Speaker Adaptation on DNN based Acoustic Model</title>
    <summary>  Speaker adaptation aims to estimate a speaker specific acoustic model from a
speaker independent one to minimize the mismatch between the training and
testing conditions arisen from speaker variabilities. A variety of neural
network adaptation methods have been proposed since deep learning models have
become the main stream. But there still lacks an experimental comparison
between different methods, especially when DNN-based acoustic models have been
advanced greatly. In this paper, we aim to close this gap by providing an
empirical evaluation of three typical speaker adaptation methods: LIN, LHUC and
KLD. Adaptation experiments, with different size of adaptation data, are
conducted on a strong TDNN-LSTM acoustic model. More challengingly, here, the
source and target we are concerned with are standard Mandarin speaker model and
accented Mandarin speaker model. We compare the performances of different
methods and their combinations. Speaker adaptation performance is also examined
by speaker's accent degree.
</summary>
    <author>
      <name>Ke Wang</name>
    </author>
    <author>
      <name>Junbo Zhang</name>
    </author>
    <author>
      <name>Yujun Wang</name>
    </author>
    <author>
      <name>Lei Xie</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.21437/Interspeech.2018-1897</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.21437/Interspeech.2018-1897" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Interspeech 2018</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Interspeech, 2018, pp. 2429-2433</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.10146v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.10146v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.10219v1</id>
    <updated>2018-03-25T04:46:17Z</updated>
    <published>2018-03-25T04:46:17Z</published>
    <title>Learning Environmental Sounds with Multi-scale Convolutional Neural
  Network</title>
    <summary>  Deep learning has dramatically improved the performance of sounds
recognition. However, learning acoustic models directly from the raw waveform
is still challenging. Current waveform-based models generally use time-domain
convolutional layers to extract features. The features extracted by single size
filters are insufficient for building discriminative representation of audios.
In this paper, we propose multi-scale convolution operation, which can get
better audio representation by improving the frequency resolution and learning
filters cross all frequency area. For leveraging the waveform-based features
and spectrogram-based features in a single model, we introduce two-phase method
to fuse the different features. Finally, we propose a novel end-to-end network
called WaveMsNet based on the multi-scale convolution operation and two-phase
method. On the environmental sounds classification datasets ESC-10 and ESC-50,
the classification accuracies of our WaveMsNet achieve 93.75% and 79.10%
respectively, which improve significantly from the previous methods.
</summary>
    <author>
      <name>Boqing Zhu</name>
    </author>
    <author>
      <name>Changjian Wang</name>
    </author>
    <author>
      <name>Feng Liu</name>
    </author>
    <author>
      <name>Jin Lei</name>
    </author>
    <author>
      <name>Zengquan Lu</name>
    </author>
    <author>
      <name>Yuxing Peng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by IJCNN 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.10219v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.10219v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.10609v1</id>
    <updated>2018-03-28T13:51:09Z</updated>
    <published>2018-03-28T13:51:09Z</published>
    <title>The fifth 'CHiME' Speech Separation and Recognition Challenge: Dataset,
  task and baselines</title>
    <summary>  The CHiME challenge series aims to advance robust automatic speech
recognition (ASR) technology by promoting research at the interface of speech
and language processing, signal processing , and machine learning. This paper
introduces the 5th CHiME Challenge, which considers the task of distant
multi-microphone conversational ASR in real home environments. Speech material
was elicited using a dinner party scenario with efforts taken to capture data
that is representative of natural conversational speech and recorded by 6
Kinect microphone arrays and 4 binaural microphone pairs. The challenge
features a single-array track and a multiple-array track and, for each track,
distinct rankings will be produced for systems focusing on robustness with
respect to distant-microphone capture vs. systems attempting to address all
aspects of the task including conversational language modeling. We discuss the
rationale for the challenge and provide a detailed description of the data
collection procedure, the task, and the baseline systems for array
synchronization, speech enhancement, and conventional and end-to-end ASR.
</summary>
    <author>
      <name>Jon Barker</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CLSP</arxiv:affiliation>
    </author>
    <author>
      <name>Shinji Watanabe</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CLSP</arxiv:affiliation>
    </author>
    <author>
      <name>Emmanuel Vincent</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MULTISPEECH</arxiv:affiliation>
    </author>
    <author>
      <name>Jan Trmal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CLSP</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1803.10609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.10609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.10916v1</id>
    <updated>2018-03-29T03:32:59Z</updated>
    <published>2018-03-29T03:32:59Z</published>
    <title>Attention-based End-to-End Models for Small-Footprint Keyword Spotting</title>
    <summary>  In this paper, we propose an attention-based end-to-end neural approach for
small-footprint keyword spotting (KWS), which aims to simplify the pipelines of
building a production-quality KWS system. Our model consists of an encoder and
an attention mechanism. The encoder transforms the input signal into a high
level representation using RNNs. Then the attention mechanism weights the
encoder features and generates a fixed-length vector. Finally, by linear
transformation and softmax function, the vector becomes a score used for
keyword detection. We also evaluate the performance of different encoder
architectures, including LSTM, GRU and CRNN. Experiments on real-world wake-up
data show that our approach outperforms the recent Deep KWS approach by a large
margin and the best performance is achieved by CRNN. To be more specific, with
~84K parameters, our attention-based model achieves 1.02% false rejection rate
(FRR) at 1.0 false alarm (FA) per hour.
</summary>
    <author>
      <name>Changhao Shan</name>
    </author>
    <author>
      <name>Junbo Zhang</name>
    </author>
    <author>
      <name>Yujun Wang</name>
    </author>
    <author>
      <name>Lei Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">attention-based model, end-to-end keyword spotting, convolutional
  neural networks, recurrent neural networks</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.10916v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.10916v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.00047v2</id>
    <updated>2018-06-07T06:37:44Z</updated>
    <published>2018-03-30T20:17:31Z</published>
    <title>Conditional End-to-End Audio Transforms</title>
    <summary>  We present an end-to-end method for transforming audio from one style to
another. For the case of speech, by conditioning on speaker identities, we can
train a single model to transform words spoken by multiple people into multiple
target voices. For the case of music, we can specify musical instruments and
achieve the same result. Architecturally, our method is a fully-differentiable
sequence-to-sequence model based on convolutional and hierarchical recurrent
neural networks. It is designed to capture long-term acoustic dependencies,
requires minimal post-processing, and produces realistic audio transforms.
Ablation studies confirm that our model can separate speaker and instrument
properties from acoustic content at different receptive fields. Empirically,
our method achieves competitive performance on community-standard datasets.
</summary>
    <author>
      <name>Albert Haque</name>
    </author>
    <author>
      <name>Michelle Guo</name>
    </author>
    <author>
      <name>Prateek Verma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Interspeech 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.00047v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.00047v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.00155v1</id>
    <updated>2018-03-31T10:49:43Z</updated>
    <published>2018-03-31T10:49:43Z</published>
    <title>Speaker Verification in Emotional Talking Environments based on
  Three-Stage Framework</title>
    <summary>  This work is dedicated to introducing, executing, and assessing a three-stage
speaker verification framework to enhance the degraded speaker verification
performance in emotional talking environments. Our framework is comprised of
three cascaded stages: gender identification stage followed by an emotion
identification stage followed by a speaker verification stage. The proposed
framework has been assessed on two distinct and independent emotional speech
datasets: our collected dataset and Emotional Prosody Speech and Transcripts
dataset. Our results demonstrate that speaker verification based on both gender
cues and emotion cues is superior to each of speaker verification based on
gender cues only, emotion cues only, and neither gender cues nor emotion cues.
The achieved average speaker verification performance based on the suggested
methodology is very similar to that attained in subjective assessment by human
listeners.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.00155v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.00155v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.00981v1</id>
    <updated>2018-03-31T10:46:38Z</updated>
    <published>2018-03-31T10:46:38Z</published>
    <title>Emirati-Accented Speaker Identification in each of Neutral and Shouted
  Talking Environments</title>
    <summary>  This work is devoted to capturing Emirati-accented speech database (Arabic
United Arab Emirates database) in each of neutral and shouted talking
environments in order to study and enhance text-independent Emirati-accented
speaker identification performance in shouted environment based on each of
First-Order Circular Suprasegmental Hidden Markov Models (CSPHMM1s),
Second-Order Circular Suprasegmental Hidden Markov Models (CSPHMM2s), and
Third-Order Circular Suprasegmental Hidden Markov Models (CSPHMM3s) as
classifiers. In this research, our database was collected from fifty Emirati
native speakers (twenty five per gender) uttering eight common Emirati
sentences in each of neutral and shouted talking environments. The extracted
features of our collected database are called Mel-Frequency Cepstral
Coefficients (MFCCs). Our results show that average Emirati-accented speaker
identification performance in neutral environment is 94.0%, 95.2%, and 95.9%
based on CSPHMM1s, CSPHMM2s, and CSPHMM3s, respectively. On the other hand, the
average performance in shouted environment is 51.3%, 55.5%, and 59.3% based,
respectively, on CSPHMM1s, CSPHMM2s, and CSPHMM3s. The achieved average speaker
identification performance in shouted environment based on CSPHMM3s is very
similar to that obtained in subjective assessment by human listeners.
</summary>
    <author>
      <name>Ismail Shahin</name>
    </author>
    <author>
      <name>Ali Bou Nassif</name>
    </author>
    <author>
      <name>Mohammed Bahutair</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10772-018-9502-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10772-018-9502-0" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 3 figures. arXiv admin note: text overlap with
  arXiv:1707.00686</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.00981v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.00981v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01146v1</id>
    <updated>2018-04-03T19:53:08Z</updated>
    <published>2018-04-03T19:53:08Z</published>
    <title>Comparing the Max and Noisy-Or Pooling Functions in Multiple Instance
  Learning for Weakly Supervised Sequence Learning Tasks</title>
    <summary>  Many sequence learning tasks require the localization of certain events in
sequences. Because it can be expensive to obtain strong labeling that specifies
the starting and ending times of the events, modern systems are often trained
with weak labeling without explicit timing information. Multiple instance
learning (MIL) is a popular framework for learning from weak labeling. In a
common scenario of MIL, it is necessary to choose a pooling function to
aggregate the predictions for the individual steps of the sequences. In this
paper, we compare the "max" and "noisy-or" pooling functions on a speech
recognition task and a sound event detection task. We find that max pooling is
able to localize phonemes and sound events, while noisy-or pooling fails. We
provide a theoretical explanation of the different behavior of the two pooling
functions on sequence learning tasks.
</summary>
    <author>
      <name>Yun Wang</name>
    </author>
    <author>
      <name>Juncheng Li</name>
    </author>
    <author>
      <name>Florian Metze</name>
    </author>
    <link href="http://arxiv.org/abs/1804.01146v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01146v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01149v1</id>
    <updated>2018-04-03T20:04:04Z</updated>
    <published>2018-04-03T20:04:04Z</published>
    <title>Music Genre Classification using Machine Learning Techniques</title>
    <summary>  Categorizing music files according to their genre is a challenging task in
the area of music information retrieval (MIR). In this study, we compare the
performance of two classes of models. The first is a deep learning approach
wherein a CNN model is trained end-to-end, to predict the genre label of an
audio signal, solely using its spectrogram. The second approach utilizes
hand-crafted features, both from the time domain and the frequency domain. We
train four traditional machine learning classifiers with these features and
compare their performance. The features that contribute the most towards this
multi-class classification task are identified. The experiments are conducted
on the Audio set data set and we report an AUC value of 0.894 for an ensemble
classifier which combines the two proposed approaches.
</summary>
    <author>
      <name>Hareesh Bahuleyan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages, 6 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.01149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01212v1</id>
    <updated>2018-04-04T02:19:27Z</updated>
    <published>2018-04-04T02:19:27Z</published>
    <title>Classification of Vehicles Based on Audio Signals using Quadratic
  Discriminant Analysis and High Energy Feature Vectors</title>
    <summary>  The focus of this paper is on classification of different vehicles using
sound emanated from the vehicles. In this paper,quadratic discriminant analysis
classifies audio signals of passing vehicles to bus, car, motor, and truck
categories based on features such as short time energy, average zero cross
rate, and pitch frequency of periodic segments of signals. Simulation results
show that just by considering high energy feature vectors, better
classification accuracy can be achieved due to the correspondence of low energy
regions with noises of the background. To separate these elements, short time
energy and average zero cross rate are used simultaneously.In our method,we
have used a few features which are easy to be calculated in time domain and
enable practical implementation of efficient classifier. Although, the
computation complexity is low, the classification accuracy is comparable with
other classification methods based on long feature vectors reported in
literature for this problem.
</summary>
    <author>
      <name>Ali Dalir</name>
    </author>
    <author>
      <name>Ali Asghar Beheshti</name>
    </author>
    <author>
      <name>Morteza Hoseini Masoom</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijsc.2015.6105</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijsc.2015.6105" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal on Soft Computing (IJSC) Vol.6, No. 1,
  February 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1804.01212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01650v1</id>
    <updated>2018-04-05T01:55:39Z</updated>
    <published>2018-04-05T01:55:39Z</published>
    <title>Jointly Detecting and Separating Singing Voice: A Multi-Task Approach</title>
    <summary>  A main challenge in applying deep learning to music processing is the
availability of training data. One potential solution is Multi-task Learning,
in which the model also learns to solve related auxiliary tasks on additional
datasets to exploit their correlation. While intuitive in principle, it can be
challenging to identify related tasks and construct the model to optimally
share information between tasks. In this paper, we explore vocal activity
detection as an additional task to stabilise and improve the performance of
vocal separation. Further, we identify problematic biases specific to each
dataset that could limit the generalisation capability of separation and
detection models, to which our proposed approach is robust. Experiments show
improved performance in separation as well as vocal detection compared to
single-task baselines. However, we find that the commonly used
Signal-to-Distortion Ratio (SDR) metrics did not capture the improvement on
non-vocal sections, indicating the need for improved evaluation methodologies.
</summary>
    <author>
      <name>Daniel Stoller</name>
    </author>
    <author>
      <name>Sebastian Ewert</name>
    </author>
    <author>
      <name>Simon Dixon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures, accepted for the 14th International Conference
  on Latent Variable Analysis and Signal Separation</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.01650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02325v1</id>
    <updated>2018-04-06T15:33:29Z</updated>
    <published>2018-04-06T15:33:29Z</published>
    <title>Does k Matter? k-NN Hubness Analysis for Kernel Additive Modelling Vocal
  Separation</title>
    <summary>  Kernel Additive Modelling (KAM) is a framework for source separation aiming
to explicitly model inherent properties of sound sources to help with their
identification and separation. KAM separates a given source by applying robust
statistics on the selection of time-frequency bins obtained through a
source-specific kernel, typically the k-NN function. Even though the parameter
k appears to be key for a successful separation, little discussion on its
influence or optimisation can be found in the literature. Here we propose a
novel method, based on graph theory statistics, to automatically optimise $k$
in a vocal separation task. We introduce the k-NN hubness as an indicator to
find a tailored k at a low computational cost. Subsequently, we evaluate our
method in comparison to the common approach to choose k. We further discuss the
influence and importance of this parameter with illuminating results.
</summary>
    <author>
      <name>Delia Fano Yela</name>
    </author>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LVA-ICA 2018 - Feedback always welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.02325v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02325v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04862v2</id>
    <updated>2018-06-14T13:17:27Z</updated>
    <published>2018-04-13T09:51:14Z</published>
    <title>Speaker Embedding Extraction with Phonetic Information</title>
    <summary>  Speaker embeddings achieve promising results on many speaker verification
tasks. Phonetic information, as an important component of speech, is rarely
considered in the extraction of speaker embeddings. In this paper, we introduce
phonetic information to the speaker embedding extraction based on the x-vector
architecture. Two methods using phonetic vectors and multi-task learning are
proposed. On the Fisher dataset, our best system outperforms the original
x-vector approach by 20% in EER, and by 15%, 15% in minDCF08 and minDCF10,
respectively. Experiments conducted on NIST SRE10 further demonstrate the
effectiveness of the proposed methods.
</summary>
    <author>
      <name>Yi Liu</name>
    </author>
    <author>
      <name>Liang He</name>
    </author>
    <author>
      <name>Jia Liu</name>
    </author>
    <author>
      <name>Michael T. Johnson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to Interspeech 2018 (accepted) and open-sourced. Please
  refer to Interspeech for the final version</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04862v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04862v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.05111v2</id>
    <updated>2020-06-28T06:36:07Z</updated>
    <published>2018-04-13T20:37:46Z</published>
    <title>Multi-Sound-Source Localization Using Machine Learning for Small
  Autonomous Unmanned Vehicles with a Self-Rotating Bi-Microphone Array</title>
    <summary>  Abstract While vision-based localization techniques have been widely studied
for small autonomous unmanned vehicles (SAUVs), sound-source localization
capabilities have not been fully enabled for SAUVs. This paper presents two
novel approaches for SAUVs to perform three-dimensional (3D)
multi-sound-sources localization (MSSL) using only the inter-channel time
difference (ICTD) signal generated by a self-rotating bi-microphone array. The
proposed two approaches are based on two machine learning techniques viz.,
Density-Based Spatial Clustering of Applications with Noise (DBSCAN) and Random
Sample Consensus (RANSAC) algorithms, respectively, whose performances are
tested and compared in both simulations and experiments. The results show that
both approaches are capable of correctly identifying the number of sound
sources along with their 3D orientations in a reverberant environment.
</summary>
    <author>
      <name>Deepak Gala</name>
    </author>
    <author>
      <name>Nathan Lindsay</name>
    </author>
    <author>
      <name>Liang Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1804.05111v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05111v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.05306v1</id>
    <updated>2018-04-15T05:50:27Z</updated>
    <published>2018-04-15T05:50:27Z</published>
    <title>Transcribing Lyrics From Commercial Song Audio: The First Step Towards
  Singing Content Processing</title>
    <summary>  Spoken content processing (such as retrieval and browsing) is maturing, but
the singing content is still almost completely left out. Songs are human voice
carrying plenty of semantic information just as speech, and may be considered
as a special type of speech with highly flexible prosody. The various problems
in song audio, for example the significantly changing phone duration over
highly flexible pitch contours, make the recognition of lyrics from song audio
much more difficult. This paper reports an initial attempt towards this goal.
We collected music-removed version of English songs directly from commercial
singing content. The best results were obtained by TDNN-LSTM with data
augmentation with 3-fold speed perturbation plus some special approaches. The
WER achieved (73.90%) was significantly lower than the baseline (96.21%), but
still relatively high.
</summary>
    <author>
      <name>Che-Ping Tsai</name>
    </author>
    <author>
      <name>Yi-Lin Tuan</name>
    </author>
    <author>
      <name>Lin-shan Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as a conference paper at ICASSP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.05306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.05486v1</id>
    <updated>2018-04-16T02:59:25Z</updated>
    <published>2018-04-16T02:59:25Z</published>
    <title>Computing Information Quantity as Similarity Measure for Music
  Classification Task</title>
    <summary>  This paper proposes a novel method that can replace compression-based
dissimilarity measure (CDM) in composer estimation task. The main features of
the proposed method are clarity and scalability. First, since the proposed
method is formalized by the information quantity, reproduction of the result is
easier compared with the CDM method, where the result depends on a particular
compression program. Second, the proposed method has a lower computational
complexity in terms of the number of learning data compared with the CDM
method. The number of correct results was compared with that of the CDM for the
composer estimation task of five composers of 75 piano musical scores. The
proposed method performed better than the CDM method that uses the file size
compressed by a particular program.
</summary>
    <author>
      <name>Ayaka Takamoto</name>
    </author>
    <author>
      <name>Mitsuo Yoshida</name>
    </author>
    <author>
      <name>Kyoji Umemura</name>
    </author>
    <author>
      <name>Yuko Ichikawa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICAICTA.2017.8090990</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICAICTA.2017.8090990" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The 2017 International Conference On Advanced Informatics: Concepts,
  Theory And Application (ICAICTA2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.05486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06775v2</id>
    <updated>2018-08-23T10:33:44Z</updated>
    <published>2018-04-18T15:02:10Z</published>
    <title>Unspeech: Unsupervised Speech Context Embeddings</title>
    <summary>  We introduce "Unspeech" embeddings, which are based on unsupervised learning
of context feature representations for spoken language. The embeddings were
trained on up to 9500 hours of crawled English speech data without
transcriptions or speaker information, by using a straightforward learning
objective based on context and non-context discrimination with negative
sampling. We use a Siamese convolutional neural network architecture to train
Unspeech embeddings and evaluate them on speaker comparison, utterance
clustering and as a context feature in TDNN-HMM acoustic models trained on
TED-LIUM, comparing it to i-vector baselines. Particularly decoding
out-of-domain speech data from the recently released Common Voice corpus shows
consistent WER reductions. We release our source code and pre-trained Unspeech
models under a permissive open source license.
</summary>
    <author>
      <name>Benjamin Milde</name>
    </author>
    <author>
      <name>Chris Biemann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at Interspeech 2018, Hyderabad, India. This version matches
  the final version submitted to the conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.06775v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06775v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.08167v2</id>
    <updated>2018-04-28T21:31:48Z</updated>
    <published>2018-04-22T20:48:29Z</published>
    <title>Tempo-Invariant Processing of Rhythm with Convolutional Neural Networks</title>
    <summary>  Rhythm patterns can be performed with a wide variation of tempi. This
presents a challenge for many music information retrieval (MIR) systems;
ideally, perceptually similar rhythms should be represented and processed
similarly, regardless of the specific tempo at which they were performed.
Several recent systems for tempo estimation, beat tracking, and downbeat
tracking have therefore sought to process rhythm in a tempo-invariant way,
often by sampling input vectors according to a precomputed pulse level. This
paper describes how a log-frequency representation of rhythm-related
activations instead can promote tempo invariance when processed with
convolutional neural networks. The strategy incorporates invariance at a
fundamental level and can be useful for most tasks related to rhythm
processing. Different methods are described, relying on magnitude, phase
relationships of different rhythm channels, as well as raw phase information.
Several variations are explored to provide direction for future
implementations.
</summary>
    <author>
      <name>Anders Elowsson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Included in doctoral dissertation "Modeling Music: Studies of Music
  Transcription, Music Perception and Music Production". 26 pages, G5 format.
  Feedback always welcome</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.08167v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.08167v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.10080v1</id>
    <updated>2018-04-26T14:22:01Z</updated>
    <published>2018-04-26T14:22:01Z</published>
    <title>On deep speaker embeddings for text-independent speaker recognition</title>
    <summary>  We investigate deep neural network performance in the textindependent speaker
recognition task. We demonstrate that using angular softmax activation at the
last classification layer of a classification neural network instead of a
simple softmax activation allows to train a more generalized discriminative
speaker embedding extractor. Cosine similarity is an effective metric for
speaker verification in this embedding space. We also address the problem of
choosing an architecture for the extractor. We found that deep networks with
residual frame level connections outperform wide but relatively shallow
architectures. This paper also proposes several improvements for previous
DNN-based extractor systems to increase the speaker recognition accuracy. We
show that the discriminatively trained similarity metric learning approach
outperforms the standard LDA-PLDA method as an embedding backend. The results
obtained on Speakers in the Wild and NIST SRE 2016 evaluation sets demonstrate
robustness of the proposed systems when dealing with close to real-life
conditions.
</summary>
    <author>
      <name>Sergey Novoselov</name>
    </author>
    <author>
      <name>Andrey Shulipa</name>
    </author>
    <author>
      <name>Ivan Kremnev</name>
    </author>
    <author>
      <name>Alexandr Kozlov</name>
    </author>
    <author>
      <name>Vadim Shchemelinin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Odyssey 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.10080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.11046v4</id>
    <updated>2018-11-27T01:51:13Z</updated>
    <published>2018-04-30T04:41:33Z</published>
    <title>Automatic Documentation of ICD Codes with Far-Field Speech Recognition</title>
    <summary>  Documentation errors increase healthcare costs and cause unnecessary patient
deaths. As the standard language for diagnoses and billing, ICD codes serve as
the foundation for medical documentation worldwide. Despite the prevalence of
electronic medical records, hospitals still witness high levels of ICD
miscoding. In this paper, we propose to automatically document ICD codes with
far-field speech recognition. Far-field speech occurs when the microphone is
located several meters from the source, as is common with smart homes and
security systems. Our method combines acoustic signal processing with recurrent
neural networks to recognize and document ICD codes in real time. To evaluate
our model, we collected a far-field speech dataset of ICD-10 codes and found
our model to achieve 87% accuracy with a BLEU score of 85%. By sampling from an
unsupervised medical language model, our method is able to outperform existing
methods. Overall, this work shows the potential of automatic speech recognition
to provide efficient, accurate, and cost-effective healthcare documentation.
</summary>
    <author>
      <name>Albert Haque</name>
    </author>
    <author>
      <name>Corinna Fukushima</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning for Health (ML4H) Workshop at NeurIPS 2018
  arXiv:1811.07216</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.11046v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.11046v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.11120v2</id>
    <updated>2018-05-02T22:28:27Z</updated>
    <published>2018-04-30T10:56:57Z</published>
    <title>WAAW Csound</title>
    <summary>  This paper describes Web Assembly Audio Worklet (WAAW) Csound, one of the
implementations of WebAudio Csound. We begin by introducing the background to
this current implementation, stemming from the two first ports of Csound to the
web platform using Native Clients and asm.js. The technology of Web Assembly is
then introduced and discussed in its more relevant aspects. The AudioWorklet
interface of Web Audio API is explored, together with its use in WAAW Csound.
We complement this discussion by considering the overarching question of
support for multiple platforms, which implement different versions of Web
Audio. Some initial examples of the system are presented to illustrate various
potential applications. Finally, we complement the paper by discussing current
issues that are fundamental for this project and others that rely on the
development of a robust support for WASM-based audio computing.
</summary>
    <author>
      <name>Steven Yi</name>
    </author>
    <author>
      <name>Victor Lazzarini</name>
    </author>
    <author>
      <name>Edward Costello</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.11120v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.11120v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00927v1</id>
    <updated>2018-06-04T02:10:48Z</updated>
    <published>2018-06-04T02:10:48Z</published>
    <title>Voice Imitating Text-to-Speech Neural Networks</title>
    <summary>  We propose a neural text-to-speech (TTS) model that can imitate a new
speaker's voice using only a small amount of speech sample. We demonstrate
voice imitation using only a 6-seconds long speech sample without any other
information such as transcripts. Our model also enables voice imitation
instantly without additional training of the model. We implemented the voice
imitating TTS model by combining a speaker embedder network with a
state-of-the-art TTS model, Tacotron. The speaker embedder network takes a new
speaker's speech sample and returns a speaker embedding. The speaker embedding
with a target sentence are fed to Tacotron, and speech is generated with the
new speaker's voice. We show that the speaker embeddings extracted by the
speaker embedder network can represent the latent structure in different
voices. The generated speech samples from our model have comparable voice
quality to the ones from existing multi-speaker TTS models.
</summary>
    <author>
      <name>Younggun Lee</name>
    </author>
    <author>
      <name>Taesu Kim</name>
    </author>
    <author>
      <name>Soo-Young Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1806.00927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00984v1</id>
    <updated>2018-06-04T06:58:45Z</updated>
    <published>2018-06-04T06:58:45Z</published>
    <title>DNN-HMM based Speaker Adaptive Emotion Recognition using Proposed Epoch
  and MFCC Features</title>
    <summary>  Speech is produced when time varying vocal tract system is excited with time
varying excitation source. Therefore, the information present in a speech such
as message, emotion, language, speaker is due to the combined effect of both
excitation source and vocal tract system. However, there is very less
utilization of excitation source features to recognize emotion. In our earlier
work, we have proposed a novel method to extract glottal closure instants
(GCIs) known as epochs. In this paper, we have explored epoch features namely
instantaneous pitch, phase and strength of epochs for discriminating emotions.
We have combined the excitation source features and the well known
Male-frequency cepstral coefficient (MFCC) features to develop an emotion
recognition system with improved performance. DNN-HMM speaker adaptive models
have been developed using MFCC, epoch and combined features. IEMOCAP emotional
database has been used to evaluate the models. The average accuracy for emotion
recognition system when using MFCC and epoch features separately is 59.25% and
54.52% respectively. The recognition performance improves to 64.2% when MFCC
and epoch features are combined.
</summary>
    <author>
      <name>Md. Shah Fahad</name>
    </author>
    <author>
      <name>Jainath Yadav</name>
    </author>
    <author>
      <name>Gyadhar Pradhan</name>
    </author>
    <author>
      <name>Akshay Deepak</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00034-020-01486-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00034-020-01486-8" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Circuits, Systems, and Signal Processing 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.00984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01180v1</id>
    <updated>2018-06-04T16:25:37Z</updated>
    <published>2018-06-04T16:25:37Z</published>
    <title>Revisiting Singing Voice Detection: a Quantitative Review and the Future
  Outlook</title>
    <summary>  Since the vocal component plays a crucial role in popular music, singing
voice detection has been an active research topic in music information
retrieval. Although several proposed algorithms have shown high performances,
we argue that there still is a room to improve to build a more robust singing
voice detection system. In order to identify the area of improvement, we first
perform an error analysis on three recent singing voice detection systems.
Based on the analysis, we design novel methods to test the systems on multiple
sets of internally curated and generated data to further examine the pitfalls,
which are not clearly revealed with the current datasets. From the experiment
results, we also propose several directions towards building a more robust
singing voice detector.
</summary>
    <author>
      <name>Kyungyun Lee</name>
    </author>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>Juhan Nam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the 19th International Society of Music Information
  Retrieval (ISMIR) Conference, Paris, France, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01506v2</id>
    <updated>2019-05-02T09:10:24Z</updated>
    <published>2018-06-05T06:00:46Z</published>
    <title>Attention Based Fully Convolutional Network for Speech Emotion
  Recognition</title>
    <summary>  Speech emotion recognition is a challenging task for three main reasons: 1)
human emotion is abstract, which means it is hard to distinguish; 2) in
general, human emotion can only be detected in some specific moments during a
long utterance; 3) speech data with emotional labeling is usually limited. In
this paper, we present a novel attention based fully convolutional network for
speech emotion recognition. We employ fully convolutional network as it is able
to handle variable-length speech, free of the demand of segmentation to keep
critical information not lost. The proposed attention mechanism can make our
model be aware of which time-frequency region of speech spectrogram is more
emotion-relevant. Considering limited data, the transfer learning is also
adapted to improve the accuracy. Especially, it's interesting to observe
obvious improvement obtained with natural scene image based pre-trained model.
Validated on the publicly available IEMOCAP corpus, the proposed model
outperformed the state-of-the-art methods with a weighted accuracy of 70.4% and
an unweighted accuracy of 63.9% respectively.
</summary>
    <author>
      <name>Yuanyuan Zhang</name>
    </author>
    <author>
      <name>Jun Du</name>
    </author>
    <author>
      <name>Zirui Wang</name>
    </author>
    <author>
      <name>Jianshu Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.23919/APSIPA.2018.8659587</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.23919/APSIPA.2018.8659587" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2018 Asia-Pacific Signal and Information Processing Association
  Annual Summit and Conference (APSIPA ASC)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.01506v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01506v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01665v1</id>
    <updated>2018-06-05T12:54:19Z</updated>
    <published>2018-06-05T12:54:19Z</published>
    <title>Singing voice phoneme segmentation by hierarchically inferring syllable
  and phoneme onset positions</title>
    <summary>  In this paper, we tackle the singing voice phoneme segmentation problem in
the singing training scenario by using language-independent information --
onset and prior coarse duration. We propose a two-step method. In the first
step, we jointly calculate the syllable and phoneme onset detection functions
(ODFs) using a convolutional neural network (CNN). In the second step, the
syllable and phoneme boundaries and labels are inferred hierarchically by using
a duration-informed hidden Markov model (HMM). To achieve the inference, we
incorporate the a priori duration model as the transition probabilities and the
ODFs as the emission probabilities into the HMM. The proposed method is
designed in a language-independent way such that no phoneme class labels are
used. For the model training and algorithm evaluation, we collect a new jingju
(also known as Beijing or Peking opera) solo singing voice dataset and manually
annotate the boundaries and labels at phrase, syllable and phoneme levels. The
dataset is publicly available. The proposed method is compared with a baseline
method based on hidden semi-Markov model (HSMM) forced alignment. The
evaluation results show that the proposed method outperforms the baseline by a
large margin regarding both segmentation and onset detection tasks.
</summary>
    <author>
      <name>Rong Gong</name>
    </author>
    <author>
      <name>Xavier Serra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Interspeech 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.02169v2</id>
    <updated>2018-06-29T06:19:34Z</updated>
    <published>2018-06-06T13:24:23Z</published>
    <title>StarGAN-VC: Non-parallel many-to-many voice conversion with star
  generative adversarial networks</title>
    <summary>  This paper proposes a method that allows non-parallel many-to-many voice
conversion (VC) by using a variant of a generative adversarial network (GAN)
called StarGAN. Our method, which we call StarGAN-VC, is noteworthy in that it
(1) requires no parallel utterances, transcriptions, or time alignment
procedures for speech generator training, (2) simultaneously learns
many-to-many mappings across different attribute domains using a single
generator network, (3) is able to generate converted speech signals quickly
enough to allow real-time implementations and (4) requires only several minutes
of training examples to generate reasonably realistic-sounding speech.
Subjective evaluation experiments on a non-parallel many-to-many speaker
identity conversion task revealed that the proposed method obtained higher
sound quality and speaker similarity than a state-of-the-art method based on
variational autoencoding GANs.
</summary>
    <author>
      <name>Hirokazu Kameoka</name>
    </author>
    <author>
      <name>Takuhiro Kaneko</name>
    </author>
    <author>
      <name>Kou Tanaka</name>
    </author>
    <author>
      <name>Nobukatsu Hojo</name>
    </author>
    <link href="http://arxiv.org/abs/1806.02169v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.02169v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04278v1</id>
    <updated>2018-06-12T00:28:50Z</updated>
    <published>2018-06-12T00:28:50Z</published>
    <title>The NES Music Database: A multi-instrumental dataset with expressive
  performance attributes</title>
    <summary>  Existing research on music generation focuses on composition, but often
ignores the expressive performance characteristics required for plausible
renditions of resultant pieces. In this paper, we introduce the Nintendo
Entertainment System Music Database (NES-MDB), a large corpus allowing for
separate examination of the tasks of composition and performance. NES-MDB
contains thousands of multi-instrumental songs composed for playback by the
compositionally-constrained NES audio synthesizer. For each song, the dataset
contains a musical score for four instrument voices as well as expressive
attributes for the dynamics and timbre of each voice. Unlike datasets comprised
of General MIDI files, NES-MDB includes all of the information needed to render
exact acoustic performances of the original compositions. Alongside the
dataset, we provide a tool that renders generated compositions as NES-style
audio by emulating the device's audio processor. Additionally, we establish
baselines for the tasks of composition, which consists of learning the
semantics of composing for the NES synthesizer, and performance, which involves
finding a mapping between a composition and realistic expressive attributes.
</summary>
    <author>
      <name>Chris Donahue</name>
    </author>
    <author>
      <name>Huanru Henry Mao</name>
    </author>
    <author>
      <name>Julian McAuley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a conference paper at ISMIR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.04278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04699v1</id>
    <updated>2018-06-12T18:22:20Z</updated>
    <published>2018-06-12T18:22:20Z</published>
    <title>Capsule Routing for Sound Event Detection</title>
    <summary>  The detection of acoustic scenes is a challenging problem in which
environmental sound events must be detected from a given audio signal. This
includes classifying the events as well as estimating their onset and offset
times. We approach this problem with a neural network architecture that uses
the recently-proposed capsule routing mechanism. A capsule is a group of
activation units representing a set of properties for an entity of interest,
and the purpose of routing is to identify part-whole relationships between
capsules. That is, a capsule in one layer is assumed to belong to a capsule in
the layer above in terms of the entity being represented. Using capsule
routing, we wish to train a network that can learn global coherence implicitly,
thereby improving generalization performance. Our proposed method is evaluated
on Task 4 of the DCASE 2017 challenge. Results show that classification
performance is state-of-the-art, achieving an F-score of 58.6%. In addition,
overfitting is reduced considerably compared to other architectures.
</summary>
    <author>
      <name>Turab Iqbal</name>
    </author>
    <author>
      <name>Yong Xu</name>
    </author>
    <author>
      <name>Qiuqiang Kong</name>
    </author>
    <author>
      <name>Wenwu Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper accepted for 26th European Signal Processing Conference
  (EUSIPCO 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.04699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04903v1</id>
    <updated>2018-06-13T09:10:43Z</updated>
    <published>2018-06-13T09:10:43Z</published>
    <title>A data-driven approach to mid-level perceptual musical feature modeling</title>
    <summary>  Musical features and descriptors could be coarsely divided into three levels
of complexity. The bottom level contains the basic building blocks of music,
e.g., chords, beats and timbre. The middle level contains concepts that emerge
from combining the basic blocks: tonal and rhythmic stability, harmonic and
rhythmic complexity, etc. High-level descriptors (genre, mood, expressive
style) are usually modeled using the lower level ones. The features belonging
to the middle level can both improve automatic recognition of high-level
descriptors, and provide new music retrieval possibilities. Mid-level features
are subjective and usually lack clear definitions. However, they are very
important for human perception of music, and on some of them people can reach
high agreement, even though defining them and therefore, designing a
hand-crafted feature extractor for them can be difficult. In this paper, we
derive the mid-level descriptors from data. We collect and release a
dataset\footnote{https://osf.io/5aupt/} of 5000 songs annotated by musicians
with seven mid-level descriptors, namely, melodiousness, tonal and rhythmic
stability, modality, rhythmic complexity, dissonance and articulation. We then
compare several approaches to predicting these descriptors from spectrograms
using deep-learning. We also demonstrate the usefulness of these mid-level
features using music emotion recognition as an application.
</summary>
    <author>
      <name>Anna Aljanaki</name>
    </author>
    <author>
      <name>Mohammad Soleymani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, ISMIR conference paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.04903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05622v2</id>
    <updated>2018-06-27T01:49:17Z</updated>
    <published>2018-06-14T15:59:12Z</published>
    <title>VoxCeleb2: Deep Speaker Recognition</title>
    <summary>  The objective of this paper is speaker recognition under noisy and
unconstrained conditions.
  We make two key contributions. First, we introduce a very large-scale
audio-visual speaker recognition dataset collected from open-source media.
Using a fully automated pipeline, we curate VoxCeleb2 which contains over a
million utterances from over 6,000 speakers. This is several times larger than
any publicly available speaker recognition dataset.
  Second, we develop and compare Convolutional Neural Network (CNN) models and
training strategies that can effectively recognise identities from voice under
various conditions. The models trained on the VoxCeleb2 dataset surpass the
performance of previous works on a benchmark dataset by a significant margin.
</summary>
    <author>
      <name>Joon Son Chung</name>
    </author>
    <author>
      <name>Arsha Nagrani</name>
    </author>
    <author>
      <name>Andrew Zisserman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.21437/Interspeech.2018-1929</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.21437/Interspeech.2018-1929" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Interspeech 2018. The audio-visual dataset can be
  downloaded from http://www.robots.ox.ac.uk/~vgg/data/voxceleb2 .
  1806.05622v2: minor fixes; 5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.05622v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05622v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05791v1</id>
    <updated>2018-06-15T02:38:40Z</updated>
    <published>2018-06-15T02:38:40Z</published>
    <title>Monaural source enhancement maximizing source-to-distortion ratio via
  automatic differentiation</title>
    <summary>  Recently, deep neural network (DNN) has made a breakthrough in monaural
source enhancement. Through a training step by using a large amount of data,
DNN estimates a mapping between mixed signals and clean signals. At this time,
we use an objective function that numerically expresses the quality of a
mapping by DNN. In the conventional methods, L1 norm, L2 norm, and
Itakura-Saito divergence are often used as objective functions. Recently, an
objective function based on short-time objective intelligibility (STOI) has
also been proposed. However, these functions only indicate similarity between
the clean signal and the estimated signal by DNN. In other words, they do not
show the quality of noise reduction or source enhancement. Motivated by the
fact, this paper adopts signal-to-distortion ratio (SDR) as the objective
function. Since SDR virtually shows signal-to-noise ratio (SNR), maximizing SDR
solves the above problem. The experimental results revealed that the proposed
method achieved better performance than the conventional methods.
</summary>
    <author>
      <name>Hiroaki Nakajima</name>
    </author>
    <author>
      <name>Yu Takahashi</name>
    </author>
    <author>
      <name>Kazunobu Kondo</name>
    </author>
    <author>
      <name>Yuji Hisaminato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is submitted to 16th International Workshop on Acoustic
  Signal Enhancement (IWAENC)</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.05791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06342v2</id>
    <updated>2019-02-26T02:51:49Z</updated>
    <published>2018-06-17T06:57:30Z</published>
    <title>Extending Recurrent Neural Aligner for Streaming End-to-End Speech
  Recognition in Mandarin</title>
    <summary>  End-to-end models have been showing superiority in Automatic Speech
Recognition (ASR). At the same time, the capacity of streaming recognition has
become a growing requirement for end-to-end models. Following these trends, an
encoder-decoder recurrent neural network called Recurrent Neural Aligner (RNA)
has been freshly proposed and shown its competitiveness on two English ASR
tasks. However, it is not clear if RNA can be further improved and applied to
other spoken language. In this work, we explore the applicability of RNA in
Mandarin Chinese and present four effective extensions: In the encoder, we
redesign the temporal down-sampling and introduce a powerful convolutional
structure. In the decoder, we utilize a regularizer to smooth the output
distribution and conduct joint training with a language model. On two Mandarin
Chinese conversational telephone speech recognition (MTS) datasets, our
Extended-RNA obtains promising performance. Particularly, it achieves 27.7%
character error rate (CER), which is superior to current state-of-the-art
result on the popular HKUST task.
</summary>
    <author>
      <name>Linhao Dong</name>
    </author>
    <author>
      <name>Shiyu Zhou</name>
    </author>
    <author>
      <name>Wei Chen</name>
    </author>
    <author>
      <name>Bo Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Interspeech 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06342v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06342v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06347v2</id>
    <updated>2018-06-29T09:58:26Z</updated>
    <published>2018-06-17T08:09:20Z</published>
    <title>Cover Song Synthesis by Analogy</title>
    <summary>  In this work, we pose and address the following "cover song analogies"
problem: given a song A by artist 1 and a cover song A' of this song by artist
2, and given a different song B by artist 1, synthesize a song B' which is a
cover of B in the style of artist 2. Normally, such a polyphonic style transfer
problem would be quite challenging, but we show how the cover songs example
constrains the problem, making it easier to solve. First, we extract the
longest common beat-synchronous subsequence between A and A', and we time
stretch the corresponding beat intervals in A' so that they align with A. We
then derive a version of joint 2D convolutional NMF, which we apply to the
constant-Q spectrograms of the synchronized segments to learn a translation
dictionary of sound templates from A to A'. Finally, we apply the learned
templates as filters to the song B, and we mash up the translated filtered
components into the synthesized song B' using audio mosaicing. We showcase our
algorithm on several examples, including a synthesized cover version of Michael
Jackson's "Bad" by Alien Ant Farm, learned from the latter's "Smooth Criminal"
cover.'
</summary>
    <author>
      <name>Christopher J. Tralie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06347v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06347v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06812v1</id>
    <updated>2018-06-18T16:35:47Z</updated>
    <published>2018-06-18T16:35:47Z</published>
    <title>Frequency domain variants of velvet noise and their application to
  speech processing and synthesis: with appendices</title>
    <summary>  We propose a new excitation source signal for VOCODERs and an all-pass
impulse response for post-processing of synthetic sounds and pre-processing of
natural sounds for data-augmentation. The proposed signals are variants of
velvet noise, which is a sparse discrete signal consisting of a few non-zero (1
or -1) elements and sounds smoother than Gaussian white noise. One of the
proposed variants, FVN (Frequency domain Velvet Noise) applies the procedure to
generate a velvet noise on the cyclic frequency domain of DFT (Discrete Fourier
Transform). Then, by smoothing the generated signal to design the phase of an
all-pass filter followed by inverse Fourier transform yields the proposed FVN.
Temporally variable frequency weighted mixing of FVN generated by frozen and
shuffled random number provides a unified excitation signal which can span from
random noise to a repetitive pulse train. The other variant, which is an
all-pass impulse response, significantly reduces "buzzy" impression of VOCODER
output by filtering. Finally, we will discuss applications of the proposed
signal for watermarking and psychoacoustic research.
</summary>
    <author>
      <name>Hideki Kawahara</name>
    </author>
    <author>
      <name>Ken-Ichi Sakakibara</name>
    </author>
    <author>
      <name>Masanori Morise</name>
    </author>
    <author>
      <name>Hideki Banno</name>
    </author>
    <author>
      <name>Tomoki Toda</name>
    </author>
    <author>
      <name>Toshio Irino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 20 figures, and 1 table, Interspeech 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07506v2</id>
    <updated>2018-06-27T19:29:00Z</updated>
    <published>2018-06-19T23:42:54Z</published>
    <title>A Simple Fusion of Deep and Shallow Learning for Acoustic Scene
  Classification</title>
    <summary>  In the past, Acoustic Scene Classification systems have been based on hand
crafting audio features that are input to a classifier. Nowadays, the common
trend is to adopt data driven techniques, e.g., deep learning, where audio
representations are learned from data. In this paper, we propose a system that
consists of a simple fusion of two methods of the aforementioned types: a deep
learning approach where log-scaled mel-spectrograms are input to a
convolutional neural network, and a feature engineering approach, where a
collection of hand-crafted features is input to a gradient boosting machine. We
first show that both methods provide complementary information to some extent.
Then, we use a simple late fusion strategy to combine both methods. We report
classification accuracy of each method individually and the combined system on
the TUT Acoustic Scenes 2017 dataset. The proposed fused system outperforms
each of the individual methods and attains a classification accuracy of 72.8%
on the evaluation set, improving the baseline system by 11.8%.
</summary>
    <author>
      <name>Eduardo Fonseca</name>
    </author>
    <author>
      <name>Rong Gong</name>
    </author>
    <author>
      <name>Xavier Serra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to SMC 2018; updated Figure 7, results unchanged</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.07506v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07506v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08002v1</id>
    <updated>2018-06-20T21:51:32Z</updated>
    <published>2018-06-20T21:51:32Z</published>
    <title>Synthesizing Diverse, High-Quality Audio Textures</title>
    <summary>  Texture synthesis techniques based on matching the Gram matrix of feature
activations in neural networks have achieved spectacular success in the image
domain. In this paper we extend these techniques to the audio domain. We
demonstrate that synthesizing diverse audio textures is challenging, and argue
that this is because audio data is relatively low-dimensional. We therefore
introduce two new terms to the original Grammian loss: an autocorrelation term
that preserves rhythm, and a diversity term that encourages the optimization
procedure to synthesize unique textures. We quantitatively study the impact of
our design choices on the quality of the synthesized audio by introducing an
audio analogue to the Inception loss which we term the VGGish loss. We show
that there is a trade-off between the diversity and quality of the synthesized
audio using this technique. We additionally perform a number of experiments to
qualitatively study how these design choices impact the quality of the
synthesized audio. Finally we describe the implications of these results for
the problem of audio style transfer.
</summary>
    <author>
      <name>Joseph Antognini</name>
    </author>
    <author>
      <name>Matt Hoffman</name>
    </author>
    <author>
      <name>Ron J. Weiss</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, submitted to TASLP</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.08002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08236v2</id>
    <updated>2019-02-04T17:09:38Z</updated>
    <published>2018-06-21T13:35:44Z</published>
    <title>Learning Transposition-Invariant Interval Features from Symbolic Music
  and Audio</title>
    <summary>  Many music theoretical constructs (such as scale types, modes, cadences, and
chord types) are defined in terms of pitch intervals---relative distances
between pitches. Therefore, when computer models are employed in music tasks,
it can be useful to operate on interval representations rather than on the raw
musical surface. Moreover, interval representations are
transposition-invariant, valuable for tasks like audio alignment, cover song
detection and music structure analysis. We employ a gated autoencoder to learn
fixed-length, invertible and transposition-invariant interval representations
from polyphonic music in the symbolic domain and in audio. An unsupervised
training method is proposed yielding an organization of intervals in the
representation space which is musically plausible. Based on the
representations, a transposition-invariant self-similarity matrix is
constructed and used to determine repeated sections in symbolic music and in
audio, yielding competitive results in the MIREX task "Discovery of Repeated
Themes and Sections".
</summary>
    <author>
      <name>Stefan Lattner</name>
    </author>
    <author>
      <name>Maarten Grachten</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper accepted at the 19th International Society for Music
  Information Retrieval Conference, ISMIR 2018, Paris, France, September 23-27;
  8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.08236v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08236v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08404v2</id>
    <updated>2018-12-04T10:20:49Z</updated>
    <published>2018-06-21T19:07:12Z</published>
    <title>On the Relationship Between Short-Time Objective Intelligibility and
  Short-Time Spectral-Amplitude Mean-Square Error for Speech Enhancement</title>
    <summary>  The majority of deep neural network (DNN) based speech enhancement algorithms
rely on the mean-square error (MSE) criterion of short-time spectral amplitudes
(STSA), which has no apparent link to human perception, e.g. speech
intelligibility. Short-Time Objective Intelligibility (STOI), a popular
state-of-the-art speech intelligibility estimator, on the other hand, relies on
linear correlation of speech temporal envelopes. This raises the question if a
DNN training criterion based on envelope linear correlation (ELC) can lead to
improved speech intelligibility performance of DNN based speech enhancement
algorithms compared to algorithms based on the STSA-MSE criterion. In this
paper we derive that, under certain general conditions, the STSA-MSE and ELC
criteria are practically equivalent, and we provide empirical data to support
our theoretical results. Furthermore, our experimental findings suggest that
the standard STSA minimum-MSE estimator is near optimal, if the objective is to
enhance noisy speech in a manner which is optimal with respect to the STOI
speech intelligibility estimator.
</summary>
    <author>
      <name>Morten Kolbæk</name>
    </author>
    <author>
      <name>Zheng-Hua Tan</name>
    </author>
    <author>
      <name>Jesper Jensen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASLP.2018.2877909</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASLP.2018.2877909" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Published in IEEE/ACM Trans. Audio, Speech, Lang. Process., vol.
  27, no. 2, pp. 283-295, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.08404v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08404v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08621v1</id>
    <updated>2018-06-22T12:15:35Z</updated>
    <published>2018-06-22T12:15:35Z</published>
    <title>Weakly Supervised Training of Speaker Identification Models</title>
    <summary>  We propose an approach for training speaker identification models in a weakly
supervised manner. We concentrate on the setting where the training data
consists of a set of audio recordings and the speaker annotation is provided
only at the recording level. The method uses speaker diarization to find unique
speakers in each recording, and i-vectors to project the speech of each speaker
to a fixed-dimensional vector. A neural network is then trained to map
i-vectors to speakers, using a special objective function that allows to
optimize the model using recording-level speaker labels. We report experiments
on two different real-world datasets. On the VoxCeleb dataset, the method
provides 94.6% accuracy on a closed set speaker identification task, surpassing
the baseline performance by a large margin. On an Estonian broadcast news
dataset, the method provides 66% time-weighted speaker identification recall at
93% precision.
</summary>
    <author>
      <name>Martin Karu</name>
    </author>
    <author>
      <name>Tanel Alumäe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Odyssey 2018 The Speaker and Language Recognition Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.08621v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08621v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09301v1</id>
    <updated>2018-06-25T06:53:54Z</updated>
    <published>2018-06-25T06:53:54Z</published>
    <title>Robust Feature Clustering for Unsupervised Speech Activity Detection</title>
    <summary>  In certain applications such as zero-resource speech processing or very-low
resource speech-language systems, it might not be feasible to collect speech
activity detection (SAD) annotations. However, the state-of-the-art supervised
SAD techniques based on neural networks or other machine learning methods
require annotated training data matched to the target domain. This paper
establish a clustering approach for fully unsupervised SAD useful for cases
where SAD annotations are not available. The proposed approach leverages
Hartigan dip test in a recursive strategy for segmenting the feature space into
prominent modes. Statistical dip is invariant to distortions that lends
robustness to the proposed method. We evaluate the method on NIST OpenSAD 2015
and NIST OpenSAT 2017 public safety communications data. The results showed the
superiority of proposed approach over the two-component GMM baseline. Index
Terms: Clustering, Hartigan dip test, NIST OpenSAD, NIST OpenSAT, speech
activity detection, zero-resource speech processing, unsupervised learning.
</summary>
    <author>
      <name>Harishchandra Dubey</name>
    </author>
    <author>
      <name>Abhijeet Sangwan</name>
    </author>
    <author>
      <name>John H. L. Hansen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages, 4 Tables, 1 Figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09325v1</id>
    <updated>2018-06-25T08:35:56Z</updated>
    <published>2018-06-25T08:35:56Z</published>
    <title>Single-channel Speech Dereverberation via Generative Adversarial
  Training</title>
    <summary>  In this paper, we propose a single-channel speech dereverberation system
(DeReGAT) based on convolutional, bidirectional long short-term memory and deep
feed-forward neural network (CBLDNN) with generative adversarial training
(GAT). In order to obtain better speech quality instead of only minimizing a
mean square error (MSE), GAT is employed to make the dereverberated speech
indistinguishable form the clean samples. Besides, our system can deal with
wide range reverberation and be well adapted to variant environments. The
experimental results show that the proposed model outperforms weighted
prediction error (WPE) and deep neural network-based systems. In addition,
DeReGAT is extended to an online speech dereverberation scenario, which reports
comparable performance with the offline case.
</summary>
    <author>
      <name>Chenxing Li</name>
    </author>
    <author>
      <name>Tieqiang Wang</name>
    </author>
    <author>
      <name>Shuang Xu</name>
    </author>
    <author>
      <name>Bo Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages. Accepted by Interspeech 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09325v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09325v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09587v1</id>
    <updated>2018-06-25T17:33:04Z</updated>
    <published>2018-06-25T17:33:04Z</published>
    <title>Frame-level Instrument Recognition by Timbre and Pitch</title>
    <summary>  Instrument recognition is a fundamental task in music information retrieval,
yet little has been done to predict the presence of instruments in
multi-instrument music for each time frame. This task is important for not only
automatic transcription but also many retrieval problems. In this paper, we use
the newly released MusicNet dataset to study this front, by building and
evaluating a convolutional neural network for making frame-level instrument
prediction. We consider it as a multi-label classification problem for each
frame and use frame-level annotations as the supervisory signal in training the
network. Moreover, we experiment with different ways to incorporate pitch
information to our model, with the premise that doing so informs the model the
notes that are active per frame, and also encourages the model to learn
relative rates of energy buildup in the harmonic partials of different
instruments. Experiments show salient performance improvement over baseline
methods. We also report an analysis probing how pitch information helps the
instrument prediction task. Code and experiment details can be found at
\url{https://biboamy.github.io/instrument-recognition/}.
</summary>
    <author>
      <name>Yun-Ning Hung</name>
    </author>
    <author>
      <name>Yi-Hsuan Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1806.09587v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09587v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09617v1</id>
    <updated>2018-06-25T14:21:37Z</updated>
    <published>2018-06-25T14:21:37Z</published>
    <title>Sounderfeit: Cloning a Physical Model using a Conditional Adversarial
  Autoencoder</title>
    <summary>  An adversarial autoencoder conditioned on known parameters of a physical
modeling bowed string synthesizer is evaluated for use in parameter estimation
and resynthesis tasks. Latent dimensions are provided to capture variance not
explained by the conditional parameters. Results are compared with and without
the adversarial training, and a system capable of "copying" a given
parameter-signal bidirectional relationship is examined. A real-time synthesis
system built on a generative, conditioned and regularized neural network is
presented, allowing to construct engaging sound synthesizers based purely on
recorded data.
</summary>
    <author>
      <name>Stephen Sinclair</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5216/mh.v18i1.53570</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5216/mh.v18i1.53570" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended conference paper published as article in Brazilian
  open-access journal Musica Hodie. 17 pages, 10 figures. ISSN 1676-3939.
  Dispon\'ivel em: https://www.revistas.ufg.br/musica/article/view/53570. arXiv
  admin note: substantial text overlap with arXiv:1802.08008</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Revista M\'usica Hodie, [S.l.], v. 18, n. 1, p. 44 - 60, jun. 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.09617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09905v1</id>
    <updated>2018-06-26T11:10:19Z</updated>
    <published>2018-06-26T11:10:19Z</published>
    <title>Conditioning Deep Generative Raw Audio Models for Structured Automatic
  Music</title>
    <summary>  Existing automatic music generation approaches that feature deep learning can
be broadly classified into two types: raw audio models and symbolic models.
Symbolic models, which train and generate at the note level, are currently the
more prevalent approach; these models can capture long-range dependencies of
melodic structure, but fail to grasp the nuances and richness of raw audio
generations. Raw audio models, such as DeepMind's WaveNet, train directly on
sampled audio waveforms, allowing them to produce realistic-sounding, albeit
unstructured music. In this paper, we propose an automatic music generation
methodology combining both of these approaches to create structured,
realistic-sounding compositions. We consider a Long Short Term Memory network
to learn the melodic structure of different styles of music, and then use the
unique symbolic generations from this model as a conditioning input to a
WaveNet-based raw audio generator, creating a model for automatic, novel music.
We then evaluate this approach by showcasing results of this work.
</summary>
    <author>
      <name>Rachel Manzelli</name>
    </author>
    <author>
      <name>Vijay Thakkar</name>
    </author>
    <author>
      <name>Ali Siahkamari</name>
    </author>
    <author>
      <name>Brian Kulis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the ISMIR 2018 Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09905v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09905v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09932v1</id>
    <updated>2018-06-26T12:05:33Z</updated>
    <published>2018-06-26T12:05:33Z</published>
    <title>Text-Independent Speaker Verification Based on Deep Neural Networks and
  Segmental Dynamic Time Warping</title>
    <summary>  In this paper we present a new method for text-independent speaker
verification that combines segmental dynamic time warping (SDTW) and the
d-vector approach. The d-vectors, generated from a feed forward deep neural
network trained to distinguish between speakers, are used as features to
perform alignment and hence calculate the overall distance between the
enrolment and test utterances.We present results on the NIST 2008 data set for
speaker verification where the proposed method outperforms the conventional
i-vector baseline with PLDA scores and outperforms d-vector approach with local
distances based on cosine and PLDA scores. Also score combination with the
i-vector/PLDA baseline leads to significant gains over both methods.
</summary>
    <author>
      <name>Mohamed Adel</name>
    </author>
    <author>
      <name>Mohamed Afify</name>
    </author>
    <author>
      <name>Akram Gaballah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to SLT 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.10474v1</id>
    <updated>2018-06-26T16:48:59Z</updated>
    <published>2018-06-26T16:48:59Z</published>
    <title>The challenge of realistic music generation: modelling raw audio at
  scale</title>
    <summary>  Realistic music generation is a challenging task. When building generative
models of music that are learnt from data, typically high-level representations
such as scores or MIDI are used that abstract away the idiosyncrasies of a
particular performance. But these nuances are very important for our perception
of musicality and realism, so in this work we embark on modelling music in the
raw audio domain. It has been shown that autoregressive models excel at
generating raw audio waveforms of speech, but when applied to music, we find
them biased towards capturing local signal structure at the expense of
modelling long-range correlations. This is problematic because music exhibits
structure at many different timescales. In this work, we explore autoregressive
discrete autoencoders (ADAs) as a means to enable autoregressive models to
capture long-range correlations in waveforms. We find that they allow us to
unconditionally generate piano music directly in the raw audio domain, which
shows stylistic consistency across tens of seconds.
</summary>
    <author>
      <name>Sander Dieleman</name>
    </author>
    <author>
      <name>Aäron van den Oord</name>
    </author>
    <author>
      <name>Karen Simonyan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures, submitted to NIPS 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.10474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.10474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.11170v5</id>
    <updated>2019-08-12T17:58:59Z</updated>
    <published>2018-06-28T20:20:04Z</published>
    <title>GenerationMania: Learning to Semantically Choreograph</title>
    <summary>  Beatmania is a rhythm action game where players must reproduce some of the
sounds of a song by pressing specific controller buttons at the correct time.
In this paper we investigate the use of deep neural networks to automatically
create game stages - called charts - for arbitrary pieces of music. Our
technique uses a multi-layer feed-forward network trained on sound sequence
summary statistics to predict which sounds in the music are to be played by the
player and which will play automatically. We use another neural network along
with rules to determine which controls should be mapped to which sounds. We
evaluated our system on the ability to reconstruct charts in a held-out test
set, achieving an $F_1$-score that significantly beats LSTM baselines.
</summary>
    <author>
      <name>Zhiyu Lin</name>
    </author>
    <author>
      <name>Kyle Xiao</name>
    </author>
    <author>
      <name>Mark Riedl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in AIIDE 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.11170v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.11170v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00069v1</id>
    <updated>2018-06-29T21:05:19Z</updated>
    <published>2018-06-29T21:05:19Z</published>
    <title>Exploratory Analysis of a Large Flamenco Corpus using an Ensemble of
  Convolutional Neural Networks as a Structural Annotation Backend</title>
    <summary>  We present computational tools that we developed for the analysis of a large
corpus of flamenco music recordings, along with the related exploratory
findings. The proposed computational backend is based on a set of Convolutional
Neural Networks that provide the structural annotation of each music recording
with respect to the presence of vocals, guitar and hand-clapping ("palmas").
The resulting, automatically extracted annotations, allowed for the
visualization of music recordings in structurally meaningful ways, the
extraction of global statistics related to the instrumentation of flamenco
music, the detection of a cappella and instrumental recordings for which no
such information existed, the investigation of differences in structure and
instrumentation across styles and the study of tonality across instrumentation
and styles. The reported findings show that it is feasible to perform a large
scale analysis of flamenco music with state-of-the-art classification
technology and produce automatically extracted descriptors that are both
musicologically valid and useful, in the sense that they can enhance
conventional metadata schemes and assist bridging the semantic gap between
audio recordings and high-level musicological concepts.
</summary>
    <author>
      <name>Nadine Kroher</name>
    </author>
    <author>
      <name>Aggelos Pikrakis</name>
    </author>
    <link href="http://arxiv.org/abs/1807.00069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00790v1</id>
    <updated>2018-07-02T17:23:06Z</updated>
    <published>2018-07-02T17:23:06Z</published>
    <title>An energy-based generative sequence model for testing sensory theories
  of Western harmony</title>
    <summary>  The relationship between sensory consonance and Western harmony is an
important topic in music theory and psychology. We introduce new methods for
analysing this relationship, and apply them to large corpora representing three
prominent genres of Western music: classical, popular, and jazz music. These
methods centre on a generative sequence model with an exponential-family
energy-based form that predicts chord sequences from continuous features. We
use this model to investigate one aspect of instantaneous consonance
(harmonicity) and two aspects of sequential consonance (spectral distance and
voice-leading distance). Applied to our three musical genres, the results
generally support the relationship between sensory consonance and harmony, but
lead us to question the high importance attributed to spectral distance in the
psychological literature. We anticipate that our methods will provide a useful
platform for future work linking music psychology to music theory.
</summary>
    <author>
      <name>Peter M. C. Harrison</name>
    </author>
    <author>
      <name>Marcus T. Pearce</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures. To appear in Proceedings of the 19th
  International Society for Music Information Retrieval Conference (ISMIR),
  Paris, France, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00868v1</id>
    <updated>2018-07-02T19:47:22Z</updated>
    <published>2018-07-02T19:47:22Z</published>
    <title>Exploring End-to-End Techniques for Low-Resource Speech Recognition</title>
    <summary>  In this work we present simple grapheme-based system for low-resource speech
recognition using Babel data for Turkish spontaneous speech (80 hours). We have
investigated different neural network architectures performance, including
fully-convolutional, recurrent and ResNet with GRU. Different features and
normalization techniques are compared as well. We also proposed CTC-loss
modification using segmentation during training, which leads to improvement
while decoding with small beam size. Our best model achieved word error rate of
45.8%, which is the best reported result for end-to-end systems using in-domain
data for this task, according to our knowledge.
</summary>
    <author>
      <name>Vladimir Bataev</name>
    </author>
    <author>
      <name>Maxim Korenevsky</name>
    </author>
    <author>
      <name>Ivan Medennikov</name>
    </author>
    <author>
      <name>Alexander Zatvornitskiy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for Specom 2018, 20th International Conference on Speech and
  Computer</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01080v1</id>
    <updated>2018-07-03T10:59:23Z</updated>
    <published>2018-07-03T10:59:23Z</published>
    <title>A Computational Study of the Role of Tonal Tension in Expressive Piano
  Performance</title>
    <summary>  Expressive variations of tempo and dynamics are an important aspect of music
performances, involving a variety of underlying factors. Previous work has
showed a relation between such expressive variations (in particular expressive
tempo) and perceptual characteristics derived from the musical score, such as
musical expectations, and perceived tension. In this work we use a
computational approach to study the role of three measures of tonal tension
proposed by Herremans and Chew (2016) in the prediction of expressive
performances of classical piano music. These features capture tonal
relationships of the music represented in Chew's spiral array model, a three
dimensional representation of pitch classes, chords and keys constructed in
such a way that spatial proximity represents close tonal relationships. We use
non-linear sequential models (recurrent neural networks) to assess the
contribution of these features to the prediction of expressive dynamics and
expressive tempo using a dataset of Mozart piano sonatas performed by a
professional concert pianist. Experiments of models trained with and without
tonal tension features show that tonal tension helps predict change of tempo
and dynamics more than absolute tempo and dynamics values. Furthermore, the
improvement is stronger for dynamics than for tempo.
</summary>
    <author>
      <name>Carlos Cancino-Chacón</name>
    </author>
    <author>
      <name>Maarten Grachten</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, accepted as poster at the ICMPC15/ESCOM10 in
  Graz, Austria</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.01080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.01898v1</id>
    <updated>2018-07-05T08:54:32Z</updated>
    <published>2018-07-05T08:54:32Z</published>
    <title>Denoising Auto-encoder with Recurrent Skip Connections and Residual
  Regression for Music Source Separation</title>
    <summary>  Convolutional neural networks with skip connections have shown good
performance in music source separation. In this work, we propose a denoising
Auto-encoder with Recurrent skip Connections (ARC). We use 1D convolution along
the temporal axis of the time-frequency feature map in all layers of the
fully-convolutional network. The use of 1D convolution makes it possible to
apply recurrent layers to the intermediate outputs of the convolution layers.
In addition, we also propose an enhancement network and a residual regression
method to further improve the separation result. The recurrent skip
connections, the enhancement module, and the residual regression all improve
the separation quality. The ARC model with residual regression achieves 5.74
siganl-to-distoration ratio (SDR) in vocals with MUSDB in SiSEC 2018. We also
evaluate the ARC model alone on the older dataset DSD100 (used in SiSEC 2016)
and it achieves 5.91 SDR in vocals.
</summary>
    <author>
      <name>Jen-Yu Liu</name>
    </author>
    <author>
      <name>Yi-Hsuan Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1807.01898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.01898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02710v3</id>
    <updated>2018-07-16T06:09:41Z</updated>
    <published>2018-07-07T19:02:36Z</published>
    <title>Improving DNN-based Music Source Separation using Phase Features</title>
    <summary>  Music source separation with deep neural networks typically relies only on
amplitude features. In this paper we show that additional phase features can
improve the separation performance. Using the theoretical relationship between
STFT phase and amplitude, we conjecture that derivatives of the phase are a
good feature representation opposed to the raw phase. We verify this conjecture
experimentally and propose a new DNN architecture which combines amplitude and
phase. This joint approach achieves a better signal-to distortion ratio on the
DSD100 dataset for all instruments compared to a network that uses only
amplitude features. Especially, the bass instrument benefits from the phase
information.
</summary>
    <author>
      <name>Joachim Muth</name>
    </author>
    <author>
      <name>Stefan Uhlich</name>
    </author>
    <author>
      <name>Nathanael Perraudin</name>
    </author>
    <author>
      <name>Thomas Kemp</name>
    </author>
    <author>
      <name>Fabien Cardinaux</name>
    </author>
    <author>
      <name>Yuki Mitsufuji</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 9 figures, Joint Workshop on Machine Learning for Music at
  ICML, IJCAI/ECAI and AAMAS, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.02710v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02710v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02776v1</id>
    <updated>2018-07-08T08:16:33Z</updated>
    <published>2018-07-08T08:16:33Z</published>
    <title>Densely Connected CNNs for Bird Audio Detection</title>
    <summary>  Detecting bird sounds in audio recordings automatically, if accurate enough,
is expected to be of great help to the research community working in bio- and
ecoacoustics, interested in monitoring biodiversity based on audio field
recordings. To estimate how accurate the state-of-the-art machine learning
approaches are, the Bird Audio Detection challenge involving large audio
datasets was recently organized. In this paper, experiments using several types
of convolutional neural networks (i.e. standard CNNs, residual nets and densely
connected nets) are reported in the framework of this challenge. DenseNets were
the preferred solution since they were the best performing and most compact
models, leading to a 88.22% area under the receiver operator curve score on the
test set of the challenge. Performance gains were obtained thank to data
augmentation through time and frequency shifting, model parameter averaging
during training and ensemble methods using the geometric mean. On the contrary,
the attempts to enlarge the training dataset with samples of the test set with
automatic predictions used as pseudo-groundtruth labels consistently degraded
performance.
</summary>
    <author>
      <name>Thomas Pellegrini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.23919/EUSIPCO.2017.8081506</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.23919/EUSIPCO.2017.8081506" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Challenge solution source code available at
  https://github.com/topel/bird_audio_detection_challenge, Proc. EUSIPCO 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.02776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03046v1</id>
    <updated>2018-07-09T11:19:42Z</updated>
    <published>2018-07-09T11:19:42Z</published>
    <title>Deep Learning for Singing Processing: Achievements, Challenges and
  Impact on Singers and Listeners</title>
    <summary>  This paper summarizes some recent advances on a set of tasks related to the
processing of singing using state-of-the-art deep learning techniques. We
discuss their achievements in terms of accuracy and sound quality, and the
current challenges, such as availability of data and computing resources. We
also discuss the impact that these advances do and will have on listeners and
singers when they are integrated in commercial applications.
</summary>
    <author>
      <name>Emilia Gómez</name>
    </author>
    <author>
      <name>Merlijn Blaauw</name>
    </author>
    <author>
      <name>Jordi Bonada</name>
    </author>
    <author>
      <name>Pritish Chandna</name>
    </author>
    <author>
      <name>Helena Cuesta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keynote speech, 2018 Joint Workshop on Machine Learning for Music.
  The Federated Artificial Intelligence Meeting (FAIM), a joint workshop
  program of ICML, IJCAI/ECAI, and AAMAS</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03046v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03046v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="97M80" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03418v3</id>
    <updated>2023-11-27T18:26:32Z</updated>
    <published>2018-07-09T23:11:17Z</published>
    <title>AudioMNIST: Exploring Explainable Artificial Intelligence for Audio
  Analysis on a Simple Benchmark</title>
    <summary>  Explainable Artificial Intelligence (XAI) is targeted at understanding how
models perform feature selection and derive their classification decisions.
This paper explores post-hoc explanations for deep neural networks in the audio
domain. Notably, we present a novel Open Source audio dataset consisting of
30,000 audio samples of English spoken digits which we use for classification
tasks on spoken digits and speakers' biological sex. We use the popular XAI
technique Layer-wise Relevance Propagation (LRP) to identify relevant features
for two neural network architectures that process either waveform or
spectrogram representations of the data. Based on the relevance scores obtained
from LRP, hypotheses about the neural networks' feature selection are derived
and subsequently tested through systematic manipulations of the input data.
Further, we take a step beyond visual explanations and introduce audible
heatmaps. We demonstrate the superior interpretability of audible explanations
over visual ones in a human user study.
</summary>
    <author>
      <name>Sören Becker</name>
    </author>
    <author>
      <name>Johanna Vielhaben</name>
    </author>
    <author>
      <name>Marcel Ackermann</name>
    </author>
    <author>
      <name>Klaus-Robert Müller</name>
    </author>
    <author>
      <name>Sebastian Lapuschkin</name>
    </author>
    <author>
      <name>Wojciech Samek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03418v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03418v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03474v1</id>
    <updated>2018-07-10T04:14:55Z</updated>
    <published>2018-07-10T04:14:55Z</published>
    <title>Phase reconstruction from amplitude spectrograms based on
  von-Mises-distribution deep neural network</title>
    <summary>  This paper presents a deep neural network (DNN)-based phase reconstruction
from amplitude spectrograms. In audio signal and speech processing, the
amplitude spectrogram is often used for processing, and the corresponding phase
spectrogram is reconstructed from the amplitude spectrogram on the basis of the
Griffin-Lim method. However, the Griffin-Lim method causes unnatural artifacts
in synthetic speech. Addressing this problem, we introduce the
von-Mises-distribution DNN for phase reconstruction. The DNN is a generative
model having the von Mises distribution that can model distributions of a
periodic variable such as a phase, and the model parameters of the DNN are
estimated on the basis of the maximum likelihood criterion. Furthermore, we
propose a group-delay loss for DNN training to make the predicted group delay
close to a natural group delay. The experimental results demonstrate that 1)
the trained DNN can predict group delay accurately more than phases themselves,
and 2) our phase reconstruction methods achieve better speech quality than the
conventional Griffin-Lim method.
</summary>
    <author>
      <name>Shinnosuke Takamichi</name>
    </author>
    <author>
      <name>Yuki Saito</name>
    </author>
    <author>
      <name>Norihiro Takamune</name>
    </author>
    <author>
      <name>Daichi Kitamura</name>
    </author>
    <author>
      <name>Hiroshi Saruwatari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Proc. of IWAENC2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03625v1</id>
    <updated>2018-07-09T17:38:23Z</updated>
    <published>2018-07-09T17:38:23Z</published>
    <title>Foreign English Accent Adjustment by Learning Phonetic Patterns</title>
    <summary>  State-of-the-art automatic speech recognition (ASR) systems struggle with the
lack of data for rare accents. For sufficiently large datasets, neural engines
tend to outshine statistical models in most natural language processing
problems. However, a speech accent remains a challenge for both approaches.
Phonologists manually create general rules describing a speaker's accent, but
their results remain underutilized. In this paper, we propose a model that
automatically retrieves phonological generalizations from a small dataset. This
method leverages the difference in pronunciation between a particular dialect
and General American English (GAE) and creates new accented samples of words.
The proposed model is able to learn all generalizations that previously were
manually obtained by phonologists. We use this statistical method to generate a
million phonological variations of words from the CMU Pronouncing Dictionary
and train a sequence-to-sequence RNN to recognize accented words with 59%
accuracy.
</summary>
    <author>
      <name>Fedor Kitashov</name>
    </author>
    <author>
      <name>Elizaveta Svitanko</name>
    </author>
    <author>
      <name>Debojyoti Dutta</name>
    </author>
    <link href="http://arxiv.org/abs/1807.03625v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03625v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04073v2</id>
    <updated>2021-03-30T05:14:42Z</updated>
    <published>2018-07-11T11:14:19Z</published>
    <title>A punishment voting algorithm based on super categories construction for
  acoustic scene classification</title>
    <summary>  In acoustic scene classification researches, audio segment is usually split
into multiple samples. Majority voting is then utilized to ensemble the results
of the samples. In this paper, we propose a punishment voting algorithm based
on the super categories construction method for acoustic scene classification.
Specifically, we propose a DenseNet-like model as the base classifier. The base
classifier is trained by the CQT spectrograms generated from the raw audio
segments. Taking advantage of the results of the base classifier, we propose a
super categories construction method using the spectral clustering. Super
classifiers corresponding to the constructed super categories are further
trained. Finally, the super classifiers are utilized to enhance the majority
voting of the base classifier by punishment voting. Experiments show that the
punishment voting obviously improves the performances on both the DCASE2017
Development dataset and the LITIS Rouen dataset.
</summary>
    <author>
      <name>Weiping Zheng</name>
    </author>
    <author>
      <name>Zhenyao Mo</name>
    </author>
    <author>
      <name>Jiantao Yi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">There is a minor mistake found in the voting process. So, We are very
  sorry about this mistake and request to withdraw this manuscript</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.04073v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04073v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.04970v1</id>
    <updated>2018-07-13T08:26:54Z</updated>
    <published>2018-07-13T08:26:54Z</published>
    <title>Analysis Acoustic Features for Acoustic Scene Classification and Score
  fusion of multi-classification systems applied to DCASE 2016 challenge</title>
    <summary>  This paper describes an acoustic scene classification method which achieved
the 4th ranking result in the IEEE AASP challenge of Detection and
Classification of Acoustic Scenes and Events 2016. In order to accomplish the
ensuing task, several methods are explored in three aspects: feature
extraction, feature transformation, and score fusion for final decision. In the
part of feature extraction, several features are investigated for effective
acoustic scene classification. For resolving the issue that the same sound can
be heard in different places, a feature transformation is applied for better
separation for classification. From these, several systems based on different
feature sets are devised for classification. The final result is determined by
fusing the individual systems. The method is demonstrated and validated by the
experiment conducted using the Challenge database.
</summary>
    <author>
      <name>Sangwook Park</name>
    </author>
    <author>
      <name>Seongkyu Mun</name>
    </author>
    <author>
      <name>Younglo Lee</name>
    </author>
    <author>
      <name>David K. Han</name>
    </author>
    <author>
      <name>Hanseok Ko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article is related to a technical report for a challenge named
  Detection and Classification of Acoustic Scenes and Events 2016</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Park, S., Mun, S., Lee, Y., and Ko, H. (2016). Score fusion of
  classification systems for acoustic scene classification. IEEE AASP Challenge
  on Detection and Classification of Acoustic Scenes and Events (DCASE)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.04970v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.04970v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05812v1</id>
    <updated>2018-07-16T12:06:13Z</updated>
    <published>2018-07-16T12:06:13Z</published>
    <title>Automatic acoustic detection of birds through deep learning: the first
  Bird Audio Detection challenge</title>
    <summary>  Assessing the presence and abundance of birds is important for monitoring
specific species as well as overall ecosystem health. Many birds are most
readily detected by their sounds, and thus passive acoustic monitoring is
highly appropriate. Yet acoustic monitoring is often held back by practical
limitations such as the need for manual configuration, reliance on example
sound libraries, low accuracy, low robustness, and limited ability to
generalise to novel acoustic conditions. Here we report outcomes from a
collaborative data challenge showing that with modern machine learning
including deep learning, general-purpose acoustic bird detection can achieve
very high retrieval rates in remote monitoring data --- with no manual
recalibration, and no pre-training of the detector for the target species or
the acoustic conditions in the target environment. Multiple methods were able
to attain performance of around 88% AUC (area under the ROC curve), much higher
performance than previous general-purpose methods. We present new acoustic
monitoring datasets, summarise the machine learning techniques proposed by
challenge teams, conduct detailed performance evaluation, and discuss how such
approaches to detection can be integrated into remote monitoring projects.
</summary>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Yannis Stylianou</name>
    </author>
    <author>
      <name>Mike Wood</name>
    </author>
    <author>
      <name>Hanna Pamuła</name>
    </author>
    <author>
      <name>Hervé Glotin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1111/2041-210X.13103</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1111/2041-210X.13103" rel="related"/>
    <link href="http://arxiv.org/abs/1807.05812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06700v1</id>
    <updated>2018-07-17T23:04:17Z</updated>
    <published>2018-07-17T23:04:17Z</published>
    <title>Psychological constraints on string-based methods for pattern discovery
  in polyphonic corpora</title>
    <summary>  Researchers often divide symbolic music corpora into contiguous sequences of
n events (called n-grams) for the purposes of pattern discovery, key finding,
classification, and prediction. What is more, several studies have reported
improved task performance when using psychologically motivated weighting
functions, which adjust the count to privilege n-grams featuring more salient
or memorable events (e.g., Krumhansl, 1990). However, these functions have yet
to appear in harmonic pattern discovery algorithms, which attempt to discover
the most recurrent chord progressions in complex polyphonic corpora. This study
examines whether psychologically-motivated weighting functions can improve
harmonic pattern discovery algorithms. Models using various n-gram selection
methods, weighting functions, and ranking algorithms attempt to discover the
most conventional closing harmonic progression in the common-practice period,
ii6-"I64"-V7-I, with the progression's mean reciprocal rank serving as an
evaluation metric for model comparison.
</summary>
    <author>
      <name>David R. W. Sears</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.06700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06899v1</id>
    <updated>2018-07-18T12:55:59Z</updated>
    <published>2018-07-18T12:55:59Z</published>
    <title>Deep neural network based speech separation optimizing an objective
  estimator of intelligibility for low latency applications</title>
    <summary>  Mean square error (MSE) has been the preferred choice as loss function in the
current deep neural network (DNN) based speech separation techniques. In this
paper, we propose a new cost function with the aim of optimizing the extended
short time objective intelligibility (ESTOI) measure. We focus on applications
where low algorithmic latency ($\leq 10$ ms) is important. We use long
short-term memory networks (LSTM) and evaluate our proposed approach on four
sets of two-speaker mixtures from extended Danish hearing in noise (HINT)
dataset. We show that the proposed loss function can offer improved or at par
objective intelligibility (in terms of ESTOI) compared to an MSE optimized
baseline while resulting in lower objective separation performance (in terms of
the source to distortion ratio (SDR)). We then proceed to propose an approach
where the network is first initialized with weights optimized for MSE criterion
and then trained with the proposed ESTOI loss criterion. This approach
mitigates some of the losses in objective separation performance while
preserving the gains in objective intelligibility.
</summary>
    <author>
      <name>Gaurav Naithani</name>
    </author>
    <author>
      <name>Joonas Nikunen</name>
    </author>
    <author>
      <name>Lars Bramsløw</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at International Workshop on Acoustic Signal Enhancement
  (IWAENC) 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.06899v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06899v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06972v2</id>
    <updated>2018-10-26T15:19:02Z</updated>
    <published>2018-07-17T14:03:55Z</published>
    <title>Data-Efficient Weakly Supervised Learning for Low-Resource Audio Event
  Detection Using Deep Learning</title>
    <summary>  We propose a method to perform audio event detection under the common
constraint that only limited training data are available. In training a deep
learning system to perform audio event detection, two practical problems arise.
Firstly, most datasets are "weakly labelled" having only a list of events
present in each recording without any temporal information for training.
Secondly, deep neural networks need a very large amount of labelled training
data to achieve good quality performance, yet in practice it is difficult to
collect enough samples for most classes of interest. In this paper, we propose
a data-efficient training of a stacked convolutional and recurrent neural
network. This neural network is trained in a multi instance learning setting
for which we introduce a new loss function that leads to improved training
compared to the usual approaches for weakly supervised learning. We
successfully test our approach on two low-resource datasets that lack temporal
labels.
</summary>
    <author>
      <name>Veronica Morfi</name>
    </author>
    <author>
      <name>Dan Stowell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:1807.03697</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.06972v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06972v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07959v1</id>
    <updated>2018-07-20T01:20:06Z</updated>
    <published>2018-07-20T01:20:06Z</published>
    <title>A Fully Convolutional Neural Network Approach to End-to-End Speech
  Enhancement</title>
    <summary>  This paper will describe a novel approach to the cocktail party problem that
relies on a fully convolutional neural network (FCN) architecture. The FCN
takes noisy audio data as input and performs nonlinear, filtering operations to
produce clean audio data of the target speech at the output. Our method learns
a model for one specific speaker, and is then able to extract that speakers
voice from babble background noise. Results from experimentation indicate the
ability to generalize to new speakers and robustness to new noise environments
of varying signal-to-noise ratios. A potential application of this method would
be for use in hearing aids. A pre-trained model could be quickly fine tuned for
an individuals family members and close friends, and deployed onto a hearing
aid to assist listeners in noisy environments.
</summary>
    <author>
      <name>Frank Longueira</name>
    </author>
    <author>
      <name>Sam Keene</name>
    </author>
    <link href="http://arxiv.org/abs/1807.07959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08636v1</id>
    <updated>2018-07-23T14:18:56Z</updated>
    <published>2018-07-23T14:18:56Z</published>
    <title>Auto-adaptive Resonance Equalization using Dilated Residual Networks</title>
    <summary>  In music and audio production, attenuation of spectral resonances is an
important step towards a technically correct result. In this paper we present a
two-component system to automate the task of resonance equalization. The first
component is a dynamic equalizer that automatically detects resonances and
offers to attenuate them by a user-specified factor. The second component is a
deep neural network that predicts the optimal attenuation factor based on the
windowed audio. The network is trained and validated on empirical data gathered
from an experiment in which sound engineers choose their preferred attenuation
factors for a set of tracks. We test two distinct network architectures for the
predictive model and find that a dilated residual network operating directly on
the audio signal is on a par with a network architecture that requires a prior
audio feature extraction stage. Both architectures predict human-preferred
resonance attenuation factors significantly better than a baseline approach.
</summary>
    <author>
      <name>Maarten Grachten</name>
    </author>
    <author>
      <name>Emmanuel Deruty</name>
    </author>
    <author>
      <name>Alexandre Tanguy</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 20th ISMIR Conference, Delft, Netherlands,
  November 4-8, 2019. Pp. 405-411.
  https://archives.ismir.net/ismir2019/paper/000048.pdf</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.08636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08869v2</id>
    <updated>2019-07-12T21:16:04Z</updated>
    <published>2018-07-24T01:01:35Z</published>
    <title>Joint Time-Frequency Scattering</title>
    <summary>  In time series classification and regression, signals are typically mapped
into some intermediate representation used for constructing models. Since the
underlying task is often insensitive to time shifts, these representations are
required to be time-shift invariant. We introduce the joint time-frequency
scattering transform, a time-shift invariant representation which characterizes
the multiscale energy distribution of a signal in time and frequency. It is
computed through wavelet convolutions and modulus non-linearities and may
therefore be implemented as a deep convolutional neural network whose filters
are not learned but calculated from wavelets. We consider the progression from
mel-spectrograms to time scattering and joint time-frequency scattering
transforms, illustrating the relationship between increased discriminability
and refinements of convolutional network architectures. The suitability of the
joint time-frequency scattering transform for time-shift invariant
characterization of time series is demonstrated through applications to chirp
signals and audio synthesis experiments. The proposed transform also obtains
state-of-the-art results on several audio classification tasks, outperforming
time scattering transforms and achieving accuracies comparable to those of
fully learned networks.
</summary>
    <author>
      <name>Joakim Andén</name>
    </author>
    <author>
      <name>Vincent Lostanlen</name>
    </author>
    <author>
      <name>Stéphane Mallat</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSP.2019.2918992</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSP.2019.2918992" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 10 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Signal Processing, vol. 67, no. 14, pp.
  3704-3718, July 15, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1807.08869v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08869v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09902v3</id>
    <updated>2018-10-07T02:25:28Z</updated>
    <published>2018-07-26T00:30:54Z</published>
    <title>General-purpose Tagging of Freesound Audio with AudioSet Labels: Task
  Description, Dataset, and Baseline</title>
    <summary>  This paper describes Task 2 of the DCASE 2018 Challenge, titled
"General-purpose audio tagging of Freesound content with AudioSet labels". This
task was hosted on the Kaggle platform as "Freesound General-Purpose Audio
Tagging Challenge". The goal of the task is to build an audio tagging system
that can recognize the category of an audio clip from a subset of 41 diverse
categories drawn from the AudioSet Ontology. We present the task, the dataset
prepared for the competition, and a baseline system.
</summary>
    <author>
      <name>Eduardo Fonseca</name>
    </author>
    <author>
      <name>Manoj Plakal</name>
    </author>
    <author>
      <name>Frederic Font</name>
    </author>
    <author>
      <name>Daniel P. W. Ellis</name>
    </author>
    <author>
      <name>Xavier Favory</name>
    </author>
    <author>
      <name>Jordi Pons</name>
    </author>
    <author>
      <name>Xavier Serra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Camera ready for DCASE Workshop 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09902v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09902v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10501v1</id>
    <updated>2018-07-27T09:15:50Z</updated>
    <published>2018-07-27T09:15:50Z</published>
    <title>Large-Scale Weakly Labeled Semi-Supervised Sound Event Detection in
  Domestic Environments</title>
    <summary>  This paper presents DCASE 2018 task 4. The task evaluates systems for the
large-scale detection of sound events using weakly labeled data (without time
boundaries). The target of the systems is to provide not only the event class
but also the event time boundaries given that multiple events can be present in
an audio recording. Another challenge of the task is to explore the possibility
to exploit a large amount of unbalanced and unlabeled training data together
with a small weakly labeled training set to improve system performance. The
data are Youtube video excerpts from domestic context which have many
applications such as ambient assisted living. The domain was chosen due to the
scientific challenges (wide variety of sounds, time-localized events.. .) and
potential industrial applications .
</summary>
    <author>
      <name>Romain Serizel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MULTISPEECH</arxiv:affiliation>
    </author>
    <author>
      <name>Nicolas Turpault</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">MULTISPEECH</arxiv:affiliation>
    </author>
    <author>
      <name>Hamid Eghbal-Zadeh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTI</arxiv:affiliation>
    </author>
    <author>
      <name>Ankit Parag Shah</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTI</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1807.10501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11161v1</id>
    <updated>2018-07-30T03:48:04Z</updated>
    <published>2018-07-30T03:48:04Z</published>
    <title>Lead Sheet Generation and Arrangement by Conditional Generative
  Adversarial Network</title>
    <summary>  Research on automatic music generation has seen great progress due to the
development of deep neural networks. However, the generation of
multi-instrument music of arbitrary genres still remains a challenge. Existing
research either works on lead sheets or multi-track piano-rolls found in MIDIs,
but both musical notations have their limits. In this work, we propose a new
task called lead sheet arrangement to avoid such limits. A new recurrent
convolutional generative model for the task is proposed, along with three new
symbolic-domain harmonic features to facilitate learning from unpaired lead
sheets and MIDIs. Our model can generate lead sheets and their arrangements of
eight-bar long. Audio samples of the generated result can be found at
https://drive.google.com/open?id=1c0FfODTpudmLvuKBbc23VBCgQizY6-Rk
</summary>
    <author>
      <name>Hao-Min Liu</name>
    </author>
    <author>
      <name>Yi-Hsuan Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures and 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.11161v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11161v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.11298v1</id>
    <updated>2018-07-30T11:51:59Z</updated>
    <published>2018-07-30T11:51:59Z</published>
    <title>Harmonic-Percussive Source Separation with Deep Neural Networks and
  Phase Recovery</title>
    <summary>  Harmonic/percussive source separation (HPSS) consists in separating the
pitched instruments from the percussive parts in a music mixture. In this
paper, we propose to apply the recently introduced Masker-Denoiser with twin
networks (MaD TwinNet) system to this task. MaD TwinNet is a deep learning
architecture that has reached state-of-the-art results in monaural singing
voice separation. Herein, we propose to apply it to HPSS by using it to
estimate the magnitude spectrogram of the percussive source. Then, we retrieve
the complex-valued short-time Fourier transform of the sources by means of a
phase recovery algorithm, which minimizes the reconstruction error and enforces
the phase of the harmonic part to follow a sinusoidal phase model. Experiments
conducted on realistic music mixtures show that this novel separation system
outperforms the previous state-of-the art kernel additive model approach.
</summary>
    <author>
      <name>Konstantinos Drossos</name>
    </author>
    <author>
      <name>Paul Magron</name>
    </author>
    <author>
      <name>Stylianos Ioannis Mimilakis</name>
    </author>
    <author>
      <name>Tuomas Virtanen</name>
    </author>
    <link href="http://arxiv.org/abs/1807.11298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.11298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00060v1</id>
    <updated>2018-07-31T20:12:15Z</updated>
    <published>2018-07-31T20:12:15Z</published>
    <title>DNN driven Speaker Independent Audio-Visual Mask Estimation for Speech
  Separation</title>
    <summary>  Human auditory cortex excels at selectively suppressing background noise to
focus on a target speaker. The process of selective attention in the brain is
known to contextually exploit the available audio and visual cues to better
focus on target speaker while filtering out other noises. In this study, we
propose a novel deep neural network (DNN) based audiovisual (AV) mask
estimation model. The proposed AV mask estimation model contextually integrates
the temporal dynamics of both audio and noise-immune visual features for
improved mask estimation and speech separation. For optimal AV features
extraction and ideal binary mask (IBM) estimation, a hybrid DNN architecture is
exploited to leverages the complementary strengths of a stacked long short term
memory (LSTM) and convolution LSTM network. The comparative simulation results
in terms of speech quality and intelligibility demonstrate significant
performance improvement of our proposed AV mask estimation model as compared to
audio-only and visual-only mask estimation approaches for both speaker
dependent and independent scenarios.
</summary>
    <author>
      <name>Mandar Gogate</name>
    </author>
    <author>
      <name>Ahsan Adeel</name>
    </author>
    <author>
      <name>Ricard Marxer</name>
    </author>
    <author>
      <name>Jon Barker</name>
    </author>
    <author>
      <name>Amir Hussain</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.21437/Interspeech.2018-2516</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.21437/Interspeech.2018-2516" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for Interspeech 2018, 5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5; I.4; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00959v2</id>
    <updated>2020-02-02T09:21:31Z</updated>
    <published>2018-08-02T13:45:37Z</published>
    <title>Histogram Transform-based Speaker Identification</title>
    <summary>  A novel text-independent speaker identification (SI) method is proposed. This
method uses the Mel-frequency Cepstral coefficients (MFCCs) and the dynamic
information among adjacent frames as feature sets to capture speaker's
characteristics. In order to utilize dynamic information, we design super-MFCCs
features by cascading three neighboring MFCCs frames together. The probability
density function (PDF) of these super-MFCCs features is estimated by the
recently proposed histogram transform~(HT) method, which generates more
training data by random transforms to realize the histogram PDF estimation and
recedes the commonly occurred discontinuity problem in multivariate histograms
computing. Compared to the conventional PDF estimation methods, such as
Gaussian mixture models, the HT model shows promising improvement in the SI
performance.
</summary>
    <author>
      <name>Zhanyu Ma</name>
    </author>
    <author>
      <name>Hong Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00959v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00959v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01603v1</id>
    <updated>2018-08-05T12:13:47Z</updated>
    <published>2018-08-05T12:13:47Z</published>
    <title>Simulating Raga Notes with a Markov Chain of Order 1-2</title>
    <summary>  Semi Natural Algorithmic composition (SNCA) is the technique of using
algorithms to create music note sequences in computer with the understanding
that how to render them would be decided by the composer. In our approach we
are proposing an SNCA2 algorithm (extension of SNCA algorithm) with an
illustrative example in Raga Bageshree. For this, Transition probability matrix
(tpm) was created for the note sequences of Raga Bageshree, then first order
Markov chain (using SNCA) and second order Markov chain (using SNCA2)
simulations were performed for generating arbitrary sequences of notes of Raga
Bageshree. The choice between first and second order Markov model, is best left
to the composer who has to decide how to render these music notes sequences. We
have confirmed that Markov chain of order of three and above are not promising,
as the tpm of these become sparse matrices.
</summary>
    <author>
      <name>Devashish Gosain</name>
    </author>
    <author>
      <name>Soubhik Chakraborty</name>
    </author>
    <author>
      <name>Mohit Sajwan</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.01935v1</id>
    <updated>2018-08-06T14:40:31Z</updated>
    <published>2018-08-06T14:40:31Z</published>
    <title>Audio Tagging With Connectionist Temporal Classification Model Using
  Sequential Labelled Data</title>
    <summary>  Audio tagging aims to predict one or several labels in an audio clip. Many
previous works use weakly labelled data (WLD) for audio tagging, where only
presence or absence of sound events is known, but the order of sound events is
unknown. To use the order information of sound events, we propose sequential
labelled data (SLD), where both the presence or absence and the order
information of sound events are known. To utilize SLD in audio tagging, we
propose a Convolutional Recurrent Neural Network followed by a Connectionist
Temporal Classification (CRNN-CTC) objective function to map from an audio clip
spectrogram to SLD. Experiments show that CRNN-CTC obtains an Area Under Curve
(AUC) score of 0.986 in audio tagging, outperforming the baseline CRNN of 0.908
and 0.815 with Max Pooling and Average Pooling, respectively. In addition, we
show CRNN-CTC has the ability to predict the order of sound events in an audio
clip.
</summary>
    <author>
      <name>Yuanbo Hou</name>
    </author>
    <author>
      <name>Qiuqiang Kong</name>
    </author>
    <author>
      <name>Shengchen Li</name>
    </author>
    <link href="http://arxiv.org/abs/1808.01935v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.01935v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.03715v1</id>
    <updated>2018-08-10T21:53:51Z</updated>
    <published>2018-08-10T21:53:51Z</published>
    <title>This Time with Feeling: Learning Expressive Musical Performance</title>
    <summary>  Music generation has generally been focused on either creating scores or
interpreting them. We discuss differences between these two problems and
propose that, in fact, it may be valuable to work in the space of direct $\it
performance$ generation: jointly predicting the notes $\it and$ $\it also$
their expressive timing and dynamics. We consider the significance and
qualities of the data set needed for this. Having identified both a problem
domain and characteristics of an appropriate data set, we show an LSTM-based
recurrent network model that subjectively performs quite well on this task.
Critically, we provide generated examples. We also include feedback from
professional composers and musicians about some of these examples.
</summary>
    <author>
      <name>Sageev Oore</name>
    </author>
    <author>
      <name>Ian Simon</name>
    </author>
    <author>
      <name>Sander Dieleman</name>
    </author>
    <author>
      <name>Douglas Eck</name>
    </author>
    <author>
      <name>Karen Simonyan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Includes links to urls for audio samples</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.03715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.03715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
