<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acs.MM%26id_list%3D%26start%3D0%26max_results%3D1100" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:cs.MM&amp;id_list=&amp;start=0&amp;max_results=1100</title>
  <id>http://arxiv.org/api/VSVzpUUQasVmwkMn4Ym6xCnUNjk</id>
  <updated>2025-05-27T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">7917</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1100</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/0809.0524v1</id>
    <updated>2008-09-02T21:48:28Z</updated>
    <published>2008-09-02T21:48:28Z</published>
    <title>Computer Art in the Former Soviet Bloc</title>
    <summary>  Documents early computer art in the Soviet bloc and describes Marxist art
theory.
</summary>
    <author>
      <name>Eric Engle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.0524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.0524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.0023v1</id>
    <updated>2011-04-29T21:34:21Z</updated>
    <published>2011-04-29T21:34:21Z</published>
    <title>Survey of Cognitive Radio Techniques in Wireless Network</title>
    <summary>  In this report, I surveyed the cognitive radio technique in wireless
networks. Researched several kinds of cognitive techniques about their
advantages and disadvantages.
</summary>
    <author>
      <name>Lu Lu</name>
    </author>
    <link href="http://arxiv.org/abs/1105.0023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.0023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.7337v1</id>
    <updated>2014-07-28T07:04:04Z</updated>
    <published>2014-07-28T07:04:04Z</published>
    <title>A Digital Watermarking Approach Based on DCT Domain Combining QR Code
  and Chaotic Theory</title>
    <summary>  This paper proposes a robust watermarking approach based on Discrete Cosine
Transform domain that combines Quick Response Code and chaotic system.
</summary>
    <author>
      <name>Qingbo Kang</name>
    </author>
    <author>
      <name>Ke Li</name>
    </author>
    <author>
      <name>Jichun Yang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/WOCN.2014.6923098</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/WOCN.2014.6923098" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.7337v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.7337v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01081v1</id>
    <updated>2018-06-04T12:49:03Z</updated>
    <published>2018-06-04T12:49:03Z</published>
    <title>Sloth Search System at the Video Browser Showdown 2018 - Final Notes</title>
    <summary>  This short paper provides further details of the Sloth Search System, which
was developed by the NECTEC team for the Video Browser Showdown (VBS) 2018.
</summary>
    <author>
      <name>Nattachai Watcharapinchai</name>
    </author>
    <author>
      <name>Sitapa Rujikietgumjorn</name>
    </author>
    <author>
      <name>Sanparith Marukatat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final note paper about the Sloth Search System at the VBS 2018
  competition</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.11426v1</id>
    <updated>2022-09-23T05:58:22Z</updated>
    <published>2022-09-23T05:58:22Z</published>
    <title>The Beauty of Repetition in Machine Composition Scenarios</title>
    <summary>  Repetition, a basic form of artistic creation, appears in most musical works
and delivers enthralling aesthetic experiences.
</summary>
    <author>
      <name>Zhejing Hu</name>
    </author>
    <author>
      <name>Xiao Ma</name>
    </author>
    <author>
      <name>Yan Liu</name>
    </author>
    <author>
      <name>Gong Chen</name>
    </author>
    <author>
      <name>Yongxu Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3503161.3548130</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3503161.3548130" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published on ACM Multimedia 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.11426v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.11426v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.12986v1</id>
    <updated>2023-09-08T07:16:45Z</updated>
    <published>2023-09-08T07:16:45Z</published>
    <title>A survey of manifold learning and its applications for multimedia</title>
    <summary>  Manifold learning is an emerging research domain of machine learning. In this
work, we give an introduction into manifold learning and how it is employed for
important application fields in multimedia.
</summary>
    <author>
      <name>Hannes Fassold</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for ICVSP 2023 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.12986v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.12986v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.15683v1</id>
    <updated>2024-12-13T20:09:27Z</updated>
    <published>2024-12-13T20:09:27Z</published>
    <title>Results of the 2024 Video Browser Showdown</title>
    <summary>  This report presents the results of the 13th Video Browser Showdown, held at
the 2024 International Conference on Multimedia Modeling on the 29th of January
2024 in Amsterdam, the Netherlands.
</summary>
    <author>
      <name>Luca Rossetto</name>
    </author>
    <author>
      <name>Klaus Schoeffmann</name>
    </author>
    <author>
      <name>Cathal Gurrin</name>
    </author>
    <author>
      <name>Jakub Lokoƒç</name>
    </author>
    <author>
      <name>Werner Bailer</name>
    </author>
    <link href="http://arxiv.org/abs/2502.15683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.15683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0409059v1</id>
    <updated>2004-09-30T19:08:55Z</updated>
    <published>2004-09-30T19:08:55Z</published>
    <title>From Digital Television to Internet?</title>
    <summary>  This paper provides a general technical overview of the Multimedia Home
Platform (MHP) specifications. MHP is a generic interface between digital
applications and user machines, whether they happen to be set top boxes,
digital TV sets or Multimedia PC's. MHP extends the DVB open standards.
Addressed are MHP architexture, System core and MHP Profiles.
</summary>
    <author>
      <name>Vita Hinze-Hoare</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0409059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0409059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0608119v1</id>
    <updated>2006-08-30T03:22:06Z</updated>
    <published>2006-08-30T03:22:06Z</published>
    <title>Security Analysis of A Chaos-based Image Encryption Algorithm</title>
    <summary>  The security of Fridrich Image Encryption Algorithm against brute-force
attack, statistical attack, known-plaintext attack and select-plaintext attack
is analyzed by investigating the properties of the involved chaotic maps and
diffusion functions. Based on the given analyses, some means are proposed to
strengthen the overall performance of the focused cryptosystem.
</summary>
    <author>
      <name>Shiguo Lian</name>
    </author>
    <author>
      <name>Jinsheng Sun</name>
    </author>
    <author>
      <name>Zhiquan Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages,4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0608119v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0608119v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4821v1</id>
    <updated>2007-10-25T12:04:15Z</updated>
    <published>2007-10-25T12:04:15Z</published>
    <title>Multimedia Applications of Multiprocessor Systems-on-Chips</title>
    <summary>  This paper surveys the characteristics of multimedia systems. Multimedia
applications today are dominated by compression and decompression, but
multimedia devices must also implement many other functions such as security
and file management. We introduce some basic concepts of multimedia algorithms
and the larger set of functions that multimedia systems-on-chips must
implement.
</summary>
    <author>
      <name>Wayne Wolf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.3538v1</id>
    <updated>2008-05-22T20:43:38Z</updated>
    <published>2008-05-22T20:43:38Z</published>
    <title>Covert Channels in SIP for VoIP signalling</title>
    <summary>  In this paper, we evaluate available steganographic techniques for SIP
(Session Initiation Protocol) that can be used for creating covert channels
during signaling phase of VoIP (Voice over IP) call. Apart from characterizing
existing steganographic methods we provide new insights by introducing new
techniques. We also estimate amount of data that can be transferred in
signalling messages for typical IP telephony call.
</summary>
    <author>
      <name>Wojciech Mazurczyk</name>
    </author>
    <author>
      <name>Krzysztof Szczypiorski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-540-69403-8_9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-540-69403-8_9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0805.3538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.3538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.5154v1</id>
    <updated>2008-09-30T09:56:35Z</updated>
    <published>2008-09-30T09:56:35Z</published>
    <title>An Export Architecture for a Multimedia Authoring Environment</title>
    <summary>  In this paper, we propose an export architecture that provides a clear
separation of authoring services from publication services. We illustrate this
architecture with the LimSee3 authoring tool and several standard publication
formats: Timesheets, SMIL, and XHTML.
</summary>
    <author>
      <name>Jan Mik√°c</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rh√¥ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</arxiv:affiliation>
    </author>
    <author>
      <name>C√©cile Roisin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rh√¥ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</arxiv:affiliation>
    </author>
    <author>
      <name>Bao Le Duc</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UPMC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans DocEng'08 (2008) 28-31</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0809.5154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.5154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.4314v1</id>
    <updated>2009-03-25T12:16:29Z</updated>
    <published>2009-03-25T12:16:29Z</published>
    <title>Virtual Reality</title>
    <summary>  This paper is focused on the presentation of Virtual Reality principles
together with the main implementation methods and techniques. An overview of
the main development directions is included.
</summary>
    <author>
      <name>Dan L. Lacrama</name>
    </author>
    <author>
      <name>Dorina Fera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, exposed on 4th International Conferences "Actualities and
  Perspectives on Hardware and Software" - APHS2007, Timisoara, Romania</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series V (2007), 137-144</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0903.4314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.4314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.3082v1</id>
    <updated>2009-08-21T09:46:07Z</updated>
    <published>2009-08-21T09:46:07Z</published>
    <title>Component based platform for multimedia applications</title>
    <summary>  We propose a platform for distributed multimedia applications which
simplifies the development process and at the same time ensures application
portability, flexibility and performance. The platform is implemented using the
Netscape Portable Runtime (NSPR) and the Cross-Platform Component Object Model
(XPCOM).
</summary>
    <author>
      <name>Ovidiu Ratoi</name>
    </author>
    <author>
      <name>Piroska Haller</name>
    </author>
    <author>
      <name>Ioan Salomie</name>
    </author>
    <author>
      <name>Bela Genge</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7th IEEE RoEduNet International Conference, Cluj-Napoca, Romania,
  Aug. 2008, pp. 40-43, ISBN 978-973-662-393-6</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.3082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.3082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.4880v1</id>
    <updated>2009-12-24T15:28:33Z</updated>
    <published>2009-12-24T15:28:33Z</published>
    <title>How Do Interactive Virtual Operas Shift Relationships between Music,
  Text and Image?</title>
    <summary>  In this paper we present the new genre of interactive operas implemented on
personal computers. They differ from traditional ones not only because they are
virtual, but mainly because they offer to composers and listeners new
perspectives of combinations and interactions between music, text and visual
aspects.
</summary>
    <author>
      <name>Alain Bonardi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">STMS</arxiv:affiliation>
    </author>
    <author>
      <name>Francis Rousseaux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">STMS, CRESTIC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Language, vision and music, John Benjamins Publishing Company
  (Ed.) (2002) 285-294</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.4880v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.4880v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.0026v1</id>
    <updated>2012-12-31T22:42:02Z</updated>
    <published>2012-12-31T22:42:02Z</published>
    <title>Bounding Lossy Compression using Lossless Codes at Reduced Precision</title>
    <summary>  An alternative approach to two-part 'critical compression' is presented.
Whereas previous results were based on summing a lossless code at reduced
precision with a lossy-compressed error or noise term, the present approach
uses a similar lossless code at reduced precision to establish absolute bounds
which constrain an arbitrary lossy data compression algorithm applied to the
original data.
</summary>
    <author>
      <name>John Scoville</name>
    </author>
    <link href="http://arxiv.org/abs/1301.0026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.4883v1</id>
    <updated>2014-09-17T07:33:46Z</updated>
    <published>2014-09-17T07:33:46Z</published>
    <title>Developing a Video Steganography Toolkit</title>
    <summary>  Although techniques for separate image and audio steganography are widely
known, relatively little has been described concerning the hiding of
information within video streams ("video steganography"). In this paper we
review the current state of the art in this field, and describe the key issues
we have encountered in developing a practical video steganography system. A
supporting video is also available online at
http://www.youtube.com/watch?v=YhnlHmZolRM
</summary>
    <author>
      <name>James Ridgway</name>
    </author>
    <author>
      <name>Mike Stannett</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.4883v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.4883v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.6656v1</id>
    <updated>2014-10-24T11:52:00Z</updated>
    <published>2014-10-24T11:52:00Z</published>
    <title>StegExpose - A Tool for Detecting LSB Steganography</title>
    <summary>  Steganalysis tools play an important part in saving time and providing new
angles of attack for forensic analysts. StegExpose is a solution designed for
use in the real world, and is able to analyse images for LSB steganography in
bulk using proven attacks in a time efficient manner. When steganalytic methods
are combined intelligently, they are able generate even more accurate results.
This is the prime focus of StegExpose.
</summary>
    <author>
      <name>Benedikt Boehm</name>
    </author>
    <link href="http://arxiv.org/abs/1410.6656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.6656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.06103v1</id>
    <updated>2015-02-21T13:19:34Z</updated>
    <published>2015-02-21T13:19:34Z</published>
    <title>Compressive sensing based velocity estimation in video data</title>
    <summary>  This paper considers the use of compressive sensing based algorithms for
velocity estimation of moving vehicles. The procedure is based on sparse
reconstruction algorithms combined with time-frequency analysis applied to
video data. This algorithm provides an accurate estimation of object's velocity
even in the case of a very reduced number of available video frames. The
influence of crucial parameters is analysed for different types of moving
vehicles.
</summary>
    <author>
      <name>Ana Miletic</name>
    </author>
    <author>
      <name>Nemanja Ivanovic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.06103v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.06103v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.04354v1</id>
    <updated>2015-11-04T12:21:04Z</updated>
    <published>2015-11-04T12:21:04Z</published>
    <title>A proposal project for a blind image quality assessment by learning
  distortions from the full reference image quality assessments</title>
    <summary>  This short paper presents a perspective plan to build a null reference image
quality assessment. Its main goal is to deliver both the objective score and
the distortion map for a given distorted image without the knowledge of its
reference image.
</summary>
    <author>
      <name>St√©fane Paris</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">QGAR</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/QoMEX.2012.6263876</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/QoMEX.2012.6263876" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Workshop on Quality of Multimedia Experience, 2012,
  Melbourne, Australia</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.04354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.04354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04930v1</id>
    <updated>2016-05-16T20:12:02Z</updated>
    <published>2016-05-16T20:12:02Z</published>
    <title>Daala: A Perceptually-Driven Still Picture Codec</title>
    <summary>  Daala is a new royalty-free video codec based on perceptually-driven coding
techniques. We explore using its keyframe format for still picture coding and
show how it has improved over the past year. We believe the technology used in
Daala could be the basis of an excellent, royalty-free image format.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Nathan E. Egge</name>
    </author>
    <author>
      <name>Thomas Daede</name>
    </author>
    <author>
      <name>Timothy B. Terriberry</name>
    </author>
    <author>
      <name>Christopher Montgomery</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for ICIP 2016, 5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04930v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04930v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06152v1</id>
    <updated>2016-06-20T14:52:47Z</updated>
    <published>2016-06-20T14:52:47Z</published>
    <title>A Note on Efficiency of Downsampling and Color Transformation in Image
  Quality Assessment</title>
    <summary>  Several existing and successful full reference image quality assessment (IQA)
models use linear color transformation and downsampling before measuring
similarity or quality of images. This paper indicates to the right order of
these two procedures and that the existing models have not chosen the more
efficient approach. In addition, efficiency of these metrics is not compared in
a fair basis in the literature.
</summary>
    <author>
      <name>Hossein Ziaei Nafchi</name>
    </author>
    <author>
      <name>Mohamed Cheriet</name>
    </author>
    <link href="http://arxiv.org/abs/1606.06152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01955v1</id>
    <updated>2016-10-06T17:12:37Z</updated>
    <published>2016-10-06T17:12:37Z</published>
    <title>MoveSteg: A Method of Network Steganography Detection</title>
    <summary>  This article presents a new method for detecting a source point of time based
network steganography - MoveSteg. A steganography carrier could be an example
of multimedia stream made with packets. These packets are then delayed
intentionally to send hidden information using time based steganography
methods. The presented analysis describes a method that allows finding the
source of steganography stream in network that is under our management.
</summary>
    <author>
      <name>Krzysztof Szczypiorski</name>
    </author>
    <author>
      <name>Tomasz Tyl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.01955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.05718v3</id>
    <updated>2017-06-21T08:13:53Z</updated>
    <published>2017-02-19T08:21:20Z</published>
    <title>Perceptual Compressive Sensing based on Contrast Sensitivity Function:
  Can we avoid non-visible redundancies acquisition?</title>
    <summary>  In this paper, we propose a novel CS approach in which the acquisition of
non-visible information is also avoided.
</summary>
    <author>
      <name>Seyed Hamid Safavi</name>
    </author>
    <author>
      <name>Farah Torkamani-Azar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in 25'th Iranian Conference on Electrical
  Engineering (ICEE2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.05718v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.05718v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.07788v1</id>
    <updated>2017-05-22T14:56:49Z</updated>
    <published>2017-05-22T14:56:49Z</published>
    <title>StegIbiza: Steganography in Club Music Implemented in Python</title>
    <summary>  This paper introduces the implementation of steganography method called
StegIbiza, which uses tempo modulation as hidden message carrier. With the use
of Python scripting language, a bit string was encoded and decoded using WAV
and MP3 files. Once the message was hidden into a music files, an internet
radio was created to evaluate broadcast possibilities. No dedicated music or
signal processing equipment was used in this StegIbiza implementation
</summary>
    <author>
      <name>Krzysztof Szczypiorski</name>
    </author>
    <author>
      <name>Wojciech Zydecki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.07788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.07788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.01482v1</id>
    <updated>2018-10-02T20:02:20Z</updated>
    <published>2018-10-02T20:02:20Z</published>
    <title>Diversifying Music Recommendations</title>
    <summary>  We compare submodular and Jaccard methods to diversify Amazon Music
recommendations. Submodularity significantly improves recommendation quality
and user engagement. Unlike the Jaccard method, our submodular approach
incorporates item relevance score within its optimization function, and
produces a relevant and uniformly diverse set.
</summary>
    <author>
      <name>Houssam Nassif</name>
    </author>
    <author>
      <name>Kemal Oral Cansizlar</name>
    </author>
    <author>
      <name>Mitchell Goodman</name>
    </author>
    <author>
      <name>SVN Vishwanathan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning for Music Discovery Workshop at the 33rd
  International Conference on Machine Learning (ICML'16), New York, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.01482v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.01482v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.11801v1</id>
    <updated>2018-10-28T11:49:24Z</updated>
    <published>2018-10-28T11:49:24Z</published>
    <title>Image Super-Resolution Using TV Priori Guided Convolutional Network</title>
    <summary>  We proposed a TV priori information guided deep learning method for single
image super-resolution(SR). The new alogorithm up-sample method based on TV
priori, new learning method and neural networks architecture are embraced in
our TV guided priori Convolutional Neural Network which diretcly learns an end
to end mapping between the low level to high level images.
</summary>
    <author>
      <name>Bo Fu</name>
    </author>
    <author>
      <name>Yi Li</name>
    </author>
    <author>
      <name>Xianghai Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is underviewring in Journal of Pattern Recognition Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.11801v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.11801v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.10647v1</id>
    <updated>2019-02-27T17:29:16Z</updated>
    <published>2019-02-27T17:29:16Z</published>
    <title>Deep Learning-based Concept Detection in vitrivr at the Video Browser
  Showdown 2019 - Final Notes</title>
    <summary>  This paper presents an after-the-fact summary of the participation of the
vitrivr system to the 2019 Video Browser Showdown. Analogously to last year's
report, the focus of this paper lies on additions made since the original
publication and the system's performance during the competition.
</summary>
    <author>
      <name>Luca Rossetto</name>
    </author>
    <author>
      <name>Mahnaz Amiri Parian</name>
    </author>
    <author>
      <name>Ralph Gasser</name>
    </author>
    <author>
      <name>Ivan Giangreco</name>
    </author>
    <author>
      <name>Silvan Heller</name>
    </author>
    <author>
      <name>Heiko Schuldt</name>
    </author>
    <link href="http://arxiv.org/abs/1902.10647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.10647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.01301v1</id>
    <updated>2021-04-03T03:23:06Z</updated>
    <published>2021-04-03T03:23:06Z</published>
    <title>Multimedia Technology Applications and Algorithms: A Survey</title>
    <summary>  Multimedia related research and development has evolved rapidly in the last
few years with advancements in hardware, software and network infrastructures.
As a result, multimedia has been integrated into domains like Healthcare and
Medicine, Human facial feature extraction and tracking, pose recognition,
disparity estimation, etc. This survey gives an overview of the various
multimedia technologies and algorithms developed in the domains mentioned.
</summary>
    <author>
      <name>Palak Tiwary</name>
    </author>
    <author>
      <name>Sanjida Ahmed</name>
    </author>
    <link href="http://arxiv.org/abs/2104.01301v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.01301v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.03532v1</id>
    <updated>2022-03-12T08:39:46Z</updated>
    <published>2022-03-12T08:39:46Z</published>
    <title>Literature Review on Image Compression, Tracking, Adaptive Training and
  3D Data Transmission</title>
    <summary>  The literature review presented below on Image Compression, Transmission of
3D data over wireless networks and tracking of objects is the in depth study of
Research Papers done in Multimedia lab. Most of the papers presented in this
literature review have tackled the problems present in the conventional system
and offered an optimal and practical solution.
</summary>
    <author>
      <name>Sravanti Chinta</name>
    </author>
    <author>
      <name>Rajat Bothra Jain</name>
    </author>
    <link href="http://arxiv.org/abs/2204.03532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.03532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.04958v1</id>
    <updated>2015-03-17T09:09:48Z</updated>
    <published>2015-03-17T09:09:48Z</published>
    <title>The blind detection for palette image watermarking without changing the
  color</title>
    <summary>  To hide a binary pattern in the palette image a steganographic scheme with
blind detection is considered. The embedding algorithm uses the Lehmer code by
palette color permutations for which the cover image palette is generally
required. The found transformation between the palette and RGB images allows to
extract the hidden data without any cover work.
</summary>
    <author>
      <name>V. N. Gorbachev</name>
    </author>
    <author>
      <name>E. M. Kaynarova</name>
    </author>
    <author>
      <name>I. K. Metelev</name>
    </author>
    <author>
      <name>O. V. Pavlovskaya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.04958v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.04958v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02371v1</id>
    <updated>2018-05-07T07:06:19Z</updated>
    <published>2018-05-07T07:06:19Z</published>
    <title>Competitive Video Retrieval with vitrivr at the Video Browser Showdown
  2018 - Final Notes</title>
    <summary>  This paper presents an after-the-fact summary of the participation of the
vitrivr system to the 2018 Video Browser Showdown. A particular focus is on
additions made since the original publication and the systems performance
during the competition.
</summary>
    <author>
      <name>Luca Rossetto</name>
    </author>
    <author>
      <name>Ivan Giangreco</name>
    </author>
    <author>
      <name>Ralph Gasser</name>
    </author>
    <author>
      <name>Heiko Schuldt</name>
    </author>
    <link href="http://arxiv.org/abs/1805.02371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.14059v1</id>
    <updated>2020-07-27T04:39:45Z</updated>
    <published>2020-07-27T04:39:45Z</published>
    <title>MUSE2020 challenge report</title>
    <summary>  This paper is a brief report for MUSE2020 challenge. We present our solution
for Muse-Wild sub challenge. The aim of this challenge is to investigate
sentiment analysis method in real-world situation. Our solutions achieve the
best CCC performance of 0.4670, 0.3571 for arousal, and valence respectively on
the challenge validation set, which outperforms the baseline system with
corresponding CCC of 0.3078 and 1506.
</summary>
    <author>
      <name>Ruichen Li</name>
    </author>
    <author>
      <name>JingWen Hu</name>
    </author>
    <author>
      <name>Shuai Guo</name>
    </author>
    <author>
      <name>Jinming Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/2009.14059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.14059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.01313v1</id>
    <updated>2021-02-02T05:10:37Z</updated>
    <published>2021-02-02T05:10:37Z</published>
    <title>Fake-image detection with Robust Hashing</title>
    <summary>  In this paper, we investigate whether robust hashing has a possibility to
robustly detect fake-images even when multiple manipulation techniques such as
JPEG compression are applied to images for the first time. In an experiment,
the proposed fake detection with robust hashing is demonstrated to outperform
state-of-the-art one under the use of various datasets including fake images
generated with GANs.
</summary>
    <author>
      <name>Miki Tanaka</name>
    </author>
    <author>
      <name>Hitoshi Kiya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to be appear in Life Tech 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.01313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.01313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.12201v1</id>
    <updated>2022-10-21T19:03:29Z</updated>
    <published>2022-10-21T19:03:29Z</published>
    <title>A computational analysis on the relationship between melodic originality
  and thematic fame in classical music from the Romantic period</title>
    <summary>  In this work, the researcher presents a novel approach to calculating melodic
originality based on the research by Simonton (1994). This novel formula is
then applied to a dataset of 428 classical music pieces from the Romantic
period to analyze the relationship between melodic originality and thematic
fame.
</summary>
    <author>
      <name>Hudson Griffith</name>
    </author>
    <link href="http://arxiv.org/abs/2210.12201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.12201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.07835v1</id>
    <updated>2022-12-15T13:44:28Z</updated>
    <published>2022-12-15T13:44:28Z</published>
    <title>You were saying? -- Spoken Language in the V3C Dataset</title>
    <summary>  This paper presents an analysis of the distribution of spoken language in the
V3C video retrieval benchmark dataset based on automatically generated
transcripts. It finds that a large portion of the dataset is covered by spoken
language. Since language transcripts can be quickly and accurately described,
this has implications for retrieval tasks such as known-item search.
</summary>
    <author>
      <name>Luca Rossetto</name>
    </author>
    <link href="http://arxiv.org/abs/2212.07835v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.07835v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.12907v1</id>
    <updated>2022-12-25T14:16:49Z</updated>
    <published>2022-12-25T14:16:49Z</published>
    <title>Technical Evaluation of HoloLens for Multimedia: A First Look</title>
    <summary>  A recently released cutting-edge AR device, Microsoft HoloLens, has attracted
considerable attention with its advanced capabilities. In this article, we
report the design and execution of a series of experiments to quantitatively
evaluate HoloLens' performance in head localization, real environment
reconstruction, spatial mapping, hologram visualization, and speech
recognition.
</summary>
    <author>
      <name>Yang Liu</name>
    </author>
    <author>
      <name>Haiwei Dong</name>
    </author>
    <author>
      <name>Longyu Zhang</name>
    </author>
    <author>
      <name>Abdulmotaleb El Saddik</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MMUL.2018.2873473</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MMUL.2018.2873473" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Multimedia, vol. 25, no. 4, pp. 8-18, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2212.12907v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.12907v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.0598v2</id>
    <updated>2008-02-03T19:41:44Z</updated>
    <published>2007-08-04T02:38:19Z</published>
    <title>An Application of Chromatic Prototypes</title>
    <summary>  This paper has been withdrawn.
</summary>
    <author>
      <name>Matthew McCool</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/0708.0598v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.0598v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410022v1</id>
    <updated>2004-10-11T12:34:02Z</updated>
    <published>2004-10-11T12:34:02Z</published>
    <title>RRL: A Rich Representation Language for the Description of Agent
  Behaviour in NECA</title>
    <summary>  In this paper, we describe the Rich Representation Language (RRL) which is
used in the NECA system. The NECA system generates interactions between two or
more animated characters. The RRL is an XML compliant framework for
representing the information that is exchanged at the interfaces between the
various NECA system modules. The full XML Schemas for the RRL are available at
http://www.ai.univie.ac.at/NECA/RRL
</summary>
    <author>
      <name>P. Piwek</name>
    </author>
    <author>
      <name>B. Krenn</name>
    </author>
    <author>
      <name>M. Schroeder</name>
    </author>
    <author>
      <name>M. Grice</name>
    </author>
    <author>
      <name>S. Baumann</name>
    </author>
    <author>
      <name>H. Pirker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the AAMAS-02 Workshop ``Embodied conversational
  agents - let's specify and evaluate them!'', July 16 2002, Bologna, Italy.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0410022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H5.1, H5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0501044v1</id>
    <updated>2005-01-20T17:12:05Z</updated>
    <published>2005-01-20T17:12:05Z</published>
    <title>Augmented Segmentation and Visualization for Presentation Videos</title>
    <summary>  We investigate methods of segmenting, visualizing, and indexing presentation
videos by separately considering audio and visual data. The audio track is
segmented by speaker, and augmented with key phrases which are extracted using
an Automatic Speech Recognizer (ASR). The video track is segmented by visual
dissimilarities and augmented by representative key frames. An interactive user
interface combines a visual representation of audio, video, text, and key
frames, and allows the user to navigate a presentation video. We also explore
clustering and labeling of speaker data and present preliminary results.
</summary>
    <author>
      <name>Alexander Haubold</name>
    </author>
    <author>
      <name>John R. Kender</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0501044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0501044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4;H.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504106v1</id>
    <updated>2005-04-28T13:40:09Z</updated>
    <published>2005-04-28T13:40:09Z</published>
    <title>A Distributed Multimedia Communication System and its Applications to
  E-Learning</title>
    <summary>  In this paper we report on a multimedia communication system including a
VCoIP (Video Conferencing over IP) software with a distributed architecture and
its applications for teaching scenarios. It is a simple, ready-to-use scheme
for distributed presenting, recording and streaming multimedia content. We also
introduce and investigate concepts and experiments to IPv6 user and session
mobility, with the special focus on real-time video group communication.
</summary>
    <author>
      <name>Hans L. Cycon</name>
    </author>
    <author>
      <name>Thomas C. Schmidt</name>
    </author>
    <author>
      <name>Matthias Waehlisch</name>
    </author>
    <author>
      <name>Mark Palkow</name>
    </author>
    <author>
      <name>Henrik Regensburg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Including 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Symposium on Consumer Electronics, Sept. 1-3,
  2004, Page(s):425 - 429</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0504106v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504106v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.2; C.2.4; H.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0506070v1</id>
    <updated>2005-06-16T10:15:05Z</updated>
    <published>2005-06-16T10:15:05Z</published>
    <title>Data Visualization on Shared Usage Multi-Screen Environment</title>
    <summary>  The modern multimedia technologies based on the whole palette of hardware and
software facilities of real-time high-speed information processing, in a
combination with effective facilities of the remote access to information
resources, allow us to visualize diverse types of information. Data
visualization facilities &amp;#8211; is the face of the Automated Control System on
whom often judge about their efficiency. They take a special place, providing
visualization of the diverse information necessary for decision-making by a
final control link - the person allocated by certain powers.
</summary>
    <author>
      <name>Ph. D. Yuriy A. Chashkov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0506070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0506070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.1.1; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0603130v1</id>
    <updated>2006-03-31T19:36:03Z</updated>
    <published>2006-03-31T19:36:03Z</published>
    <title>Digital watermarking in the singular vector domain</title>
    <summary>  Many current watermarking algorithms insert data in the spatial or transform
domains like the discrete cosine, the discrete Fourier, and the discrete
wavelet transforms. In this paper, we present a data-hiding algorithm that
exploits the singular value decomposition (SVD) representation of the data. We
compute the SVD of the host image and the watermark and embed the watermark in
the singular vectors of the host image. The proposed method leads to an
imperceptible scheme for digital images, both in grey scale and color and is
quite robust against attacks like noise and JPEG compression.
</summary>
    <author>
      <name>Rashmi Agarwal</name>
    </author>
    <author>
      <name>M. S. Santhanam</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/S0219467808003131</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/S0219467808003131" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 21 figures, Elsevier class</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Image and Graphics, volume 8, page 351
  (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0603130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0603130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607087v1</id>
    <updated>2006-07-18T08:16:37Z</updated>
    <published>2006-07-18T08:16:37Z</published>
    <title>Un filtre temporel cr√©dibiliste pour la reconnaissance d'actions
  humaines dans les vid√©os</title>
    <summary>  In the context of human action recognition in video sequences, a temporal
belief filter is presented. It allows to cope with human action disparity and
low quality videos. The whole system of action recognition is based on the
Transferable Belief Model (TBM) proposed by P. Smets. The TBM allows to
explicitly model the doubt between actions. Furthermore, the TBM emphasizes the
conflict which is exploited for action recognition. The filtering performance
is assessed on real video sequences acquired by a moving camera and under
several unknown view angles.
</summary>
    <author>
      <name>Emmanuel Ramasso</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIS</arxiv:affiliation>
    </author>
    <author>
      <name>Mich√®le Rombaut</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIS</arxiv:affiliation>
    </author>
    <author>
      <name>Denis Pellerin</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIS</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0607087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.0625v1</id>
    <updated>2008-01-04T03:15:56Z</updated>
    <published>2008-01-04T03:15:56Z</published>
    <title>On the Robustness of the Delay-Based Fingerprint Embedding Scheme</title>
    <summary>  The delay-based fingerprint embedding was recently proposed to support more
users in secure media distribution scenario. In this embedding scheme, some
users are assigned the same fingerprint code with only different embedding
delay. The algorithm's robustness against collusion attacks is investigated.
However, its robustness against common desynchronization attacks, e.g.,
cropping and time shifting, is not considered. In this paper, desynchronization
attacks are used to break the delay-based fingerprint embedding algorithm. To
improve the robustness, two means are proposed to keep the embedded fingerprint
codes synchronized, i.e., adding a synchronization fingerprint and adopting the
relative delay to detect users. Analyses and experiments are given to show the
improvements.
</summary>
    <author>
      <name>Shiguo Lian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages,6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0801.0625v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.0625v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.11; H.5.1; H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.4865v1</id>
    <updated>2008-04-30T16:39:32Z</updated>
    <published>2008-04-30T16:39:32Z</published>
    <title>Characterizing Video Responses in Social Networks</title>
    <summary>  Video sharing sites, such as YouTube, use video responses to enhance the
social interactions among their users. The video response feature allows users
to interact and converse through video, by creating a video sequence that
begins with an opening video and followed by video responses from other users.
Our characterization is over 3.4 million videos and 400,000 video responses
collected from YouTube during a 7-day period. We first analyze the
characteristics of the video responses, such as popularity, duration, and
geography. We then examine the social networks that emerge from the video
response interactions.
</summary>
    <author>
      <name>Fabricio Benevenuto</name>
    </author>
    <author>
      <name>Fernando Duarte</name>
    </author>
    <author>
      <name>Tiago Rodrigues</name>
    </author>
    <author>
      <name>Virgilio Almeida</name>
    </author>
    <author>
      <name>Jussara Almeida</name>
    </author>
    <author>
      <name>Keith Ross</name>
    </author>
    <link href="http://arxiv.org/abs/0804.4865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.4865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4; H.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.1543v1</id>
    <updated>2008-06-09T22:03:40Z</updated>
    <published>2008-06-09T22:03:40Z</published>
    <title>On the Superdistribution of Digital Goods</title>
    <summary>  Business models involving buyers of digital goods in the distribution process
are called superdistribution schemes. We review the state-of-the art of
research and application of superdistribution and propose systematic approach
to market mechanisms using super-distribution and technical system
architectures supporting it. The limiting conditions on such markets are of
economic, legal, technical, and psychological nature.
</summary>
    <author>
      <name>Andreas U. Schmidt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Invited paper at the Wokshop 2008 International Workshop on
  Multimedia Security in Communication (MUSIC'08) To appear in: Proceedings of
  2008 Third International Conference on Communications and Networking in China
  (CHINACOM'08), August 25-27, 2008, Hangzhou, China</arxiv:comment>
    <link href="http://arxiv.org/abs/0806.1543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.1543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.0309v1</id>
    <updated>2008-08-03T10:17:03Z</updated>
    <published>2008-08-03T10:17:03Z</published>
    <title>A Reliable SVD based Watermarking Schem</title>
    <summary>  We propose a novel scheme for watermarking of digital images based on
singular value decomposition (SVD), which makes use of the fact that the SVD
subspace preserves significant amount of information of an image, as compared
to its singular value matrix, Zhang and Li (2005). The principal components of
the watermark are embedded in the original image, leaving the detector with a
complimentary set of singular vectors for watermark extraction. The above step
invariably ensures that watermark extraction from the embedded watermark image,
using a modified matrix, is not possible, thereby removing a major drawback of
an earlier proposed algorithm by Liu and Tan (2002).
</summary>
    <author>
      <name>Chirag Jain</name>
    </author>
    <author>
      <name>Siddharth Arora</name>
    </author>
    <author>
      <name>Prasanta K. Panigrahi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 7 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.0309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.0309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.3485v1</id>
    <updated>2008-09-20T05:52:47Z</updated>
    <published>2008-09-20T05:52:47Z</published>
    <title>A First Step to Convolutive Sparse Representation</title>
    <summary>  In this paper an extension of the sparse decomposition problem is considered
and an algorithm for solving it is presented. In this extension, it is known
that one of the shifted versions of a signal s (not necessarily the original
signal itself) has a sparse representation on an overcomplete dictionary, and
we are looking for the sparsest representation among the representations of all
the shifted versions of s. Then, the proposed algorithm finds simultaneously
the amount of the required shift, and the sparse representation. Experimental
results emphasize on the performance of our algorithm.
</summary>
    <author>
      <name>Hamed Firouzi</name>
    </author>
    <author>
      <name>Massoud Babaie-Zadeh</name>
    </author>
    <author>
      <name>Aria Ghasemian</name>
    </author>
    <author>
      <name>Christian Jutten</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 Pages-In Proceeding of ICASSP 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/0809.3485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.3485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.OH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.1959v1</id>
    <updated>2008-11-12T20:10:00Z</updated>
    <published>2008-11-12T20:10:00Z</published>
    <title>Characterization and collection of information from heterogeneous
  multimedia sources with users' parameters for decision support</title>
    <summary>  No single information source can be good enough to satisfy the divergent and
dynamic needs of users all the time. Integrating information from divergent
sources can be a solution to deficiencies in information content. We present
how Information from multimedia document can be collected based on associating
a generic database to a federated database. Information collected in this way
is brought into relevance by integrating the parameters of usage and user's
parameter for decision making. We identified seven different classifications of
multimedia document.
</summary>
    <author>
      <name>Charles A. B. Robert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LORIA</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0811.1959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.1959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.4700v1</id>
    <updated>2008-11-28T16:07:33Z</updated>
    <published>2008-11-28T16:07:33Z</published>
    <title>Trellis-coded quantization for public-key steganography</title>
    <summary>  This paper deals with public-key steganography in the presence of a passive
warden. The aim is to hide secret messages within cover-documents without
making the warden suspicious, and without any preliminar secret key sharing.
Whereas a practical attempt has been already done to provide a solution to this
problem, it suffers of poor flexibility (since embedding and decoding steps
highly depend on cover-signals statistics) and of little capacity compared to
recent data hiding techniques. Using the same framework, this paper explores
the use of trellis-coded quantization techniques (TCQ and turbo TCQ) to design
a more efficient public-key scheme. Experiments on audio signals show great
improvements considering Cachin's security criterion.
</summary>
    <author>
      <name>Ga√´tan Le Guelvouit</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0811.4700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.4700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.2405v1</id>
    <updated>2008-12-12T15:56:42Z</updated>
    <published>2008-12-12T15:56:42Z</published>
    <title>A New Trend in Optimization on Multi Overcomplete Dictionary toward
  Inpainting</title>
    <summary>  Recently, great attention was intended toward overcomplete dictionaries and
the sparse representations they can provide. In a wide variety of signal
processing problems, sparsity serves a crucial property leading to high
performance. Inpainting, the process of reconstructing lost or deteriorated
parts of images or videos, is an interesting application which can be handled
by suitably decomposition of an image through combination of overcomplete
dictionaries. This paper addresses a novel technique of such a decomposition
and investigate that through inpainting of images. Simulations are presented to
demonstrate the validation of our approach.
</summary>
    <author>
      <name>SeyyedMajid Valiollahzadeh</name>
    </author>
    <author>
      <name>Mohammad Nazari</name>
    </author>
    <author>
      <name>Massoud Babaie-Zadeh</name>
    </author>
    <author>
      <name>Christian Jutten</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0812.2405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.2405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.1407v1</id>
    <updated>2009-01-11T02:25:17Z</updated>
    <published>2009-01-11T02:25:17Z</published>
    <title>Condition for Energy Efficient Watermarking with Random Vector Model
  without WSS Assumption</title>
    <summary>  Energy efficient watermarking preserves the watermark energy after linear
attack as much as possible. We consider in this letter non-stationary signal
models and derive conditions for energy efficient watermarking under random
vector model without WSS assumption. We find that the covariance matrix of the
energy efficient watermark should be proportional to host covariance matrix to
best resist the optimal linear removal attacks. In WSS process our result
reduces to the well known power spectrum condition. Intuitive geometric
interpretation of the results are also discussed which in turn also provide
more simpler proof of the main results.
</summary>
    <author>
      <name>Bin Yan</name>
    </author>
    <author>
      <name>Zheming Lu</name>
    </author>
    <author>
      <name>Yinjing Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures, submitted to IEEE Signal Processing Letter for
  review</arxiv:comment>
    <link href="http://arxiv.org/abs/0901.1407v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.1407v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.3103v1</id>
    <updated>2009-03-18T08:17:05Z</updated>
    <published>2009-03-18T08:17:05Z</published>
    <title>Efficiently Learning a Detection Cascade with Sparse Eigenvectors</title>
    <summary>  In this work, we first show that feature selection methods other than
boosting can also be used for training an efficient object detector. In
particular, we introduce Greedy Sparse Linear Discriminant Analysis (GSLDA)
\cite{Moghaddam2007Fast} for its conceptual simplicity and computational
efficiency; and slightly better detection performance is achieved compared with
\cite{Viola2004Robust}. Moreover, we propose a new technique, termed Boosted
Greedy Sparse Linear Discriminant Analysis (BGSLDA), to efficiently train a
detection cascade. BGSLDA exploits the sample re-weighting property of boosting
and the class-separability criterion of GSLDA.
</summary>
    <author>
      <name>Chunhua Shen</name>
    </author>
    <author>
      <name>Sakrapee Paisitkriangkrai</name>
    </author>
    <author>
      <name>Jian Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, conference version published in CVPR2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0903.3103v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.3103v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.4205v1</id>
    <updated>2009-05-26T14:06:05Z</updated>
    <published>2009-05-26T14:06:05Z</published>
    <title>Development and Optimization of a Multimedia Product</title>
    <summary>  This article presents a new concept of a multimedia interactive product. It
is a multiuser versatile platform that can be used for different purposes. The
first implementation of the platform is a multiplayer game called Texas Hold
'em, which is a very popular community card game. The paper shows the product's
multimedia structure where Hardware and Software work together in creating a
realistic feeling for the users.
</summary>
    <author>
      <name>Cristian Anghel</name>
    </author>
    <author>
      <name>Vlad Muia</name>
    </author>
    <author>
      <name>Miodrag Stoianovici</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, exposed on 5th International Conference "Actualities and
  Perspectives on Hardware and Software" - APHS2009, Timisoara, Romania</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series VII(2009), 31-36</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0905.4205v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.4205v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.0667v1</id>
    <updated>2009-06-03T09:31:34Z</updated>
    <published>2009-06-03T09:31:34Z</published>
    <title>Quality assessment of the MPEG-4 scalable video CODEC</title>
    <summary>  In this paper, the performance of the emerging MPEG-4 SVC CODEC is evaluated.
In the first part, a brief introduction on the subject of quality assessment
and the development of the MPEG-4 SVC CODEC is given. After that, the used test
methodologies are described in detail, followed by an explanation of the actual
test scenarios. The main part of this work concentrates on the performance
analysis of the MPEG-4 SVC CODEC - both objective and subjective.
</summary>
    <author>
      <name>Florian Niedermeier</name>
    </author>
    <author>
      <name>Michael Niedermeier</name>
    </author>
    <author>
      <name>Harald Kosch</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">published in a shorter version at ICIAP 2009 Conference</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0906.0667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.0667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.0866v1</id>
    <updated>2009-06-04T09:52:22Z</updated>
    <published>2009-06-04T09:52:22Z</published>
    <title>Web Publishing of the Files Obtained by Flash</title>
    <summary>  The aim of this article is to familiarize the user with the Web publishing of
the files obtained by Flash. The article contains an overview of Macromedia
Flash 5, as well as the running of a Playing Flash movie, information on Flash
and Generator, the publishing of Flash movies, a HTLM publishing for Flash
Player files and publishing by Generator templates.
</summary>
    <author>
      <name>Virgiliu Streian</name>
    </author>
    <author>
      <name>Adela Ionescu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, exposed on 5th International Conference "Actualities and
  Perspectives on Hardware and Software" - APHS2009, Timisoara, Romania</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series VII(2009),349-358</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0906.0866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.0866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.4607v1</id>
    <updated>2009-06-25T05:40:46Z</updated>
    <published>2009-06-25T05:40:46Z</published>
    <title>A Bandwidth Characterization Tool For MPEG-2 File</title>
    <summary>  This paper proposes the design and development of MPEG 2 Video Decoder to
offer flexible and effective utilization of bandwidth services. The decoder is
capable of decoding the MPEG 2 bit stream on a single host machine. The present
decoder is designed to be simple, but yet effectively reconstruct the video
from MPEG 2 bit stream.
</summary>
    <author>
      <name>Sandeep. Kugali</name>
    </author>
    <author>
      <name>S. S. Manvi</name>
    </author>
    <author>
      <name>A. V. Sutagundar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, International Journal of Computer Science and Information
  Security (IJCSIS)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSIS, June 2009 Issue, Vol. 2, No. 1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0906.4607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.4607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.5073v1</id>
    <updated>2009-06-27T11:59:15Z</updated>
    <published>2009-06-27T11:59:15Z</published>
    <title>TTSS Packet Classification Algorithm to enhance Multimedia Applications
  in Network Processor based Router</title>
    <summary>  The objective of this paper is to implement the Trie based Tuple Space
Search(TTSS) packet classification algorithm for Network Processor(NP) based
router to enhance multimedia applications. The performance is evaluated using
Intel IXP2400 NP Simulator. The results demonstrate that, TTSS has better
performance than Tuple Space Search algorithm and is well suited to achieve
high speed packet classification to support multimedia applications.
</summary>
    <author>
      <name>R. Avudaiammal</name>
    </author>
    <author>
      <name>R. SivaSubramanian</name>
    </author>
    <author>
      <name>R. Pandian</name>
    </author>
    <author>
      <name>P. Seethalakshmi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages, International Journal of Computer Science and Information
  Security (IJCSIS)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSIS June 2009 Issue, Vol. 2, No. 1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0906.5073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.5073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.0443v1</id>
    <updated>2010-01-04T05:44:57Z</updated>
    <published>2010-01-04T05:44:57Z</published>
    <title>Discovering Knowledge from Multi-modal Lecture Recordings</title>
    <summary>  Educational media mining is the process of converting raw media data from
educational systems to useful information that can be used to design learning
systems, answer research questions and allow personalized learning experiences.
Knowledge discovery encompasses a wide range of techniques ranging from
database queries to more recent developments in machine learning and language
technology. Educational media mining techniques are now being used in IT
Services research worldwide. Multi-modal Lecture Recordings is one of the
important types of educational media and this paper explores the research
challenges for mining lecture recordings for the efficient personalized
learning experiences. Keywords: Educational Media Mining; Lecture Recordings,
Multimodal Information System, Personalized Learning; Online Course Ware;
Skills and Competences;
</summary>
    <author>
      <name>Rajkumar Kannan</name>
    </author>
    <author>
      <name>Christian Guetl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First International Conference on Data Engineering and Management
  2008, India</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.0443v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.0443v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; H.5.4; H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.1168v1</id>
    <updated>2010-02-05T09:31:58Z</updated>
    <published>2010-02-05T09:31:58Z</published>
    <title>Shape-Adaptive Motion Estimation Algorithm for MPEG-4 Video Coding</title>
    <summary>  This paper presents a gradient based motion estimation algorithm based on
shape-motion prediction, which takes advantage of the correlation between
neighboring Binary Alpha Blocks (BABs), to match with the Mpeg-4 shape coding
case and speed up the estimation process. The PSNR and computation time
achieved by the proposed algorithm seem to be better than those obtained by
most popular motion estimation techniques.
</summary>
    <author>
      <name>F. Benboubker</name>
    </author>
    <author>
      <name>F. Abdi</name>
    </author>
    <author>
      <name>A. Ahaitouf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, IJCSI, Vol. 7,
  Issue 1, No. 2, January 2010,
  http://ijcsi.org/articles/Shape-Adaptive-Motion-Estimation-Algorithm-for-MPEG-4-Video-Coding.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, IJCSI, Vol. 7,
  Issue 1, No. 2, January 2010,
  http://ijcsi.org/articles/Shape-Adaptive-Motion-Estimation-Algorithm-for-MPEG-4-Video-Coding.php</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.1168v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.1168v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.1791v1</id>
    <updated>2010-04-11T11:11:59Z</updated>
    <published>2010-04-11T11:11:59Z</published>
    <title>Reversible Image data Hiding using Lifting wavelet Transform and
  Histogram Shifting</title>
    <summary>  A method of lossless data hiding in images using integer wavelet transform
and histogram shifting for gray scale images is proposed. The method shifts
part of the histogram, to create space for embedding the watermark information
bits. The method embeds watermark while maintaining the visual quality well.
The method is completely reversible. The original image and the watermark data
can be recovered without any loss.
</summary>
    <author>
      <name>S. Kurshid Jinna</name>
    </author>
    <author>
      <name>L. Ganesan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Publication format, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSIS, Vol. 7 No. 3, March 2010, 283-289</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.1791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.1791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3275v1</id>
    <updated>2010-04-19T18:26:09Z</updated>
    <published>2010-04-19T18:26:09Z</published>
    <title>C Implementation &amp; comparison of companding &amp; silence audio compression
  techniques</title>
    <summary>  Just about all the newest living room audio-video electronics and PC
multimedia products being designed today will incorporate some form of
compressed digitized-audio processing capability. Audio compression reduces the
bit rate required to represent an analog audio signal while maintaining the
perceived audio quality. Discarding inaudible data reduces the storage,
transmission and compute requirements of handling high-quality audio files.
This paper covers wave audio file format &amp; algorithm of silence compression
method and companding method to compress and decompress wave audio file. Then
it compares the result of these two methods.
</summary>
    <author>
      <name>Kruti Dangarwala</name>
    </author>
    <author>
      <name>Jigar Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues online at
  http://ijcsi.org/articles/C-Implementation-comparison-of-companding-silence-audio-compression-techniques.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI, Volume 7, Issue 2, March 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.3275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3556v1</id>
    <updated>2010-04-20T20:14:19Z</updated>
    <published>2010-04-20T20:14:19Z</published>
    <title>Policies and Economics of Digital Multimedia Transmission</title>
    <summary>  There are different Standards of digital multimedia transmission, for example
DVB in Europe and ISDB in Japan and DMB in Korea, with different delivery
system (example MPEG-2, MPEG-4).This paper describe an overview of Digital
Multimedia Transmission (DMT) technologies. The economic aspects of digital
content &amp; software solution industry as a strategic key in the future will be
discussed. The study then focuses on some important policy and technology
issues, such S-DMB, T-DMB, Digital Video Broadcasting Handheld (DVB-H) and
concludes DMT policies for convergence of telecommunications and broadcasting.
</summary>
    <author>
      <name>Mohsen Gerami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues online at
  http://ijcsi.org/articles/Policies-and-Economics-of-Digital-Multimedia-Transmission.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI, Volume 7, Issue 2, March 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.3556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.4241v1</id>
    <updated>2010-04-24T00:29:00Z</updated>
    <published>2010-04-24T00:29:00Z</published>
    <title>Error Concealment in Image Communication Using Edge Map Watermarking and
  Spatial Smoothing</title>
    <summary>  We propose a novel error concealment algorithm to be used at the receiver
side of a lossy image transmission system. Our algorithm involves hiding the
edge map of the original image at the transmitter within itself using a robust
watermarking scheme. At the receiver, wherever a lost block is detected, the
extracted edge information is used as border constraint for the spatial
smoothing employing the intact neighboring blocks in order to conceal errors.
Simulation results show the superiority of our technique over existing methods
even in case of high packet loss ratios in the communication network.
</summary>
    <author>
      <name>Shabnam Sodagari</name>
    </author>
    <author>
      <name>Peyman Hesami</name>
    </author>
    <author>
      <name>Alireza Nasiri Avanaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Proceeding of ICCET 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.4241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.4241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.4134v1</id>
    <updated>2010-07-23T13:59:42Z</updated>
    <published>2010-07-23T13:59:42Z</published>
    <title>Human Daily Activities Indexing in Videos from Wearable Cameras for
  Monitoring of Patients with Dementia Diseases</title>
    <summary>  Our research focuses on analysing human activities according to a known
behaviorist scenario, in case of noisy and high dimensional collected data. The
data come from the monitoring of patients with dementia diseases by wearable
cameras. We define a structural model of video recordings based on a Hidden
Markov Model. New spatio-temporal features, color features and localization
features are proposed as observations. First results in recognition of
activities are promising.
</summary>
    <author>
      <name>Svebor Karaman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Jenny Benois-Pineau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>R√©mi M√©gret</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMS</arxiv:affiliation>
    </author>
    <author>
      <name>Vladislavs Dovgalecs</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMS</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-Fran√ßois Dartigues</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISPED</arxiv:affiliation>
    </author>
    <author>
      <name>Yann Ga√´stel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISPED</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICPR.2010.999</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICPR.2010.999" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICPR 2010, Istanbul : Turquie (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1007.4134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.4134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.1478v1</id>
    <updated>2010-09-08T08:23:47Z</updated>
    <published>2010-09-08T08:23:47Z</published>
    <title>A Block Based Scheme for Enhancing Low Luminated Images</title>
    <summary>  In this paper the background detection in images in poor lighting can be done
by the use of morphological filters. Lately contrast image enhancement
technique is used to detect the background in image which uses Weber's Law. The
proposed technique is more effective one in which the background detection in
image can be done in color images. The given image obtained in this method is
very effective one. More enhancement can be obtained while comparing the
results. In this technique compressed domain enhancement has been used for
better result.
</summary>
    <author>
      <name>A. Saradha Devi</name>
    </author>
    <author>
      <name>S. Suja Priyadharsini</name>
    </author>
    <author>
      <name>S. Athinarayanan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures, 2 national conferences</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Multimedia &amp; Its Applications(IJMA),
  Vol.2, No.3, August 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.1478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.1478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.3785v1</id>
    <updated>2010-09-20T12:15:46Z</updated>
    <published>2010-09-20T12:15:46Z</published>
    <title>Improved Iterative Techniques to Compensate for Interpolation
  Distortions</title>
    <summary>  In this paper a novel hybrid approach for compensating the distortion of any
interpolation has been proposed. In this hybrid method, a modular approach was
incorporated in an iterative fashion. By using this approach we can get drastic
improvement with less computational complexity. The extension of the proposed
approach to two dimensions was also studied. Both the simulation results and
mathematical analyses confirmed the superiority of the hybrid method. The
proposed method was also shown to be robust against additive noise.
</summary>
    <author>
      <name>A. ParandehGheibi</name>
    </author>
    <author>
      <name>M. A. Akhaee</name>
    </author>
    <author>
      <name>A. Ayremlou</name>
    </author>
    <author>
      <name>M. A. Rahimian</name>
    </author>
    <author>
      <name>F. Marvasti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on Signal Processing, Elsevier</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.3785v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.3785v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.2432v1</id>
    <updated>2010-10-12T16:13:21Z</updated>
    <published>2010-10-12T16:13:21Z</published>
    <title>Transmitting Video-on-Demand Effectively</title>
    <summary>  Now-a-days internet has become a vast source of entertainment &amp; new services
are available in quick succession which provides entertainment to the users.
One of this service i.e. Video-on-Demand is most hyped service in this context.
Transferring the video over the network with less error is the main objective
of the service providers. In this paper we present an algorithm for routing the
video to the user in an effective manner along with a method that ensures less
error rate than others.
</summary>
    <author>
      <name>Rachit Mohan Garg</name>
    </author>
    <author>
      <name>Shipra Kapoor</name>
    </author>
    <author>
      <name>Kapil Kumar</name>
    </author>
    <author>
      <name>Mohd. Dilshad Ansari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Universal Journal of Computer Science and Engineering Technology
  (1) 1, October 2010, UniCSE</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1010.2432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.2432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.3951v1</id>
    <updated>2010-10-19T15:24:33Z</updated>
    <published>2010-10-19T15:24:33Z</published>
    <title>Alternatives to speech in low bit rate communication systems</title>
    <summary>  This paper describes a framework and a method with which speech communication
can be analyzed. The framework consists of a set of low bit rate, short-range
acoustic communication systems, such as speech, but that are quite different
from speech. The method is to systematically compare these systems according to
different objective functions such as data rate, computational overhead,
psychoacoustic effects and semantics. One goal of this study is to better
understand the nature of human communication. Another goal is to identify
acoustic communication systems that are more efficient than human speech for
some specific purposes.
</summary>
    <author>
      <name>Cristina Videira Lopes</name>
    </author>
    <author>
      <name>Pedro M. Q. Aguiar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1010.3951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.3951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.2753v1</id>
    <updated>2010-11-11T20:20:36Z</updated>
    <published>2010-11-11T20:20:36Z</published>
    <title>Compensating Interpolation Distortion by New Optimized Modular Method</title>
    <summary>  A modular method was suggested before to recover a band limited signal from
the sample and hold and linearly interpolated (or, in general, an
nth-order-hold) version of the regular samples. In this paper a novel approach
for compensating the distortion of any interpolation based on modular method
has been proposed. In this method the performance of the modular method is
optimized by adding only some simply calculated coefficients. This approach
causes drastic improvement in terms of SNRs with fewer modules compared to the
classical modular method. Simulation results clearly confirm the improvement of
the proposed method and also its superior robustness against additive noise.
</summary>
    <author>
      <name>Ali Ayremlou</name>
    </author>
    <author>
      <name>Mohammad Tofighi</name>
    </author>
    <author>
      <name>Farokh Marvasti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to International Conference on Telecommunications(ICT) 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.2753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.2753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.5458v1</id>
    <updated>2010-11-24T18:58:53Z</updated>
    <published>2010-11-24T18:58:53Z</published>
    <title>Image Inpainting Using Sparsity of the Transform Domain</title>
    <summary>  In this paper, we propose a new image inpainting method based on the property
that much of the image information in the transform domain is sparse. We add a
redundancy to the original image by mapping the transform coefficients with
small amplitudes to zero and the resultant sparsity pattern is used as the side
information in the recovery stage. If the side information is not available,
the receiver has to estimate the sparsity pattern. At the end, the recovery is
done by consecutive projecting between two spatial and transform sets.
Experimental results show that our method works well for both structural and
texture images and outperforms other techniques in objective and subjective
performance measures.
</summary>
    <author>
      <name>H. Hosseini</name>
    </author>
    <author>
      <name>N. B. Marvasti</name>
    </author>
    <author>
      <name>F. Marvasti</name>
    </author>
    <link href="http://arxiv.org/abs/1011.5458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.5458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.0397v2</id>
    <updated>2011-04-29T20:06:42Z</updated>
    <published>2010-12-02T13:02:06Z</published>
    <title>A proposed Optimized Spline Interpolation</title>
    <summary>  The goal of this paper is to design compact support basis spline functions
that best approximate a given filter (e.g., an ideal Lowpass filter). The
optimum function is found by minimizing the least square problem ($\ell$2 norm
of the difference between the desired and the approximated filters) by means of
the calculus of variation; more precisely, the introduced splines give optimal
filtering properties with respect to their time support interval. Both
mathematical analysis and simulation results confirm the superiority of these
splines.
</summary>
    <author>
      <name>Ramtin Madani</name>
    </author>
    <author>
      <name>Ali Ayremlou</name>
    </author>
    <author>
      <name>Arash Amini</name>
    </author>
    <author>
      <name>Farrokh Marvasti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SampTA 2011, Accepted</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.0397v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.0397v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.5573v1</id>
    <updated>2010-12-27T07:44:21Z</updated>
    <published>2010-12-27T07:44:21Z</published>
    <title>Image Sterilization to Prevent LSB-based Steganographic Transmission</title>
    <summary>  Sterilization is a very popular word used in biomedical testing (like removal
of all microorganisms on surface of an article or in fluid using appropriate
chemical products). Motivated by this biological analogy, we, for the first
time, introduce the concept of sterilization of an image, i.e., removing any
steganographic information embedded in the image. Experimental results show
that our technique succeeded in sterilizing around 76% to 91% of stego pixels
in an image on average, where data is embedded using LSB-based steganography.
</summary>
    <author>
      <name>Goutam Paul</name>
    </author>
    <author>
      <name>Imon Mukherjee</name>
    </author>
    <link href="http://arxiv.org/abs/1012.5573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.5573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.2859v1</id>
    <updated>2011-07-14T15:52:21Z</updated>
    <published>2011-07-14T15:52:21Z</published>
    <title>Label-Specific Training Set Construction from Web Resource for Image
  Annotation</title>
    <summary>  Recently many research efforts have been devoted to image annotation by
leveraging on the associated tags/keywords of web images as training labels. A
key issue to resolve is the relatively low accuracy of the tags. In this paper,
we propose a novel semi-automatic framework to construct a more accurate and
effective training set from these web media resources for each label that we
want to learn. Experiments conducted on a real-world dataset demonstrate that
the constructed training set can result in higher accuracy for image
annotation.
</summary>
    <author>
      <name>Jinhui Tang</name>
    </author>
    <author>
      <name>Shuicheng Yan</name>
    </author>
    <author>
      <name>Tat-Seng Chua</name>
    </author>
    <author>
      <name>Ramesh Jain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.2859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.2859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.4661v1</id>
    <updated>2011-07-23T06:30:44Z</updated>
    <published>2011-07-23T06:30:44Z</published>
    <title>MediaWiki Grammar Recovery</title>
    <summary>  The paper describes in detail the recovery effort of one of the official
MediaWiki grammars. Over two hundred grammar transformation steps are reported
and annotated, leading to delivery of a level 2 grammar, semi-automatically
extracted from a community created semi-formal text using at least five
different syntactic notations, several non-enforced naming conventions,
multiple misspellings, obsolete parsing technology idiosyncrasies and other
problems commonly encountered in grammars that were not engineered properly.
Having a quality grammar will allow to test and validate it further, without
alienating the community with a separately developed grammar.
</summary>
    <author>
      <name>Vadim Zaytsev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">47 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.4661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.4661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.6290v2</id>
    <updated>2011-09-01T07:35:06Z</updated>
    <published>2011-08-31T16:39:09Z</published>
    <title>Compression and Quantitative Analysis of Buffer Map Message in P2P
  Streaming System</title>
    <summary>  BM compression is a straightforward and operable way to reduce buffer message
length as well as to improve system performance. In this paper, we thoroughly
discuss the principles and protocol progress of different compression schemes,
and for the first time present an original compression scheme which can nearly
remove all redundant information from buffer message. Theoretical limit of
compression rates are deduced in the theory of information. Through the
analysis of information content and simulation with our measured BM trace of
UUSee, the validity and superiority of our compression scheme are validated in
term of compression ratio.
</summary>
    <author>
      <name>Chunxi Li</name>
    </author>
    <author>
      <name>Changjia Chen</name>
    </author>
    <author>
      <name>DahMing Chiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13pages,12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.6290v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.6290v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.0753v1</id>
    <updated>2011-09-04T20:03:32Z</updated>
    <published>2011-09-04T20:03:32Z</published>
    <title>Transmission of Successful Route Error Message(RERR) in Routing Aware
  Multiple Description Video Coding over Mobile Ad-Hoc Network</title>
    <summary>  Video transmission over mobile ad-hoc networks is becoming important as these
networks become more widely used in the wireless networks. We propose a
routing-aware multiple description video coding approach to support video
transmission over mobile ad-hoc networks with single and multiple path
transport. We build a model to estimate the packet loss probability of each
packet transmitted over the network based on the standard ad-hoc routing
messages and network parameters without losing the RERR message. We then
calculate the frame loss probability in order to eliminate error without any
loss of data.
</summary>
    <author>
      <name>Kinjal Shah</name>
    </author>
    <author>
      <name>Gagan Dua</name>
    </author>
    <author>
      <name>Dharmendar Sharma</name>
    </author>
    <author>
      <name>Priyanka Mishra</name>
    </author>
    <author>
      <name>Nitin Rakesh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijma.2011.3305</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijma.2011.3305" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages,2 figures, 1 table for algorithm</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.0753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.0753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.1641v1</id>
    <updated>2012-05-08T09:27:29Z</updated>
    <published>2012-05-08T09:27:29Z</published>
    <title>Content based video retrieval systems</title>
    <summary>  With the development of multimedia data types and available bandwidth there
is huge demand of video retrieval systems, as users shift from text based
retrieval systems to content based retrieval systems. Selection of extracted
features play an important role in content based video retrieval regardless of
video attributes being under consideration. These features are intended for
selecting, indexing and ranking according to their potential interest to the
user. Good features selection also allows the time and space costs of the
retrieval process to be reduced. This survey reviews the interesting features
that can be extracted from video data for indexing and retrieval along with
similarity measurement methods. We also identify present research issues in
area of content based video retrieval systems.
</summary>
    <author>
      <name>B V Patel</name>
    </author>
    <author>
      <name>B B Meshram</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/iju.2012.3202</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/iju.2012.3202" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.1641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.4572v1</id>
    <updated>2012-05-21T11:57:55Z</updated>
    <published>2012-05-21T11:57:55Z</published>
    <title>A Novel Video Compression Approach Based on Underdetermined Blind Source
  Separation</title>
    <summary>  This paper develops a new video compression approach based on underdetermined
blind source separation. Underdetermined blind source separation, which can be
used to efficiently enhance the video compression ratio, is combined with
various off-the-shelf codecs in this paper. Combining with MPEG-2, video
compression ratio could be improved slightly more than 33%. As for combing with
H.264, 4X~12X more compression ratio could be achieved with acceptable PSNR,
according to different kinds of video sequences.
</summary>
    <author>
      <name>Jing Liu</name>
    </author>
    <author>
      <name>Fei Qiao</name>
    </author>
    <author>
      <name>Qi Wei</name>
    </author>
    <author>
      <name>Huazhong Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages with 4 figures and 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1205.4572v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.4572v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1; H.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.1866v1</id>
    <updated>2012-06-08T20:15:30Z</updated>
    <published>2012-06-08T20:15:30Z</published>
    <title>Perceptual quality comparison between single-layer and scalable videos
  at the same spatial, temporal and amplitude resolutions</title>
    <summary>  In this paper, the perceptual quality difference between scalable and
single-layer videos coded at the same spatial, temporal and amplitude
resolution (STAR) is investigated through a subjective test using a mobile
platform. Three source videos are considered and for each source video
single-layer and scalable video are compared at 9 different STARs. We utilize
paired comparison methods with and without tie option. Results collected from
10 subjects in the without "tie" option and 6 subjects in the with "tie" option
show that there is no significant quality difference between scalable and
singlelayer video when coded at the same STAR. An analysis of variance (ANOVA)
test is also performed to further confirm the finding.
</summary>
    <author>
      <name>Yuanyi Xue</name>
    </author>
    <author>
      <name>Yao Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1206.1866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.1866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.2070v1</id>
    <updated>2012-08-15T16:00:06Z</updated>
    <published>2012-08-15T16:00:06Z</published>
    <title>Content-based Multi-media Retrieval Technology</title>
    <summary>  This paper gives a summary of the content-based Image Retrieval and
Content-based Audio Retrieval, which are two parts of the Content-based
Retrieval. Content-based Retrieval is the retrieval based on the features of
the content. Generally, it is a way to extract features of the media data and
find other data with the similar features from the database automatically.
Content-based Retrieval can not only work on discrete media like texts, but
also can be used on continuous media, such as video and audio.
</summary>
    <author>
      <name>Yi Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1209.2070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.2070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.2855v1</id>
    <updated>2012-09-13T11:25:19Z</updated>
    <published>2012-09-13T11:25:19Z</published>
    <title>Investigating Streaming Techniques and Energy Efficiency of Mobile Video
  Services</title>
    <summary>  We report results from a measurement study of three video streaming services,
YouTube, Dailymotion and Vimeo on six different smartphones. We measure and
analyze the traffic and energy consumption when streaming different quality
videos over Wi-Fi and 3G. We identify five different techniques to deliver the
video and show that the use of a particular technique depends on the device,
player, quality, and service. The energy consumption varies dramatically
between devices, services, and video qualities depending on the streaming
technique used. As a consequence, we come up with suggestions on how to improve
the energy efficiency of mobile video streaming services.
</summary>
    <author>
      <name>Mohammad Ashraful Hoque</name>
    </author>
    <author>
      <name>Matti Siekkinen</name>
    </author>
    <author>
      <name>Jukka K. Nurminen</name>
    </author>
    <author>
      <name>Mika Aalto</name>
    </author>
    <link href="http://arxiv.org/abs/1209.2855v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.2855v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.1947v1</id>
    <updated>2013-02-08T05:18:49Z</updated>
    <published>2013-02-08T05:18:49Z</published>
    <title>A new compressive video sensing framework for mobile broadcast</title>
    <summary>  A new video coding method based on compressive sampling is proposed. In this
method, a video is coded using compressive measurements on video cubes. Video
reconstruction is performed by minimization of total variation (TV) of the
pixelwise DCT coefficients along the temporal direction. A new reconstruction
algorithm is developed from TVAL3, an efficient TV minimization algorithm based
on the alternating minimization and augmented Lagrangian methods. Video coding
with this method is inherently scalable, and has applications in mobile
broadcast.
</summary>
    <author>
      <name>Chengbo Li</name>
    </author>
    <author>
      <name>Hong Jiang</name>
    </author>
    <author>
      <name>Paul Wilford</name>
    </author>
    <author>
      <name>Yin Zhang</name>
    </author>
    <author>
      <name>Mike Scheutzow</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TBC.2012.2226509</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TBC.2012.2226509" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 12 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Broadcasting VOL 59 NO 1 MARCH 2013 pp. 197 -
  205</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1302.1947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.1947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.4233v1</id>
    <updated>2013-02-18T11:41:09Z</updated>
    <published>2013-02-18T11:41:09Z</published>
    <title>The Robust Digital Image Watermarking using Quantization and Fuzzy Logic
  Approach in DWT Domain</title>
    <summary>  In this paper a novel approach to embed watermark into the host image using
quantization with the help of Dynamic Fuzzy Inference System (DFIS) is
proposed. The cover image is decomposed up to 3- levels using quantization and
Discrete Wavelet Transform (DWT). A bitmap of size 64x64 pixels is embedded
into the host image using DFIS rule base. The DFIS is utilized to generate the
watermark weighting function to embed the imperceptible watermark. The
implemented watermarking algorithm is imperceptible and robust to some normal
attacks such as JPEG Compression, salt&amp;pepper noise, median filtering, rotation
and cropping.
  Keywords: Watermark, Quantization, Dynamic Fuzzy Inference System,
Imperceptible, Robust, JPEG Compression, Cropping.
</summary>
    <author>
      <name>Nallagarla Ramamurthy</name>
    </author>
    <author>
      <name>S. Varadarajan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 11 figures, IJCSN Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.4233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.4233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.2330v1</id>
    <updated>2013-03-10T15:29:26Z</updated>
    <published>2013-03-10T15:29:26Z</published>
    <title>Image compression using anti-forensics method</title>
    <summary>  A large number of image forensics methods are available which are capable of
identifying image tampering. But these techniques are not capable of addressing
the anti-forensics method which is able to hide the trace of image tampering.
In this paper anti-forensics method for digital image compression has been
proposed. This anti-forensics method is capable of removing the traces of image
compression. Additionally, technique is also able to remove the traces of
blocking artifact that are left by image compression algorithms that divide an
image into segments during compression process. This method is targeted to
remove the compression fingerprints of JPEG compression.
</summary>
    <author>
      <name>M. S. Sreelakshmi</name>
    </author>
    <author>
      <name>D. Venkataraman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages 8 figures IJCSEA journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.2330v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.2330v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.4264v1</id>
    <updated>2013-03-18T14:36:53Z</updated>
    <published>2013-03-18T14:36:53Z</published>
    <title>StegTorrent: a Steganographic Method for the P2P File Sharing Service</title>
    <summary>  The paper proposes StegTorrent a new network steganographic method for the
popular P2P file transfer service-BitTorrent. It is based on modifying the
order of data packets in the peer-peer data exchange protocol. Unlike other
existing steganographic methods that modify the packets' order it does not
require any synchronization. Experimental results acquired from prototype
implementation proved that it provides high steganographic bandwidth of up to
270 b/s while introducing little transmission distortion and providing
difficult detectability.
</summary>
    <author>
      <name>Pawel Kopiczko</name>
    </author>
    <author>
      <name>Wojciech Mazurczyk</name>
    </author>
    <author>
      <name>Krzysztof Szczypiorski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.4264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.4264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.7324v1</id>
    <updated>2013-04-27T05:05:52Z</updated>
    <published>2013-04-27T05:05:52Z</published>
    <title>Adaptive Software Radio Steganography</title>
    <summary>  This paper presents an adaptable steganography (information hiding) method
for digital radio communication. Many radio steganography methods exist, but
most are defined at higher levels of the protocol stack and are thus protocol
dependent. In contrast, this method is defined at the physical layer, which
makes it widely applicable regardless of the protocols used at higher layers.
This approach is also adaptive; the covertness of the hidden channel is simple
to control via a single continuous parameter either manually or automatically.
Several variations are introduced, each with performance evaluated by
simulation. Results show this to be a feasible method with a reasonable
trade-off between performance and covertness.
</summary>
    <author>
      <name>David E. Robillard</name>
    </author>
    <link href="http://arxiv.org/abs/1304.7324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.7324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.4102v1</id>
    <updated>2013-04-11T14:30:46Z</updated>
    <published>2013-04-11T14:30:46Z</published>
    <title>Using Bias Optimization for Reversible Data Hiding Using Image
  Interpolation</title>
    <summary>  In this paper, we propose a reversible data hiding method in the spatial
domain for compressed grayscale images. The proposed method embeds secret bits
into a compressed thumbnail of the original image by using a novel
interpolation method and the Neighbour Mean Interpolation (NMI) technique as
scaling up to the original image occurs. Experimental results presented in this
paper show that the proposed method has significantly improved embedding
capacities over the approach proposed by Jung and Yoo.
</summary>
    <author>
      <name>Andrew Rudder</name>
    </author>
    <author>
      <name>Wayne Goodridge</name>
    </author>
    <author>
      <name>Shareeda Mohammed</name>
    </author>
    <link href="http://arxiv.org/abs/1305.4102v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.4102v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.6328v1</id>
    <updated>2013-07-24T08:32:38Z</updated>
    <published>2013-07-24T08:32:38Z</published>
    <title>Digital Watermarking for Image AuthenticationBased on Combined DCT, DWT
  and SVD Transformation</title>
    <summary>  This paper presents a hybrid digital image watermarking based on Discrete
Wavelet Transform (DWT), Discrete Cosine Transform (DCT) and Singular Value
Decomposition (SVD) in a zigzag order. From DWT we choose the high band to
embed the watermark that facilities to add more information, gives more
invisibility and robustness against some attacks. Such as geometric attack.
Zigzag method is applied to map DCT coefficients into four quadrants that
represent low, mid and high bands. Finally, SVD is applied to each quadrant.
</summary>
    <author>
      <name>Mohammad Ibrahim Khan</name>
    </author>
    <author>
      <name>Md. Maklachur Rahman</name>
    </author>
    <author>
      <name>Md. Iqbal Hasan Sarker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures and 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 10,
  Issue 3, No 1, May 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.6328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.6328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.4263v1</id>
    <updated>2013-08-20T09:02:57Z</updated>
    <published>2013-08-20T09:02:57Z</published>
    <title>Compressive Sampling for the Packet Loss Recovery in Audio Multimedia
  Streaming</title>
    <summary>  The aim of this paper is to introduce a new schema, based on a Compressive
Sampling technique, for the recovery of lost data in multimedia streaming. The
audio streaming data are encapsuled in different packets by using an
interleaving technique. The Compressive Sampling technique is used to recover
audio information in case of lost packets. Experimental results are presented
on speech and musical audio signals to illustrate the performances and the
capabilities of the proposed methodology.
</summary>
    <author>
      <name>Angelo Ciaramella</name>
    </author>
    <author>
      <name>Giulio Giunta</name>
    </author>
    <link href="http://arxiv.org/abs/1308.4263v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.4263v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.5653v1</id>
    <updated>2013-10-09T10:24:07Z</updated>
    <published>2013-10-09T10:24:07Z</published>
    <title>Blind and robust images watermarking based on wavelet and edge insertion</title>
    <summary>  This paper gives a new scheme of watermarking technique related to insert the
mark by adding edge in HH sub-band of the host image after wavelet
decomposition. Contrary to most of the watermarking algorithms in wavelet
domain, our method is blind and results show that it is robust against the JPEG
and GIF compression, histogram and spectrum spreading, noise adding and small
rotation. Its robustness against compression is better than others watermarking
algorithms reported in the literature. The algorithm is flexible because its
capacity or robustness can be improved by modifying some parameters.
</summary>
    <author>
      <name>Henri Bruno Razafindradina</name>
    </author>
    <author>
      <name>Attoumani Mohamed Karim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCIS, Vol.3, No. 3, September 2013, pp 23-30</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1310.5653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.5653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Computer Science" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.6.5; C.2.0; D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5050v1</id>
    <updated>2013-12-18T06:41:12Z</updated>
    <published>2013-12-18T06:41:12Z</published>
    <title>Fake View Analytics in Online Video Services</title>
    <summary>  Online video-on-demand(VoD) services invariably maintain a view count for
each video they serve, and it has become an important currency for various
stakeholders, from viewers, to content owners, advertizers, and the online
service providers themselves. There is often significant financial incentive to
use a robot (or a botnet) to artificially create fake views. How can we detect
the fake views? Can we detect them (and stop them) using online algorithms as
they occur? What is the extent of fake views with current VoD service
providers? These are the questions we study in the paper. We develop some
algorithms and show that they are quite effective for this problem.
</summary>
    <author>
      <name>Liang Chen</name>
    </author>
    <author>
      <name>Yipeng Zhou</name>
    </author>
    <author>
      <name>Dah Ming Chiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 15 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.5050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.5561v1</id>
    <updated>2014-01-22T05:58:55Z</updated>
    <published>2014-01-22T05:58:55Z</published>
    <title>A Study of Various Steganographic Techniques Used for Information Hiding</title>
    <summary>  Steganography derives from the Greek word steganos, meaning covered or
secret, and graphy (writing or drawing). Steganography is a technology where
modern data compression, information theory, spread spectrum, and cryptography
technologies are brought together to satisfy the need for privacy on the
Internet. This paper is an attempt to analyse the various techniques used in
steganography and to identify areas in which this technique can be applied, so
that the human race can be benefited at large.
</summary>
    <author>
      <name>C. P. Sumathi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer Science, SDNB Vaishnav College For Women, Chennai, India</arxiv:affiliation>
    </author>
    <author>
      <name>T. Santanam</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer Science, DG Vaishnav College For Men, Chennai, India</arxiv:affiliation>
    </author>
    <author>
      <name>G. Umamaheswari</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer Science, SDNB Vaishnav College For Women, Chennai, India</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 Pages, International Journal of Computer Science &amp; Engineering
  Survey (IJCSES) Vol.4, No.6, December 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.5561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.5561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.6034v1</id>
    <updated>2014-02-25T02:49:01Z</updated>
    <published>2014-02-25T02:49:01Z</published>
    <title>A DCT Approximation for Image Compression</title>
    <summary>  An orthogonal approximation for the 8-point discrete cosine transform (DCT)
is introduced. The proposed transformation matrix contains only zeros and ones;
multiplications and bit-shift operations are absent. Close spectral behavior
relative to the DCT was adopted as design criterion. The proposed algorithm is
superior to the signed discrete cosine transform. It could also outperform
state-of-the-art algorithms in low and high image compression scenarios,
exhibiting at the same time a comparable computational complexity.
</summary>
    <author>
      <name>R. J. Cintra</name>
    </author>
    <author>
      <name>F. M. Bayer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2011.2163394</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2011.2163394" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Signal Processing Letters, 18(10):579-582, October 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.6034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.6034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.4158v1</id>
    <updated>2014-03-17T16:42:43Z</updated>
    <published>2014-03-17T16:42:43Z</published>
    <title>A Methodology for Implementation of MMS Client on Embedded Platforms</title>
    <summary>  MMS (Multimedia Messaging Service) is the next generation of messaging
services in multimedia mobile communications. MMS enables messaging with full
multimedia content including images, audios, videos, texts and data, from
client to client or e-mail. MMS is based on WAP technology, so it is technology
independent. This means that enabling messages from a GSM/GPRS network to be
sent to a TDMA or WCDMA network. In this paper a methodology for implementing
MMS client on embedded platforms especially on Wince OS is described.
</summary>
    <author>
      <name>A. A. Milani</name>
    </author>
    <author>
      <name>Reza Rahimi</name>
    </author>
    <link href="http://arxiv.org/abs/1403.4158v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.4158v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.1313v1</id>
    <updated>2014-03-31T14:19:04Z</updated>
    <published>2014-03-31T14:19:04Z</published>
    <title>Color to Gray and Back transformation for distributing color digital
  images</title>
    <summary>  The Color to Gray and Back transformation watermarking with a secrete key is
considered. Color is embedded into the bit planes of the luminosity component
of the YUV color space with the help of a block algorithm that allows using not
only the least significant bits. An application of the problem of distributing
color digital images from a data base among legitimate users is discussed. The
proposed protocol can protect original images from unauthorized copying.
</summary>
    <author>
      <name>V. N. Gorbachev</name>
    </author>
    <author>
      <name>E. M. Kaynarova</name>
    </author>
    <author>
      <name>I. K. Metelev</name>
    </author>
    <author>
      <name>E. S. Yakovleva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, submitted to The International Conference on
  Computer Graphics and Vision, GraphiCon'2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.1313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.1313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.2237v1</id>
    <updated>2014-02-02T17:09:56Z</updated>
    <published>2014-02-02T17:09:56Z</published>
    <title>Steganography -- coding and intercepting the information from encoded
  pictures in the absence of any initial information</title>
    <summary>  The work includes implementation and extraction algorithms capabilities test,
without any additional data (starting position, the number of bits used, gap
between the amount of data encoded) information from encoded files (mostly
images). The software is written using OpenMP standard [1], which allowed them
to run on parallel computers. Performance tests were carried out on computers,
Blue Gene/P [2], Blue Gene/Q [3] and the system consisting of four AMD Opteron
6272 [4]. Source code is available under GNU GPL v3 license and are available
in a repository OLib [5].
</summary>
    <author>
      <name>Monika Kwiatkowska</name>
    </author>
    <author>
      <name>Lukasz Swierczewski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures, 5 tables, LVEE 2014 Conference Proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.2237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.2237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.2592v1</id>
    <updated>2014-04-09T01:35:26Z</updated>
    <published>2014-04-09T01:35:26Z</published>
    <title>Reduction of Field Loss by a Video Processing System</title>
    <summary>  Streaming of 60 de-interlaced fields per second digital uncompressed video
with 720x480 resolution without a loss of video fields is one of the desired
technologies by scientists in biomechanics. If it is possible to stream digital
uncompressed video without dropped video fields, then a sophisticated computer
analysis of the transmitted via IEEE 1394a connection video is possible. Such
process is used in biomechanics when it is important to analyze athletes
performance via streaming digital uncompressed video to a computer and then
analyzing it using specific software such as Arial Performance Analysis
Systems.
</summary>
    <author>
      <name>Dr. Timur Mirzoev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1404.2344</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Southeast Decision Sciences Institute. Conference Proceedings.
  SEDSI 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.2592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.2592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.7335v1</id>
    <updated>2014-04-29T12:30:36Z</updated>
    <published>2014-04-29T12:30:36Z</published>
    <title>Antescofo Intermediate Representation</title>
    <summary>  We describe an intermediate language designed as a medium-level internal
representation of programs of the interactive music system Antescofo. This
representation is independent both of the Antescofo source language and of the
architecture of the execution platform. It is used in tasks such as
verification of timings, model-based conformance testing, static control-flow
analysis or simulation. This language is essentially a flat representation of
Antescofo's code, as a finite state machine extended with local and global
variables, with delays and with concurrent threads creation. It features a
small number of simple instructions which are either blocking (wait for
external event, signal or duration) or not (variable assignment, message
emission and control).
</summary>
    <author>
      <name>Florent Jacquemard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria Paris-Rocquencourt, STMS</arxiv:affiliation>
    </author>
    <author>
      <name>Cl√©ment Poncelet Sanchez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria Paris-Rocquencourt, STMS</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">RR-8520 (2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.7335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.7335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.3207v1</id>
    <updated>2014-05-12T16:45:21Z</updated>
    <published>2014-05-12T16:45:21Z</published>
    <title>An Adaptive Watermarking Process in Hadamard Transform</title>
    <summary>  An adaptive visible/invisible watermarking scheme is done to prevent the
privacy and preserving copyright protection of digital data using Hadamard
transform based on the scaling factor of the image. The value of scaling factor
depends on the control parameter. The scaling factor is calculated to embedded
the watermark. Depend upon the control parameter the visible and invisible
watermarking is determined. The proposed Hadamard transform domain method is
more robust again image/signal processing attacks. Furthermore, it also shows
that the proposed method confirm the efficiency through various performance
analysis and experimental results.
</summary>
    <author>
      <name>Parvathavarthini S.</name>
    </author>
    <author>
      <name>Shanthakumari R</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Pages, 10 Figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.3207v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.3207v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.5119v1</id>
    <updated>2014-05-20T15:21:52Z</updated>
    <published>2014-05-20T15:21:52Z</published>
    <title>Steganalysis: Detecting LSB Steganographic Techniques</title>
    <summary>  Steganalysis means analysis of stego images. Like cryptanalysis, steganalysis
is used to detect messages often encrypted using secret key from stego images
produced by steganography techniques. Recently lots of new and improved
steganography techniques are developed and proposed by researchers which
require robust steganalysis techniques to detect the stego images having
minimum false alarm rate. This paper discusses about the different Steganalysis
techniques and help to understand how, where and when this techniques can be
used based on different situations.
</summary>
    <author>
      <name>Tanmoy Sarkar</name>
    </author>
    <author>
      <name>Sugata Sanyal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.5119v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5119v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.5948v1</id>
    <updated>2014-05-23T02:24:11Z</updated>
    <published>2014-05-23T02:24:11Z</published>
    <title>Low-complexity video encoder for smart eyes based on underdetermined
  blind signal separation</title>
    <summary>  This paper presents a low complexity video coding method based on
Underdetermined Blind Signal Separation (UBSS). The detailed coding framework
is designed. Three key techniques are proposed to enhance the compression ratio
and the quality of the decoded frames. The experiments validate that the
proposed method costs 30ms encoding time less than DISCOVER. The simulation
shows that this new method can save 50% energy compared with H.264.
</summary>
    <author>
      <name>Jing Liu</name>
    </author>
    <author>
      <name>Fei Qiao</name>
    </author>
    <author>
      <name>Zhijian Ou</name>
    </author>
    <author>
      <name>Huazhong Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1405.5948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.6147v1</id>
    <updated>2014-05-08T08:04:21Z</updated>
    <published>2014-05-08T08:04:21Z</published>
    <title>Jpeg Image Compression Using Discrete Cosine Transform - A Survey</title>
    <summary>  Due to the increasing requirements for transmission of images in computer,
mobile environments, the research in the field of image compression has
increased significantly. Image compression plays a crucial role in digital
image processing, it is also very important for efficient transmission and
storage of images. When we compute the number of bits per image resulting from
typical sampling rates and quantization methods, we find that Image compression
is needed. Therefore development of efficient techniques for image compression
has become necessary .This paper is a survey for lossy image compression using
Discrete Cosine Transform, it covers JPEG compression algorithm which is used
for full-colour still image applications and describes all the components of
it.
</summary>
    <author>
      <name>A. M. Raid</name>
    </author>
    <author>
      <name>W. M. Khedr</name>
    </author>
    <author>
      <name>M. A. El-dosuky</name>
    </author>
    <author>
      <name>Wesam Ahmed</name>
    </author>
    <link href="http://arxiv.org/abs/1405.6147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.6147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.6012v1</id>
    <updated>2014-06-23T18:29:59Z</updated>
    <published>2014-06-23T18:29:59Z</published>
    <title>Designing Sound Collaboratively - Perceptually Motivated Audio Synthesis</title>
    <summary>  In this contribution, we will discuss a prototype that allows a group of
users to design sound collaboratively in real time using a multi-touch
tabletop. We make use of a machine learning method to generate a mapping from
perceptual audio features to synthesis parameters. This mapping is then used
for visualization and interaction. Finally, we discuss the results of a
comparative evaluation study.
</summary>
    <author>
      <name>Niklas Kl√ºgel</name>
    </author>
    <author>
      <name>Timo Becker</name>
    </author>
    <author>
      <name>Georg Groh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of submission to conference proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.6012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.6012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.2221v1</id>
    <updated>2014-07-06T06:29:34Z</updated>
    <published>2014-07-06T06:29:34Z</published>
    <title>Sonic interaction with a virtual orchestra of factory machinery</title>
    <summary>  This paper presents an immersive application where users receive sound and
visual feedbacks on their interactions with a virtual environment. In this
application, the users play the part of conductors of an orchestra of factory
machines since each of their actions on interaction devices triggers a pair of
visual and audio responses. Audio stimuli were spatialized around the listener.
The application was exhibited during the 2013 Science and Music day and
designed to be used in a large immersive system with head tracking, shutter
glasses and a 10.2 loudspeaker configuration.
</summary>
    <author>
      <name>Laurent Simon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIMSI</arxiv:affiliation>
    </author>
    <author>
      <name>Florian Nouviale</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INSA Rennes, INRIA - IRISA</arxiv:affiliation>
    </author>
    <author>
      <name>Ronan Gaugne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">UR1</arxiv:affiliation>
    </author>
    <author>
      <name>Val√©rie Gouranton</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INSA Rennes, INRIA - IRISA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Sonic Interaction for Virtual Environments, Minneapolis : United
  States (2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.2221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.2221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.2729v1</id>
    <updated>2014-07-10T09:07:03Z</updated>
    <published>2014-07-10T09:07:03Z</published>
    <title>Genetic Algorithm in Audio Steganography</title>
    <summary>  With the advancement of communication technology,data is exchanged digitally
over the network. At the other side the technology is also proven as a tool for
unauthorized access to attackers. Thus the security of data to be transmitted
digitally should get prime focus. Data hiding is the common approach to secure
data. In steganography technique, the existence of data is concealed. GA is an
emerging component of AI to provide suboptimal solutions. In this paper the use
of GA in Steganography is explored to find future scope of research.
</summary>
    <author>
      <name>Manisha Rana</name>
    </author>
    <author>
      <name>Rohit Tanwar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22315381/IJETT-V13P206</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22315381/IJETT-V13P206" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,2 figures Published with International Journal of Engineering
  Trends and Technology (IJETT). arXiv admin note: text overlap with
  arXiv:1003.4084, arXiv:1205.2800 by other authors without attribution</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJETT, V13(1),29-34 July 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1407.2729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.2729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.4735v1</id>
    <updated>2014-07-17T16:56:15Z</updated>
    <published>2014-07-17T16:56:15Z</published>
    <title>A Survey of Digital Watermarking Techniques and its Applications</title>
    <summary>  Digital media is the need of a people now a day as the alternate of paper
media.As the technology grown up digital media required protection while
transferring through internet or others mediums.Watermarking techniques have
been developed to fulfill this requirement.This paper aims to provide a
detailed survey of all watermarking techniques specially focuses on image
watermarking types and its applications in today world.
</summary>
    <author>
      <name>Lalit Kumar Saini</name>
    </author>
    <author>
      <name>Vishal Shrivastava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCST V2(3): Page(70-73) May-June 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1407.4735v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.4735v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.4865v4</id>
    <updated>2014-08-14T12:10:59Z</updated>
    <published>2014-07-18T01:37:21Z</published>
    <title>Robust Lossless Semi Fragile Information Protection in Images</title>
    <summary>  Internet security finds it difficult to keep the information secure and to
maintain the integrity of the data. Sending messages over the internet secretly
is one of the major tasks as it is widely used for passing the message.
</summary>
    <author>
      <name>Pushkar Dixit</name>
    </author>
    <author>
      <name>Nishant Singh</name>
    </author>
    <author>
      <name>Jay Prakash Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to a crucial error in
  diffusion process</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.4865v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.4865v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0915v1</id>
    <updated>2014-09-02T22:59:52Z</updated>
    <published>2014-09-02T22:59:52Z</published>
    <title>An Approach for Text Steganography Based on Markov Chains</title>
    <summary>  A text steganography method based on Markov chains is introduced, together
with a reference implementation. This method allows for information hiding in
texts that are automatically generated following a given Markov model. Other
Markov - based systems of this kind rely on big simplifications of the language
model to work, which produces less natural looking and more easily detectable
texts. The method described here is designed to generate texts within a good
approximation of the original language model provided.
</summary>
    <author>
      <name>H. Hernan Moraldo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at 41 JAIIO - WSegI 2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">41 JAIIO - WSegI 2012, ISSN: 2313-9110, pages 21 - 35</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1409.0915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68P25, 94A60" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.2100v1</id>
    <updated>2014-10-08T13:24:06Z</updated>
    <published>2014-10-08T13:24:06Z</published>
    <title>A New Method for Estimating the Widths of JPEG Images</title>
    <summary>  Image width is important for image understanding. We propose a novel method
to estimate widths for JPEG images when their widths are not available. The key
idea is that the distance between two decoded MCUs (Minimum Coded Unit)
adjacent in the vertical direction is usually small, which is measured by the
average Euclidean distance between the pixels from the bottom row of the top
MCU and the top row of the bottom MCU. On PASCAL VOC 2010 challenge dataset and
USC-SIPI image database, experimental results show the high performance of the
proposed approach.
</summary>
    <author>
      <name>Wu Xianyan</name>
    </author>
    <author>
      <name>Han Qi</name>
    </author>
    <author>
      <name>Le Dan</name>
    </author>
    <author>
      <name>Niu Xiamu</name>
    </author>
    <link href="http://arxiv.org/abs/1410.2100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.2100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.2324v1</id>
    <updated>2014-10-09T00:53:11Z</updated>
    <published>2014-10-09T00:53:11Z</published>
    <title>Recommendation Scheme Based on Converging Properties for Contents
  Broadcasting</title>
    <summary>  Popular videos are often clicked by a mount of users in a short period. With
content recommendation, the popular contents could be broadcast to the
potential users in wireless network, to save huge transmitting resource. In
this paper, the contents propagation model is analyzed due to users' historical
behavior, location, and the converging properties in wireless data
transmission, with the users' communication log in the Chinese commercial
cellular network. And a recommendation scheme is proposed to achieve high
energy efficiency.
</summary>
    <author>
      <name>Jian Sun</name>
    </author>
    <author>
      <name>Xiaofeng Zhong</name>
    </author>
    <author>
      <name>Xuan Zhou</name>
    </author>
    <author>
      <name>Xiaolong Fu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages. This work is present at 2015 International Workshop on
  Networking Issues in Multimedia Entertainment (NIME'15)</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.2324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.2324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.6928v1</id>
    <updated>2014-11-25T17:23:19Z</updated>
    <published>2014-11-25T17:23:19Z</published>
    <title>A Tag Identification Approach Based On Fragile Watermark</title>
    <summary>  This paper proposes a tag identify approach based on fragile Watermark that
based on Least significant bit of the replacement that we first use a special
way to initialize the cover to ensure that we can use random positions to embed
the information of tag. Using this way enhance the security of other to get the
right information of this tag. Finally as long as the covered information can
be decoded, the completeness and accuracy of the tag information can be
guaranteed. the result of simulation experiment show that this approach has
high sensitivity and security .
</summary>
    <author>
      <name>Jianbiao Lin</name>
    </author>
    <author>
      <name>Ke Ji</name>
    </author>
    <author>
      <name>Hui Lin</name>
    </author>
    <author>
      <name>Enyan Wu</name>
    </author>
    <author>
      <name>Xin Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1411.6928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.6928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.4576v1</id>
    <updated>2014-12-15T13:19:24Z</updated>
    <published>2014-12-15T13:19:24Z</published>
    <title>Multi-Hypothesis Compressed Video Sensing Technique</title>
    <summary>  In this paper, we present a compressive sampling and Multi-Hypothesis (MH)
reconstruction strategy for video sequences which has a rather simple encoder,
while the decoding system is not that complex. We introduce a convex cost
function that incorporates the MH technique with the sparsity constraint and
the Tikhonov regularization. Consequently, we derive a new iterative algorithm
based on these criteria. This algorithm surpasses its counterparts (Elasticnet
and Tikhonov) in the recovery performance. Besides it is computationally much
faster than the Elasticnet and comparable to the Tikhonov. Our extensive
simulation results confirm these claims.
</summary>
    <author>
      <name>Masoumeh Azghani</name>
    </author>
    <author>
      <name>Mostafa Karimi</name>
    </author>
    <author>
      <name>Farokh Marvasti</name>
    </author>
    <link href="http://arxiv.org/abs/1412.4576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.4576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.01758v1</id>
    <updated>2015-01-08T08:08:50Z</updated>
    <published>2015-01-08T08:08:50Z</published>
    <title>Enhance Robustness of Image-in-Image Watermarking through Data
  Partitioning</title>
    <summary>  Vulnerability of watermarking schemes against intense signal processing
attacks is generally a major concern, particularly when there are techniques to
reproduce an acceptable copy of the original signal with no chance for
detecting the watermark. In this paper, we propose a two-layer, data
partitioning (DP) based, image in image watermarking method in the DCT domain
to improve the watermark detection performance. Truncated singular value
decomposition, binary wavelet decomposition and spatial scalability idea in
H.264/SVC are analyzed and employed as partitioning methods. It is shown that
the proposed scheme outperforms its two recent competitors in terms of both
data payload and robustness to intense attacks.
</summary>
    <author>
      <name>Hossein Bakhshi Golestani</name>
    </author>
    <author>
      <name>Shahrokh Ghaemmaghami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 7 figures, IEEE TENCON2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.01758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.01758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.02659v1</id>
    <updated>2015-01-12T14:27:37Z</updated>
    <published>2015-01-12T14:27:37Z</published>
    <title>PacMap: Transferring PacMan to the Physical Realm</title>
    <summary>  This paper discusses the implementation of the pervasive game PacMap.
Openness and portability have been the main design objectives for PacMap. We
elaborate on programming techniques which may be applicable to a broad range of
location-based games that involve the movement of virtual characters over map
interfaces. In particular, we present techniques to execute shortest path
algorithms on spatial environments bypassing the restrictions imposed by
commercial mapping services. Last, we present ways to improve the movement and
enhance the intelligence of virtual characters taking into consideration the
actions and position of players in location-based games.
</summary>
    <author>
      <name>Thomas Chatzidimitris</name>
    </author>
    <author>
      <name>Damianos Gavalas</name>
    </author>
    <author>
      <name>Vlasios Kasapakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, Proceedings of the International Conference on
  Pervasive Games (PERGAMES'2014), Rome, Italy, 27 October 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.02659v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.02659v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.07034v1</id>
    <updated>2015-01-28T08:58:20Z</updated>
    <published>2015-01-28T08:58:20Z</published>
    <title>Embedding of binary image in the Gray planes</title>
    <summary>  For watermarking of the digital grayscale image its Gray planes have been
used. With the help of the introduced representation over Gray planes the LSB
embedding method and detection have been discussed. It found that data, a
binary image, hidden in the Gray planes is more robust to JPEG lossy
compression than in the bit planes.
</summary>
    <author>
      <name>V. N. Gorbachev</name>
    </author>
    <author>
      <name>L. A. Denisov</name>
    </author>
    <author>
      <name>E. M. Kainarova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures, Proceeding of 24rd International Conference on
  Computer Graphics and Vision GraphiCon'2014, Sept.30 - Oct.3,2014,
  Rostov-on-Don, Russia</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.07034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.07034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.05226v1</id>
    <updated>2015-04-20T20:53:29Z</updated>
    <published>2015-04-20T20:53:29Z</published>
    <title>On the Security of a Revised Fragile Watermarking Scheme</title>
    <summary>  This paper analyzes a revised fragile watermarking scheme proposed by Botta
et al. which was developed as a revision of the watermarking scheme previously
proposed by Rawat et al. A new attack is presented that allows an attacker to
apply a valid watermark on tampered images, therefore circumventing the
protection that the watermarking scheme under study was supposed to offer.
Furthermore, the presented attack has very low computational and memory
requirements.
</summary>
    <author>
      <name>Daniel Caragata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be submitted to AEU INT J ELECTRON C</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.05226v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.05226v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.05407v1</id>
    <updated>2015-05-20T14:46:51Z</updated>
    <published>2015-05-20T14:46:51Z</published>
    <title>Compressive Sensing of Large-Scale Images: An Assumption-Free Approach</title>
    <summary>  Cost-efficient compressive sensing of big media data with fast reconstructed
high-quality results is very challenging. In this paper, we propose a new
large-scale image compressive sensing method, composed of operator-based
strategy in the context of fixed point continuation method and weighted LASSO
with tree structure sparsity pattern. The main characteristic of our method is
free from any assumptions and restrictions. The feasibility of our method is
verified via simulations and comparisons with state-of-the-art algorithms.
</summary>
    <author>
      <name>Wei-Jie Liang</name>
    </author>
    <author>
      <name>Gang-Xuan Lin</name>
    </author>
    <author>
      <name>Chun-Shien Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.05407v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.05407v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.07757v1</id>
    <updated>2015-05-28T16:54:13Z</updated>
    <published>2015-05-28T16:54:13Z</published>
    <title>Micro protocol engineering for unstructured carriers: On the embedding
  of steganographic control protocols into audio transmissions</title>
    <summary>  Network steganography conceals the transfer of sensitive information within
unobtrusive data in computer networks. So-called micro protocols are
communication protocols placed within the payload of a network steganographic
transfer. They enrich this transfer with features such as reliability, dynamic
overlay routing, or performance optimization --- just to mention a few. We
present different design approaches for the embedding of hidden channels with
micro protocols in digitized audio signals under consideration of different
requirements. On the basis of experimental results, our design approaches are
compared, and introduced into a protocol engineering approach for micro
protocols.
</summary>
    <author>
      <name>Matthias Naumann</name>
    </author>
    <author>
      <name>Steffen Wendzel</name>
    </author>
    <author>
      <name>Wojciech Mazurczyk</name>
    </author>
    <author>
      <name>J√∂rg Keller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 7 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.07757v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.07757v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.01501v1</id>
    <updated>2015-06-04T08:10:13Z</updated>
    <published>2015-06-04T08:10:13Z</published>
    <title>Optimum Decoder for an Additive Video Watermarking with Laplacian Noise
  in H.264</title>
    <summary>  In this paper, we investigate an additive video watermarking method in H.264
standard in presence of the Laplacian noise. In some applications, due to the
loss of some pixels or a region of a frame, we resort to Laplacian noise rather
than Gaussian one. The embedding is performed in the transform domain; while an
optimum and a sub-optimum decoder are derived for the proposed Laplacian model.
Simulation results show that the proposed watermarking scheme has suitable
performance with enough transparency required for watermarking applications.
</summary>
    <author>
      <name>Nematollah Zarmehi</name>
    </author>
    <author>
      <name>Morteza Banagar</name>
    </author>
    <author>
      <name>Mohammad Ali Akhaee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISCISC.2013.6767352</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISCISC.2013.6767352" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2013 10th International ISC Conference on Information Security and
  Cryptology (ISCISC),Aug. 2013, pp. 1-5</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.01501v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.01501v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.03681v1</id>
    <updated>2015-06-11T14:05:45Z</updated>
    <published>2015-06-11T14:05:45Z</published>
    <title>A Novel Approach for Image Steganography in Spatial Domain</title>
    <summary>  This paper presents a new approach for hiding information in digital image in
spatial domain. In this approach three bits of message is embedded in a pixel
using Lucas number system but only one bit plane is allowed for alternation.
The experimental results show that the proposed method has the larger capacity
of embedding data, high peak signal to noise ratio compared to existing methods
and is hardly detectable for steganolysis algorithm.
</summary>
    <author>
      <name>Fatema Akhter</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Global Journal of Computer Science and Technology, Volume 13,
  Issue 7, pp. 1-6 Year 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1506.03681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.03681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.08811v1</id>
    <updated>2015-05-23T21:43:59Z</updated>
    <published>2015-05-23T21:43:59Z</published>
    <title>A new approach for image compression using normal matrices</title>
    <summary>  In this paper, we present methods for image compression on the basis of
eigenvalue decomposition of normal matrices. The proposed methods are
convenient and self-explanatory, requiring fewer and easier computations as
compared to some existing methods. Through the proposed techniques, the image
is transformed to the space of normal matrices. Then, the properties of
spectral decomposition are dealt with to obtain compressed images. Experimental
results are provided to illustrate the validity of the methods.
</summary>
    <author>
      <name>E. Kokabifar</name>
    </author>
    <author>
      <name>G. B. Loghmani</name>
    </author>
    <author>
      <name>A. Latif</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1506.01952</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.08811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01485v1</id>
    <updated>2015-08-06T18:39:19Z</updated>
    <published>2015-08-06T18:39:19Z</published>
    <title>Arabic Text Watermarking: A Review</title>
    <summary>  The using of the internet with its technologies and applications have been
increased rapidly. So, protecting the text from illegal use is too needed .
Text watermarking is used for this purpose. Arabic text has many
characteristics such existing of diacritics , kashida (extension character) and
points above or under its letters .Each of Arabic letters can take different
shapes with different Unicode. These characteristics are utilized in the
watermarking process. In this paper, several methods are discussed in the area
of Arabic text watermarking with its advantages and disadvantages .Comparison
of these methods is done in term of capacity, robustness and Imperceptibility.
</summary>
    <author>
      <name>Reem Ahmed Alotaibi</name>
    </author>
    <author>
      <name>Lamiaa A. Elrefaei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 4 tables and 19 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Artificial Intelligence &amp; Applications
  (IJAIA) Vol. 6, No. 4, July 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1508.01485v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.01485v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.03278v1</id>
    <updated>2015-09-10T19:01:14Z</updated>
    <published>2015-09-10T19:01:14Z</published>
    <title>A New Method For Digital Watermarking Based on Combination of DCT and
  PCA</title>
    <summary>  In the digital watermarking with DCT method,the watermark is located within a
range of DCT coefficients of the cover image. In this paper to use the
low-frequency band, a new method is proposed by using a combination of the DCT
and PCA transform. The proposed method is compared to other DCT methods, our
method is robust and keeps the quality of cover image, also increases capacity
of the watermarking.
</summary>
    <author>
      <name>Arash Saboori</name>
    </author>
    <author>
      <name>S. Abolfazl Hosseini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TELFOR.2014.7034461</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TELFOR.2014.7034461" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Telecommunications Forum Telfor (TELFOR), 2014 22nd</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.03278v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.03278v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.03090v1</id>
    <updated>2015-10-11T19:15:56Z</updated>
    <published>2015-10-11T19:15:56Z</published>
    <title>An Extension of Interactive Scores for Multimedia Scenarios with
  Temporal Relations for Micro and Macro Controls</title>
    <summary>  Software to design multimedia scenarios is usually based either on a fixed
timeline or on cue lists, but both models are unrelated temporally. On the
contrary, the formalism of interactive scores can describe multimedia scenarios
with flexible and fixed temporal relations among the objects of the scenario,
but cannot express neither temporal relations for micro controls nor signal
processing. We extend interactive scores with such relations and with sound
processing. We show some applications and we describe how they can be
implemented in Pure Data. Our implementation has low average relative jitter
even under high cpu load.
</summary>
    <author>
      <name>Mauricio Toro</name>
    </author>
    <author>
      <name>Myriam Desainte-Catherine</name>
    </author>
    <author>
      <name>Julien Castet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">extended version of article presented in the Sound and Music
  Computing Conference 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.03090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.03090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.3; D.1.6; H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.00118v1</id>
    <updated>2015-10-31T12:30:52Z</updated>
    <published>2015-10-31T12:30:52Z</published>
    <title>A new chaos-based watermarking algorithm</title>
    <summary>  This paper introduces a new watermarking algorithm based on discrete chaotic
iterations. After defining some coefficients deduced from the description of
the carrier medium, chaotic discrete iterations are used to mix the watermark
and to embed it in the carrier medium. It can be proved that this procedure
generates topological chaos, which ensures that desired properties of a
watermarking algorithm are satisfied.
</summary>
    <author>
      <name>Jacques M. Bahi</name>
    </author>
    <author>
      <name>Christophe Guyeux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SECRYPT 2010, International Conference on Security and Cryptograph.
  Submitted as a regular paper, accepted as a short one. arXiv admin note: text
  overlap with arXiv:0810.4713, arXiv:1012.4620</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.00118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.00118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.03398v1</id>
    <updated>2015-11-11T06:44:31Z</updated>
    <published>2015-11-11T06:44:31Z</published>
    <title>A GMM-Based Stair Quality Model for Human Perceived JPEG Images</title>
    <summary>  Based on the notion of just noticeable differences (JND), a stair quality
function (SQF) was recently proposed to model human perception on JPEG images.
Furthermore, a k-means clustering algorithm was adopted to aggregate JND data
collected from multiple subjects to generate a single SQF. In this work, we
propose a new method to derive the SQF using the Gaussian Mixture Model (GMM).
The newly derived SQF can be interpreted as a way to characterize the mean
viewer experience. Furthermore, it has a lower information criterion (BIC)
value than the previous one, indicating that it offers a better model. A
specific example is given to demonstrate the advantages of the new approach.
</summary>
    <author>
      <name>Sudeng Hu</name>
    </author>
    <author>
      <name>Haiqiang Wang</name>
    </author>
    <author>
      <name>C. -C. Jay Kuo</name>
    </author>
    <link href="http://arxiv.org/abs/1511.03398v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.03398v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.07004v1</id>
    <updated>2015-11-22T12:33:08Z</updated>
    <published>2015-11-22T12:33:08Z</published>
    <title>Understanding Music Playlists</title>
    <summary>  As music streaming services dominate the music industry, the playlist is
becoming an increasingly crucial element of music consumption. Con- sequently,
the music recommendation problem is often casted as a playlist generation prob-
lem. Better understanding of the playlist is there- fore necessary for
developing better playlist gen- eration algorithms. In this work, we analyse
two playlist datasets to investigate some com- monly assumed hypotheses about
playlists. Our findings indicate that deeper understanding of playlists is
needed to provide better prior infor- mation and improve machine learning
algorithms in the design of recommendation systems.
</summary>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>George Fazekas</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Machine Learning (ICML) 2015, Machine
  Learning for Music Discovery Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.07004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.07004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.00299v1</id>
    <updated>2016-01-03T14:23:32Z</updated>
    <published>2016-01-03T14:23:32Z</published>
    <title>Capacity Enlargement Of The PVD Steganography Method Using The GLM
  Technique</title>
    <summary>  In most steganographic methods, increasing in the capacity leads to decrease
in the quality of the stego-image, so in this paper, we propose to combine two
existing techniques, Pixel value differencing and Gray Level Modification, to
come up with a hybrid steganography scheme which can hide more information
without having to compromise much on the quality of the stego-image.
Experimental results demonstrate that the proposed approach has larger capacity
while its results are imperceptible. In comparison with original PVD method
criterion of the quality is declined by 2% dB averagely while the capacity is
increased around 25%.
</summary>
    <author>
      <name>Mehdi Safarpour</name>
    </author>
    <author>
      <name>Mostafa Charmi</name>
    </author>
    <link href="http://arxiv.org/abs/1601.00299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.00299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01386v1</id>
    <updated>2016-01-07T03:32:27Z</updated>
    <published>2016-01-07T03:32:27Z</published>
    <title>A New Image Steganographic Technique using Pattern based Bits Shuffling
  and Magic LSB for Grayscale Images</title>
    <summary>  Image Steganography is a growing research area of information security where
secret information is embedded in innocent-looking public communication. This
paper proposes a novel crystographic technique for grayscale images in spatial
domain. The secret data is encrypted and shuffled using pattern based bits
shuffling algorithm (PBSA) and a secret key. The encrypted data is then
embedded in the cover image using magic least significant bit (M-LSB) method.
Experimentally, the proposed method is evaluated by qualitative and
quantitative analysis which validates the effectiveness of the proposed method
in contrast to several state-of-the-art methods.
</summary>
    <author>
      <name>Khan Muhammad</name>
    </author>
    <author>
      <name>Jamil Ahmad</name>
    </author>
    <author>
      <name>Haleem Farman</name>
    </author>
    <author>
      <name>Zahoor Jan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A short paper of 6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Sindh University Research Journal-SURJ (Science Series) 47.4
  (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.01386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.07232v1</id>
    <updated>2016-01-27T00:18:26Z</updated>
    <published>2016-01-27T00:18:26Z</published>
    <title>Robust Image Watermarking Using Non-Regular Wavelets</title>
    <summary>  An approach to watermarking digital images using non-regular wavelets is
advanced. Non-regular transforms spread the energy in the transform domain. The
proposed method leads at the same time to increased image quality and increased
robustness with respect to lossy compression. The approach provides robust
watermarking by suitably creating watermarked messages that have energy
compaction and frequency spreading. Our experimental results show that the
application of non-regular wavelets, instead of regular ones, can furnish a
superior robust watermarking scheme. The generated watermarked data is more
immune against non-intentional JPEG and JPEG2000 attacks.
</summary>
    <author>
      <name>R. J. Cintra</name>
    </author>
    <author>
      <name>T. V. Cooklev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11760-008-0070-7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11760-008-0070-7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 11 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Signal, Image and Video Processing, September 2009, Volume 3,
  Issue 3, pp 241-250</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.07232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.07232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01178v1</id>
    <updated>2016-02-03T03:32:31Z</updated>
    <published>2016-02-03T03:32:31Z</published>
    <title>GECKA3D: A 3D Game Engine for Commonsense Knowledge Acquisition</title>
    <summary>  Commonsense knowledge representation and reasoning is key for tasks such as
artificial intelligence and natural language understanding. Since commonsense
consists of information that humans take for granted, gathering it is an
extremely difficult task. In this paper, we introduce a novel 3D game engine
for commonsense knowledge acquisition (GECKA3D) which aims to collect
commonsense from game designers through the development of serious games.
GECKA3D integrates the potential of serious games and games with a purpose.
This provides a platform for the acquisition of re-usable and multi-purpose
knowledge, and also enables the development of games that can provide
entertainment value and teach players something meaningful about the actual
world they live in.
</summary>
    <author>
      <name>Erik Cambria</name>
    </author>
    <author>
      <name>Tam V. Nguyen</name>
    </author>
    <author>
      <name>Brian Cheng</name>
    </author>
    <author>
      <name>Kenneth Kwok</name>
    </author>
    <author>
      <name>Jose Sepulveda</name>
    </author>
    <link href="http://arxiv.org/abs/1602.01178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.04845v1</id>
    <updated>2016-02-15T21:30:54Z</updated>
    <published>2016-02-15T21:30:54Z</published>
    <title>High-Quality, Low-Delay Music Coding in the Opus Codec</title>
    <summary>  The IETF recently standardized the Opus codec as RFC6716. Opus targets a wide
range of real-time Internet applications by combining a linear prediction coder
with a transform coder. We describe the transform coder, with particular
attention to the psychoacoustic knowledge built into the format. The result
out-performs existing audio codecs that do not operate under real-time
constraints.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Gregory Maxwell</name>
    </author>
    <author>
      <name>Timothy B. Terriberry</name>
    </author>
    <author>
      <name>Koen Vos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 135th AES Convention. Proceedings of the 135th AES
  Convention, October 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.04845v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.04845v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.05311v1</id>
    <updated>2016-02-17T05:50:50Z</updated>
    <published>2016-02-17T05:50:50Z</published>
    <title>A Full-Bandwidth Audio Codec With Low Complexity And Very Low Delay</title>
    <summary>  We propose an audio codec that addresses the low-delay requirements of some
applications such as network music performance. The codec is based on the
modified discrete cosine transform (MDCT) with very short frames and uses
gain-shape quantization to preserve the spectral envelope. The short frame
sizes required for low delay typically hinder the performance of transform
codecs. However, at 96 kbit/s and with only 4 ms algorithmic delay, the
proposed codec out-performs the ULD codec operating at the same rate. The total
complexity of the codec is small, at only 17 WMOPS for real-time operation at
48 kHz.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Timothy B. Terriberry</name>
    </author>
    <author>
      <name>Gregory Maxwell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, Proceedings of EUSIPCO 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.05311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.05975v3</id>
    <updated>2017-10-28T17:18:28Z</updated>
    <published>2016-02-18T21:14:25Z</published>
    <title>The AV1 Constrained Directional Enhancement Filter (CDEF)</title>
    <summary>  This paper presents the constrained directional enhancement filter designed
for the AV1 royalty-free video codec. The in-loop filter is based on a
non-linear low-pass filter and is designed for vectorization efficiency. It
takes into account the direction of edges and patterns being filtered. The
filter works by identifying the direction of each block and then adaptively
filtering with a high degree of control over the filter strength along the
direction and across it. The proposed enhancement filter is shown to improve
the quality of the Alliance for Open Media (AOM) AV1 and Thor video codecs in
particular in low complexity configurations.
</summary>
    <author>
      <name>Steinar Midtskogen</name>
    </author>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.05975v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05975v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03129v1</id>
    <updated>2016-03-10T02:40:05Z</updated>
    <published>2016-03-10T02:40:05Z</published>
    <title>Daala: A Perceptually-Driven Next Generation Video Codec</title>
    <summary>  The Daala project is a royalty-free video codec that attempts to compete with
the best patent-encumbered codecs. Part of our strategy is to replace core
tools of traditional video codecs with alternative approaches, many of them
designed to take perceptual aspects into account, rather than optimizing for
simple metrics like PSNR. This paper documents some of our experiences with
these tools, which ones worked and which did not, and what we've learned from
them. The result is a codec which compares favorably with HEVC on still images,
and is on a path to do so for video as well.
</summary>
    <author>
      <name>Thomas J. Daede</name>
    </author>
    <author>
      <name>Nathan E. Egge</name>
    </author>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Guillaume Martres</name>
    </author>
    <author>
      <name>Timothy B. Terriberry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.03129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.04270v2</id>
    <updated>2016-10-03T08:34:11Z</updated>
    <published>2016-05-13T17:44:52Z</published>
    <title>A First Look at Quality of Mobile Live Streaming Experience: the Case of
  Periscope</title>
    <summary>  Live multimedia streaming from mobile devices is rapidly gaining popularity
but little is known about the QoE they provide. In this paper, we examine the
Periscope service. We first crawl the service in order to understand its usage
patterns. Then, we study the protocols used, the typical quality of experience
indicators, such as playback smoothness and latency, video quality, and the
energy consumption of the Android application.
</summary>
    <author>
      <name>Matti Siekkinen</name>
    </author>
    <author>
      <name>Enrico Masala</name>
    </author>
    <author>
      <name>Teemu K√§m√§r√§inen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2987443.2987472</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2987443.2987472" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the ACM Internet Measurement Conference (IMC) 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.04270v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.04270v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05118v1</id>
    <updated>2016-05-17T11:32:13Z</updated>
    <published>2016-05-17T11:32:13Z</published>
    <title>Lossless Compression in HEVC with Integer-to-Integer Transforms</title>
    <summary>  Many approaches have been proposed to support lossless coding within video
coding standards that are primarily designed for lossy coding. The simplest
approach is to just skip transform and quantization and directly entropy code
the prediction residual, which is used in HEVC version 1. However, this simple
approach is inefficient for compression. More efficient approaches include
processing the residual with DPCM prior to entropy coding. This paper explores
an alternative approach based on processing the residual with
integer-to-integer (i2i) transforms. I2i transforms map integers to integers,
however, unlike the integer transforms used in HEVC for lossy coding, they do
not increase the dynamic range at the output and can be used in lossless
coding. Experiments with the HEVC reference software show competitive results.
</summary>
    <author>
      <name>Fatih Kamisli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MMSP.2016.7813364</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MMSP.2016.7813364" rel="related"/>
    <link href="http://arxiv.org/abs/1605.05118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09486v1</id>
    <updated>2016-05-31T03:59:03Z</updated>
    <published>2016-05-31T03:59:03Z</published>
    <title>Drone Streaming with Wi-Fi Grid Aggregation for Virtual Tour</title>
    <summary>  To provide a live, active and high-quality virtual touring streaming
experience, we propose an unmanned drone stereoscopic streaming paradigm using
a control and streaming infrastructure of a 2.4GHz Wi-Fi grid. Our system
allows users to actively control the streaming captured by a drone, receive and
watch the streaming using a head mount display (HMD); a Wi-Fi grid is deployed
across the remote scene with multi-channel support to enable high-bitrate
stream- ing broadcast from the drones. The system adopt a joint view adaptation
and drone control scheme to enable fast viewer movement including both head
rotation and touring. We implement the prototype on Dji M100 quadcopter and HTC
Vive in a demo scene.
</summary>
    <author>
      <name>Chenglei Wu</name>
    </author>
    <author>
      <name>Zhi Wang</name>
    </author>
    <author>
      <name>Shiqiang Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1605.09486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02424v1</id>
    <updated>2016-06-08T07:08:10Z</updated>
    <published>2016-06-08T07:08:10Z</published>
    <title>Generic-Precision algorithm for DCT-Cordic architectures</title>
    <summary>  In this paper we propose a generic algorithm to calculate the rotation
parameters of CORDIC angles required for the Discrete Cosine Transform
algorithm (DCT). This leads us to increase the precision of calculation meeting
any accuracy.Our contribution is to use this decomposition in CORDIC based DCT
which is appropriate for domains which require high quality and top precision.
We then propose a hardware implementation of the novel transformation, and as
expected, a substantial improvement in PSNR quality is found.
</summary>
    <author>
      <name>Imen Ben Saad</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAMSIN</arxiv:affiliation>
    </author>
    <author>
      <name>Younes Lahbib</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAMSIN</arxiv:affiliation>
    </author>
    <author>
      <name>Yassine Hacha√Øchi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAMSIN</arxiv:affiliation>
    </author>
    <author>
      <name>Sonia Mami</name>
    </author>
    <author>
      <name>Abdelkader Mami</name>
    </author>
    <link href="http://arxiv.org/abs/1606.02424v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02424v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00321v1</id>
    <updated>2016-07-01T17:19:27Z</updated>
    <published>2016-07-01T17:19:27Z</published>
    <title>Formal Definition of QoE Metrics</title>
    <summary>  This technical report formally defines the QoE metrics which are introduced
and discussed in the article "QoE Beyond the MOS: An In-Depth Look at QoE via
Better Metrics and their Relation to MOS" by Tobias Ho{\ss}feld, Poul E.
Heegaard, Martin Varela, Sebastian M\"oller, accepted for publication in the
Springer journal "Quality and User Experience". Matlab scripts for computing
the QoE metrics for given data sets are available in GitHub.
</summary>
    <author>
      <name>Tobias Hossfeld</name>
    </author>
    <author>
      <name>Poul E. Heegaard</name>
    </author>
    <author>
      <name>Martin Varela</name>
    </author>
    <author>
      <name>Sebastian M√∂ller</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s41233-016-0002-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s41233-016-0002-1" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Quality and User Experience (2016) 1: 2</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.00321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.04868v2</id>
    <updated>2017-01-15T05:23:51Z</updated>
    <published>2016-08-17T06:24:46Z</published>
    <title>Towards Music Captioning: Generating Music Playlist Descriptions</title>
    <summary>  Descriptions are often provided along with recommendations to help users'
discovery. Recommending automatically generated music playlists (e.g.
personalised playlists) introduces the problem of generating descriptions. In
this paper, we propose a method for generating music playlist descriptions,
which is called as music captioning. In the proposed method, audio content
analysis and natural language processing are adopted to utilise the information
of each track.
</summary>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>George Fazekas</name>
    </author>
    <author>
      <name>Brian McFee</name>
    </author>
    <author>
      <name>Kyunghyun Cho</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, ISMIR 2016 Late-breaking/session extended abstract</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.04868v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.04868v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05054v1</id>
    <updated>2016-08-17T19:24:23Z</updated>
    <published>2016-08-17T19:24:23Z</published>
    <title>MT3S: Mobile Turkish Scene Text-to-Speech System for the Visually
  Impaired</title>
    <summary>  Reading text is one of the essential needs of the visually impaired people.
We developed a mobile system that can read Turkish scene and book text, using a
fast gradient-based multi-scale text detection algorithm for real-time
operation and Tesseract OCR engine for character recognition. We evaluated the
OCR accuracy and running time of our system on a new, publicly available mobile
Turkish scene text dataset we constructed and also compared with
state-of-the-art systems. Our system proved to be much faster, able to run on a
mobile device, with OCR accuracy comparable to the state-of-the-art.
</summary>
    <author>
      <name>Muhammet Bastan</name>
    </author>
    <author>
      <name>Hilal Kandemir</name>
    </author>
    <author>
      <name>Busra Canturk</name>
    </author>
    <link href="http://arxiv.org/abs/1608.05054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07848v1</id>
    <updated>2016-09-26T05:09:32Z</updated>
    <published>2016-09-26T05:09:32Z</published>
    <title>Location-Based and Audience-Aware Storytelling</title>
    <summary>  While the daily user of digital, Internet-enabled devices has some explicit
control over what they read and see, the providers fulfilling searches,
offering options, and presenting material are using increasingly sophisticated
real-time algorithms that tune and target content for the particular user. They
redefine the historical relationships between tellers and users, providing a
responsiveness paralleled only by forms of live performance incorporating
elements of improvisation and audience interaction. The general accessibility
of algorithmically driven content delivery techniques suggests significant
untapped potential for new approaches to narrative beyond advertising and
commercially orientated customization.
</summary>
    <author>
      <name>Jeff Burke</name>
    </author>
    <author>
      <name>Jared J. Stein</name>
    </author>
    <link href="http://arxiv.org/abs/1609.07848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.04455v1</id>
    <updated>2016-11-14T16:48:12Z</updated>
    <published>2016-11-14T16:48:12Z</published>
    <title>Columbia MVSO Image Sentiment Dataset</title>
    <summary>  The Multilingual Visual Sentiment Ontology (MVSO) consists of 15,600 concepts
in 12 different languages that are strongly related to emotions and sentiments
expressed in images. These concepts are defined in the form of Adjective-Noun
Pair (ANP), which are crawled and discovered from online image forum Flickr. In
this work, we used Amazon Mechanical Turk as a crowd-sourcing platform to
collect human judgments on sentiments expressed in images that are uniformly
sampled over 3,911 English ANPs extracted from a tag-restricted subset of MVSO.
Our goal is to use the dataset as a benchmark for the evaluation of systems
that automatically predict sentiments in images or ANPs.
</summary>
    <author>
      <name>Vaidehi Dalmia</name>
    </author>
    <author>
      <name>Hongyi Liu</name>
    </author>
    <author>
      <name>Shih-Fu Chang</name>
    </author>
    <link href="http://arxiv.org/abs/1611.04455v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.04455v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.03461v1</id>
    <updated>2016-12-11T19:41:44Z</updated>
    <published>2016-12-11T19:41:44Z</published>
    <title>Low-complexity Pruned 8-point DCT Approximations for Image Encoding</title>
    <summary>  Two multiplierless pruned 8-point discrete cosine transform (DCT)
approximation are presented. Both transforms present lower arithmetic
complexity than state-of-the-art methods. The performance of such new methods
was assessed in the image compression context. A JPEG-like simulation was
performed, demonstrating the adequateness and competitiveness of the introduced
methods. Digital VLSI implementation in CMOS technology was also considered.
Both presented methods were realized in Berkeley Emulation Engine (BEE3).
</summary>
    <author>
      <name>V. A. Coutinho</name>
    </author>
    <author>
      <name>R. J. Cintra</name>
    </author>
    <author>
      <name>F. M. Bayer</name>
    </author>
    <author>
      <name>S. Kulasekera</name>
    </author>
    <author>
      <name>A. Madanayake</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CONIELECOMP.2015.7086923</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CONIELECOMP.2015.7086923" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.03461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.03461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.08350v1</id>
    <updated>2016-12-26T09:40:45Z</updated>
    <published>2016-12-26T09:40:45Z</published>
    <title>Streaming Virtual Reality Content</title>
    <summary>  The recent rise of interest in Virtual Reality (VR) came with the
availability of commodity commercial VR prod- ucts, such as the Head Mounted
Displays (HMD) created by Oculus and other vendors. To accelerate the user
adoption of VR headsets, content providers should focus on producing high
quality immersive content for these devices. Similarly, multimedia streaming
service providers should enable the means to stream 360 VR content on their
platforms. In this study, we try to cover different aspects related to VR
content representation, streaming, and quality assessment that will help
establishing the basic knowledge of how to build a VR streaming system.
</summary>
    <author>
      <name>Tarek El-Ganainy</name>
    </author>
    <author>
      <name>Mohamed Hefeeda</name>
    </author>
    <link href="http://arxiv.org/abs/1612.08350v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.08350v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00817v1</id>
    <updated>2017-02-02T20:08:42Z</updated>
    <published>2017-02-02T20:08:42Z</published>
    <title>DCT-like Transform for Image Compression Requires 14 Additions Only</title>
    <summary>  A low-complexity 8-point orthogonal approximate DCT is introduced. The
proposed transform requires no multiplications or bit-shift operations. The
derived fast algorithm requires only 14 additions, less than any existing DCT
approximation. Moreover, in several image compression scenarios, the proposed
transform could outperform the well-known signed DCT, as well as
state-of-the-art algorithms.
</summary>
    <author>
      <name>F. M. Bayer</name>
    </author>
    <author>
      <name>R. J. Cintra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1049/el.2012.1148</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1049/el.2012.1148" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronics Letters, Volume 48, Issue 15, pp. 919-921 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1702.00817v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00817v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00190v1</id>
    <updated>2017-03-01T09:22:47Z</updated>
    <published>2017-03-01T09:22:47Z</published>
    <title>Video transrating in AVC and HEVC transcoding</title>
    <summary>  HEVC (MPEG-H Part 2 and H.265) is a new coding technology which is expected
to be deployed on the market along with new video services in the near future.
HEVC is a successor of currently widely used AVC (MPEG-4 Part 10 and H.264). In
this paper, the quality coding gains obtained for the Cascaded Pixel Domain
Transcoder of AVC-coded material to HEVC standard are reported. Extensive
experiments showed that transcoding with bitrate reduction allows the
achievement of better rate-distortion performance than by compressing an
original video sequence with the use of AVC at the same (reduced) bitrate.
</summary>
    <author>
      <name>Krzysztof Wegner</name>
    </author>
    <author>
      <name>Tomasz Grajek</name>
    </author>
    <author>
      <name>Jakub Stankowski</name>
    </author>
    <author>
      <name>Marek Domanski</name>
    </author>
    <link href="http://arxiv.org/abs/1703.00190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05778v1</id>
    <updated>2017-03-16T18:05:32Z</updated>
    <published>2017-03-16T18:05:32Z</published>
    <title>Medical Image Watermarking using 2D-DWT with Enhanced security and
  capacity</title>
    <summary>  Teleradiology enables medical images to be transferred over the computer
networks for many purposes including clinical interpretation, diagnosis,
archive, etc. In telemedicine, medical images can be manipulated while
transferring. In addition, medical information security requirements are
specified by the legislative rules, and concerned entities must adhere to them.
In this research, we propose a new scheme based on 2-dimensional Discrete
Wavelet Transform (2D DWT) to improve the robustness and authentication of
medical images. In addition, the current research improves security and
capacity of watermarking using encryption and compression in medical images.
The evaluation is performed on the personal dataset, which contains 194 CTI and
68 MRI cases.
</summary>
    <author>
      <name>Ali Sharifara</name>
    </author>
    <author>
      <name>Amir Ghaderi</name>
    </author>
    <link href="http://arxiv.org/abs/1703.05778v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05778v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02754v1</id>
    <updated>2017-04-10T08:25:36Z</updated>
    <published>2017-04-10T08:25:36Z</published>
    <title>A Synchronization Algorithm Based on Moving Average for Robust Audio
  Watermarking Scheme</title>
    <summary>  A synchronization code scheme based on moving average is proposed for robust
audio watermarking in the paper. Two proper positive integers are chosen to
compute the moving average sequence by sliding one sample every time. The
synchronization bits are embedded at crosses of the two moving average
sequences with the quantization index modulation. The experimental results show
that the proposed watermarking scheme maintains high audio quality and is
robust to common attacks such as additive white Gaussian noise, re-sampling,
low-pass filtering, random cropping, MP3 compression, jitter attack and time
scale modification. Simultaneously, the algorithm has high search efficiency
and low false alarm rate.
</summary>
    <author>
      <name>Zhang Jin-quan</name>
    </author>
    <author>
      <name>Han Bin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 7 tables,5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.02754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02755v1</id>
    <updated>2017-04-10T08:26:19Z</updated>
    <published>2017-04-10T08:26:19Z</published>
    <title>Robust Audio Watermarking Algorithm Based on Moving Average and DCT</title>
    <summary>  Noise is often brought to host audio by common signal processing operation,
and it usually changes the high-frequency component of an audio signal. So
embedding watermark by adjusting low-frequency coefficient can improve the
robustness of a watermark scheme. Moving Average sequence is a low-frequency
feature of an audio signal. This work proposed a method which embedding
watermark into the maximal coefficient in discrete cosine transform domain of a
moving average sequence. Subjective and objective tests reveal that the
proposed watermarking scheme maintains highly audio quality, and
simultaneously, the algorithm is highly robust to common digital signal
processing operations, including additive noise, sampling rate change, bit
resolution transformation, MP3 compression, and random cropping, especially
low-pass filtering.
</summary>
    <author>
      <name>Jinquan Zhang</name>
    </author>
    <author>
      <name>Bin Han</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.02755v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02755v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01340v1</id>
    <updated>2017-07-05T12:02:42Z</updated>
    <published>2017-07-05T12:02:42Z</published>
    <title>Web Video in Numbers - An Analysis of Web-Video Metadata</title>
    <summary>  Web video is often used as a source of data in various fields of study. While
specialized subsets of web video, mainly earmarked for dedicated purposes, are
often analyzed in detail, there is little information available about the
properties of web video as a whole. In this paper we present insights gained
from the analysis of the metadata associated with more than 120 million videos
harvested from two popular web video platforms, vimeo and YouTube, in 2016 and
compare their properties with the ones found in commonly used video
collections. This comparison has revealed that existing collections do not (or
no longer) properly reflect the properties of web video "in the wild".
</summary>
    <author>
      <name>Luca Rossetto</name>
    </author>
    <author>
      <name>Heiko Schuldt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Dataset available from http://download-dbis.dmi.unibas.ch/WWIN/</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.01340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01513v1</id>
    <updated>2017-07-05T18:02:29Z</updated>
    <published>2017-07-05T18:02:29Z</published>
    <title>On the steganographic image based approach to PDF files protection</title>
    <summary>  Digital images can be copied without authorization and have to be protected.
Two schemes for watermarking images in PDF document were considered. Both
schemes include a converter to extract images from PDF pages and return the
protected images back. Frequency and spatial domain embedding were used for
hiding a message presented by a binary pattern. We considered visible and
invisible watermarking and found that spatial domain LSB technique can be more
preferable than frequency embedding using DWT.
</summary>
    <author>
      <name>V. N. Gorbachev</name>
    </author>
    <author>
      <name>L. A. Denisov</name>
    </author>
    <author>
      <name>E. M. Kaynarova</name>
    </author>
    <author>
      <name>I. K. Metelev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.01513v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01513v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02709v1</id>
    <updated>2017-07-10T06:30:59Z</updated>
    <published>2017-07-10T06:30:59Z</published>
    <title>An Augmented Autoregressive Approach to HTTP Video Stream Quality
  Prediction</title>
    <summary>  HTTP-based video streaming technologies allow for flexible rate selection
strategies that account for time-varying network conditions. Such rate changes
may adversely affect the user's Quality of Experience; hence online prediction
of the time varying subjective quality can lead to perceptually optimised
bitrate allocation policies. Recent studies have proposed to use dynamic
network approaches for continuous-time prediction; yet they do not consider
multiple video quality models as inputs nor consider forecasting ensembles.
Here we address the problem of predicting continuous-time subjective quality
using multiple inputs fed to a non-linear autoregressive network. By
considering multiple network configurations and by applying simple averaging
forecasting techniques, we are able to considerably improve prediction
performance and decrease forecasting errors.
</summary>
    <author>
      <name>Christos G. Bampis</name>
    </author>
    <author>
      <name>Alan C. Bovik</name>
    </author>
    <link href="http://arxiv.org/abs/1707.02709v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02709v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09538v1</id>
    <updated>2017-07-29T16:40:50Z</updated>
    <published>2017-07-29T16:40:50Z</published>
    <title>Benchmarking Multimodal Sentiment Analysis</title>
    <summary>  We propose a framework for multimodal sentiment analysis and emotion
recognition using convolutional neural network-based feature extraction from
text and visual modalities. We obtain a performance improvement of 10% over the
state of the art by combining visual, text and audio features. We also discuss
some major issues frequently ignored in multimodal sentiment analysis research:
the role of speaker-independent models, importance of the modalities and
generalizability. The paper thus serve as a new benchmark for further research
in multimodal sentiment analysis and also demonstrates the different facets of
analysis to be considered while performing such tasks.
</summary>
    <author>
      <name>Erik Cambria</name>
    </author>
    <author>
      <name>Devamanyu Hazarika</name>
    </author>
    <author>
      <name>Soujanya Poria</name>
    </author>
    <author>
      <name>Amir Hussain</name>
    </author>
    <author>
      <name>R. B. V. Subramaanyam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in CICLing 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.02991v1</id>
    <updated>2017-08-09T19:59:49Z</updated>
    <published>2017-08-09T19:59:49Z</published>
    <title>Robust Video Watermarking against H.264 and H.265 Compression Attacks</title>
    <summary>  This paper proposes a robust watermarking method for uncompressed video data
against H.264/AVC and H.265/HEVC compression standards. We embed the watermark
data in the mid-range transform coefficients of a block that is less similar to
its corresponding block in the previous and next frames. This idea makes the
watermark robust against the compression standards that use the inter
prediction technique. The last two video compression standards also use inter
prediction for motion compensation like previous video compression standards.
Therefore, the proposed method is also well suited with these standards.
Simulation results show the adequate robustness and transparency of our
watermarking scheme.
</summary>
    <author>
      <name>Nematollah Zarmehi</name>
    </author>
    <author>
      <name>Mohammad Javad Barikbin</name>
    </author>
    <link href="http://arxiv.org/abs/1708.02991v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.02991v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05970v1</id>
    <updated>2017-08-20T14:22:48Z</updated>
    <published>2017-08-20T14:22:48Z</published>
    <title>An improved watermarking scheme for Internet applications</title>
    <summary>  In this paper, a data hiding scheme ready for Internet applications is
proposed. An existing scheme based on chaotic iterations is improved, to
respond to some major Internet security concerns, such as digital rights
management, communication over hidden channels, and social search engines. By
using Reed Solomon error correcting codes and wavelets domain, we show that
this data hiding scheme can be improved to solve issues and requirements raised
by these Internet fields.
</summary>
    <author>
      <name>Christophe Guyeux</name>
    </author>
    <author>
      <name>Jacques M. Bahi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of INTERNET'2010, 2nd Int. Conf. on Evolving Internet.
  Valencia (Spain), September 20-25, 2010. pp. 119-124</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.05970v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05970v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09980v1</id>
    <updated>2017-10-27T03:54:38Z</updated>
    <published>2017-10-27T03:54:38Z</published>
    <title>Watching Videos with Certain and Constant Quality: PID-based Quality
  Control Method</title>
    <summary>  In video coding, compressed videos with certain and constant quality can
ensure quality of experience (QoE). To this end, we propose in this paper a
novel PID-based quality control (PQC) method for video coding. Specifically, a
formulation is modelled to control quality of video coding with two objectives:
minimizing control error and quality fluctuation. Then, we apply the Laplace
domain analysis to model the relationship between quantization parameter (QP)
and control error in this formulation. Given the relationship between QP and
control error, we propose a solution to the PQC formulation, such that videos
can be compressed at certain and constant quality. Finally, experimental
results show that our PQC method is effective in both control accuracy and
quality fluctuation.
</summary>
    <author>
      <name>Yuhang Song</name>
    </author>
    <author>
      <name>Mai Xu</name>
    </author>
    <author>
      <name>Shengxi Li</name>
    </author>
    <link href="http://arxiv.org/abs/1710.09980v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09980v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.02438v1</id>
    <updated>2017-12-06T23:17:00Z</updated>
    <published>2017-12-06T23:17:00Z</published>
    <title>Real-time Video Processing in Web Applications</title>
    <summary>  The OpenGL ES standard is implemented in modern desktop and mobile browsers
through the WebGL API. This paper explores the potential for using OpenGL ES
hardware acceleration for real time video processing in standard HTML5
applications. It analyses the WebGL performance across device types and
compares it with the standard JavaScript and canvas performance.
</summary>
    <author>
      <name>Cristian Ionita</name>
    </author>
    <author>
      <name>Alexandru Barbulescu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12th Romanian Human-Computer Interaction Conference, RoCHI 2015,
  Bucharest, Romania, September 24-25, 2015,
  http://rochi.utcluj.ro/articole/3/RoCHI-2015-Ionita.pdf</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Cristian Ionita, Alexandru Barbulescu, Realtime Video Processing
  in Web Applications, 12th Romanian Human-Computer Interaction Conference,
  RoCHI 2015, Bucharest, Romania, September 24-25, 2015, p129-132</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.02438v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.02438v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.05596v1</id>
    <updated>2018-03-15T05:23:12Z</updated>
    <published>2018-03-15T05:23:12Z</published>
    <title>A nonlinear transform based analog video transmission framework</title>
    <summary>  Soft-cast, a cross-layer design for wireless video transmission, is proposed
to solve the drawbacks of digital video transmission: threshold transmission
framework achieving the same effect. Specifically, in encoder, we carry out
power allocation on the transformed coefficients and encode the coefficients
based on the new formulation of power distortion. In decoder, the process of
LLSE estimator is also improved. Accompanied with the inverse nonlinear
transform, DCT coefficients can be recovered depending on the scaling factors ,
LLSE estimator coefficients and metadata. Experiment results show that our
proposed framework outperforms the Soft-cast in PSNR 1.08 dB and the MSSIM gain
reaches to 2.35% when transmitting under the same bandwidth and total power.
</summary>
    <author>
      <name>Yongtao Liu</name>
    </author>
    <author>
      <name>Xiaopeng Fan</name>
    </author>
    <author>
      <name>Yang Wang</name>
    </author>
    <author>
      <name>Debin Zhao</name>
    </author>
    <author>
      <name>Wen Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages,7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.05596v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.05596v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.10889v1</id>
    <updated>2018-03-29T01:10:22Z</updated>
    <published>2018-03-29T01:10:22Z</published>
    <title>Weakening the Detecting Capability of CNN-based Steganalysis</title>
    <summary>  Recently, the application of deep learning in steganalysis has drawn many
researchers' attention. Most of the proposed steganalytic deep learning models
are derived from neural networks applied in computer vision. These kinds of
neural networks have distinguished performance. However, all these kinds of
back-propagation based neural networks may be cheated by forging input named
the adversarial example. In this paper we propose a method to generate
steganographic adversarial example in order to enhance the steganographic
security of existing algorithms. These adversarial examples can increase the
detection error of steganalytic CNN. The experiments prove the effectiveness of
the proposed method.
</summary>
    <author>
      <name>Sai Ma</name>
    </author>
    <author>
      <name>Qingxiao Guan</name>
    </author>
    <author>
      <name>Xianfeng Zhao</name>
    </author>
    <author>
      <name>Yaqi Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.10889v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.10889v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.01863v1</id>
    <updated>2018-04-05T14:08:22Z</updated>
    <published>2018-04-05T14:08:22Z</published>
    <title>The diveXplore System at the Video Browser Showdown 2018 - Final Notes</title>
    <summary>  This short paper provides further details of the diveXplore system (formerly
known as CoViSS), which has been used by team ITEC1 for the Video Browser
Showdown (VBS) 2018. In particular, it gives a short overview of search
features and some details of final system changes, not included in the
corresponding VBS2018 paper, as well as a basic analysis of how the system has
been used for VBS2018 (from a user perspective).
</summary>
    <author>
      <name>Klaus Schoeffmann</name>
    </author>
    <author>
      <name>Bernd M√ºnzer</name>
    </author>
    <author>
      <name>J√ºrgen Primus</name>
    </author>
    <author>
      <name>Andreas Leibetseder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.01863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.01863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06645v3</id>
    <updated>2018-05-27T07:16:02Z</updated>
    <published>2018-04-18T10:49:19Z</published>
    <title>An Improved Reversible Data Hiding Scheme by Changing Modification
  Direction of Partial Coefficients in JPEG Images</title>
    <summary>  This paper first reviews the reversible data hiding scheme, of Liu et al. in
2018, for JPEG images. After that, a novel reversible data hiding scheme, in
which modification directions of partial nonzero quantized alternating current
(AC) coefficients are utilized to decrease distortion and file size increase
caused by data hiding, is proposed. Experimental results have shown that the
proposed scheme has indeed advantages in visual quality and smaller increase in
file size of marked JPEG images while compared to the state-of-the-art scheme
with the same embedding payload so far.
</summary>
    <author>
      <name>Yi Chen</name>
    </author>
    <author>
      <name>Hongxia Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1804.06645v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06645v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.07019v1</id>
    <updated>2018-04-19T07:24:55Z</updated>
    <published>2018-04-19T07:24:55Z</published>
    <title>Simple Yet Efficient Content Based Video Copy Detection</title>
    <summary>  Given a collection of videos, how to detect content-based copies efficiently
with high accuracy? Detecting copies in large video collections still remains
one of the major challenges of multimedia retrieval. While many video copy
detection approaches show high computation times and insufficient quality, we
propose a new efficient content-based video copy detection algorithm improving
both aspects. The idea of our approach consists in utilizing self-similarity
matrices as video descriptors in order to capture different visual properties.
We benchmark our algorithm on the MuscleVCD ST1 benchmark dataset and show that
our approach is able to achieve a score of 100\% and a score of at least 93\%
in a wide range of parameters.
</summary>
    <author>
      <name>J√∂rg P. Bachmann</name>
    </author>
    <author>
      <name>Benjamin Hauskeller</name>
    </author>
    <link href="http://arxiv.org/abs/1804.07019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.07019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06357v1</id>
    <updated>2018-06-17T10:06:29Z</updated>
    <published>2018-06-17T10:06:29Z</published>
    <title>StegNet: Mega Image Steganography Capacity with Deep Convolutional
  Network</title>
    <summary>  Traditional image steganography often leans interests towards safely
embedding hidden information into cover images with payload capacity almost
neglected. This paper combines recent deep convolutional neural network methods
with image-into-image steganography. It successfully hides the same size images
with a decoding rate of 98.2% or bpp (bits per pixel) of 23.57 by changing only
0.76% of the cover image on average. Our method directly learns end-to-end
mappings between the cover image and the embedded image and between the hidden
image and the decoded image. We~further show that our embedded image, while
with mega payload capacity, is still robust to statistical analysis.
</summary>
    <author>
      <name>Pin Wu</name>
    </author>
    <author>
      <name>Yang Yang</name>
    </author>
    <author>
      <name>Xiaoqiang Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/fi10060054</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/fi10060054" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">https://github.com/adamcavendish/StegNet-Mega-Image-Steganography-Capacity-with-Deep-Convolutional-Network</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.06357v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06357v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07617v1</id>
    <updated>2018-07-19T19:18:51Z</updated>
    <published>2018-07-19T19:18:51Z</published>
    <title>SoniControl - A Mobile Ultrasonic Firewall</title>
    <summary>  The exchange of data between mobile devices in the near-ultrasonic frequency
band is a new promising technology for near field communication (NFC) but also
raises a number of privacy concerns. We present the first ultrasonic firewall
that reliably detects ultrasonic communication and provides the user with
effective means to prevent hidden data exchange. This demonstration showcases a
new media-based communication technology ("data over audio") together with its
related privacy concerns. It enables users to (i) interactively test out and
experience ultrasonic information exchange and (ii) shows how to protect
oneself against unwanted tracking.
</summary>
    <author>
      <name>Matthias Zeppelzauer</name>
    </author>
    <author>
      <name>Alexis Ringot</name>
    </author>
    <author>
      <name>Florian Taurer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3240508.3241393</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3240508.3241393" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in proceedings of 2018 ACM Multimedia Conference October
  22--26, 2018, Seoul, Republic of Korea</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.07617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.06616v1</id>
    <updated>2018-11-15T22:50:54Z</updated>
    <published>2018-11-15T22:50:54Z</published>
    <title>Motion Style Extraction Based on Sparse Coding Decomposition</title>
    <summary>  We present a sparse coding-based framework for motion style decomposition and
synthesis. Dynamic Time Warping is firstly used to synchronized input motions
in the time domain as a pre-processing step. A sparse coding-based
decomposition has been proposed, we also introduce the idea of core component
and basic motion. Decomposed motions are then combined, transfer to synthesize
new motions. Lastly, we develop limb length constraint as a post-processing
step to remove distortion skeletons. Our framework has the advantage of less
time-consuming, no manual alignment and large dataset requirement. As a result,
our experiments show smooth and natural synthesized motion.
</summary>
    <author>
      <name>Xuan Thanh Nguyen</name>
    </author>
    <author>
      <name>Thanh Ha Le</name>
    </author>
    <author>
      <name>Hongchuan Yu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at ACM SIGGRAPH ASIA Workshop: Data-Driven Animation
  Techniques (D2AT)</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.06616v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.06616v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.12915v2</id>
    <updated>2019-01-22T18:40:07Z</updated>
    <published>2018-11-30T17:57:58Z</published>
    <title>Large-Scale and Fine-Grained Evaluation of Popular JPEG Forgery
  Localization Schemes</title>
    <summary>  Over the years, researchers have proposed various approaches to JPEG forgery
detection and localization. In most cases, experimental evaluation was limited
to JPEG quality levels that are multiples of 5 or 10. Each study used a
different dataset, making it difficult to directly compare the reported
results. The goal of this work is to perform a unified, large-scale and
fine-grained evaluation of the most popular state-of-the-art detectors. The
obtained results allow to compare the detectors with respect to various
criteria, and shed more light on the compression configurations where reliable
tampering localization can be expected.
</summary>
    <author>
      <name>Pawel Korus</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Supplementary materials for online code publication</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.12915v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.12915v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.05940v1</id>
    <updated>2019-03-14T12:26:23Z</updated>
    <published>2019-03-14T12:26:23Z</published>
    <title>Notation for Subject Answer Analysis</title>
    <summary>  It is believed that consistent notation helps the research community in many
ways. First and foremost, it provides a consistent interface of communication.
Subjective experiments described according to uniform rules are easier to
understand and analyze. Additionally, a comparison of various results is less
complicated. In this publication we describe notation proposed by VQEG (Video
Quality Expert Group) working group SAM (Statistical Analysis and Methods).
</summary>
    <author>
      <name>Lucjan Janowski</name>
    </author>
    <author>
      <name>Jakub Nawa≈Ça</name>
    </author>
    <author>
      <name>Werner Robitza</name>
    </author>
    <author>
      <name>Zhi Li</name>
    </author>
    <author>
      <name>Luk√°≈° Kasula</name>
    </author>
    <author>
      <name>Krzysztof Rusek</name>
    </author>
    <link href="http://arxiv.org/abs/1903.05940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.05940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.08238v1</id>
    <updated>2019-03-19T19:49:33Z</updated>
    <published>2019-03-19T19:49:33Z</published>
    <title>Audio Watermarking over the Air With Modulated Self-Correlation</title>
    <summary>  We propose a novel audio watermarking system that is robust to the distortion
due to the indoor acoustic propagation channel between the loudspeaker and the
receiving microphone. The system utilizes a set of new algorithms that
effectively mitigate the impact of room reverberation and interfering sound
sources without using dereverberation procedures. The decoder has low-latency
and it operates asynchronously, which alleviates the need for explicit
synchronization with the encoder. It is also robust to standard audio
processing operations in legacy watermarking systems, e.g., compression and
volume change. The effectiveness of the system is established with a real-time
system under general room conditions.
</summary>
    <author>
      <name>Yuan-Yen Tai</name>
    </author>
    <author>
      <name>Mohamed F. Mansour</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, conference; preprint, ICASSP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.08238v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.08238v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94A12, 94A13" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1; K.6.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.03823v1</id>
    <updated>2019-05-09T19:21:45Z</updated>
    <published>2019-05-09T19:21:45Z</published>
    <title>A Taxonomy and Dataset for 360¬∞ Videos</title>
    <summary>  In this paper, we propose a taxonomy for 360{\deg} videos that categorizes
videos based on moving objects and camera motion. We gathered and produced 28
videos based on the taxonomy, and recorded viewport traces from 60 participants
watching the videos. In addition to the viewport traces, we provide the
viewers' feedback on their experience watching the videos, and we also analyze
viewport patterns on each category.
</summary>
    <author>
      <name>Afshin Taghavi Nasrabadi</name>
    </author>
    <author>
      <name>Aliehsan Samiei</name>
    </author>
    <author>
      <name>Anahita Mahzari</name>
    </author>
    <author>
      <name>Mylene C. Q. Farias</name>
    </author>
    <author>
      <name>Marcelo M. Carvalho</name>
    </author>
    <author>
      <name>Ryan P. McMahan</name>
    </author>
    <author>
      <name>Ravi Prakash</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3304109.3325812</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3304109.3325812" rel="related"/>
    <link href="http://arxiv.org/abs/1905.03823v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.03823v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.04807v2</id>
    <updated>2019-08-29T13:35:31Z</updated>
    <published>2019-07-10T15:56:33Z</published>
    <title>Hacking VMAF with Video Color and Contrast Distortion</title>
    <summary>  Video quality measurement takes an important role in many applications.
Full-reference quality metrics which are usually used in video codecs
comparisons are expected to reflect any changes in videos. In this article, we
consider different color corrections of compressed videos which increase the
values of full-reference metric VMAF and almost don't decrease other
widely-used metric SSIM. The proposed video contrast enhancement approach shows
the metric inapplicability in some cases for video codecs comparisons, as it
may be used for cheating in the comparisons via tuning to improve this metric
values.
</summary>
    <author>
      <name>Anastasia Zvezdakova</name>
    </author>
    <author>
      <name>Sergey Zvezdakov</name>
    </author>
    <author>
      <name>Dmitriy Kulikov</name>
    </author>
    <author>
      <name>Dmitriy Vatolin</name>
    </author>
    <link href="http://arxiv.org/abs/1907.04807v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04807v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.10898v1</id>
    <updated>2019-08-28T18:28:04Z</updated>
    <published>2019-08-28T18:28:04Z</published>
    <title>A steganographic approach based on the chaotic fractional map and in the
  DCT domain</title>
    <summary>  A steganographic method based on the chaotic fractional map and in the DCT
domain is proposed. This method embeds a secret message in some high frequency
coefficients of the image using a 128-bit private key and a chaotic fractional
map which generate a permutation indicating the positions where the secret bits
will be embedded. An experimental work on the validation of the proposed method
is also presented, showing performance in imperceptibility, quality, similarity
and security analysis of the steganographic system. The proposed algorithm
improved the level of imperceptibility and Cachin's security of stego-system
analyzed through the values of Peak Signal-to-Noise Ratio (PSNR) and the
Relative Entropy (RE).
</summary>
    <author>
      <name>A. Soria-Lorente</name>
    </author>
    <author>
      <name>E. P√©rez-Michel</name>
    </author>
    <author>
      <name>E. Avila-Domenech</name>
    </author>
    <link href="http://arxiv.org/abs/1908.10898v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10898v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.00772v1</id>
    <updated>2019-11-02T19:19:24Z</updated>
    <published>2019-11-02T19:19:24Z</published>
    <title>Robustness and Imperceptibility Enhancement in Watermarked Images by
  Color Transformation</title>
    <summary>  One of the effective methods for the preservation of copyright ownership of
digital media is watermarking. Different watermarking techniques try to set a
tradeoff between robustness and transparency of the process. In this research
work, we have used color space conversion and frequency transform to achieve
high robustness and transparency. Due to the distribution of image information
in the RGB domain, we use the YUV color space, which concentrates the visual
information in the Y channel. Embedding of the watermark is performed in the
DCT coefficients of the specific wavelet subbands. Experimental results show
high transparency and robustness of the proposed method.
</summary>
    <author>
      <name>Maedeh Jamali</name>
    </author>
    <author>
      <name>Mahnoosh Bagheri</name>
    </author>
    <author>
      <name>Nader Karimi</name>
    </author>
    <author>
      <name>Shadrokh Samavi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.00772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.00772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.13279v1</id>
    <updated>2019-11-29T18:24:44Z</updated>
    <published>2019-11-29T18:24:44Z</published>
    <title>A Graph-based Ranking Approach to Extract Key-frames for Static Video
  Summarization</title>
    <summary>  Video abstraction has become one of the efficient approaches to grasp the
content of a video without seeing it entirely. Key frame-based static video
summarization falls under this category. In this paper, we propose a
graph-based approach which summarizes the video with best user satisfaction. We
treated each video frame as a node of the graph and assigned a rank to each
node by our proposed VidRank algorithm. We developed three different models of
VidRank algorithm and performed a comparative study on those models. A
comprehensive evaluation of 50 videos from open video database using objective
and semi-objective measures indicates the superiority of our static video
summary generation method.
</summary>
    <author>
      <name>Saikat Chakraborty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.13279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.13279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.10809v1</id>
    <updated>2019-11-25T12:02:15Z</updated>
    <published>2019-11-25T12:02:15Z</published>
    <title>Visual Summarization of Scholarly Videos using Word Embeddings and
  Keyphrase Extraction</title>
    <summary>  Effective learning with audiovisual content depends on many factors. Besides
the quality of the learning resource's content, it is essential to discover the
most relevant and suitable video in order to support the learning process most
effectively. Video summarization techniques facilitate this goal by providing a
quick overview over the content. It is especially useful for longer recordings
such as conference presentations or lectures. In this paper, we present an
approach that generates a visual summary of video content based on semantic
word embeddings and keyphrase extraction. For this purpose, we exploit video
annotations that are automatically generated by speech recognition and video
OCR (optical character recognition).
</summary>
    <author>
      <name>Hang Zhou</name>
    </author>
    <author>
      <name>Christian Otto</name>
    </author>
    <author>
      <name>Ralph Ewerth</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-30760-8_28</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-30760-8_28" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.10809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.10809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.12275v1</id>
    <updated>2020-02-27T17:34:34Z</updated>
    <published>2020-02-27T17:34:34Z</published>
    <title>Subjective Quality Assessment for YouTube UGC Dataset</title>
    <summary>  Due to the scale of social video sharing, User Generated Content (UGC) is
getting more attention from academia and industry. To facilitate
compression-related research on UGC, YouTube has released a large-scale
dataset. The initial dataset only provided videos, limiting its use in quality
assessment. We used a crowd-sourcing platform to collect subjective quality
scores for this dataset. We analyzed the distribution of Mean Opinion Score
(MOS) in various dimensions, and investigated some fundamental questions in
video quality assessment, like the correlation between full video MOS and
corresponding chunk MOS, and the influence of chunk variation in quality score
aggregation.
</summary>
    <author>
      <name>Joong Gon Yim</name>
    </author>
    <author>
      <name>Yilin Wang</name>
    </author>
    <author>
      <name>Neil Birkbeck</name>
    </author>
    <author>
      <name>Balu Adsumilli</name>
    </author>
    <link href="http://arxiv.org/abs/2002.12275v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.12275v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.07505v1</id>
    <updated>2020-03-17T02:49:35Z</updated>
    <published>2020-03-17T02:49:35Z</published>
    <title>Hide Secret Information in Blocks: Minimum Distortion Embedding</title>
    <summary>  In this paper, a new steganographic method is presented that provides minimum
distortion in the stego image. The proposed encoding algorithm focuses on DCT
rounding error and optimizes that in a way to reduce distortion in the stego
image, and the proposed algorithm produces less distortion than existing
methods (e.g., F5 algorithm). The proposed method is based on DCT rounding
error which helps to lower distortion and higher embedding capacity.
</summary>
    <author>
      <name>Md Amiruzzaman</name>
    </author>
    <author>
      <name>Rizal Mohd Nor</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SPIN48934.2020.9071138</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SPIN48934.2020.9071138" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accepted for publication in IEEE SPIN 2020 conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2020 7th International Conference on Signal Processing and
  Integrated Networks (SPIN)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.07505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.07505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.12470v1</id>
    <updated>2020-04-26T20:17:36Z</updated>
    <published>2020-04-26T20:17:36Z</published>
    <title>Secure Steganography Technique Based on Bitplane Indexes</title>
    <summary>  This paper is concerned with secret hiding in multiple image bitplanes for
increased security without undermining capacity. A secure steganographic
algorithm based on bitplanes index manipulation is proposed. The index
manipulation is confined to the first two Least Significant Bits of the cover
image. The proposed algorithm has the property of un-detectability with respect
to stego quality and payload capacity. Experimental results demonstrate that
the proposed technique is secure against statistical attacks such as pair of
value (PoV), Weighted Stego steganalyser (WS), and Multi Bitplane Weighted
Stego steganalyser (MLSB-WS).
</summary>
    <author>
      <name>Alan Anwer Abdulla</name>
    </author>
    <author>
      <name>Sabah A. Jassim</name>
    </author>
    <author>
      <name>Harin Sellahewa</name>
    </author>
    <link href="http://arxiv.org/abs/2004.12470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.12470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.03756v1</id>
    <updated>2020-05-07T21:10:53Z</updated>
    <published>2020-05-07T21:10:53Z</published>
    <title>White Paper: Recommendations for immersive accessibility services</title>
    <summary>  This paper provides recommendations on how to integrate accessibility
solutions, like subtitling, audio description and sign language, with immersive
media services, with a focus on 360-degree video and spatial audio. It provides
an in-depth analysis of the features provided by state-of-the-art standard
solutions to achieve this goal, and elaborates on the finding and proposed
solutions from the EU H2020 ImAc project to address existing gaps. The proposed
solutions are described qualitatively and technically, including example
implementations. The document is intended to serve as a valuable information
source for early adopters who plan to provide accessibility services to their
portfolio with standard-compliant solutions.
</summary>
    <author>
      <name>Peter tho Pesch</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRT, Germany</arxiv:affiliation>
    </author>
    <author>
      <name>Romain Bouqueau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Motion Spell, France</arxiv:affiliation>
    </author>
    <author>
      <name>Mario Montagud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">i2CAT, Spain</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, White Paper</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.03756v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.03756v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.14317v1</id>
    <updated>2020-05-28T22:07:29Z</updated>
    <published>2020-05-28T22:07:29Z</published>
    <title>Augmenting reality: On the shared history of perceptual illusion and
  video projection mapping</title>
    <summary>  Perceptual illusions based on the spatial correspondence between objects and
displayed images have been pursued by artists and scientists since the 15th
century, mastering optics to create crucial techniques as the linear
perspective and devices as the Magic Lantern. Contemporary video projection
mapping inherits and further extends this drive to produce perceptual illusions
in space by incorporating the required real time capabilities for dynamically
superposing the imaginary onto physical objects under fluid real world
conditions. A critical milestone has been reached in the creation of the
technical possibilities for all encompassing, untethered synthetic reality
experiences available to the plain senses, where every surface may act as a
screen and the relation to everyday objects is open to alterations.
</summary>
    <author>
      <name>Alvaro Pastor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.14317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.14317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.13477v2</id>
    <updated>2020-08-04T04:16:20Z</updated>
    <published>2020-05-27T17:28:31Z</published>
    <title>Ari: The Automated R Instructor</title>
    <summary>  We present the ari package for automatically generating technology-focused
educational videos. The goal of the package is to create reproducible videos,
with the ability to change and update video content seamlessly. We present
several examples of generating videos including using R Markdown slide decks,
PowerPoint slides, or simple images as source material. We also discuss how ari
can help instructors reach new audiences through programmatically translating
materials into other languages.
</summary>
    <author>
      <name>Sean Kross</name>
    </author>
    <author>
      <name>Jeffrey T. Leek</name>
    </author>
    <author>
      <name>John Muschelli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">- reformatted section headings - added several citations - linted and
  reformatted code chunks</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.13477v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.13477v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.08742v1</id>
    <updated>2020-12-16T04:37:28Z</updated>
    <published>2020-12-16T04:37:28Z</published>
    <title>An adaptive algorithm for embedding information into compressed JPEG
  images using the QIM method</title>
    <summary>  The widespread use of JPEG images makes them good covers for secret messages
storing and transmitting. This paper proposes a new algorithm for embedding
information in JPEG images based on the steganographic QIM method. The main
problem of such embedding is the vulnerability to statistical steganalysis. To
solve this problem, it is proposed to use a variable quantization step, which
is adaptively selected for each block of the JPEG cover image. Experimental
results show that the proposed approach successfully increases the security of
embedding.
</summary>
    <author>
      <name>Anna Melman</name>
    </author>
    <author>
      <name>Pavel Petrov</name>
    </author>
    <author>
      <name>Alexander Shelupanov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.08742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.08742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.15641v1</id>
    <updated>2020-12-31T15:01:44Z</updated>
    <published>2020-12-31T15:01:44Z</published>
    <title>Investigating Memorability of Dynamic Media</title>
    <summary>  The Predicting Media Memorability task in MediaEval'20 has some challenging
aspects compared to previous years. In this paper we identify the high-dynamic
content in videos and dataset of limited size as the core challenges for the
task, we propose directions to overcome some of these challenges and we present
our initial result in these directions.
</summary>
    <author>
      <name>Phuc H. Le-Khac</name>
    </author>
    <author>
      <name>Ayush K. Rai</name>
    </author>
    <author>
      <name>Graham Healy</name>
    </author>
    <author>
      <name>Alan F. Smeaton</name>
    </author>
    <author>
      <name>Noel E. O'Connor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 figure. 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">MediaEval Multimedia Benchmark Workshop Working Notes, 14-15
  December 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2012.15641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.15641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.02453v1</id>
    <updated>2021-03-03T14:59:58Z</updated>
    <published>2021-03-03T14:59:58Z</published>
    <title>Reversible Data Hiding Associated with Digital Halftoning That Allows
  Printing with Special Color Ink by Using Single Color Layer</title>
    <summary>  We propose an efficient framework of reversible data hiding to preserve
compatibility between normal printing and printing with a special color ink by
using a single common image. The special color layer is converted to a binary
image by digital halftoning and losslessly compressed using JBIG2. Then, the
compressed information of the binarized special color layer is reversibly
embedded into the general color layer without significant distortion. Our
experimental results show the availability of the proposed method in terms of
the marked image quality.
</summary>
    <author>
      <name>Minagi Ueda</name>
    </author>
    <author>
      <name>Shoko Imaizumi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1541/ieejeiss.141.163</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1541/ieejeiss.141.163" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.02453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.02453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.03539v1</id>
    <updated>2021-03-05T08:39:47Z</updated>
    <published>2021-03-05T08:39:47Z</published>
    <title>Extend the FFmpeg Framework to Analyze Media Content</title>
    <summary>  This paper introduces a new set of video analytics plugins developed for the
FFmpeg framework. Multimedia applications that increasingly utilize the FFmpeg
media features for its comprehensive media encoding, decoding, muxing, and
demuxing capabilities can now additionally analyze the video content based on
AI models. The plugins are thread optimized for best performance overcoming
certain FFmpeg threading limitations. The plugins utilize the Intel OpenVINO
Toolkit inference engine as the backend. The analytics workloads are
accelerated on different platforms such as CPU, GPU, FPGA or specialized
analytics accelerators. With our reference implementation, the feature of
OpenVINO as inference backend has been pushed into FFmpeg mainstream
repository. We plan to submit more patches later.
</summary>
    <author>
      <name>Xintian Wu</name>
    </author>
    <author>
      <name>Pengfei Qu</name>
    </author>
    <author>
      <name>Shaofei Wang</name>
    </author>
    <author>
      <name>Lin Xie</name>
    </author>
    <author>
      <name>Jie Dong</name>
    </author>
    <link href="http://arxiv.org/abs/2103.03539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.03539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.02618v2</id>
    <updated>2022-07-20T14:09:01Z</updated>
    <published>2021-04-06T15:55:13Z</published>
    <title>Subjective Assessment Experiments That Recruit Few Observers With
  Repetitions (FOWR)</title>
    <summary>  Recent studies have shown that it is possible to characterize subject bias
and variance in subjective assessment tests. Apparent differences among
subjects can, for the most part, be explained by random factors. Building on
that theory, we propose a subjective test design where three to four team
members each rate the stimuli multiple times. The results are comparable to a
high performing objective metric. This provides a quick and simple way to
analyze new technologies and perform pre-tests for subjective assessment.
</summary>
    <author>
      <name>Pablo Perez</name>
    </author>
    <author>
      <name>Lucjan Janowski</name>
    </author>
    <author>
      <name>Narciso Garcia</name>
    </author>
    <author>
      <name>Margaret Pinson</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2021.3098450</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2021.3098450" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Multimedia</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.02618v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.02618v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04669v1</id>
    <updated>2021-04-10T02:42:43Z</updated>
    <published>2021-04-10T02:42:43Z</published>
    <title>Spike Camera and Its Coding Methods</title>
    <summary>  This paper introduces a spike camera with a distinct video capture scheme and
proposes two methods of decoding the spike stream for texture reconstruction.
The spike camera captures light and accumulates the converted luminance
intensity at each pixel. A spike is fired when the accumulated intensity
exceeds the dispatch threshold. The spike stream generated by the camera
indicates the luminance variation. Analyzing the patterns of the spike stream
makes it possible to reconstruct the picture of any moment which enables the
playback of high speed movement.
</summary>
    <author>
      <name>Siwei Dong</name>
    </author>
    <author>
      <name>Tiejun Huang</name>
    </author>
    <author>
      <name>Yonghong Tian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in DCC 2017. arXiv admin note: text overlap with
  arXiv:1907.08769</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.04669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.00641v1</id>
    <updated>2021-05-03T06:16:33Z</updated>
    <published>2021-05-03T06:16:33Z</published>
    <title>Naturalistic audio-visual volumetric sequences dataset of sounding
  actions for six degree-of-freedom interaction</title>
    <summary>  As audio-visual systems increasingly bring immersive and interactive
capabilities into our work and leisure activities, so the need for naturalistic
test material grows. New volumetric datasets have captured high-quality 3D
video, but accompanying audio is often neglected, making it hard to test an
integrated bimodal experience. Designed to cover diverse sound types and
features, the presented volumetric dataset was constructed from audio and video
studio recordings of scenes to yield forty short action sequences. Potential
uses in technical and scientific tests are discussed.
</summary>
    <author>
      <name>Hanne Stenzel</name>
    </author>
    <author>
      <name>Davide Berghi</name>
    </author>
    <author>
      <name>Marco Volino</name>
    </author>
    <author>
      <name>Philip J. B. Jackson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">for dataset visit cvssp.org/data/navvs; accepted as poster in IEEE VR
  2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.00641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.00641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.01475v1</id>
    <updated>2021-05-04T13:10:00Z</updated>
    <published>2021-05-04T13:10:00Z</published>
    <title>Insights on the V3C2 Dataset</title>
    <summary>  For research results to be comparable, it is important to have common
datasets for experimentation and evaluation. The size of such datasets,
however, can be an obstacle to their use. The Vimeo Creative Commons Collection
(V3C) is a video dataset designed to be representative of video content found
on the web, containing roughly 3800 hours of video in total, split into three
shards. In this paper, we present insights on the second of these shards (V3C2)
and discuss their implications for research areas, such as video retrieval, for
which the dataset might be particularly useful. We also provide all the
extracted data in order to simplify the use of the dataset.
</summary>
    <author>
      <name>Luca Rossetto</name>
    </author>
    <author>
      <name>Klaus Schoeffmann</name>
    </author>
    <author>
      <name>Abraham Bernstein</name>
    </author>
    <link href="http://arxiv.org/abs/2105.01475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.01475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.07135v1</id>
    <updated>2021-05-15T04:14:47Z</updated>
    <published>2021-05-15T04:14:47Z</published>
    <title>Analyzing Images for Music Recommendation</title>
    <summary>  Experiencing images with suitable music can greatly enrich the overall user
experience. The proposed image analysis method treats an artwork image
differently from a photograph image. Automatic image classification is
performed using deep-learning based models. An illustrative analysis showcasing
the ability of our deep-models to inherently learn and utilize perceptually
relevant features when classifying artworks is also presented. The Mean Opinion
Score (MOS) obtained from subjective assessments of the respective image and
recommended music pairs supports the effectiveness of our approach.
</summary>
    <author>
      <name>Anant Baijal</name>
    </author>
    <author>
      <name>Vivek Agarwal</name>
    </author>
    <author>
      <name>Danny Hyun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICCE50685.2021.9427619</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICCE50685.2021.9427619" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Consumer Electronics (IEEE ICCE
  2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.07135v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.07135v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.07990v1</id>
    <updated>2022-01-28T18:13:03Z</updated>
    <published>2022-01-28T18:13:03Z</published>
    <title>UofA-Truth at Factify 2022 : Transformer And Transfer Learning Based
  Multi-Modal Fact-Checking</title>
    <summary>  Identifying fake news is a very difficult task, especially when considering
the multiple modes of conveying information through text, image, video and/or
audio. We attempted to tackle the problem of automated
misinformation/disinformation detection in multi-modal news sources (including
text and images) through our simple, yet effective, approach in the FACTIFY
shared task at De-Factify@AAAI2022. Our model produced an F1-weighted score of
74.807%, which was the fourth best out of all the submissions. In this paper we
will explain our approach to undertake the shared task.
</summary>
    <author>
      <name>Abhishek Dhankar</name>
    </author>
    <author>
      <name>Osmar R. Za√Øane</name>
    </author>
    <author>
      <name>Francois Bolduc</name>
    </author>
    <link href="http://arxiv.org/abs/2203.07990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.07990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.0011v1</id>
    <updated>2011-04-29T20:05:52Z</updated>
    <published>2011-04-29T20:05:52Z</published>
    <title>Optimized Spline Interpolation</title>
    <summary>  In this paper, we investigate the problem of designing compact support
interpolation kernels for a given class of signals. By using calculus of
variations, we simplify the optimization problem from an infinite nonlinear
problem to a finite dimensional linear case, and then find the optimum compact
support function that best approximates a given filter in the least square
sense (l2 norm). The benefit of compact support interpolants is the low
computational complexity in the interpolation process while the optimum compact
support interpolant gaurantees the highest achivable Signal to Noise Ratio
(SNR). Our simulation results confirm the superior performance of the proposed
splines compared to other conventional compact support interpolants such as
cubic spline.
</summary>
    <author>
      <name>Ramtin Madani</name>
    </author>
    <author>
      <name>Ali Ayremlou</name>
    </author>
    <author>
      <name>Arash Amini</name>
    </author>
    <author>
      <name>Farrokh Marvasti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Signal Processing, Submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.0011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.0011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.2899v2</id>
    <updated>2014-01-23T13:33:06Z</updated>
    <published>2011-05-14T13:57:35Z</published>
    <title>Fast restoration of natural images corrupted by high-density impulse
  noise</title>
    <summary>  In this paper, we suggest a general model for the fixed-valued impulse noise
and propose a two-stage method for high density noise suppression while
preserving the image details. In the first stage, we apply an iterative impulse
detector, exploiting the image entropy, to identify the corrupted pixels and
then employ an Adaptive Iterative Mean filter to restore them. The filter is
adaptive in terms of the number of iterations, which is different for each
noisy pixel, according to the Euclidean distance from the nearest uncorrupted
pixel. Experimental results show that the proposed filter is fast and
outperforms the best existing techniques in both objective and subjective
performance measures.
</summary>
    <author>
      <name>Hossein Hosseini</name>
    </author>
    <author>
      <name>Farokh Marvasti</name>
    </author>
    <link href="http://arxiv.org/abs/1105.2899v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.2899v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.2040v1</id>
    <updated>2011-12-09T08:16:03Z</updated>
    <published>2011-12-09T08:16:03Z</published>
    <title>Recent Trends and Research Issues in Video Association Mining</title>
    <summary>  With the ever-growing digital libraries and video databases, it is
increasingly important to understand and mine the knowledge from video database
automatically. Discovering association rules between items in a large video
database plays a considerable role in the video data mining research areas.
Based on the research and development in the past years, application of
association rule mining is growing in different domains such as surveillance,
meetings, broadcast news, sports, archives, movies, medical data, as well as
personal and online media collections. The purpose of this paper is to provide
general framework of mining the association rules from video database. This
article is also represents the research issues in video association mining
followed by the recent trends.
</summary>
    <author>
      <name>Vijayakumar V</name>
    </author>
    <author>
      <name>Nedunchezhian R</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages; 1 Figure; 1 Table</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.2040v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.2040v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.2044v1</id>
    <updated>2011-12-09T08:35:14Z</updated>
    <published>2011-12-09T08:35:14Z</published>
    <title>Modelling Gesture Based Ubiquitous Applications</title>
    <summary>  A cost effective, gesture based modelling technique called Virtual
Interactive Prototyping (VIP) is described in this paper. Prototyping is
implemented by projecting a virtual model of the equipment to be prototyped.
Users can interact with the virtual model like the original working equipment.
For capturing and tracking the user interactions with the model image and sound
processing techniques are used. VIP is a flexible and interactive prototyping
method that has much application in ubiquitous computing environments.
Different commercial as well as socio-economic applications and extension to
interactive advertising of VIP are also discussed.
</summary>
    <author>
      <name>Kurien Zacharia</name>
    </author>
    <author>
      <name>Eldo P. Elias</name>
    </author>
    <author>
      <name>Surekha Mariam Varghese</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijma.2011.3403</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijma.2011.3403" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages; The International Journal of Multimedia &amp; Its Applications
  (IJMA) Vol.3, No.4, November 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1112.2044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.2044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68U20, 68U05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.0297v2</id>
    <updated>2012-10-06T03:52:57Z</updated>
    <published>2012-10-01T07:10:12Z</published>
    <title>Comparison of Speech Activity Detection Techniques for Speaker
  Recognition</title>
    <summary>  Speech activity detection (SAD) is an essential component for a variety of
speech processing applications. It has been observed that performances of
various speech based tasks are very much dependent on the efficiency of the
SAD. In this paper, we have systematically reviewed some popular SAD techniques
and their applications in speaker recognition. Speaker verification system
using different SAD technique are experimentally evaluated on NIST speech
corpora using Gaussian mixture model- universal background model (GMM-UBM)
based classifier for clean and noisy conditions. It has been found that two
Gaussian modeling based SAD is comparatively better than other SAD techniques
for different types of noises.
</summary>
    <author>
      <name>Md. Sahidullah</name>
    </author>
    <author>
      <name>Goutam Saha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.0297v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.0297v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.07551v1</id>
    <updated>2015-02-05T03:15:25Z</updated>
    <published>2015-02-05T03:15:25Z</published>
    <title>A Low-throughput Wavelet-based Steganography Audio Scheme</title>
    <summary>  This paper presents the preliminary of a novel scheme of steganography, and
introduces the idea of combining two secret keys in the operation. The first
secret key encrypts the text using a standard cryptographic scheme (e.g. IDEA,
SAFER+, etc.) prior to the wavelet audio decomposition. The way in which the
cipher text is embedded in the file requires another key, namely a stego-key,
which is associated with features of the audio wavelet analysis.
</summary>
    <author>
      <name>P. Carrion</name>
    </author>
    <author>
      <name>H. M. de Oliveira</name>
    </author>
    <author>
      <name>R. M. Campello de Souza</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure, conference: 8th Brazilian Symposium on Information
  and Computer System Security, 2008, Gramado, RS, Brazil</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.07551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.07551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.02581v1</id>
    <updated>2017-11-06T18:51:07Z</updated>
    <published>2017-11-06T18:51:07Z</published>
    <title>Convolutional Neural Network Steganalysis's Application to Steganography</title>
    <summary>  This paper presents a novel approach to increase the performance bounds of
image steganography under the criteria of minimizing distortion. The proposed
approach utilizes a steganalysis convolutional neural network (CNN) framework
to understand an image's model and embed in less detectable regions to preserve
the model. In other word, the trained steganalysis CNN is used to calculate
derivatives of the statistical model of an image with respect to embedding
changes. The experimental results show that the proposed algorithm outperforms
previous state-of-the-art methods in a wide range of low relative payloads when
compared with HUGO, S-UNIWARD, and HILL by the state-of-the-art steganalysis.
</summary>
    <author>
      <name>Mehdi Sharifzadeh</name>
    </author>
    <author>
      <name>Chirag Agarwal</name>
    </author>
    <author>
      <name>Mohammed Aloraini</name>
    </author>
    <author>
      <name>Dan Schonfeld</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1705.08616</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.02581v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.02581v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.03951v2</id>
    <updated>2018-01-17T15:51:46Z</updated>
    <published>2017-11-10T18:26:04Z</published>
    <title>Predicting Chroma from Luma in AV1</title>
    <summary>  Chroma from luma (CfL) prediction is a new and promising chroma-only intra
predictor that models chroma pixels as a linear function of the coincident
reconstructed luma pixels. In this paper, we present the CfL predictor adopted
in Alliance Video 1 (AV1), a royalty-free video codec developed by the Alliance
for Open Media (AOM). The proposed CfL distinguishes itself from prior art not
only by reducing decoder complexity, but also by producing more accurate
predictions. On average, CfL reduces the BD-rate, when measured with CIEDE2000,
by 5% for still images and 2% for video sequences.
</summary>
    <author>
      <name>Luc N. Trudeau</name>
    </author>
    <author>
      <name>Nathan E. Egge</name>
    </author>
    <author>
      <name>David Barr</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.03951v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.03951v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.04837v1</id>
    <updated>2018-05-13T07:54:34Z</updated>
    <published>2018-05-13T07:54:34Z</published>
    <title>Video Processing on the Edge for Multimedia IoT Systems</title>
    <summary>  In this article, we first survey the current situation of video processing on
the edge for multimedia Internet-of-Things (M-IoT) systems in three typical
scenarios, i.e., smart cities, satellite networks, and Internet-of-Vehicles. By
summarizing a general model of the edge video processing, the importance of
developing an edge computing platform is highlighted. Then, we give a method of
implementing cooperative video processing on an edge computing platform based
on light-weighted virtualization technologies. Performance evaluation is
conducted and some insightful observations can be obtained. Moreover, we
summarize challenges and opportunities of realizing effective edge video
processing for M-IoT systems.
</summary>
    <author>
      <name>Yang Cao</name>
    </author>
    <author>
      <name>Zeyu Xu</name>
    </author>
    <author>
      <name>Peng Qin</name>
    </author>
    <author>
      <name>Tao Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 5 figures, one table</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.04837v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.04837v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.02137v1</id>
    <updated>2018-12-05T17:52:16Z</updated>
    <published>2018-12-05T17:52:16Z</published>
    <title>HEVC Inter Coding Using Deep Recurrent Neural Networks and Artificial
  Reference Pictures</title>
    <summary>  The efficiency of motion compensated prediction in modern video codecs highly
depends on the available reference pictures. Occlusions and non-linear motion
pose challenges for the motion compensation and often result in high bit rates
for the prediction error. We propose the generation of artificial reference
pictures using deep recurrent neural networks. Conceptually, a reference
picture at the time instance of the currently coded picture is generated from
previously reconstructed conventional reference pictures. Based on these
artificial reference pictures, we propose a complete coding pipeline based on
HEVC. By using the artificial reference pictures for motion compensated
prediction, average BD-rate gains of 1.5% over HEVC are achieved.
</summary>
    <author>
      <name>Felix Haub</name>
    </author>
    <author>
      <name>Thorsten Laude</name>
    </author>
    <author>
      <name>J√∂rn Ostermann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures, under review for ICME 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.02137v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.02137v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.08615v1</id>
    <updated>2019-06-20T13:45:17Z</updated>
    <published>2019-06-20T13:45:17Z</published>
    <title>Zero-shot Learning and Knowledge Transfer in Music Classification and
  Tagging</title>
    <summary>  Music classification and tagging is conducted through categorical supervised
learning with a fixed set of labels. In principle, this cannot make predictions
on unseen labels. Zero-shot learning is an approach to solve the problem by
using side information about the semantic labels. We recently investigated this
concept of zero-shot learning in music classification and tagging task by
projecting both audio and label space on a single semantic space. In this work,
we extend the work to verify the generalization ability of zero-shot learning
model by conducting knowledge transfer to different music corpora.
</summary>
    <author>
      <name>Jeong Choi</name>
    </author>
    <author>
      <name>Jongpil Lee</name>
    </author>
    <author>
      <name>Jiyoung Park</name>
    </author>
    <author>
      <name>Juhan Nam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Machine Learning (ICML) 2019, Machine
  Learning for Music Discovery Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.08615v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.08615v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.03251v1</id>
    <updated>2020-01-09T23:08:34Z</updated>
    <published>2020-01-09T23:08:34Z</published>
    <title>Adaptive Control of Embedding Strength in Image Watermarking using
  Neural Networks</title>
    <summary>  Digital image watermarking has been widely used in different applications
such as copyright protection of digital media, such as audio, image, and video
files. Two opposing criteria of robustness and transparency are the goals of
watermarking methods. In this paper, we propose a framework for determining the
appropriate embedding strength factor. The framework can use most DWT and DCT
based blind watermarking approaches. We use Mask R-CNN on the COCO dataset to
find a good strength factor for each sub-block. Experiments show that this
method is robust against different attacks and has good transparency.
</summary>
    <author>
      <name>Mahnoosh Bagheri</name>
    </author>
    <author>
      <name>Majid Mohrekesh</name>
    </author>
    <author>
      <name>Nader Karimi</name>
    </author>
    <author>
      <name>Shadrokh Samavi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.03251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.03251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.04558v1</id>
    <updated>2020-08-11T07:14:17Z</updated>
    <published>2020-08-11T07:14:17Z</published>
    <title>Extension of JPEG XS for Two-Layer Lossless Coding</title>
    <summary>  A two-layer lossless image coding method compatible with JPEG XS is proposed.
JPEG XS is a new international standard for still image coding that has the
characteristics of very low latency and very low complexity. However, it does
not support lossless coding, although it can achieve visual lossless coding.
The proposed method has a two-layer structure similar to JPEG XT, which
consists of JPEG XS coding and a lossless coding method. As a result, it
enables us to losslessly restore original images, while maintaining
compatibility with JPEG XS.
</summary>
    <author>
      <name>Hiroyuki Kobayashi</name>
    </author>
    <author>
      <name>Hitoshi Kiya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in 2020 IEEE 9th Global Conference on Consumer Electronics</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.04558v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.04558v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.09641v1</id>
    <updated>2020-10-19T16:35:30Z</updated>
    <published>2020-10-19T16:35:30Z</published>
    <title>DIME: An Online Tool for the Visual Comparison of Cross-Modal Retrieval
  Models</title>
    <summary>  Cross-modal retrieval relies on accurate models to retrieve relevant results
for queries across modalities such as image, text, and video. In this paper, we
build upon previous work by tackling the difficulty of evaluating models both
quantitatively and qualitatively quickly. We present DIME (Dataset, Index,
Model, Embedding), a modality-agnostic tool that handles multimodal datasets,
trained models, and data preprocessors to support straightforward model
comparison with a web browser graphical user interface. DIME inherently
supports building modality-agnostic queryable indexes and extraction of
relevant feature embeddings, and thus effectively doubles as an efficient
cross-modal tool to explore and search through datasets.
</summary>
    <author>
      <name>Tony Zhao</name>
    </author>
    <author>
      <name>Jaeyoung Choi</name>
    </author>
    <author>
      <name>Gerald Friedland</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-37734-2_61</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-37734-2_61" rel="related"/>
    <link href="http://arxiv.org/abs/2010.09641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.04510v2</id>
    <updated>2021-08-16T07:04:36Z</updated>
    <published>2021-07-09T15:53:47Z</published>
    <title>Hacking VMAF and VMAF NEG: vulnerability to different preprocessing
  methods</title>
    <summary>  Video-quality measurement plays a critical role in the development of
video-processing applications. In this paper, we show how video preprocessing
can artificially increase the popular quality metric VMAF and its
tuning-resistant version, VMAF NEG. We propose a pipeline that tunes
processing-algorithm parameters to increase VMAF by up to 218.8%. A subjective
comparison revealed that for most preprocessing methods, a video's visual
quality drops or stays unchanged. We also show that some preprocessing methods
can increase VMAF NEG scores by up to 23.6%.
</summary>
    <author>
      <name>Maksim Siniukov</name>
    </author>
    <author>
      <name>Anastasia Antsiferova</name>
    </author>
    <author>
      <name>Dmitriy Kulikov</name>
    </author>
    <author>
      <name>Dmitriy Vatolin</name>
    </author>
    <link href="http://arxiv.org/abs/2107.04510v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.04510v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.03018v1</id>
    <updated>2022-06-07T05:22:42Z</updated>
    <published>2022-06-07T05:22:42Z</published>
    <title>What is the Metaverse? An Immersive Cyberspace and Open Challenges</title>
    <summary>  The Metaverse refers to a virtual-physical blended space in which multiple
users can concurrently interact with a unified computer-generated environment
and other users, which can be regarded as the next significant milestone of the
current cyberspace. This article primarily discusses the development and
challenges of the Metaverse. We first briefly describe the development of
cyberspace and the necessity of technology enablers. Accordingly, our bottom-up
approach highlights three critical technology enablers for the Metaverse:
networks, systems, and users. Also, we highlight a number of indispensable
issues, under technological and ecosystem perspectives, that build and sustain
the Metaverse.
</summary>
    <author>
      <name>Lik-Hang Lee</name>
    </author>
    <author>
      <name>Pengyuan Zhou</name>
    </author>
    <author>
      <name>Tristan Braud</name>
    </author>
    <author>
      <name>Pan Hui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.03018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.03018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A.1; K.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.02927v1</id>
    <updated>2022-09-07T04:41:11Z</updated>
    <published>2022-09-07T04:41:11Z</published>
    <title>Network-aware Prefetching Method for Short-Form Video Streaming</title>
    <summary>  Recent years have witnessed the rising of short-form video platforms such as
TikTok. Apart from conventional videos, short-form videos are much shorter and
users frequently change the content to watch. Thus, it is crucial to have an
effective streaming method for this new type of video. In this paper, we
propose a resource-efficient prefetching method for short-form video streaming.
Taking into account network throughput conditions and user viewing behaviors,
the proposed method dynamically adapts the amount of prefetched video data.
Experiment results show that our method can reduce the data waste by 37~52%
compared to other existing methods.
</summary>
    <author>
      <name>Duc Nguyen</name>
    </author>
    <author>
      <name>Phong Nguyen</name>
    </author>
    <author>
      <name>Vu Long</name>
    </author>
    <author>
      <name>Truong Thu Huong</name>
    </author>
    <author>
      <name>Pham Ngoc Nam</name>
    </author>
    <link href="http://arxiv.org/abs/2209.02927v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.02927v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.15557v1</id>
    <updated>2022-09-30T16:15:57Z</updated>
    <published>2022-09-30T16:15:57Z</published>
    <title>Explaining Hierarchical Features in Dynamic Point Cloud Processing</title>
    <summary>  This paper aims at bringing some light and understanding to the field of deep
learning for dynamic point cloud processing. Specifically, we focus on the
hierarchical features learning aspect, with the ultimate goal of understanding
which features are learned at the different stages of the process and what
their meaning is. Last, we bring clarity on how hierarchical components of the
network affect the learned features and their importance for a successful
learning model. This study is conducted for point cloud prediction tasks,
useful for predicting coding applications.
</summary>
    <author>
      <name>Pedro Gomes</name>
    </author>
    <author>
      <name>Silvia Rossi</name>
    </author>
    <author>
      <name>Laura Toni</name>
    </author>
    <link href="http://arxiv.org/abs/2209.15557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.15557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.06794v1</id>
    <updated>2022-10-13T07:13:16Z</updated>
    <published>2022-10-13T07:13:16Z</published>
    <title>Towards Holographic Video Communications: A Promising AI-driven Solution</title>
    <summary>  Real-time holographic video communications enable immersive experiences for
next-generation video services in the future metaverse era. However,
high-fidelity holographic videos require high bandwidth and significant
computation resources, which exceed the transferring and computing capacity of
5G networks. This article reviews state-of-the-art holographic point cloud
video (PCV) transmission techniques and highlights the critical challenges of
delivering such immersive services. We further implement a preliminary
prototype of an AI-driven holographic video communication system and present
critical experimental results to evaluate its performance. Finally, we identify
future research directions and discuss potential solutions for providing
real-time and high-quality holographic experiences.
</summary>
    <author>
      <name>Yakun Huang</name>
    </author>
    <author>
      <name>Yuanwei Zhu</name>
    </author>
    <author>
      <name>Xiuquan Qiao</name>
    </author>
    <author>
      <name>Xiang Su</name>
    </author>
    <author>
      <name>Schahram Dustdar</name>
    </author>
    <author>
      <name>Ping Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2210.06794v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.06794v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.02240v1</id>
    <updated>2022-11-04T03:16:45Z</updated>
    <published>2022-11-04T03:16:45Z</published>
    <title>DaI: Decrypt and Infer the Quality of Real-Time Video Streaming</title>
    <summary>  Inferring the quality of network services is the vital basis of optimization
for network operators. However, prevailing real-time video streaming
applications adopt encryption for security, leaving it a problem to extract
Quality of Service (QoS) indicators of real-time video. In this paper, we
propose DaI, a traffic-based real-time video quality estimator. DaI can
partially decrypt the encrypted real-time video data and applies machine
learning methods to estimate key objective Quality of Experience (QoE) metrics
of real-time video. According to the experimental results, DaI can estimate
objective QoE metrics with an average accuracy of 79%.
</summary>
    <author>
      <name>Sheng Cheng</name>
    </author>
    <link href="http://arxiv.org/abs/2211.02240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.02240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.08878v1</id>
    <updated>2022-11-16T12:36:48Z</updated>
    <published>2022-11-16T12:36:48Z</published>
    <title>Video-Music Retrieval:A Dual-Path Cross-Modal Network</title>
    <summary>  We propose a method to recommend background music for videos. Current work
rarely considers the emotional information of music, which is essential for
video music retrieval. To achieve this, we design two paths to process content
information and emotional information between modal. Based on characteristics
of video and music, we design various feature extraction schemes and common
representation spaces. More importantly, we propose a way to combine content
information with emotional information. Additionally, we make improvements to
the classical metric loss to be more suited to this task. Experiments show that
this dual path video music retrieval network can effectively merge information.
Compare with existing methods, the retrieval task evaluation index: increasing
Recall@1 by 3.94 and Recall@25 by 16.36.
</summary>
    <author>
      <name>Xin Gu</name>
    </author>
    <author>
      <name>Yinghua Shen</name>
    </author>
    <author>
      <name>Chaohui Lv</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5pages,3figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.08878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.08878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.13484v1</id>
    <updated>2022-11-24T09:21:14Z</updated>
    <published>2022-11-24T09:21:14Z</published>
    <title>Robust-MSA: Understanding the Impact of Modality Noise on Multimodal
  Sentiment Analysis</title>
    <summary>  Improving model robustness against potential modality noise, as an essential
step for adapting multimodal models to real-world applications, has received
increasing attention among researchers. For Multimodal Sentiment Analysis
(MSA), there is also a debate on whether multimodal models are more effective
against noisy features than unimodal ones. Stressing on intuitive illustration
and in-depth analysis of these concerns, we present Robust-MSA, an interactive
platform that visualizes the impact of modality noise as well as simple defence
methods to help researchers know better about how their models perform with
imperfect real-world data.
</summary>
    <author>
      <name>Huisheng Mao</name>
    </author>
    <author>
      <name>Baozheng Zhang</name>
    </author>
    <author>
      <name>Hua Xu</name>
    </author>
    <author>
      <name>Ziqi Yuan</name>
    </author>
    <author>
      <name>Yihe Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accept by AAAI 2023. Code is available at
  https://github.com/thuiar/Robust-MSA</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.13484v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.13484v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2212.13842v1</id>
    <updated>2022-12-25T15:15:29Z</updated>
    <published>2022-12-25T15:15:29Z</published>
    <title>Towards a QoE Model to Evaluate Holographic Augmented Reality Devices</title>
    <summary>  Augmented reality (AR) technology is developing fast and provides users with
new ways to interact with the real-world surrounding environment. Although the
performance of holographic AR multimedia devices can be measured with
traditional quality-of-service parameters, a quality-of-experience (QoE) model
can better evaluate the device from the perspective of users. As there are
currently no well-recognized models for measuring the QoE of a holographic AR
multimedia device, we present a QoE framework and model it with a fuzzy
inference system to quantitatively evaluate the device.
</summary>
    <author>
      <name>Longyu Zhang</name>
    </author>
    <author>
      <name>Haiwei Dong</name>
    </author>
    <author>
      <name>Abdulmotaleb El Saddik</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MMUL.2018.2873843</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MMUL.2018.2873843" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Multimedia, vol. 26, no. 2, pp. 21-32, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2212.13842v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2212.13842v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.07740v1</id>
    <updated>2023-01-18T19:10:28Z</updated>
    <published>2023-01-18T19:10:28Z</published>
    <title>The Metaverse from a Multimedia Communications Perspective</title>
    <summary>  eXtended reality (XR) technologies such as virtual reality and 360{\deg}
stereoscopic streaming enable the concept of the Metaverse, an immersive
virtual space for collaboration and interaction. To ensure high fidelity
display of immersive media, the bandwidth, latency and network traffic patterns
will need to be considered to ensure a user's Quality of Experience (QoE). In
this article, examples and calculations are explored to demonstrate the
requirements of the abovementioned parameters. Additionally, future methods
such as network-awareness using reinforcement learning (RL) and XR content
awareness using spatial or temporal difference in the frames could be explored
from a multimedia communications perspective.
</summary>
    <author>
      <name>Haiwei Dong</name>
    </author>
    <author>
      <name>Jeannie S. A. Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MMUL.2022.3217627</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MMUL.2022.3217627" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Multimedia Magazine, vol. 29, no. 4, pp. 123-127, 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2301.07740v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.07740v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.08249v1</id>
    <updated>2023-02-16T12:11:55Z</updated>
    <published>2023-02-16T12:11:55Z</published>
    <title>Mixing Levels -- A Rock Music Spirit Level App</title>
    <summary>  To date, sonification apps are rare. Music apps on the other hand are widely
used. Smartphone users like to play with music. In this manuscript, we present
Mixing Levels, a spirit level sonification based on music mixing. Tilting the
smartphone adjusts the volumes of 5 musical instruments in a rock music loop.
Only when perfectly leveled, all instruments in the mix are well-audible. The
app is supposed to be useful and fun. Since the app appears like a music mixing
console, people have fun to interact with Mixing Levels, so that learning the
sonification is a playful experience.
</summary>
    <author>
      <name>Tim Ziemer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2302.08249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.08249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.12393v1</id>
    <updated>2023-02-24T01:47:13Z</updated>
    <published>2023-02-24T01:47:13Z</published>
    <title>Blind Omnidirectional Image Quality Assessment: Integrating Local
  Statistics and Global Semantics</title>
    <summary>  Omnidirectional image quality assessment (OIQA) aims to predict the
perceptual quality of omnidirectional images that cover the whole
180$\times$360$^{\circ}$ viewing range of the visual environment. Here we
propose a blind/no-reference OIQA method named S$^2$ that bridges the gap
between low-level statistics and high-level semantics of omnidirectional
images. Specifically, statistic and semantic features are extracted in separate
paths from multiple local viewports and the hallucinated global omnidirectional
image, respectively. A quality regression along with a weighting process is
then followed that maps the extracted quality-aware features to a perceptual
quality prediction. Experimental results demonstrate that the proposed S$^2$
method offers highly competitive performance against state-of-the-art methods.
</summary>
    <author>
      <name>Wei Zhou</name>
    </author>
    <author>
      <name>Zhou Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2302.12393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.12393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.03176v1</id>
    <updated>2023-05-04T21:47:43Z</updated>
    <published>2023-05-04T21:47:43Z</published>
    <title>NeRF-QA: Neural Radiance Fields Quality Assessment Database</title>
    <summary>  This short paper proposes a new database - NeRF-QA - containing 48 videos
synthesized with seven NeRF based methods, along with their perceived quality
scores, resulting from subjective assessment tests; for the videos selection,
both real and synthetic, 360 degrees scenes were considered. This database will
allow to evaluate the suitability, to NeRF based synthesized views, of existing
objective quality metrics and also the development of new quality metrics,
specific for this case.
</summary>
    <author>
      <name>Pedro Martin</name>
    </author>
    <author>
      <name>Ant√≥nio Rodrigues</name>
    </author>
    <author>
      <name>Jo√£o Ascenso</name>
    </author>
    <author>
      <name>Maria Paula Queluz</name>
    </author>
    <link href="http://arxiv.org/abs/2305.03176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.03176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.10112v1</id>
    <updated>2023-05-17T10:24:39Z</updated>
    <published>2023-05-17T10:24:39Z</published>
    <title>A dual watermaking scheme based on Sobolev type orthogonal moments for
  document authentication</title>
    <summary>  A dual watermarking scheme based on Sobolev type orthogonal moments, Charlier
and Meixner, is proposed based on different discrete measures. The existing
relation through the connection formulas allows to provide with structure and
recurrence relations, together with two difference equations satisfied by such
families. Weighted polynomials derived from them are being applied in an
embedding and extraction watermarking algorithm, comparing the results obtained
in imperceptibly and robustness tests with other families of polynomials.
</summary>
    <author>
      <name>Alicia Mar√≠a Centuri√≥n-Fajardo</name>
    </author>
    <author>
      <name>Alberto Lastra</name>
    </author>
    <author>
      <name>Anier Soria-Lorente</name>
    </author>
    <link href="http://arxiv.org/abs/2305.10112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.10112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.09948v1</id>
    <updated>2023-08-19T08:55:40Z</updated>
    <published>2023-08-19T08:55:40Z</published>
    <title>Bamboo: Boosting Training Efficiency for Real-Time Video Streaming via
  Online Grouped Federated Transfer Learning</title>
    <summary>  Most of the learning-based algorithms for bitrate adaptation are limited to
offline learning, which inevitably suffers from the simulation-to-reality gap.
Online learning can better adapt to dynamic real-time communication scenes but
still face the challenge of lengthy training convergence time. In this paper,
we propose a novel online grouped federated transfer learning framework named
Bamboo to accelerate training efficiency. The preliminary experiments validate
that our method remarkably improves online training efficiency by up to 302%
compared to other reinforcement learning algorithms in various network
conditions while ensuring the quality of experience (QoE) of real-time video
communication.
</summary>
    <author>
      <name>Qianyuan Zheng</name>
    </author>
    <author>
      <name>Hao Chen</name>
    </author>
    <author>
      <name>Zhan Ma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3600061.3600069</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3600061.3600069" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper will be presented at Apnet 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.09948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.09948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.03326v1</id>
    <updated>2023-09-06T19:17:46Z</updated>
    <published>2023-09-06T19:17:46Z</published>
    <title>Detecting False Alarms and Misses in Audio Captions</title>
    <summary>  Metrics to evaluate audio captions simply provide a score without much
explanation regarding what may be wrong in case the score is low. Manual human
intervention is needed to find any shortcomings of the caption. In this work,
we introduce a metric which automatically identifies the shortcomings of an
audio caption by detecting the misses and false alarms in a candidate caption
with respect to a reference caption, and reports the recall, precision and
F-score. Such a metric is very useful in profiling the deficiencies of an audio
captioning model, which is a milestone towards improving the quality of audio
captions.
</summary>
    <author>
      <name>Rehana Mahfuz</name>
    </author>
    <author>
      <name>Yinyi Guo</name>
    </author>
    <author>
      <name>Arvind Krishna Sridhar</name>
    </author>
    <author>
      <name>Erik Visser</name>
    </author>
    <link href="http://arxiv.org/abs/2309.03326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.03326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.05395v1</id>
    <updated>2023-10-09T04:19:27Z</updated>
    <published>2023-10-09T04:19:27Z</published>
    <title>Robust Image Watermarking based on Cross-Attention and Invariant Domain
  Learning</title>
    <summary>  Image watermarking involves embedding and extracting watermarks within a
cover image, with deep learning approaches emerging to bolster generalization
and robustness. Predominantly, current methods employ convolution and
concatenation for watermark embedding, while also integrating conceivable
augmentation in the training process. This paper explores a robust image
watermarking methodology by harnessing cross-attention and invariant domain
learning, marking two novel, significant advancements. First, we design a
watermark embedding technique utilizing a multi-head cross attention mechanism,
enabling information exchange between the cover image and watermark to identify
semantically suitable embedding locations. Second, we advocate for learning an
invariant domain representation that encapsulates both semantic and
noise-invariant information concerning the watermark, shedding light on
promising avenues for enhancing image watermarking techniques.
</summary>
    <author>
      <name>Agnibh Dasgupta</name>
    </author>
    <author>
      <name>Xin Zhong</name>
    </author>
    <link href="http://arxiv.org/abs/2310.05395v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.05395v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.06309v1</id>
    <updated>2023-10-10T04:58:43Z</updated>
    <published>2023-10-10T04:58:43Z</published>
    <title>Encoding and Decoding Narratives: Datafication and Alternative Access
  Models for Audiovisual Archives</title>
    <summary>  Situated in the intersection of audiovisual archives, computational methods,
and immersive interactions, this work probes the increasingly important
accessibility issues from a two-fold approach. Firstly, the work proposes an
ontological data model to handle complex descriptors (metadata, feature
vectors, etc.) with regard to user interactions. Secondly, this work examines
text-to-video retrieval from an implementation perspective by proposing a
classifier-enhanced workflow to deal with complex and hybrid queries and a
training data augmentation workflow to improve performance. This work serves as
the foundation for experimenting with novel public-facing access models to
large audiovisual archives
</summary>
    <author>
      <name>Yuchen Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2310.05825</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.06309v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.06309v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.10406v1</id>
    <updated>2023-12-16T10:14:16Z</updated>
    <published>2023-12-16T10:14:16Z</published>
    <title>Statistical Analysis of Inter Coding in VVC Test Model (VTM)</title>
    <summary>  The promising improvement in compression efficiency of Versatile Video Coding
(VVC) compared to High Efficiency Video Coding (HEVC) comes at the cost of a
non-negligible encoder side complexity. The largely increased complexity
overhead is a possible obstacle towards its industrial implementation. Many
papers have proposed acceleration methods for VVC. Still, a better
understanding of VVC complexity, especially related to new partitions and
coding tools, is desirable to help the design of new and better acceleration
methods. For this purpose, statistical analyses have been conducted, with a
focus on Coding Unit (CU) sizes and inter coding modes.
</summary>
    <author>
      <name>Yiqun Liu</name>
    </author>
    <author>
      <name>Mohsen Abdoli</name>
    </author>
    <author>
      <name>Thomas Guionnet</name>
    </author>
    <author>
      <name>Christine Guillemot</name>
    </author>
    <author>
      <name>Aline Roumy</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICIP46576.2022.9897595</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICIP46576.2022.9897595" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICIP 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.10406v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.10406v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.17151v1</id>
    <updated>2024-01-30T16:32:37Z</updated>
    <published>2024-01-30T16:32:37Z</published>
    <title>An Open Software Suite for Event-Based Video</title>
    <summary>  While traditional video representations are organized around discrete image
frames, event-based video is a new paradigm that forgoes image frames
altogether. Rather, pixel samples are temporally asynchronous and independent
of one another. Until now, researchers have lacked a cohesive software
framework for exploring the representation, compression, and applications of
event-based video. I present the AD$\Delta$ER software suite to fill this gap.
This framework includes utilities for transcoding framed and multimodal
event-based video sources to a common representation, rate control mechanisms,
lossy compression, application support, and an interactive GUI for transcoding
and playback. In this paper, I describe these various software components and
their usage.
</summary>
    <author>
      <name>Andrew C. Freeman</name>
    </author>
    <link href="http://arxiv.org/abs/2401.17151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.17151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.18007v1</id>
    <updated>2024-04-09T07:17:08Z</updated>
    <published>2024-04-09T07:17:08Z</published>
    <title>Deep Mamba Multi-modal Learning</title>
    <summary>  Inspired by the excellent performance of Mamba networks, we propose a novel
Deep Mamba Multi-modal Learning (DMML). It can be used to achieve the fusion of
multi-modal features. We apply DMML to the field of multimedia retrieval and
propose an innovative Deep Mamba Multi-modal Hashing (DMMH) method. It combines
the advantages of algorithm accuracy and inference speed. We validated the
effectiveness of DMMH on three public datasets and achieved state-of-the-art
results.
</summary>
    <author>
      <name>Jian Zhu</name>
    </author>
    <author>
      <name>Xin Zou</name>
    </author>
    <author>
      <name>Yu Cui</name>
    </author>
    <author>
      <name>Zhangmin Huang</name>
    </author>
    <author>
      <name>Chenshu Hu</name>
    </author>
    <author>
      <name>Bo Lyu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Deep Mamba Multi-modal Learning; Deep Mamba Multi-modal Hashing</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.18007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.18007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.04383v1</id>
    <updated>2024-08-08T11:38:27Z</updated>
    <published>2024-08-08T11:38:27Z</published>
    <title>The algorithmic nature of song-sequencing: statistical regularities in
  music albums</title>
    <summary>  Based on a review of anecdotal beliefs, we explored patterns of
track-sequencing within professional music albums. We found that songs with
high levels of valence, energy and loudness are more likely to be positioned at
the beginning of each album. We also found that transitions between consecutive
tracks tend to alternate between increases and decreases of valence and energy.
These findings were used to build a system which automates the process of
album-sequencing. Our results and hypothesis have both practical and
theoretical applications. Practically, sequencing regularities can be used to
inform playlist generation systems. Theoretically, we show weak to moderate
support for the idea that music is perceived in both global and local contexts.
</summary>
    <author>
      <name>Pedro Neto</name>
    </author>
    <author>
      <name>Martin Hartmann</name>
    </author>
    <author>
      <name>Geoff Luck</name>
    </author>
    <author>
      <name>Petri Toiviainen</name>
    </author>
    <link href="http://arxiv.org/abs/2408.04383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.04383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.06709v1</id>
    <updated>2024-08-25T04:56:08Z</updated>
    <published>2024-08-25T04:56:08Z</published>
    <title>Unveiling Visual Biases in Audio-Visual Localization Benchmarks</title>
    <summary>  Audio-Visual Source Localization (AVSL) aims to localize the source of sound
within a video. In this paper, we identify a significant issue in existing
benchmarks: the sounding objects are often easily recognized based solely on
visual cues, which we refer to as visual bias. Such biases hinder these
benchmarks from effectively evaluating AVSL models. To further validate our
hypothesis regarding visual biases, we examine two representative AVSL
benchmarks, VGG-SS and EpicSounding-Object, where the vision-only models
outperform all audiovisual baselines. Our findings suggest that existing AVSL
benchmarks need further refinement to facilitate audio-visual learning.
</summary>
    <author>
      <name>Liangyu Chen</name>
    </author>
    <author>
      <name>Zihao Yue</name>
    </author>
    <author>
      <name>Boshen Xu</name>
    </author>
    <author>
      <name>Qin Jin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ECCV24 AVGenL Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.06709v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.06709v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.12825v3</id>
    <updated>2025-03-08T03:30:10Z</updated>
    <published>2024-11-19T19:22:24Z</published>
    <title>TopoCode: Topologically Informed Error Detection and Correction in
  Communication Systems</title>
    <summary>  Traditional error detection and correction codes focus on bit-level fidelity,
which is insufficient for emerging technologies like eXtended Reality (XR) and
holographic communications requiring high-data-rate, low-latency systems.
Bit-level metrics cannot comprehensively evaluate Quality-of-Service (QoS) in
these scenarios. This letter proposes TopoCode which leverages Topological Data
Analysis (TDA) and persistent homology to encode topological information for
message-level error detection and correction. It introduces minimal redundancy
while enabling effective data reconstruction, especially in low Signal-to-Noise
Ratio (SNR) conditions. TopoCode offers a promising approach to meet the
demands of next-generation communication systems prioritizing semantic accuracy
and message-level integrity.
</summary>
    <author>
      <name>Hongzhi Guo</name>
    </author>
    <link href="http://arxiv.org/abs/2411.12825v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.12825v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.12907v1</id>
    <updated>2024-11-19T22:51:31Z</updated>
    <published>2024-11-19T22:51:31Z</published>
    <title>Narrative Information Theory</title>
    <summary>  We propose an information-theoretic framework to measure narratives,
providing a formalism to understand pivotal moments, cliffhangers, and plot
twists. This approach offers creatives and AI researchers tools to analyse and
benchmark human- and AI-created stories. We illustrate our method in TV shows,
showing its ability to quantify narrative complexity and emotional dynamics
across genres. We discuss applications in media and in human-in-the-loop
generative AI storytelling.
</summary>
    <author>
      <name>Lion Schulz</name>
    </author>
    <author>
      <name>Miguel Patr√≠cio</name>
    </author>
    <author>
      <name>Daan Odijk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in NeurIPS 2024 Workshop on Creativity &amp; Generative
  AI. 7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.12907v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.12907v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.11818v1</id>
    <updated>2024-12-16T14:35:32Z</updated>
    <published>2024-12-16T14:35:32Z</published>
    <title>Leveraging User-Generated Metadata of Online Videos for Cover Song
  Identification</title>
    <summary>  YouTube is a rich source of cover songs. Since the platform itself is
organized in terms of videos rather than songs, the retrieval of covers is not
trivial. The field of cover song identification addresses this problem and
provides approaches that usually rely on audio content. However, including the
user-generated video metadata available on YouTube promises improved
identification results. In this paper, we propose a multi-modal approach for
cover song identification on online video platforms. We combine the entity
resolution models with audio-based approaches using a ranking model. Our
findings implicate that leveraging user-generated metadata can stabilize cover
song identification performance on YouTube.
</summary>
    <author>
      <name>Simon Hachmeier</name>
    </author>
    <author>
      <name>Robert J√§schke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for presentation at NLP for Music and Audio (NLP4MusA) 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.11818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.11818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.01001v1</id>
    <updated>2025-05-02T04:56:57Z</updated>
    <published>2025-05-02T04:56:57Z</published>
    <title>Photoshop Batch Rendering Using Actions for Stylistic Video Editing</title>
    <summary>  My project looks at an efficient workflow for creative image/video editing
using Adobe Photoshop Actions tool and Batch Processing System. This innovative
approach to video editing through Photoshop creates a fundamental shift to
creative workflow management through the integration of industry-leading image
manipulation with video editing techniques. Through systematic automation of
Actions, users can achieve a simple and consistent application of visual edits
across a string of images. This approach provides an alternative method to
optimize productivity while ensuring uniform results across image collections
through a post-processing pipeline.
</summary>
    <author>
      <name>Tessa De La Fuente</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.01001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.01001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.0256v1</id>
    <updated>2010-04-01T23:42:11Z</updated>
    <published>2010-04-01T23:42:11Z</published>
    <title>From Playability to a Hierarchical Game Usability Model</title>
    <summary>  This paper presents a brief review of current game usability models. This
leads to the conception of a high-level game development-centered usability
model that integrates current usability approaches in game industry and game
research.
</summary>
    <author>
      <name>Lennart E. Nacke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1639601.1639609</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1639601.1639609" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.0256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.0256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="97Rxx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.8.0; H.5.1; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10204v1</id>
    <updated>2018-07-26T15:41:35Z</updated>
    <published>2018-07-26T15:41:35Z</published>
    <title>Visual Display and Retrieval of Music Information</title>
    <summary>  This paper describes computational methods for the visual display and
analysis of music information. We provide a concise description of software,
music descriptors and data visualization techniques commonly used in music
information retrieval. Finally, we provide use cases where the described
software, descriptors and visualizations are showcased.
</summary>
    <author>
      <name>Rafael Valle</name>
    </author>
    <link href="http://arxiv.org/abs/1807.10204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.09388v1</id>
    <updated>2022-01-23T22:44:45Z</updated>
    <published>2022-01-23T22:44:45Z</published>
    <title>A Survey on Patients Privacy Protection with Stganography and Visual
  Encryption</title>
    <summary>  In this survey, thirty models for steganography and visual encryption methods
have been discussed to provide patients privacy protection.
</summary>
    <author>
      <name>Hussein K. Alzubaidy</name>
    </author>
    <author>
      <name>Dhiah Al-Shammary</name>
    </author>
    <author>
      <name>Mohammed Hamzah Abed</name>
    </author>
    <link href="http://arxiv.org/abs/2201.09388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.09388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.02435v2</id>
    <updated>2016-05-17T13:23:08Z</updated>
    <published>2015-05-10T21:04:48Z</published>
    <title>Cloud for Gaming</title>
    <summary>  Cloud for Gaming refers to the use of cloud computing technologies to build
large-scale gaming infrastructures, with the goal of improving scalability and
responsiveness, improve the user's experience and enable new business models.
</summary>
    <author>
      <name>Gabriele D'Angelo</name>
    </author>
    <author>
      <name>Stefano Ferretti</name>
    </author>
    <author>
      <name>Moreno Marzolla</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-08234-9_39-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-08234-9_39-1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Encyclopedia of Computer Graphics and Games. Newton Lee (Editor).
  Springer International Publishing, 2015, ISBN 978-3-319-08234-9</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.02435v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.02435v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.4; I.6.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07990v1</id>
    <updated>2016-03-25T19:35:59Z</updated>
    <published>2016-03-25T19:35:59Z</published>
    <title>Modeling and Resource Allocation for HD Videos over WiMAX Broadband
  Wireless Networks</title>
    <summary>  Mobile video is considered a major upcoming application and revenue generator
for broadband wireless networks like WiMAX and LTE. Therefore, it is important
to design a proper resource allocation scheme for mobile video, since video
traffic is both throughput consuming and delay sensitive.
</summary>
    <author>
      <name>Abdel-Karim Al-Tamimi</name>
    </author>
    <author>
      <name>Raj Jain</name>
    </author>
    <author>
      <name>Chakchai So-In</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Communication Society Multimedia Communications Technical
  Committee, E-letter Vol. 5, No. 3, May 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.07990v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07990v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.01058v1</id>
    <updated>2016-12-04T03:36:51Z</updated>
    <published>2016-12-04T03:36:51Z</published>
    <title>Algorithmic Songwriting with ALYSIA</title>
    <summary>  This paper introduces ALYSIA: Automated LYrical SongwrIting Application.
ALYSIA is based on a machine learning model using Random Forests, and we
discuss its success at pitch and rhythm prediction. Next, we show how ALYSIA
was used to create original pop songs that were subsequently recorded and
produced. Finally, we discuss our vision for the future of Automated
Songwriting for both co-creative and autonomous systems.
</summary>
    <author>
      <name>Margareta Ackerman</name>
    </author>
    <author>
      <name>David Loker</name>
    </author>
    <link href="http://arxiv.org/abs/1612.01058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.01058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.04574v1</id>
    <updated>2017-03-14T14:07:19Z</updated>
    <published>2017-03-14T14:07:19Z</published>
    <title>Causes of discomfort in stereoscopic content: a review</title>
    <summary>  This paper reviews the causes of discomfort in viewing stereoscopic content.
These include objective factors, such as misaligned images, as well as
subjective factors, such as excessive disparity. Different approaches to the
measurement of visual discomfort are also reviewed, in relation to the
underlying physiological and psychophysical processes. The importance of
understanding these issues, in the context of new display technologies, is
emphasized.
</summary>
    <author>
      <name>Kasim Terzic</name>
    </author>
    <author>
      <name>Miles Hansard</name>
    </author>
    <link href="http://arxiv.org/abs/1703.04574v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.04574v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08675v1</id>
    <updated>2017-06-27T05:28:06Z</updated>
    <published>2017-06-27T05:28:06Z</published>
    <title>Proceedings of the First International Workshop on Deep Learning and
  Music</title>
    <summary>  Proceedings of the First International Workshop on Deep Learning and Music,
joint with IJCNN, Anchorage, US, May 17-18, 2017
</summary>
    <author>
      <name>Dorien Herremans</name>
    </author>
    <author>
      <name>Ching-Hua Chuan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.22227.99364/1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.22227.99364/1" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Workshop on Deep Learning
  and Music, joint with IJCNN, Anchorage, US, May 17-18, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.08675v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08675v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.05785v2</id>
    <updated>2018-01-18T20:24:40Z</updated>
    <published>2017-12-15T18:41:53Z</published>
    <title>Sentiment Predictability for Stocks</title>
    <summary>  In this work, we present our findings and experiments for stock-market
prediction using various textual sentiment analysis tools, such as mood
analysis and event extraction, as well as prediction models, such as LSTMs and
specific convolutional architectures.
</summary>
    <author>
      <name>Jordan Prosky</name>
    </author>
    <author>
      <name>Xingyou Song</name>
    </author>
    <author>
      <name>Andrew Tan</name>
    </author>
    <author>
      <name>Michael Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.05785v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.05785v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.11037v1</id>
    <updated>2021-03-21T13:02:29Z</updated>
    <published>2021-03-21T13:02:29Z</published>
    <title>10 Years of the PCG workshop: Past and Future Trends</title>
    <summary>  As of 2020, the international workshop on Procedural Content Generation
enters its second decade. The annual workshop, hosted by the international
conference on the Foundations of Digital Games, has collected a corpus of 95
papers published in its first 10 years. This paper provides an overview of the
workshop's activities and surveys the prevalent research topics emerging over
the years.
</summary>
    <author>
      <name>Antonios Liapis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3402942.3409598</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3402942.3409598" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the FDG Workshop on Procedural Content Generation,
  2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.11037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.11037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.m; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01328v1</id>
    <updated>2020-10-03T11:22:48Z</updated>
    <published>2020-10-03T11:22:48Z</published>
    <title>HCI Models for Digital Musical Instruments: Methodologies for Rigorous
  Testing of Digital Musical Instruments</title>
    <summary>  Here we present an analysis of literature relating to the evaluation
methodologies of Digital Musical Instruments (DMIs) derived from the field of
Human-Computer Interaction (HCI). We then apply choice aspects from these
existing evaluation models and apply them to an optimized evaluation for
assessing new DMIs.
</summary>
    <author>
      <name>Gareth W. Young</name>
    </author>
    <author>
      <name>Dave Murphy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">CMMR 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.01328v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01328v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.01727v1</id>
    <updated>2022-01-01T13:26:04Z</updated>
    <published>2022-01-01T13:26:04Z</published>
    <title>X3: Lossless Data Compressor</title>
    <summary>  X3 is a lossless optimizing dictionary-based data compressor. The algorithm
uses a combination of a dictionary, context modeling, and arithmetic coding.
Optimization adds the ability to find the most appropriate parameters for each
file. Even without optimization, x3 can compress data with a compression ratio
comparable to the best dictionary compression methods like LZMA, zstd, or
Brotli.
</summary>
    <author>
      <name>David Barina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted as a poster on the DCC 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.01727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.01727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.02099v1</id>
    <updated>2022-01-06T15:30:33Z</updated>
    <published>2022-01-06T15:30:33Z</published>
    <title>Implementing simple spectral denoising for environmental audio
  recordings</title>
    <summary>  This technical report details changes applied to a noise filter to facilitate
its application and improve its results. The filter is applied to denoise
natural sounds recorded in the wild and to generate an acoustic index used in
soundscape analysis.
</summary>
    <author>
      <name>F√°bio Felix Dias</name>
    </author>
    <author>
      <name>Moacir Antonelli Ponti</name>
    </author>
    <author>
      <name>Rosane Minghim</name>
    </author>
    <link href="http://arxiv.org/abs/2201.02099v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.02099v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.03732v1</id>
    <updated>2023-04-07T16:55:52Z</updated>
    <published>2023-04-07T16:55:52Z</published>
    <title>Enabling immersive experiences in challenging network conditions</title>
    <summary>  Immersive experiences, such as remote collaboration and augmented and virtual
reality, require delivery of large volumes of data with consistent ultra-low
latency across wireless networks in fluctuating network conditions. We describe
the high-level design behind a data delivery solution that meets these
requirements and provide synthetic simulations and test results running in
network conditions based on real-world measurements demonstrating the efficacy
of the solution.
</summary>
    <author>
      <name>Pooja Aggarwal</name>
    </author>
    <author>
      <name>Michael Luby</name>
    </author>
    <author>
      <name>Lorenz Minder</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.03732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.03732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0002004v1</id>
    <updated>2000-02-04T18:42:13Z</updated>
    <published>2000-02-04T18:42:13Z</published>
    <title>Stochastic Model Checking for Multimedia</title>
    <summary>  Modern distributed systems include a class of applications in which
non-functional requirements are important. In particular, these applications
include multimedia facilities where real time constraints are crucial to their
correct functioning. In order to specify such systems it is necessary to
describe that events occur at times given by probability distributions and
stochastic automata have emerged as a useful technique by which such systems
can be specified and verified.
  However, stochastic descriptions are very general, in particular they allow
the use of general probability distribution functions, and therefore their
verification can be complex. In the last few years, model checking has emerged
as a useful verification tool for large systems.
  In this paper we describe two model checking algorithms for stochastic
automata. These algorithms consider how properties written in a simple
probabilistic real-time logic can be checked against a given stochastic
automaton.
</summary>
    <author>
      <name>Jeremy Bryans</name>
    </author>
    <author>
      <name>Howard Bowman</name>
    </author>
    <author>
      <name>John Derrick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages; 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0002004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0002004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.3.1; F.4.1; G.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0109041v2</id>
    <updated>2001-09-27T23:06:04Z</updated>
    <published>2001-09-20T23:48:11Z</published>
    <title>Open Access beyond cable: The case of Interactive TV</title>
    <summary>  In this paper we analyze the development of interactive TV in the U.S. and
Western Europe. We argue that despite the nascent character of the market there
are important regulatory issues at stake, as exemplified by the AOL/TW merger
and the British Interactive Broadcasting case. Absent rules that provide for
non-discriminatory access to network components (including terminal equipment
specifications), dominant platform operators are likely to leverage ownership
of delivery infrastructure into market power over interactive TV services.
While integration between platform operator, service provider and terminal
vendor may facilitate the introduction of services in the short-term, the
lasting result will be a collection of fragmented "walled gardens" offering
limited content and applications. Would interactive TV develop under such
model, the exciting opportunities for broad-based innovation and extended
access to multiple information, entertainment and educational services opened
by the new generation of broadcasting technologies will be foregone
</summary>
    <author>
      <name>Hernan Galperin</name>
    </author>
    <author>
      <name>Francois Bar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">typos corrected, content changed section 3(a), new abstract</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0109041v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0109041v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0501014v3</id>
    <updated>2007-03-27T17:12:49Z</updated>
    <published>2005-01-08T15:55:30Z</published>
    <title>On the Design of Perceptual MPEG-Video Encryption Algorithms</title>
    <summary>  In this paper, some existing perceptual encryption algorithms of MPEG videos
are reviewed and some problems, especially security defects of two recently
proposed MPEG-video perceptual encryption schemes, are pointed out. Then, a
simpler and more effective design is suggested, which selectively encrypts
fixed-length codewords (FLC) in MPEG-video bitstreams under the control of
three perceptibility factors. The proposed design is actually an encryption
configuration that can work with any stream cipher or block cipher. Compared
with the previously-proposed schemes, the new design provides more useful
features, such as strict size-preservation, on-the-fly encryption and multiple
perceptibility, which make it possible to support more applications with
different requirements. In addition, four different measures are suggested to
provide better security against known/chosen-plaintext attacks.
</summary>
    <author>
      <name>Shujun Li</name>
    </author>
    <author>
      <name>Guanrong Chen</name>
    </author>
    <author>
      <name>Albert Cheung</name>
    </author>
    <author>
      <name>Bharat Bhargava</name>
    </author>
    <author>
      <name>Kwok-Tung Lo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCSVT.2006.888840</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCSVT.2006.888840" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures, IEEEtran.cls</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Circuits and Systems for Video Technology,
  vol. 17, no. 2, pp. 214-223, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0501014v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0501014v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0508066v1</id>
    <updated>2005-08-13T14:46:16Z</updated>
    <published>2005-08-13T14:46:16Z</published>
    <title>Can Small Museums Develop Compelling, Educational and Accessible Web
  Resources? The Case of Accademia Carrara</title>
    <summary>  Due to the lack of budget, competence, personnel and time, small museums are
often unable to develop compelling, educational and accessible web resources
for their permanent collections or temporary exhibitions. In an attempt to
prove that investing in these types of resources can be very fruitful even for
small institutions, we will illustrate the case of Accademia Carrara, a museum
in Bergamo, northern Italy, which, for a current temporary exhibition on
Cezanne and Renoir's masterpieces from the Paul Guillaume collection, developed
a series of multimedia applications, including an accessible website, rich in
content and educational material [www.cezannerenoir.it].
</summary>
    <author>
      <name>Silvia Filippini-Fantoni</name>
    </author>
    <author>
      <name>Jonathan P. Bowen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In James Hemsley, Vito Cappellini and Gerd Stanke (eds.), EVA 2005
  London Conference Proceedings, University College London, UK, 25-29 July
  2005, pages 18.1-18.14. ISBN: 0-9543146-6-2</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0508066v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0508066v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.5; H.3.7; H.4.3; H.5.1; H.5.2; H.5.3; H.5.4; K.3.1; K.4.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0509035v2</id>
    <updated>2007-10-24T15:45:10Z</updated>
    <published>2005-09-13T10:44:31Z</published>
    <title>Cryptanalysis of an MPEG-Video Encryption Scheme Based on Secret Huffman
  Tables</title>
    <summary>  This paper studies the security of a recently-proposed MPEG-video encryption
scheme based on secret Huffman tables. Our cryptanalysis shows that: 1) the key
space of the encryption scheme is not sufficiently large against
divide-and-conquer (DAC) attack and known-plaintext attack; 2) it is possible
to decrypt a cipher-video with a partially-known key, thus dramatically
reducing the complexity of the DAC brute-force attack in some cases; 3) its
security against the chosen-plaintext attack is very weak. Some experimental
results are included to support the cryptanalytic results with a brief discuss
on how to improve this MPEG-video encryption scheme.
</summary>
    <author>
      <name>Shujun Li</name>
    </author>
    <author>
      <name>Guanrong Chen</name>
    </author>
    <author>
      <name>Albert Cheung</name>
    </author>
    <author>
      <name>Kwok-Tung Lo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-540-92957-4_78</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-540-92957-4_78" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Image and Video Technology - Third Pacific Rim
  Symposium, PSIVT 2009, Tokyo, Japan, January 13-16, 2009. Proceedings,
  Lecture Notes in Computer Science, vol. 5414, pp. 898-909, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0509035v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0509035v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605002v3</id>
    <updated>2006-06-21T03:22:38Z</updated>
    <published>2006-04-30T13:35:54Z</published>
    <title>A Hybrid Quantum Encoding Algorithm of Vector Quantization for Image
  Compression</title>
    <summary>  Many classical encoding algorithms of Vector Quantization (VQ) of image
compression that can obtain global optimal solution have computational
complexity O(N). A pure quantum VQ encoding algorithm with probability of
success near 100% has been proposed, that performs operations 45sqrt(N) times
approximately. In this paper, a hybrid quantum VQ encoding algorithm between
classical method and quantum algorithm is presented. The number of its
operations is less than sqrt(N) for most images, and it is more efficient than
the pure quantum algorithm.
  Key Words: Vector Quantization, Grover's Algorithm, Image Compression,
Quantum Algorithm
</summary>
    <author>
      <name>Chao-Yang Pang</name>
    </author>
    <author>
      <name>Zheng-Wei Zhou</name>
    </author>
    <author>
      <name>Guang-Can Guo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1009-1963/15/12/044</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1009-1963/15/12/044" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Modify on June 21. 10pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0605002v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605002v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1; F.2.1; F.2.2; F.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0606034v2</id>
    <updated>2007-01-12T13:11:09Z</updated>
    <published>2006-06-08T13:28:07Z</published>
    <title>A constructive and unifying framework for zero-bit watermarking</title>
    <summary>  In the watermark detection scenario, also known as zero-bit watermarking, a
watermark, carrying no hidden message, is inserted in content. The watermark
detector checks for the presence of this particular weak signal in content. The
article looks at this problem from a classical detection theory point of view,
but with side information enabled at the embedding side. This means that the
watermark signal is a function of the host content. Our study is twofold. The
first step is to design the best embedding function for a given detection
function, and the best detection function for a given embedding function. This
yields two conditions, which are mixed into one `fundamental' partial
differential equation. It appears that many famous watermarking schemes are
indeed solution to this `fundamental' equation. This study thus gives birth to
a constructive framework unifying solutions, so far perceived as very
different.
</summary>
    <author>
      <name>Teddy Furon</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRISA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to IEEE Trans. on Information Forensics and Security</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0606034v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0606034v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0609131v1</id>
    <updated>2006-09-24T09:55:57Z</updated>
    <published>2006-09-24T09:55:57Z</published>
    <title>A Fast Block Matching Algorithm for Video Motion Estimation Based on
  Particle Swarm Optimization and Motion Prejudgment</title>
    <summary>  In this paper, we propose a fast 2-D block-based motion estimation algorithm
called Particle Swarm Optimization - Zero-motion Prejudgment(PSO-ZMP) which
consists of three sequential routines: 1)Zero-motion prejudgment. The routine
aims at finding static macroblocks(MB) which do not need to perform remaining
search thus reduces the computational cost; 2)Predictive image coding and 3)PSO
matching routine. Simulation results obtained show that the proposed PSO-ZMP
algorithm achieves over 10 times of computation less than Diamond Search(DS)
and 5 times less than the recent proposed Adaptive Rood Pattern
Searching(ARPS). Meanwhile the PSNR performances using PSO-ZMP are very close
to that using DS and ARPS in some less-motioned sequences. While in some
sequences containing dense and complex motion contents, the PSNR performances
of PSO-ZMP are several dB lower than that using DS and ARPS but in an
acceptable degree.
</summary>
    <author>
      <name>Ran Ren</name>
    </author>
    <author>
      <name>Madan mohan Manokar</name>
    </author>
    <author>
      <name>Yaogang Shi</name>
    </author>
    <author>
      <name>Baoyu Zheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 12 figures, submitted to ACM Symposium of Applied
  Computing(SAC)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0609131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0609131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.1141v1</id>
    <updated>2007-06-08T09:31:46Z</updated>
    <published>2007-06-08T09:31:46Z</published>
    <title>Multimedia Content Distribution in Hybrid Wireless Networks using
  Weighted Clustering</title>
    <summary>  Fixed infrastructured networks naturally support centralized approaches for
group management and information provisioning. Contrary to infrastructured
networks, in multi-hop ad-hoc networks each node acts as a router as well as
sender and receiver. Some applications, however, requires hierarchical
arrangements that-for practical reasons-has to be done locally and
self-organized. An additional challenge is to deal with mobility that causes
permanent network partitioning and re-organizations. Technically, these
problems can be tackled by providing additional uplinks to a backbone network,
which can be used to access resources in the Internet as well as to inter-link
multiple ad-hoc network partitions, creating a hybrid wireless network. In this
paper, we present a prototypically implemented hybrid wireless network system
optimized for multimedia content distribution. To efficiently manage the ad-hoc
communicating devices a weighted clustering algorithm is introduced. The
proposed localized algorithm deals with mobility, but does not require
geographical information or distances.
</summary>
    <author>
      <name>Adrian Andronache</name>
    </author>
    <author>
      <name>Matthias R. Brust</name>
    </author>
    <author>
      <name>Steffen Rothkugel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2nd ACM Workshop on Wireless Multimedia Networking and Performance
  Modeling 2006 (ISBN 1-59593-485)</arxiv:comment>
    <link href="http://arxiv.org/abs/0706.1141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.1141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.3076v1</id>
    <updated>2007-06-21T02:44:44Z</updated>
    <published>2007-06-21T02:44:44Z</published>
    <title>On the Performance of Joint Fingerprint Embedding and Decryption Scheme</title>
    <summary>  Till now, few work has been done to analyze the performances of joint
fingerprint embedding and decryption schemes. In this paper, the security of
the joint fingerprint embedding and decryption scheme proposed by Kundur et al.
is analyzed and improved. The analyses include the security against
unauthorized customer, the security against authorized customer, the
relationship between security and robustness, the relationship between
secu-rity and imperceptibility and the perceptual security. Based these
analyses, some means are proposed to strengthen the system, such as multi-key
encryp-tion and DC coefficient encryption. The method can be used to analyze
other JFD schemes. It is expected to provide valuable information to design JFD
schemes.
</summary>
    <author>
      <name>Shiguo Lian</name>
    </author>
    <author>
      <name>Zhongxuan Liu</name>
    </author>
    <author>
      <name>Zhen Ren</name>
    </author>
    <author>
      <name>Haila Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,9 figures. To be submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/0706.3076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.3076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.0; D.2.11; D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.0802v1</id>
    <updated>2007-07-05T15:11:24Z</updated>
    <published>2007-07-05T15:11:24Z</published>
    <title>Very fast watermarking by reversible contrast mapping</title>
    <summary>  Reversible contrast mapping (RCM) is a simple integer transform that applies
to pairs of pixels. For some pairs of pixels, RCM is invertible, even if the
least significant bits (LSBs) of the transformed pixels are lost. The data
space occupied by the LSBs is suitable for data hiding. The embedded
information bit-rates of the proposed spatial domain reversible watermarking
scheme are close to the highest bit-rates reported so far. The scheme does not
need additional data compression, and, in terms of mathematical complexity, it
appears to be the lowest complexity one proposed up to now. A very fast lookup
table implementation is proposed. Robustness against cropping can be ensured as
well.
</summary>
    <author>
      <name>Dinu Coltuc</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GIPSA-lab</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-Marc Chassery</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GIPSA-lab</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2006.884895</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2006.884895" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Signal Processing Letters 14, 4 (04/2007) pp 255-258</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0707.0802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.0802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.4524v1</id>
    <updated>2007-07-31T02:27:10Z</updated>
    <published>2007-07-31T02:27:10Z</published>
    <title>Image Authentication Based on Neural Networks</title>
    <summary>  Neural network has been attracting more and more researchers since the past
decades. The properties, such as parameter sensitivity, random similarity,
learning ability, etc., make it suitable for information protection, such as
data encryption, data authentication, intrusion detection, etc. In this paper,
by investigating neural networks' properties, the low-cost authentication
method based on neural networks is proposed and used to authenticate images or
videos. The authentication method can detect whether the images or videos are
modified maliciously. Firstly, this chapter introduces neural networks'
properties, such as parameter sensitivity, random similarity, diffusion
property, confusion property, one-way property, etc. Secondly, the chapter
gives an introduction to neural network based protection methods. Thirdly, an
image or video authentication scheme based on neural networks is presented, and
its performances, including security, robustness and efficiency, are analyzed.
Finally, conclusions are drawn, and some open issues in this field are
presented.
</summary>
    <author>
      <name>Shiguo Lian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages,10 figures, submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/0707.4524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.4524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; E.3.x; F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4180v1</id>
    <updated>2007-10-23T03:06:53Z</updated>
    <published>2007-10-23T03:06:53Z</published>
    <title>A quick search method for audio signals based on a piecewise linear
  representation of feature trajectories</title>
    <summary>  This paper presents a new method for a quick similarity-based search through
long unlabeled audio streams to detect and locate audio clips provided by
users. The method involves feature-dimension reduction based on a piecewise
linear representation of a sequential feature trajectory extracted from a long
audio stream. Two techniques enable us to obtain a piecewise linear
representation: the dynamic segmentation of feature trajectories and the
segment-based Karhunen-L\'{o}eve (KL) transform. The proposed search method
guarantees the same search results as the search method without the proposed
feature-dimension reduction method in principle. Experiment results indicate
significant improvements in search speed. For example the proposed method
reduced the total search time to approximately 1/12 that of previous methods
and detected queries in approximately 0.3 seconds from a 200-hour audio
database.
</summary>
    <author>
      <name>Akisato Kimura</name>
    </author>
    <author>
      <name>Kunio Kashino</name>
    </author>
    <author>
      <name>Takayuki Kurozumi</name>
    </author>
    <author>
      <name>Hiroshi Murase</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TASL.2007.912362</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TASL.2007.912362" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, to appear in IEEE Transactions on Audio, Speech and
  Language Processing</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Audio, Speech and Language Processing,
  Vol.16, No.2, pp.396-407, February 2008.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4819v1</id>
    <updated>2007-10-25T12:03:15Z</updated>
    <published>2007-10-25T12:03:15Z</published>
    <title>A High Quality/Low Computational Cost Technique for Block Matching
  Motion Estimation</title>
    <summary>  Motion estimation is the most critical process in video coding systems. First
of all, it has a definitive impact on the rate-distortion performance given by
the video encoder. Secondly, it is the most computationally intensive process
within the encoding loop. For these reasons, the design of high-performance
low-cost motion estimators is a crucial task in the video compression field. An
adaptive cost block matching (ACBM) motion estimation technique is presented in
this paper, featuring an excellent tradeoff between the quality of the
reconstructed video sequences and the computational effort. Simulation results
demonstrate that the ACBM algorithm achieves a slight better rate-distortion
performance than the one given by the well-known full search algorithm block
matching algorithm with reductions of up to 95% in the computational load.
</summary>
    <author>
      <name>S. Lopez</name>
    </author>
    <author>
      <name>G. M. Callico</name>
    </author>
    <author>
      <name>J. F. Lopez</name>
    </author>
    <author>
      <name>R. Sarmiento</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4823v1</id>
    <updated>2007-10-25T12:04:42Z</updated>
    <published>2007-10-25T12:04:42Z</published>
    <title>A Coprocessor for Accelerating Visual Information Processing</title>
    <summary>  Visual information processing will play an increasingly important role in
future electronics systems. In many applications, e.g. video surveillance
cameras, data throughput of microprocessors is not sufficient and power
consumption is too high. Instruction profiling on a typical test algorithm has
shown that pixel address calculations are the dominant operations to be
optimized. Therefore AddressLib, a structured scheme for pixel addressing was
developed, that can be accelerated by AddressEngine, a coprocessor for visual
information processing. In this paper, the architectural design of
AddressEngine is described, which in the first step supports a subset of the
AddressLib. Dataflow and memory organization are optimized during architectural
design. AddressEngine was implemented in a FPGA and was tested with MPEG-7
Global Motion Estimation algorithm. Results on processing speed and circuit
complexity are given and compared to a pure software implementation. The next
step will be the support for the full AddressLib, including segment addressing.
An outlook on further investigations on dynamic reconfiguration capabilities is
given.
</summary>
    <author>
      <name>W. Stechele</name>
    </author>
    <author>
      <name>L. Alvado Carcel</name>
    </author>
    <author>
      <name>S. Herrmann</name>
    </author>
    <author>
      <name>J. Lidon Simon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4823v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4823v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4846v1</id>
    <updated>2007-10-25T12:24:16Z</updated>
    <published>2007-10-25T12:24:16Z</published>
    <title>An Integrated Design and Verification Methodology for Reconfigurable
  Multimedia Systems</title>
    <summary>  Recently a lot of multimedia applications are emerging on portable
appliances. They require both the flexibility of upgradeable devices
(traditionally software based) and a powerful computing engine (typically
hardware). In this context, programmable HW and dynamic reconfiguration allow
novel approaches to the migration of algorithms from SW to HW. Thus, in the
frame of the Symbad project, we propose an industrial design flow for
reconfigurable SoC's. The goal of Symbad consists of developing a system level
design platform for hardware and software SoC systems including formal and
semi-formal verification techniques.
</summary>
    <author>
      <name>M. Borgatti</name>
    </author>
    <author>
      <name>A. Capello</name>
    </author>
    <author>
      <name>U. Rossi</name>
    </author>
    <author>
      <name>J. -L. Lambert</name>
    </author>
    <author>
      <name>I. Moussa</name>
    </author>
    <author>
      <name>F. Fummi</name>
    </author>
    <author>
      <name>G. Pravadelli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe | Designers'Forum -
  DATE'05, Munich : Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.3500v1</id>
    <updated>2007-11-22T03:53:29Z</updated>
    <published>2007-11-22T03:53:29Z</published>
    <title>Secure Fractal Image Coding</title>
    <summary>  In recent work, various fractal image coding methods are reported, which
adopt the self-similarity of images to compress the size of images. However,
till now, no solutions for the security of fractal encoded images have been
provided. In this paper, a secure fractal image coding scheme is proposed and
evaluated, which encrypts some of the fractal parameters during fractal
encoding, and thus, produces the encrypted and encoded image. The encrypted
image can only be recovered by the correct key. To keep secure and efficient,
only the suitable parameters are selected and encrypted through in-vestigating
the properties of various fractal parameters, including parameter space,
parameter distribu-tion and parameter sensitivity. The encryption process does
not change the file format, keeps secure in perception, and costs little time
or computational resources. These properties make it suitable for secure image
encoding or transmission.
</summary>
    <author>
      <name>Shiguo Lian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 8 figures. To be submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/0711.3500v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.3500v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.0; D.2.11; D.4.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.2938v2</id>
    <updated>2008-08-25T10:40:57Z</updated>
    <published>2008-05-19T20:46:38Z</published>
    <title>Steganography of VoIP Streams</title>
    <summary>  The paper concerns available steganographic techniques that can be used for
creating covert channels for VoIP (Voice over Internet Protocol) streams. Apart
from characterizing existing steganographic methods we provide new insights by
presenting two new techniques. The first one is network steganography solution
which exploits free/unused protocols' fields and is known for IP, UDP or TCP
protocols but has never been applied to RTP (Real-Time Transport Protocol) and
RTCP (Real-Time Control Protocol) which are characteristic for VoIP. The second
method, called LACK (Lost Audio Packets Steganography), provides hybrid
storage-timing covert channel by utilizing delayed audio packets. The results
of the experiment, that was performed to estimate a total amount of data that
can be covertly transferred during typical VoIP conversation phase, regardless
of steganalysis, are also included in this paper.
</summary>
    <author>
      <name>Wojciech Mazurczyk</name>
    </author>
    <author>
      <name>Krzysztof Szczypiorski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 9 figures, content changed, accepted to The 3rd
  International Symposium on Information Security (IS'08), Monterrey, Mexico,
  November 10-11, 2008 (Proceedings will be published by Springer LNCS)</arxiv:comment>
    <link href="http://arxiv.org/abs/0805.2938v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.2938v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.4293v1</id>
    <updated>2008-06-26T12:19:27Z</updated>
    <published>2008-06-26T12:19:27Z</published>
    <title>Scalar Quantization for Audio Data Coding</title>
    <summary>  This paper is concerned with scalar quantization of transform coefficients in
an audio codec. The generalized Gaussian distribution (GGD) is used as an
approximation of one-dimensional probability density function for transform
coefficients obtained by modulated lapped transform (MLT) or modified cosine
transform (MDCT) filterbank. The rationale of the model is provided in
comparison with theoretically achievable rate-distortion function. The
rate-distortion function computed for the random sequence obtained from a real
sequence of samples from a large database is compared with that computed for
random sequence obtained by a GGD random generator. A simple algorithm of
constructing the Extended Zero Zone (EZZ) quantizer is proposed. Simulation
results show that the EZZ quantizer yields a negligible loss in terms of coding
efficiency compared to optimal scalar quantizers. Furthermore, we describe an
adaptive version of the EZZ quantizer which works efficiently with low bitrate
requirements for transmitting side information
</summary>
    <author>
      <name>Boris D. Kudryashov</name>
    </author>
    <author>
      <name>Anton V. Porov</name>
    </author>
    <author>
      <name>Eunmi L. Oh</name>
    </author>
    <link href="http://arxiv.org/abs/0806.4293v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.4293v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.2063v1</id>
    <updated>2008-10-12T00:15:27Z</updated>
    <published>2008-10-12T00:15:27Z</published>
    <title>Initial Offset Placement in p2p Live Streaming Systems</title>
    <summary>  Initial offset placement in p2p streaming systems is studied in this paper.
Proportional placement (PP) scheme is proposed. In this scheme, peer places the
initial offset as the offset reported by other reference peer with a shift
proportional to the buffer width or offset lag of this reference peer. This
will introduce a stable placement that supports larger buffer width for peers
and small buffer width for tracker. Real deployed placement method in PPLive is
studied through measurement. It shows that, instead of based on offset lag, the
placement is based on buffer width of the reference peer to facilitate the
initial chunk fetching. We will prove that, such a PP scheme may not be stable
under arbitrary buffer occupation in the reference peer. The required average
buffer width then is derived. A simple good peer selection mechanism to check
the buffer occupation of reference peer is proposed for a stable PP scheme
based on buffer width
</summary>
    <author>
      <name>Chunxi Li</name>
    </author>
    <author>
      <name>Changjia Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.2063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.2063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.0582v1</id>
    <updated>2008-11-04T19:36:16Z</updated>
    <published>2008-11-04T19:36:16Z</published>
    <title>Optimization of automatically generated multi-core code for the LTE
  RACH-PD algorithm</title>
    <summary>  Embedded real-time applications in communication systems require high
processing power. Manual scheduling devel-oped for single-processor
applications is not suited to multi-core architectures. The Algorithm
Architecture Matching (AAM) methodology optimizes static application
implementation on multi-core architectures. The Random Access Channel Preamble
Detection (RACH-PD) is an algorithm for non-synchronized access of Long Term
Evolu-tion (LTE) wireless networks. LTE aims to improve the spectral efficiency
of the next generation cellular system. This paper de-scribes a complete
methodology for implementing the RACH-PD. AAM prototyping is applied to the
RACH-PD which is modelled as a Synchronous DataFlow graph (SDF). An efficient
implemen-tation of the algorithm onto a multi-core DSP, the TI C6487, is then
explained. Benchmarks for the solution are given.
</summary>
    <author>
      <name>Maxime Pelcat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IETR</arxiv:affiliation>
    </author>
    <author>
      <name>Slaheddine Aridhi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IETR</arxiv:affiliation>
    </author>
    <author>
      <name>Jean Fran√ßois Nezan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IETR</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">DASIP 2008, Bruxelles : Belgique (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0811.0582v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.0582v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.2868v1</id>
    <updated>2008-11-18T09:14:18Z</updated>
    <published>2008-11-18T09:14:18Z</published>
    <title>Approximate Sparse Decomposition Based on Smoothed L0-Norm</title>
    <summary>  In this paper, we propose a method to address the problem of source
estimation for Sparse Component Analysis (SCA) in the presence of additive
noise. Our method is a generalization of a recently proposed method (SL0),
which has the advantage of directly minimizing the L0-norm instead of L1-norm,
while being very fast. SL0 is based on minimization of the smoothed L0-norm
subject to As=x. In order to better estimate the source vector for noisy
mixtures, we suggest then to remove the constraint As=x, by relaxing exact
equality to an approximation (we call our method Smoothed L0-norm Denoising or
SL0DN). The final result can then be obtained by minimization of a proper
linear combination of the smoothed L0-norm and a cost function for the
approximation. Experimental results emphasize on the significant enhancement of
the modified method in noisy cases.
</summary>
    <author>
      <name>Hamed Firouzi</name>
    </author>
    <author>
      <name>Masoud Farivar</name>
    </author>
    <author>
      <name>Massoud Babaie-Zadeh</name>
    </author>
    <author>
      <name>Christian Jutten</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 Pages, Submitted to ICASSP 2009</arxiv:comment>
    <link href="http://arxiv.org/abs/0811.2868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.2868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0811.4483v1</id>
    <updated>2008-11-28T16:28:59Z</updated>
    <published>2008-11-28T16:28:59Z</published>
    <title>Wide spread spectrum watermarking with side information and interference
  cancellation</title>
    <summary>  Nowadays, a popular method used for additive watermarking is wide spread
spectrum. It consists in adding a spread signal into the host document. This
signal is obtained by the sum of a set of carrier vectors, which are modulated
by the bits to be embedded. To extract these embedded bits, weighted
correlations between the watermarked document and the carriers are computed.
Unfortunately, even without any attack, the obtained set of bits can be
corrupted due to the interference with the host signal (host interference) and
also due to the interference with the others carriers (inter-symbols
interference (ISI) due to the non-orthogonality of the carriers). Some recent
watermarking algorithms deal with host interference using side informed
methods, but inter-symbols interference problem is still open. In this paper,
we deal with interference cancellation methods, and we propose to consider ISI
as side information and to integrate it into the host signal. This leads to a
great improvement of extraction performance in term of signal-to-noise ratio
and/or watermark robustness.
</summary>
    <author>
      <name>Ga√´tan Le Guelvouit</name>
    </author>
    <author>
      <name>St√©phane Pateux</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.476839</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.476839" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. IS&amp;T/SPIE Electronic Imaging, vol. 5020, Santa Clara, CA,
  Jan. 2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0811.4483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0811.4483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.2411v1</id>
    <updated>2008-12-12T16:08:04Z</updated>
    <published>2008-12-12T16:08:04Z</published>
    <title>Probabilistic SVM/GMM Classifier for Speaker-Independent Vowel
  Recognition in Continues Speech</title>
    <summary>  In this paper, we discuss the issues in automatic recognition of vowels in
Persian language. The present work focuses on new statistical method of
recognition of vowels as a basic unit of syllables. First we describe a vowel
detection system then briefly discuss how the detected vowels can feed to
recognition unit. According to pattern recognition, Support Vector Machines
(SVM) as a discriminative classifier and Gaussian mixture model (GMM) as a
generative model classifier are two most popular techniques. Current
state-ofthe- art systems try to combine them together for achieving more power
of classification and improving the performance of the recognition systems. The
main idea of the study is to combine probabilistic SVM and traditional GMM
pattern classification with some characteristic of speech like band-pass energy
to achieve better classification rate. This idea has been analytically
formulated and tested on a FarsDat based vowel recognition system. The results
show inconceivable increases in recognition accuracy. The tests have been
carried out by various proposed vowel recognition algorithms and the results
have been compared.
</summary>
    <author>
      <name>Mohammad Nazari</name>
    </author>
    <author>
      <name>Abolghasem Sayadiyan</name>
    </author>
    <author>
      <name>SeyedMajid Valiollahzadeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0812.2411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.2411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.2529v1</id>
    <updated>2008-12-13T07:46:52Z</updated>
    <published>2008-12-13T07:46:52Z</published>
    <title>Kalinahia: Considering Quality of Service to Design and Execute
  Distributed Multimedia Applications</title>
    <summary>  One of the current challenges of Information Systems is to ensure
semi-structured data transmission, such as multimedia data, in a distributed
and pervasive environment. Information Sytems must then guarantee users a
quality of service ensuring data accessibility whatever the hardware and
network conditions may be. They must also guarantee information coherence and
particularly intelligibility that imposes a personalization of the service.
Within this framework, we propose a design method based on original models of
multimedia applications and quality of service. We also define a supervision
platform Kalinahia using a user centered heuristic allowing us to define at any
moment which configuration of software components constitutes the best answers
to users' wishes in terms of service.
</summary>
    <author>
      <name>Sophie Laplace</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <author>
      <name>Marc Dalmau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Roose</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE/IFIP Int'l Conference on Network Management and Management
  Symposium, Salvador de Bahia : Br\'esil (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0812.2529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.2529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.2988v1</id>
    <updated>2008-12-16T07:46:49Z</updated>
    <published>2008-12-16T07:46:49Z</published>
    <title>The Korrontea Data Modeling</title>
    <summary>  Needs of multimedia systems evolved due to the evolution of their
architecture which is now distributed into heterogeneous contexts. A critical
issue lies in the fact that they handle, process, and transmit multimedia data.
This data integrates several properties which should be considered since it
holds a considerable part of its semantics, for instance the lips
synchronization in a video. In this paper, we focus on the definition of a
model as a basic abstraction for describing and modeling media in multimedia
systems by taking into account their properties. This model will be used in
software architecture in order to handle data in efficient way. The provided
model is an interesting solution for the integration of media into
applications; we propose to consider and to handle them in a uniform way. This
model is proposed with synchronization policies to ensure synchronous transport
of media. Therefore, we use it in a component model that we develop for the
design and deployment of distributed multimedia systems.
</summary>
    <author>
      <name>Emmanuel Bouix</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Roose</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <author>
      <name>Marc Dalmau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ambisys, Quebec City : Canada (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0812.2988v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.2988v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.2989v1</id>
    <updated>2008-12-16T07:47:22Z</updated>
    <published>2008-12-16T07:47:22Z</published>
    <title>Heterogeneous component interactions: Sensors integration into
  multimedia applications</title>
    <summary>  Resource-constrained embedded and mobile devices are becoming increasingly
common. Since few years, some mobile and ubiquitous devices such as wireless
sensor, able to be aware of their physical environment, appeared. Such devices
enable proposing applications which adapt to user's need according the context
evolution. It implies the collaboration of sensors and software components
which differ on their nature and their communication mechanisms. This paper
proposes a unified component model in order to easily design applications based
on software components and sensors without taking care of their nature. Then it
presents a state of the art of communication problems linked to heterogeneous
components and proposes an interaction mechanism which ensures information
exchanges between wireless sensors and software components.
</summary>
    <author>
      <name>Christine Louberry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Roose</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <author>
      <name>Marc Dalmau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIUPPA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Networks, Issue N6, Academy Publisher 3, 4 (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0812.2989v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.2989v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.2272v1</id>
    <updated>2009-03-13T07:11:31Z</updated>
    <published>2009-03-13T07:11:31Z</published>
    <title>A Novel Approach for Compression of Images Captured using Bayer Color
  Filter Arrays</title>
    <summary>  We propose a new approach for image compression in digital cameras, where the
goal is to achieve better quality at a given rate by using the characteristics
of a Bayer color filter array. Most digital cameras produce color images by
using a single CCD plate, so that each pixel in an image has only one color
component and therefore an interpolation method is needed to produce a full
color image. After the image processing stage, in order to reduce the memory
requirements of the camera, a lossless or lossy compression stage often
follows. But in this scheme, before decreasing redundancy through compression,
redundancy is increased in an interpolation stage. In order to avoid increasing
the redundancy before compression, we propose algorithms for image compression
in which the order of the compression and interpolation stages is reversed. We
introduce image transform algorithms, since non interpolated images cannot be
directly compressed with general image coders. The simulation results show that
our algorithm outperforms conventional methods with various color interpolation
methods in a wide range of compression ratios. Our proposed algorithm provides
not only better quality but also lower encoding complexity because the amount
of luminance data used is only half of that in conventional methods.
</summary>
    <author>
      <name>Sang-Yong Lee</name>
    </author>
    <author>
      <name>Antonio Ortega</name>
    </author>
    <link href="http://arxiv.org/abs/0903.2272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.2272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.3995v1</id>
    <updated>2009-03-24T01:33:15Z</updated>
    <published>2009-03-24T01:33:15Z</published>
    <title>Gradient-based adaptive interpolation in super-resolution image
  restoration</title>
    <summary>  This paper presents a super-resolution method based on gradient-based
adaptive interpolation. In this method, in addition to considering the distance
between the interpolated pixel and the neighboring valid pixel, the
interpolation coefficients take the local gradient of the original image into
account. The smaller the local gradient of a pixel is, the more influence it
should have on the interpolated pixel. And the interpolated high resolution
image is finally deblurred by the application of wiener filter. Experimental
results show that our proposed method not only substantially improves the
subjective and objective quality of restored images, especially enhances edges,
but also is robust to the registration error and has low computational
complexity.
</summary>
    <author>
      <name>Jinyu Chu</name>
    </author>
    <author>
      <name>Ju Liu</name>
    </author>
    <author>
      <name>Jianping Qiao</name>
    </author>
    <author>
      <name>Xiaoling Wang</name>
    </author>
    <author>
      <name>Yujun Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4pages, 7figures, This paper presents a super-resolution method based
  on gradient-based adaptive interpolation</arxiv:comment>
    <link href="http://arxiv.org/abs/0903.3995v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.3995v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.3693v1</id>
    <updated>2009-04-23T13:50:36Z</updated>
    <published>2009-04-23T13:50:36Z</published>
    <title>The Multimedia Product - between Design and Information, Design and
  Utility and Design and Entertainment</title>
    <summary>  The paper investigates the possible coherent and effective alternatives to
solve the problems related to the communication needs of any multimedia
product. In essence, the presentation will focus on identifying the issues and
principles governing three types of the design - in fact, the multimedia design
in a broader sense - namely the information design - precisely aiming at ways
of organization and presentation of information in a useful and significant
form, the graphical user interface design, whose sub-domain consists of the
information displayed on the monitor screen and of interactivity between user,
computer and electronic devices, meaning, in fact, everything the user sees,
touches, hears and all the elements with which he interacts, the graphic
design, whose main concern is to create an aesthetic layout arrangement (from
the visual and perceptive) information.
</summary>
    <author>
      <name>Dieter Penteliuc Cotosman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages,exposed on 1st "European Conference on Computer Sciences &amp;
  Applications" - XA2006, Timisoara, Romania</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series IV (2006), 167-194</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0904.3693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.3693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.3694v1</id>
    <updated>2009-04-23T13:57:23Z</updated>
    <published>2009-04-23T13:57:23Z</published>
    <title>The new multimedia educational technologies, used in open and distance
  learning</title>
    <summary>  This paper reviews and refers to the latest telematics technology that has
turned the open system learning and helped it to become an institutional
alternative to the face-to-face traditional one. Most technologies, briefly
presented here, will be implemented in the "ARTeFACt" project - telematic
system for vocational education system of open system learning, system which
will be officially launched at the end of 2006, in the institutional offer of
the Faculty of Arts of the University West of Timisoara. The scientific
coordination of the doctoral project "ARTeFACt" is done by Mr. Prof. Dr. Eng.
Savi G. George, representing the Department of Mechatronics Faculty of
Mechanical Engineering from the University "Politehnica" of Timisoara, Romania
</summary>
    <author>
      <name>Dieter Penteliuc-Cotosman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages,exposed on 1st "European Conference on Computer Sciences &amp;
  Applications" - XA2006, Timisoara, Romania</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series IV (2006), 195-204</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0904.3694v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.3694v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.4936v1</id>
    <updated>2009-06-26T13:40:38Z</updated>
    <published>2009-06-26T13:40:38Z</published>
    <title>A New Approach to Manage QoS in Distributed Multimedia Systems</title>
    <summary>  Dealing with network congestion is a criterion used to enhance quality of
service (QoS) in distributed multimedia systems. The existing solutions for the
problem of network congestion ignore scalability considerations because they
maintain a separate classification for each video stream. In this paper, we
propose a new method allowing to control QoS provided to clients according to
the network congestion, by discarding some frames when needed. The technique
proposed, called (m,k)-frame, is scalable with little degradation in
application performances. (m,k)-frame method is issued from the notion of
(m,k)-firm realtime constraints which means that among k invocations of a task,
m invocations must meet their deadline. Our simulation studies show the
usefulness of (m,k)-frame method to adapt the QoS to the real conditions in a
multimedia application, according to the current system load. Notably, the
system must adjust the QoS provided to active clients1 when their number
varies, i.e. dynamic arrival of clients.
</summary>
    <author>
      <name>Bechir Alaya</name>
    </author>
    <author>
      <name>Claude Duvallet</name>
    </author>
    <author>
      <name>Bruno Sadeg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, International Journal of Computer Science and Information
  Security (IJCSIS)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSIS June 2009 Issue, Vol. 2, No. 1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0906.4936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.4936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.0118v1</id>
    <updated>2009-09-01T08:19:18Z</updated>
    <published>2009-09-01T08:19:18Z</published>
    <title>Dynamic Multimedia Content Retrieval System in Distributed Environment</title>
    <summary>  WiCoM enables remote management of web resources. Our application Mobile
reporter is aimed at Journalist, who will be able to capture the events in
real-time using their mobile phones and update their web server on the latest
event. WiCoM has been developed using J2ME technology on the client side and
PHP on the server side. The communication between the client and the server is
established through GPRS. Mobile reporter will be able to upload, edit and
remove both textual as well as multimedia contents in the server.
</summary>
    <author>
      <name>R. Sivaraman</name>
    </author>
    <author>
      <name>R. Prabakaran</name>
    </author>
    <author>
      <name>S. Sujatha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 Pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 4, No. 1 &amp; 2, August 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.0118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.0118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.0245v1</id>
    <updated>2009-09-01T18:57:06Z</updated>
    <published>2009-09-01T18:57:06Z</published>
    <title>Enhanced Mode Selection Algorithm for H.264 encoder for Application in
  Low Computational power devices</title>
    <summary>  The intent of the H.264 AVC project was to create a standard capable of
providing good video quality at substantially lower bit rates than previous
standards without increasing the complexity of design so much that it would be
impractical or excessively expensive to implement. An additional goal was to
provide enough flexibility to allow the standard to be applied to a wide
variety of applications. To achieve better coding efficiency, H.264 AVC uses
several techniques such as inter mode and intra mode prediction with variable
size motion compensation, which adopts Rate Distortion Optimization (RDO). This
increases the computational complexity of the encoder especially for devices
with lower processing capabilities such as mobile and other handheld devices.
In this paper, we propose an algorithm to reduce the number of mode and sub
mode evaluations in inter mode prediction. Experimental results show that this
fast intra mode selection algorithm can lessen about 75 percent encoding time
with little loss of bit rate and visual quality.
</summary>
    <author>
      <name>Sourabh Rungta</name>
    </author>
    <author>
      <name>Kshitij Verma</name>
    </author>
    <author>
      <name>Neeta Tripathi</name>
    </author>
    <author>
      <name>Anupam Shukla</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact factor 0.423
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 4, No. 1 &amp; 2, August 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.0245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.0245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.2816v1</id>
    <updated>2009-09-15T14:44:21Z</updated>
    <published>2009-09-15T14:44:21Z</published>
    <title>Efficient Quality-Based Playout Buffer Algorithm</title>
    <summary>  Playout buffers are used in VoIP systems to compensate for network delay
jitter by making a trade-off between delay and loss. In this work we propose a
playout buffer algorithm that makes the trade-off based on maximization of
conversational speech quality, aiming to keep the computational complexity
lowest possible. We model the network delay using a Pareto distribution and
show that it is a good compromise between providing an appropriate fit to the
network delay characteristics and yielding a low arithmetical complexity. We
use the ITU-T E-Model as the quality model and simplify its delay impairment
function. The proposed playout buffer algorithm finds the optimum playout delay
using a closed-form solution that minimizes the sum of the simplified delay
impairment factor and the loss-dependent equipment impairment factor of the
E-model. The simulation results show that our proposed algorithm outperforms
existing state-of-the-art algorithms with a reduced complexity for a
quality-based algorithm.
</summary>
    <author>
      <name>Emine Zerrin Sakir</name>
    </author>
    <author>
      <name>Christian Feldbauer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0909.2816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.2816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0909.3554v1</id>
    <updated>2009-09-19T03:06:57Z</updated>
    <published>2009-09-19T03:06:57Z</published>
    <title>Robustness of the Digital Image Watermarking Techniques against
  Brightness and Rotation Attack</title>
    <summary>  The recent advent in the field of multimedia proposed a many facilities in
transport, transmission and manipulation of data. Along with this advancement
of facilities there are larger threats in authentication of data, its licensed
use and protection against illegal use of data. A lot of digital image
watermarking techniques have been designed and implemented to stop the illegal
use of the digital multimedia images. This paper compares the robustness of
three different watermarking schemes against brightness and rotation attacks.
The robustness of the watermarked images has been verified on the parameters of
PSNR (Peak Signal to Noise Ratio), RMSE (Root Mean Square Error) and MAE (Mean
Absolute Error).
</summary>
    <author>
      <name>Harsh K Verma</name>
    </author>
    <author>
      <name>Abhishek Narain Singh</name>
    </author>
    <author>
      <name>Raman Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 Pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact factor 0.423</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Harsh K Verma, Abhishek Narain Singh, Raman Kumar, International
  Journal of Computer Science and Information Security, IJCSIS, Vol. 5, No. 1,
  September 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0909.3554v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0909.3554v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.0179v1</id>
    <updated>2009-10-01T14:03:51Z</updated>
    <published>2009-10-01T14:03:51Z</published>
    <title>Analysis, Design and Simulation of a New System for Internet Multimedia
  Transmission Guarantee</title>
    <summary>  QoS is a very important issue for multimedia communication systems. In this
paper, a new system that reinstalls the relation between the QoS elements
(RSVP, routing protocol, sender, and receiver) during the multimedia
transmission is proposed, then an alternative path is created in case of
original multimedia path failure. The suggested system considers the resulting
problems that may be faced within and after the creation of rerouting path.
Finally, the proposed system is simulated using OPNET 11.5 simulation package.
Simulation results show that our proposed system outperforms the old one in
terms of QoS parameters like packet loss and delay jitter.
</summary>
    <author>
      <name>O. Said</name>
    </author>
    <author>
      <name>S. Bahgat</name>
    </author>
    <author>
      <name>M. Ghoniemy</name>
    </author>
    <author>
      <name>Y. Elawdy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 5, No. 1, pp. 77-86, September 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0910.0179v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.0179v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.1468v1</id>
    <updated>2009-10-08T11:21:21Z</updated>
    <published>2009-10-08T11:21:21Z</published>
    <title>Prefetching of VoD Programs Based On ART1 Requesting Clustering</title>
    <summary>  In this paper, we propose a novel approach to group users according to the
VoD user request pattern. We cluster the user requests based on ART1 neural
network algorithm. The knowledge extracted from the cluster is used to prefetch
the multimedia object from each cluster before the users request. We have
developed an algorithm to cluster users according to the users request patterns
based on ART1 neural network algorithm that offers an unsupervised clustering.
This approach adapts to changes in user request patterns over period without
losing previous information. Each cluster is represented as prototype vector by
generalizing the most frequently used URLs that are accessed by all the cluster
members. The simulation results of our proposed clustering and prefetching
algorithm, shows enormous increase in the performance of streaming server. Our
algorithm helps the servers agent to learn user preferences and discover the
information about the corresponding sources and other similar interested
individuals.
</summary>
    <author>
      <name>P. Jayarekha</name>
    </author>
    <author>
      <name>T. R. GopalaKrishnan Nair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 5, No. 1, pp. 128-134, September 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0910.1468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.1468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.0399v1</id>
    <updated>2009-11-02T20:07:52Z</updated>
    <published>2009-11-02T20:07:52Z</published>
    <title>A Wavelet-Based Digital Watermarking for Video</title>
    <summary>  A novel video watermarking system operating in the three dimensional wavelet
transform is here presented. Specifically the video sequence is partitioned
into spatio temporal units and the single shots are projected onto the 3D
wavelet domain. First a grayscale watermark image is decomposed into a series
of bitplanes that are preprocessed with a random location matrix. After that
the preprocessed bitplanes are adaptively spread spectrum and added in 3D
wavelet coefficients of the video shot. Our video watermarking algorithm is
robust against the attacks of frame dropping, averaging and swapping.
Furthermore, it allows blind retrieval of embedded watermark which does not
need the original video and the watermark is perceptually invisible. The
algorithm design, evaluation, and experimentation of the proposed scheme are
described in this paper.
</summary>
    <author>
      <name>A. Essaouabi</name>
    </author>
    <author>
      <name>F. Regragui</name>
    </author>
    <author>
      <name>E. Ibnelhaj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS 2009, ISSN 1947 5500, Impact Factor 0.423,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 6, No. 1, pp. 029-033, October 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.0399v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.0399v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.1005v1</id>
    <updated>2009-12-05T12:33:09Z</updated>
    <published>2009-12-05T12:33:09Z</published>
    <title>Performance analysis of Non Linear Filtering Algorithms for underwater
  images</title>
    <summary>  Image filtering algorithms are applied on images to remove the different
types of noise that are either present in the image during capturing or
injected in to the image during transmission. Underwater images when captured
usually have Gaussian noise, speckle noise and salt and pepper noise. In this
work, five different image filtering algorithms are compared for the three
different noise types. The performances of the filters are compared using the
Peak Signal to Noise Ratio (PSNR) and Mean Square Error (MSE). The modified
spatial median filter gives desirable results in terms of the above two
parameters for the three different noise. Forty underwater images are taken for
study.
</summary>
    <author>
      <name>Dr. G. Padmavathi</name>
    </author>
    <author>
      <name>Dr. P. Subashini</name>
    </author>
    <author>
      <name>Mr. M. Muthu Kumar</name>
    </author>
    <author>
      <name>Suresh Kumar Thakur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS November 2009, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 6, No. 2, pp. 232-238, November 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.1005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.1005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.1011v1</id>
    <updated>2009-12-05T13:03:17Z</updated>
    <published>2009-12-05T13:03:17Z</published>
    <title>A Reliable Replication Strategy for VoD System using Markov Chain</title>
    <summary>  In this paper we have investigated on the reliability of streams for a VoD
system. The objective of the paper is to maximize the availability of streams
for the peers in the VoD system. We have achieved this by using data
replication technique in the peers. Hence, we proposed a new data replication
technique to optimally store the videos in the peers. The new data replication
technique generates more number of replicas than the existing techniques such
as random, minimum request and maximize hit. We have also investigated by
applying the CTMC model for the reliability of replications during the peer
failures. Our result shows that the mean lifetime of replicas are more under
various circumstances. We have addressed the practical issues of efficient
utilization of overall bandwidth and buffer in the VoD system. We achieved
greater success playback probability of videos than the existing techniques.
</summary>
    <author>
      <name>R. Ashok Kumar</name>
    </author>
    <author>
      <name>K. Ganesan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS November 2009, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 6, No. 2, pp. 281-290, November 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.1011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.1011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.4881v1</id>
    <updated>2009-12-24T15:28:53Z</updated>
    <published>2009-12-24T15:28:53Z</published>
    <title>Music-ripping: des pratiques qui provoquent la musicologie</title>
    <summary>  Out of the scope of the usual positions of computing in the field of music
and musicology, one notices the emergence of human-computer systems that do
exist by breaking off. Though these singular systems take effect in the usual
fields of expansion of music, they do not make any systematic reference to
known musicological categories. On the contrary, they make possible experiments
that open uses where listening, composition and musical transmission get merged
in a gesture sometimes named as ?music-ripping?. We will show in which way the
music-ripping practices provoke traditional musicology, whose canonical
categories happen to be ineffectual to explain here. To achieve that purpose,
we shall need: - to make explicit a minimal set of categories that is
sufficient to underlie the usual models of computer assisted music;- to do the
same for human-computer systems (anti-musicological?) that disturb us; - to
examine the possibility conditions of reduction of the second set to the first;
- to conclude on the nature of music-ripping.
</summary>
    <author>
      <name>Francis Rousseaux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">STMS, CRESTIC</arxiv:affiliation>
    </author>
    <author>
      <name>Alain Bonardi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">STMS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Musicae Scientiae Special issue 2004 (2004)
  http://musicweb.hmt-hannover.de/escom/english/MusicScE/MSstart.htm</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.4881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.4881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.0441v1</id>
    <updated>2010-01-04T05:32:05Z</updated>
    <published>2010-01-04T05:32:05Z</published>
    <title>Semantic Modeling and Retrieval of Dance Video Annotations</title>
    <summary>  Dance video is one of the important types of narrative videos with semantic
rich content. This paper proposes a new meta model, Dance Video Content Model
(DVCM) to represent the expressive semantics of the dance videos at multiple
granularity levels. The DVCM is designed based on the concepts such as video,
shot, segment, event and object, which are the components of MPEG-7 MDS. This
paper introduces a new relationship type called Temporal Semantic Relationship
to infer the semantic relationships between the dance video objects. Inverted
file based index is created to reduce the search time of the dance queries. The
effectiveness of containment queries using precision and recall is depicted.
Keywords: Dance Video Annotations, Effectiveness Metrics, Metamodeling,
Temporal Semantic Relationships.
</summary>
    <author>
      <name>Rajkumar Kannan</name>
    </author>
    <author>
      <name>Balakrishnan Ramadoss</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">INFOCOMP Journal of Computer Science, Brazil</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.0441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.0441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; H.5.4; H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.0442v1</id>
    <updated>2010-01-04T05:41:01Z</updated>
    <published>2010-01-04T05:41:01Z</published>
    <title>Modeling and Annotating the Expressive Semantics of Dance Videos</title>
    <summary>  Dance videos are interesting and semantics-intensive. At the same time, they
are the complex type of videos compared to all other types such as sports, news
and movie videos. In fact, dance video is the one which is less explored by the
researchers across the globe. Dance videos exhibit rich semantics such as macro
features and micro features and can be classified into several types. Hence,
the conceptual modeling of the expressive semantics of the dance videos is very
crucial and complex. This paper presents a generic Dance Video Semantics Model
(DVSM) in order to represent the semantics of the dance videos at different
granularity levels, identified by the components of the accompanying song. This
model incorporates both syntactic and semantic features of the videos and
introduces a new entity type called, Agent, to specify the micro features of
the dance videos. The instantiations of the model are expressed as graphs. The
model is implemented as a tool using J2SE and JMF to annotate the macro and
micro features of the dance videos. Finally examples and evaluation results are
provided to depict the effectiveness of the proposed dance video model.
Keywords: Agents, Dance videos, Macro features, Micro features, Video
annotation, Video semantics.
</summary>
    <author>
      <name>Rajkumar Kannan</name>
    </author>
    <author>
      <name>Balakrishnan Ramadoss</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Information Technologies and Knowledge</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.0442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.0442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.5; H.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.1794v1</id>
    <updated>2010-01-12T05:06:16Z</updated>
    <published>2010-01-12T05:06:16Z</published>
    <title>Designing a Truly Integrated (Onsite and Online) Conference: Concept,
  Processes, Solutions</title>
    <summary>  Web conferencing tools have entered the mainstream of business applications.
Using web conferencing for IEEE conferences has a good potential of adding
value to both organizers and participants. Authors propose a concept of Truly
Integrated Conference (TIC) according to which a multi-point
worldwide-distributed network of conference online authors/participants will
enhance the standard (centralized) IEEE conference model, which requires
attendance of the participants in person at the main conference location. The
concept entails seamless integration of the onsite and online conference
systems, including data/presentation, video, audio channels. Benefits and
challenges of the TIC concept are analyzed. Requirements to the web
conferencing system capable of supporting the TIC conference are presented and
reviewed against commercial web conferencing tools. Case study of the IEEE
Toronto International Conference ? Science and Technology for Humanity, which
was the first realization of TIC, is presented which analyzes various aspects
(organizational, technological, and financial) of the integrated conference.
</summary>
    <author>
      <name>Alexei Botchkarev</name>
    </author>
    <author>
      <name>Lian Zhao</name>
    </author>
    <author>
      <name>Hamed Rasouli</name>
    </author>
    <link href="http://arxiv.org/abs/1001.1794v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.1794v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.1937v2</id>
    <updated>2010-02-04T16:26:43Z</updated>
    <published>2010-01-12T16:13:50Z</published>
    <title>Avoiding Interruptions - QoE Trade-offs in Block-coded Streaming Media
  Applications</title>
    <summary>  We take an analytical approach to study Quality of user Experience (QoE) for
video streaming applications. First, we show that random linear network coding
applied to blocks of video frames can significantly simplify the packet
requests at the network layer and save resources by avoiding duplicate packet
reception. Network coding allows us to model the receiver's buffer as a queue
with Poisson arrivals and deterministic departures. We consider the probability
of interruption in video playback as well as the number of initially buffered
packets (initial waiting time) as the QoE metrics. We characterize the optimal
trade-off between these metrics by providing upper and lower bounds on the
minimum initial buffer size, required to achieve certain level of interruption
probability for different regimes of the system parameters. Our bounds are
asymptotically tight as the file size goes to infinity.
</summary>
    <author>
      <name>Ali Parandehgheibi</name>
    </author>
    <author>
      <name>Muriel Medard</name>
    </author>
    <author>
      <name>Srinivas Shakkottai</name>
    </author>
    <author>
      <name>Asu Ozdaglar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ISIT 2010 - Full version</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.1937v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.1937v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.1972v1</id>
    <updated>2010-01-12T18:30:20Z</updated>
    <published>2010-01-12T18:30:20Z</published>
    <title>A New Image Steganography Based On First Component Alteration Technique</title>
    <summary>  In this paper, A new image steganography scheme is proposed which is a kind
of spatial domain technique. In order to hide secret data in cover-image, the
first component alteration technique is used. Techniques used so far focuses
only on the two or four bits of a pixel in a image (at the most five bits at
the edge of an image) which results in less peak to signal noise ratio and high
root mean square error. In this technique, 8 bits of blue components of pixels
are replaced with secret data bits. Proposed scheme can embed more data than
previous schemes and shows better image quality. To prove this scheme, several
experiments are performed, and are compared the experimental results with the
related previous works.
</summary>
    <author>
      <name>Amanpreet Kaur</name>
    </author>
    <author>
      <name>Renu Dhir</name>
    </author>
    <author>
      <name>Geeta Sikka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS December 2009, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 6, No. 3, pp. 053-056, December 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.1972v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.1972v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.1974v1</id>
    <updated>2010-01-12T18:34:24Z</updated>
    <published>2010-01-12T18:34:24Z</published>
    <title>Evaluating Effectiveness of Tamper Proofing on Dynamic Graph Software
  Watermarks</title>
    <summary>  For enhancing the protection level of dynamic graph software watermarks and
for the purpose of conducting the analysis which evaluates the effect of
integrating two software protection techniques such as software watermarking
and tamper proofing, constant encoding technique along with the enhancement
through the idea of constant splitting is proposed. In this paper Thomborson
technique has been implemented with the scheme of breaking constants which
enables to encode all constants without having any consideration about their
values with respect to the value of watermark tree. Experimental analysis which
have been conducted and provided in this paper concludes that the constant
encoding process significantly increases the code size, heap space usage, and
execution time, while making the tamper proofed code resilient to variety of
semantic preserving program transformation attacks.
</summary>
    <author>
      <name>Malik Sikandar Hayat Khiyal</name>
    </author>
    <author>
      <name>Aihab Khan</name>
    </author>
    <author>
      <name>Sehrish Amjad</name>
    </author>
    <author>
      <name>M. Shahid Khalil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS December 2009, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 6, No. 3, pp. 057-063, December 2009, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.1974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.1974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.3744v1</id>
    <updated>2010-01-21T08:57:27Z</updated>
    <published>2010-01-21T08:57:27Z</published>
    <title>Multicast Transmission Prefix and Popularity Aware Interval Caching
  Based Admission Control Policy</title>
    <summary>  Admission control is a key component in multimedia servers, which will allow
the resources to be used by the client only when they are available. A problem
faced by numerous content serving machines is overload, when there are too many
clients who need to be served, the server tends to slow down. An admission
control algorithm for a multimedia server is responsible for determining if a
new request can be accepted without violating the QoS requirements of the
existing requests in the system. By caching and streaming only the data in the
interval between two successive requests on the same object, the following
request can be serviced directly from the buffer cache without disk operations
and within the deadline of the request. An admission control strategy based on
Popularity-aware interval caching for Prefix [3] scheme extends the interval
caching by considering different popularity of multimedia objects. The method
of Prefix caching with multicast transmission of popular objects utilizes the
hard disk and network bandwidth efficiently and increases the number of
requests being served.
</summary>
    <author>
      <name>P. Jayarekha</name>
    </author>
    <author>
      <name>T. R. Gopalakrishnan Nair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages,</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Innovations-2008, pp 15-31, 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.3744v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.3744v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.3774v1</id>
    <updated>2010-01-21T11:08:42Z</updated>
    <published>2010-01-21T11:08:42Z</published>
    <title>Cooperative Proxy Servers Architecture for VoD to Achieve High QoS with
  Reduced Transmission Time and Cost</title>
    <summary>  - The aim of this paper is to propose a novel Voice On Demand (VoD)
architecture and implementation of an efficient load sharing algorithm to
achieve Quality of Service (QoS). This scheme reduces the transmission cost
from the Centralized Multimedia Sever (CMS) to Proxy Servers (PS) by sharing
the videos among the proxy servers of the Local Proxy Servers Group [LPSG] and
among the neighboring LPSGs, which are interconnected in a ring fashion. This
results in very low request rejection ratio, reduction in transmission time and
cost, reduction of load on the CMS and high QoS for the users. Simulation
results indicate acceptable initial startup latency, reduced transmission cost
and time, load sharing among the proxy servers, among the LPSGs and between the
CMS and the PS.
</summary>
    <author>
      <name>M. Dakshayini</name>
    </author>
    <author>
      <name>T. R. Gopalakrishan Nair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Innovations-2008, pp 103-114, 2008</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1001.3774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.3774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1001.4135v1</id>
    <updated>2010-01-23T07:20:30Z</updated>
    <published>2010-01-23T07:20:30Z</published>
    <title>An Adaptive Dynamic Replacement Approach for a Multicast based
  Popularity Aware Prefix Cache Memory System</title>
    <summary>  In this paper we have proposed an adaptive dynamic cache replacement
algorithm for a multimedia servers cache system. The goal is to achieve an
effective utilization of the cache memory which stores the prefix of popular
videos. A replacement policy is usually evaluated using hit ratio, the
frequency with which any video is requested. Usually discarding the least
recently used page is the policy of choice in cache management. The adaptive
dynamic replacement approach for prefix cache is a self tuning, low overhead
algorithm that responds online to changing access patterns. It constantly
balances between lru and lfu to improve combined result. It automatically
adapts to evolving workloads. Since in our algorithm we have considered a
prefix caching with multicast transmission of popular objects it utilizes the
hard disk and network bandwidth efficiently and increases the number of
requests being served.
</summary>
    <author>
      <name>P. Jayarekha</name>
    </author>
    <author>
      <name>T. R. Gopalakrishnan Nair</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1001.4135v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1001.4135v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.1727v3</id>
    <updated>2010-06-21T20:50:53Z</updated>
    <published>2010-02-08T22:05:04Z</published>
    <title>An Improved DC Recovery Method from AC Coefficients of DCT-Transformed
  Images</title>
    <summary>  Motivated by the work of Uehara et al. [1], an improved method to recover DC
coefficients from AC coefficients of DCT-transformed images is investigated in
this work, which finds applications in cryptanalysis of selective multimedia
encryption. The proposed under/over-flow rate minimization (FRM) method employs
an optimization process to get a statistically more accurate estimation of
unknown DC coefficients, thus achieving a better recovery performance. It was
shown by experimental results based on 200 test images that the proposed DC
recovery method significantly improves the quality of most recovered images in
terms of the PSNR values and several state-of-the-art objective image quality
assessment (IQA) metrics such as SSIM and MS-SSIM.
</summary>
    <author>
      <name>Shujun Li</name>
    </author>
    <author>
      <name>Junaid Jameel Ahmad</name>
    </author>
    <author>
      <name>Dietmar Saupe</name>
    </author>
    <author>
      <name>C. -C. Jay Kuo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICIP.2010.5653467</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICIP.2010.5653467" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures, ICIP 2010</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 2010 17th IEEE International Conference on Image
  Processing (ICIP 2010), pages 2085-2088, IEEE, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.1727v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.1727v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.2; E.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.1951v1</id>
    <updated>2010-02-09T19:43:44Z</updated>
    <published>2010-02-09T19:43:44Z</published>
    <title>Image Retrieval Techniques based on Image Features, A State of Art
  approach for CBIR</title>
    <summary>  The purpose of this Paper is to describe our research on different feature
extraction and matching techniques in designing a Content Based Image Retrieval
(CBIR) system. Due to the enormous increase in image database sizes, as well as
its vast deployment in various applications, the need for CBIR development
arose. Firstly, this paper outlines a description of the primitive feature
extraction techniques like, texture, colour, and shape. Once these features are
extracted and used as the basis for a similarity check between images, the
various matching techniques are discussed. Furthermore, the results of its
performance are illustrated by a detailed example.
</summary>
    <author>
      <name>Mr. Kondekar V. H.</name>
    </author>
    <author>
      <name>Mr. Kolkure V. S.</name>
    </author>
    <author>
      <name>Prof. Kore S. N</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS January 2010, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 7, No. 1, pp. 69-76, January 2010, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.1951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.1951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.2184v1</id>
    <updated>2010-02-10T19:27:25Z</updated>
    <published>2010-02-10T19:27:25Z</published>
    <title>The Fast Haar Wavelet Transform for Signal &amp; Image Processing</title>
    <summary>  A method for the design of Fast Haar wavelet for signal processing and image
processing has been proposed. In the proposed work, the analysis bank and
synthesis bank of Haar wavelet is modified by using polyphase structure.
Finally, the Fast Haar wavelet was designed and it satisfies alias free and
perfect reconstruction condition. Computational time and computational
complexity is reduced in Fast Haar wavelet transform.
</summary>
    <author>
      <name>V. Ashok</name>
    </author>
    <author>
      <name>T. Balakumaran</name>
    </author>
    <author>
      <name>C. Gowrishankar</name>
    </author>
    <author>
      <name>I. L. A. Vennila</name>
    </author>
    <author>
      <name>A. Nirmal kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS January 2010, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 7, No. 1, pp. 126-130, January 2010, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.2184v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.2184v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.2193v1</id>
    <updated>2010-02-10T19:52:12Z</updated>
    <published>2010-02-10T19:52:12Z</published>
    <title>Using Statistical Moment Invariants and Entropy in Image Retrieval</title>
    <summary>  Although content-based image retrieval (CBIR) is not a new subject, it keeps
attracting more and more attention, as the amount of images grow tremendously
due to internet, inexpensive hardware and automation of image acquisition. One
of the applications of CBIR is fetching images from a database. This paper
presents a new method for automatic image retrieval using moment invariants and
image entropy, our technique could be used to find semi or perfect matches
based on query by example manner, experimental results demonstrate that the
purposed technique is scalable and efficient.
</summary>
    <author>
      <name>Ismail I. Amr</name>
    </author>
    <author>
      <name>Mohamed Amin</name>
    </author>
    <author>
      <name>Passent El Kafrawy</name>
    </author>
    <author>
      <name>Amr M. Sauber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS January 2010, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 7, No. 1, pp. 160-164, January 2010, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.2193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.2193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.2414v1</id>
    <updated>2010-02-11T20:05:22Z</updated>
    <published>2010-02-11T20:05:22Z</published>
    <title>Dual Watermarking Scheme with Encryption</title>
    <summary>  Digital Watermarking is used for copyright protection and authentication. In
the proposed system, a Dual Watermarking Scheme based on DWT SVD with chaos
encryption algorithm, will be developed to improve the robustness and
protection along with security. DWT and SVD have been used as a mathematical
tool to embed watermark in the image. Two watermarks are embedded in the host
image. The secondary is embedded into primary watermark and the resultant
watermarked image is encrypted using chaos based logistic map. This provides an
efficient and secure way for image encryption and transmission. The watermarked
image is decrypted and a reliable watermark extraction scheme is developed for
the extraction of the primary as well as secondary watermark from the distorted
image.
</summary>
    <author>
      <name>R. Dhanalakshmi</name>
    </author>
    <author>
      <name>K. Thaiyalnayaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE format, International Journal of Computer Science and
  Information Security, IJCSIS January 2010, ISSN 1947 5500,
  http://sites.google.com/site/ijcsis/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, IJCSIS, Vol. 7, No. 1, pp. 248-253, January 2010, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.2414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.2414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.3984v1</id>
    <updated>2010-02-21T18:29:53Z</updated>
    <published>2010-02-21T18:29:53Z</published>
    <title>Effect of Embedding Watermark on Compression of the Digital Images</title>
    <summary>  Image Compression plays a very important role in image processing especially
when we are to send the image on the internet. The threat to the information on
the internet increases and image is no exception. Generally the image is sent
on the internet as the compressed image to optimally use the bandwidth of the
network. But as we are on the network, at any intermediate level the image can
be changed intentionally or unintentionally. To make sure that the correct
image is being delivered at the other end we embed the water mark to the image.
The watermarked image is then compressed and sent on the network. When the
image is decompressed at the other end we can extract the watermark and make
sure that the image is the same that was sent by the other end. Though
watermarking the image increases the size of the uncompressed image but that
has to done to achieve the high degree of robustness i.e. how an image sustains
the attacks on it. The present paper is an attempt to make transmission of the
images secure from the intermediate attacks by applying the generally used
compression transforms.
</summary>
    <author>
      <name>Deepak Aggarwal</name>
    </author>
    <author>
      <name>Kanwalvir Singh Dhindsa</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 2, Issue 2, February 2010,
  https://sites.google.com/site/journalofcomputing/</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1002.3984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.3984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.3080v1</id>
    <updated>2010-03-16T05:07:10Z</updated>
    <published>2010-03-16T05:07:10Z</published>
    <title>An Algorithm for Index Multimedia Data (Video) using the Movement
  Oriented Method for Real-time Online Services</title>
    <summary>  Multimedia data is a form of data that can represent all types of data
(images, sound and text). The use of multimedia data for the online application
requires a more comprehensive database in the use of storage media, Sorting /
indexing, search and system / data searching. This is necessary in order to
help providers and users to access multimedia data online. Systems that use of
the index image as a reference requires storage media so that the rules and
require special expertise to obtain the desired file. Changes in multimedia
data into a series of stories / storyboard in the form of a text will help
reduce the consumption of media storage, system index / sorting and search
applications. Oriented Movement is one method that is being developed to change
the form of multimedia data into a storyboard.
</summary>
    <author>
      <name>A. Muslim</name>
    </author>
    <author>
      <name>A. B. Mutiara</name>
    </author>
    <author>
      <name>C. M. Karyati</name>
    </author>
    <author>
      <name>P. Musa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, International Conference on Robotics, Informatics,
  Intelligence control system Technologies (RIIT'09)</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.3080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.3080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.3533v1</id>
    <updated>2010-03-18T09:27:16Z</updated>
    <published>2010-03-18T09:27:16Z</published>
    <title>Towards Automated Lecture Capture, Navigation and Delivery System for
  Web-Lecture on Demand</title>
    <summary>  Institutions all over the world are continuously exploring ways to use ICT in
improving teaching and learning effectiveness. The use of course web pages,
discussion groups, bulletin boards, and e-mails have shown considerable impact
on teaching and learning in significant ways, across all disciplines. ELearning
has emerged as an alternative to traditional classroom-based education and
training and web lectures can be a powerful addition to traditional lectures.
They can even serve as a main content source for learning, provided users can
quickly navigate and locate relevant pages in a web lecture. A web lecture
consists of video and audio of the presenter and slides complemented with
screen capturing. In this paper, an automated approach for recording live
lectures and for browsing available web lectures for on-demand applications by
end users is presented.
</summary>
    <author>
      <name>Rajkumar Kannan</name>
    </author>
    <author>
      <name>Frederic Andres</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3rd International Conference on Data Management, March 2010, India</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Book Chapter in Innovations and Advances in Computer Science and
  Engineering, MacMillan Publishers, 386-394, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.3533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.3533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4049v1</id>
    <updated>2010-03-22T03:31:50Z</updated>
    <published>2010-03-22T03:31:50Z</published>
    <title>An Optimal Prefix Replication Strategy for VoD Services</title>
    <summary>  In this paper we propose scalable proxy servers cluster architecture of
interconnected proxy servers for high quality and high availability services.
We also propose an optimal regional popularity based video prefix replication
strategy and a scene change based replica caching algorithm that utilizes the
zipf-like video popularity distribution to maximize the availability of videos
closer to the client and request-servicing rate thereby reducing the client
rejection ratio and the response time for the client. The simulation results of
our proposed architecture and algorithm show the greater achievement in
maximizing the availability of videos, client request-servicing rate and in
reduction of initial start-up latency and client rejection ratio.
</summary>
    <author>
      <name>M Dakshayini</name>
    </author>
    <author>
      <name>T R GopalaKrishnan Nair</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 2, Issue 3, March 2010,
  https://sites.google.com/site/journalofcomputing/</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.4049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4084v1</id>
    <updated>2010-03-22T06:42:05Z</updated>
    <published>2010-03-22T06:42:05Z</published>
    <title>New Classification Methods for Hiding Information into Two Parts:
  Multimedia Files and Non Multimedia Files</title>
    <summary>  With the rapid development of various multimedia technologies, more and more
multimedia data are generated and transmitted in the medical, commercial, and
military fields, which may include some sensitive information which should not
be accessed by or can only be partially exposed to the general users.
Therefore, security and privacy has become an important, Another problem with
digital document and video is that undetectable modifications can be made with
very simple and widely available equipment, which put the digital material for
evidential purposes under question .With the large flood of information and the
development of the digital format Information hiding considers one of the
techniques which used to protect the important information. The main goals for
this paper, provides a general overview of the New Classification Methods for
Hiding Information into Two Parts: Multimedia Files and Non Multimedia Files.
</summary>
    <author>
      <name>Hamdan. O. Alanazi</name>
    </author>
    <author>
      <name>A. A. Zaidan</name>
    </author>
    <author>
      <name>B. B. Zaidan</name>
    </author>
    <author>
      <name>Hamid A. Jalab</name>
    </author>
    <author>
      <name>Zaidoon Kh. AL-Ani</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 2, Issue 3, March 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.4084v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4084v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.4637v1</id>
    <updated>2010-03-24T13:07:08Z</updated>
    <published>2010-03-24T13:07:08Z</published>
    <title>Context-Oriented Web Video Tag Recommendation</title>
    <summary>  Tag recommendation is a common way to enrich the textual annotation of
multimedia contents. However, state-of-the-art recommendation methods are built
upon the pair-wised tag relevance, which hardly capture the context of the web
video, i.e., when who are doing what at where. In this paper we propose the
context-oriented tag recommendation (CtextR) approach, which expands tags for
web videos under the context-consistent constraint. Given a web video, CtextR
first collects the multi-form WWW resources describing the same event with the
video, which produce an informative and consistent context; and then, the tag
recommendation is conducted based on the obtained context. Experiments on an
80,031 web video collection show CtextR recommends various relevant tags to web
videos. Moreover, the enriched tags improve the performance of web video
categorization.
</summary>
    <author>
      <name>Zhineng Chen</name>
    </author>
    <author>
      <name>Juan Cao</name>
    </author>
    <author>
      <name>Yicheng Song</name>
    </author>
    <author>
      <name>Junbo Guo</name>
    </author>
    <author>
      <name>Yongdong Zhang</name>
    </author>
    <author>
      <name>Jintao Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 19th international conference on World wide web
  (WWW2010), April 26-30, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.4637v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.4637v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.1676v1</id>
    <updated>2010-04-10T03:42:06Z</updated>
    <published>2010-04-10T03:42:06Z</published>
    <title>A reversible high embedding capacity data hiding technique for hiding
  secret data in images</title>
    <summary>  As the multimedia and internet technologies are growing fast, the
transmission of digital media plays an important role in communication. The
various digital media like audio, video and images are being transferred
through internet. There are a lot of threats for the digital data that are
transferred through internet. Also, a number of security techniques have been
employed to protect the data that is transferred through internet. This paper
proposes a new technique for sending secret messages securely, using
steganographic technique. Since the proposed system uses multiple level of
security for data hiding, where the data is hidden in an image file and the
stego file is again concealed in another image. Previously, the secret message
is being encrypted with the encryption algorithm which ensures the achievement
of high security enabled data transfer through internet.
</summary>
    <author>
      <name>P. Mohan Kumar</name>
    </author>
    <author>
      <name>K. L. Shunmuganathan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 7 No. 3, March 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.1676v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.1676v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.1682v1</id>
    <updated>2010-04-10T04:29:01Z</updated>
    <published>2010-04-10T04:29:01Z</published>
    <title>Design And Implementation Of Multilevel Access Control In Medical Image
  Transmission Using Symmetric Polynomial Based Audio Steganography</title>
    <summary>  ...The steganography scheme makes it possible to hide the medical image in
different bit locations of host media without inviting suspicion. The Secret
file is embedded in a cover media with a key. At the receiving end the key can
be derived by all the classes which are higher in the hierarchy using symmetric
polynomial and the medical image file can be retrieved. The system is
implemented and found to be secure, fast and scalable. Simulation results show
that the system is dynamic in nature and allows any type of hierarchy. The
proposed approach performs better even during frequent member joins and leaves.
The computation cost is reduced as the same algorithm is used for key
computation and descendant key derivation. Steganographic technique used in
this paper does not use the conventional LSB's and uses two bit positions and
the hidden data occurs only from a frame which is dictated by the key that is
used. Hence the quality of stego data is improved.
</summary>
    <author>
      <name>J. Nafeesa Begum</name>
    </author>
    <author>
      <name>K. Kumar</name>
    </author>
    <author>
      <name>V. Sumathy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 7 No. 3, March 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.1682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.1682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.1789v1</id>
    <updated>2010-04-11T11:05:33Z</updated>
    <published>2010-04-11T11:05:33Z</published>
    <title>SAR Image Segmentation using Vector Quantization Technique on Entropy
  Images</title>
    <summary>  The development and application of various remote sensing platforms result in
the production of huge amounts of satellite image data. Therefore, there is an
increasing need for effective querying and browsing in these image databases.
In order to take advantage and make good use of satellite images data, we must
be able to extract meaningful information from the imagery. Hence we proposed a
new algorithm for SAR image segmentation. In this paper we propose segmentation
using vector quantization technique on entropy image. Initially, we obtain
entropy image and in second step we use Kekre's Fast Codebook Generation (KFCG)
algorithm for segmentation of the entropy image. Thereafter, a codebook of size
128 was generated for the Entropy image. These code vectors were further
clustered in 8 clusters using same KFCG algorithm and converted into 8 images.
These 8 images were displayed as a result. This approach does not lead to over
segmentation or under segmentation. We compared these results with well known
Gray Level Co-occurrence Matrix. The proposed algorithm gives better
segmentation with less complexity.
</summary>
    <author>
      <name>H. B. Kekre</name>
    </author>
    <author>
      <name>Saylee Gharge</name>
    </author>
    <author>
      <name>Tanuja K. Sarode</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 7 No. 3, March 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.1789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.1789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.4457v1</id>
    <updated>2010-04-26T09:57:23Z</updated>
    <published>2010-04-26T09:57:23Z</published>
    <title>Combination of Subtractive Clustering and Radial Basis Function in
  Speaker Identification</title>
    <summary>  Speaker identification is the process of determining which registered speaker
provides a given utterance. Speaker identification required to make a claim on
the identity of speaker from the Ns trained speaker in its user database. In
this study, we propose the combination of clustering algorithm and the
classification technique - subtractive and Radial Basis Function (RBF). The
proposed technique is chosen because RBF is a simpler network structures and
faster learning algorithm. RBF finds the input to output map using the local
approximators which will combine the linear of the approximators and cause the
linear combiner have few weights. Besides that, RBF neural network model using
subtractive clustering algorithm for selecting the hidden node centers, which
can achieve faster training speed. In the meantime, the RBF network was trained
with a regularization term so as to minimize the variances of the nodes in the
hidden layer and perform more accu-rate prediction.
</summary>
    <author>
      <name>Ibrahim A. Albidewi</name>
    </author>
    <author>
      <name>Yap Teck Ann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing online at
  https://sites.google.com/site/journalofcomputing/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 2, Issue 4, April 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.4457v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.4457v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.4459v1</id>
    <updated>2010-04-26T10:01:53Z</updated>
    <published>2010-04-26T10:01:53Z</published>
    <title>Visual Infrared Video Fusion for Night Vision using Background
  Estimation</title>
    <summary>  Video fusion is a process that combines visual data from different sensors to
obtain a single composite video preserving the information of the sources. The
availability of a system, enhancing human ability to perceive the observed
scenario, is crucial to improve the performance of a surveillance system. The
infrared (IR) camera captures thermal image of object in night-time
environment, when only limited visual information can be captured by RGB
camera. The fusion of data recorded by an IR sensor and a visible RGB camera
can produce information otherwise not obtainable by viewing the sensor outputs
separately. In this paper we consider the problem of fusing two video streams
acquired by an RGB camera and an IR sensor. The pedestrians, distinctly
captured by IR video, are separated and fused with the RGB video. The
algorithms implemented involve estimation of the background, followed by
detection of object from the IR Video, after necessary denoising. Finally a
suitable fusion algorithm is employed to combine the extracted pedestrians with
the visual output. The obtained results clearly demonstrate the effectiveness
of the proposed video fusion scheme, for night vision.
</summary>
    <author>
      <name>Anjali Malviya</name>
    </author>
    <author>
      <name>S. G. Bhirud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">https://sites.google.com/site/journalofcomputing/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 2, Issue 4, April 2010, 66-69</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.4459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.4459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.1757v1</id>
    <updated>2010-05-11T08:42:18Z</updated>
    <published>2010-05-11T08:42:18Z</published>
    <title>Architecture for Cooperative Prefetching in P2P Video-on- Demand System</title>
    <summary>  Most P2P VoD schemes focused on service architectures and overlays
optimization without considering segments rarity and the performance of
prefetching strategies. As a result, they cannot better support VCRoriented
service in heterogeneous environment having clients using free VCR controls.
Despite the remarkable popularity in VoD systems, there exist no prior work
that studies the performance gap between different prefetching strategies. In
this paper, we analyze and understand the performance of different prefetching
strategies. Our analytical characterization brings us not only a better
understanding of several fundamental tradeoffs in prefetching strategies, but
also important insights on the design of P2P VoD system. On the basis of this
analysis, we finally proposed a cooperative prefetching strategy called
"cooching". In this strategy, the requested segments in VCR interactivities are
prefetched into session beforehand using the information collected through
gossips. We evaluate our strategy through extensive simulations. The results
indicate that the proposed strategy outperforms the existing prefetching
mechanisms.
</summary>
    <author>
      <name>Ubaid Abbasi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Bordeaux, France</arxiv:affiliation>
    </author>
    <author>
      <name>Toufik Ahmed</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Bordeaux, France</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcnc.2010.2310</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcnc.2010.2310" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 Pages, IJCNC</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Networks &amp; Communications 2.3
  (2010) 126-138</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.1757v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.1757v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.4014v1</id>
    <updated>2010-05-21T17:18:29Z</updated>
    <published>2010-05-21T17:18:29Z</published>
    <title>A Study on Potential of Integrating Multimodal Interaction into Musical
  Conducting Education</title>
    <summary>  With the rapid development of computer technology, computer music has begun
to appear in the laboratory. Many potential utility of computer music is
gradually increasing. The purpose of this paper is attempted to analyze the
possibility of integrating multimodal interaction such as vision-based hand
gesture and speech interaction into musical conducting education. To achieve
this purpose, this paper is focus on discuss some related research and the
traditional musical conducting education. To do so, six musical conductors had
been interviewed to share their musical conducting learning/ teaching
experience. These interviews had been analyzed in this paper to show the
syllabus and the focus of musical conducting education for beginners.
</summary>
    <author>
      <name>Gilbert Phuah Leong Siang</name>
    </author>
    <author>
      <name>Nor Azman Ismail</name>
    </author>
    <author>
      <name>Pang Yee Yong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://www.journalofcomputing.org</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Computing, Volume 2, Issue 5, May 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.4014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.4014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.4267v1</id>
    <updated>2010-05-24T07:28:24Z</updated>
    <published>2010-05-24T07:28:24Z</published>
    <title>Content Base Image Retrieval Using Phong Shading</title>
    <summary>  The digital image data is rapidly expanding in quantity and heterogeneity.
The traditional information retrieval techniques does not meet the user's
demand, so there is need to develop an efficient system for content based image
retrieval. Content based image retrieval means retrieval of images from
database on the basis of visual features of image like as color, texture etc.
In our proposed method feature are extracted after applying Phong shading on
input image. Phong shading, flattering out the dull surfaces of the image The
features are extracted using color, texture &amp; edge density methods. Feature
extracted values are used to find the similarity between input query image and
the data base image. It can be measure by the Euclidean distance formula. The
experimental result shows that the proposed approach has a better retrieval
results with phong shading.
</summary>
    <author>
      <name>Uday Pratap Singh</name>
    </author>
    <author>
      <name>Sanjeev Jain</name>
    </author>
    <author>
      <name>Gulfishan Firdose Ahmed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Publication format, International Journal of Computer Science
  and Information Security, IJCSIS, Vol. 8 No. 1, April 2010, USA. ISSN 1947
  5500, http://sites.google.com/site/ijcsis/</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.4267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.4267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.5436v1</id>
    <updated>2010-05-29T08:00:39Z</updated>
    <published>2010-05-29T08:00:39Z</published>
    <title>Client-to-Client Streaming Scheme for VOD Applications</title>
    <summary>  In this paper, we propose an efficient client-to-client streaming approach to
cooperatively stream the video using chaining technique with unicast
communication among the clients. This approach considers two major issues of
VoD 1) Prefix caching scheme to accommodate more number of videos closer to
client, so that the request-service delay for the user can be minimized. 2)
Cooperative proxy and client chaining scheme for streaming the videos using
unicasting. This approach minimizes the client rejection rate and bandwidth
requirement on server to proxy and proxy to client path. Our simulation results
show that the proposed approach achieves reduced client waiting time and
optimal prefix caching of videos minimizing server to proxy path bandwidth
usage by utilizing the client to client bandwidth, which is occasionally used
when compared to busy server to proxy path bandwidth.
</summary>
    <author>
      <name>M. Dakshayini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Dr.MGR University, India</arxiv:affiliation>
    </author>
    <author>
      <name>T. R. Gopala Krishnan Nair</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Research and Industry Incubation Centre - Bangalore, India</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijma.2010.2204</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijma.2010.2204" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 Pages, IJMA</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International journal of Multimedia &amp; Its Applications 2.2 (2010)
  46-55</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.5436v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.5436v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.5613v1</id>
    <updated>2010-05-31T08:11:59Z</updated>
    <published>2010-05-31T08:11:59Z</published>
    <title>An Automated Algorithm for Approximation of Temporal Video Data Using
  Linear B'EZIER Fitting</title>
    <summary>  This paper presents an efficient method for approximation of temporal video
data using linear Bezier fitting. For a given sequence of frames, the proposed
method estimates the intensity variations of each pixel in temporal dimension
using linear Bezier fitting in Euclidean space. Fitting of each segment ensures
upper bound of specified mean squared error. Break and fit criteria is employed
to minimize the number of segments required to fit the data. The proposed
method is well suitable for lossy compression of temporal video data and
automates the fitting process of each pixel. Experimental results show that the
proposed method yields good results both in terms of objective and subjective
quality measurement parameters without causing any blocking artifacts.
</summary>
    <author>
      <name>Murtaza Ali Khan</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Royal University for Women, Bahrain</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijma.2010.2207</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijma.2010.2207" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 Pages, IJMA 2010</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International journal of Multimedia &amp; Its Applications 2.2 (2010)
  81-94</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1005.5613v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.5613v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.1233v1</id>
    <updated>2010-07-01T09:40:40Z</updated>
    <published>2010-07-01T09:40:40Z</published>
    <title>An Alternative Approach of Steganography using Reference Image</title>
    <summary>  This paper is to create a practical steganographic implementation for 4-bit
images.The proposed technique converts 4 bit image into 4 shaded Gray Scale
image. This image will be act as reference image to hide the text. Using this
grey scale reference image any text can be hidden. Single character of a text
can be represented by 8-bit. The 8-bit character can be split into 4X2 bit
information. If the reference image and the data file are transmitted through
network separately, we can achieve the effect of Steganography. Here the image
is not at all distorted because said image is only used for referencing. Any
huge mount of text material can be hidden using a very small image. Decipher
the text is not possible intercepting the image or data file separately. So, it
is more secure.
</summary>
    <author>
      <name>Samir Kumar Bandyopadhyay</name>
    </author>
    <author>
      <name>Indra Kanta Maitra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://ijict.org/index.php/ijoat/article/view/approach-of-stegnagraphy .
  arXiv admin note: text overlap with
  http://dx.doi.org/10.1016/j.sigpro.2009.08.010 by other authors without
  attribution</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advancements in Technology, Vol 1, No 1
  (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1007.1233v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.1233v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.5136v1</id>
    <updated>2010-07-29T07:41:51Z</updated>
    <published>2010-07-29T07:41:51Z</published>
    <title>Perceptual Copyright Protection Using Multiresolution Wavelet-Based
  Watermarking And Fuzzy Logic</title>
    <summary>  In this paper, an efficiently DWT-based watermarking technique is proposed to
embed signatures in images to attest the owner identification and discourage
the unauthorized copying. This paper deals with a fuzzy inference filter to
choose the larger entropy of coefficients to embed watermarks. Unlike most
previous watermarking frameworks which embedded watermarks in the larger
coefficients of inner coarser subbands, the proposed technique is based on
utilizing a context model and fuzzy inference filter by embedding watermarks in
the larger-entropy coefficients of coarser DWT subbands. The proposed
approaches allow us to embed adaptive casting degree of watermarks for
transparency and robustness to the general image-processing attacks such as
smoothing, sharpening, and JPEG compression. The approach has no need the
original host image to extract watermarks. Our schemes have been shown to
provide very good results in both image transparency and robustness.
</summary>
    <author>
      <name>Ming-Shing Hsieh</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Aletheia University, Taiwan</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijaia.2010.1304</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijaia.2010.1304" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Artificial Intelligence &amp; Applications
  1.3 (2010) 45-57</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1007.5136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.5136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.0757v1</id>
    <updated>2010-08-04T12:01:39Z</updated>
    <published>2010-08-04T12:01:39Z</published>
    <title>Web Video Categorization based on Wikipedia Categories and
  Content-Duplicated Open Resources</title>
    <summary>  This paper presents a novel approach for web video categorization by
leveraging Wikipedia categories (WikiCs) and open resources describing the same
content as the video, i.e., content-duplicated open resources (CDORs). Note
that current approaches only col-lect CDORs within one or a few media forms and
ignore CDORs of other forms. We explore all these resources by utilizing WikiCs
and commercial search engines. Given a web video, its discrimin-ative Wikipedia
concepts are first identified and classified. Then a textual query is
constructed and from which CDORs are collected. Based on these CDORs, we
propose to categorize web videos in the space spanned by WikiCs rather than
that spanned by raw tags. Experimental results demonstrate the effectiveness of
both the proposed CDOR collection method and the WikiC voting catego-rization
algorithm. In addition, the categorization model built based on both WikiCs and
CDORs achieves better performance compared with the models built based on only
one of them as well as state-of-the-art approach.
</summary>
    <author>
      <name>Zhineng Chen</name>
    </author>
    <author>
      <name>Juan Cao</name>
    </author>
    <author>
      <name>Yicheng Song</name>
    </author>
    <author>
      <name>Yongdong Zhang</name>
    </author>
    <author>
      <name>Jintao Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1008.0757v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.0757v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1008.3741v2</id>
    <updated>2012-03-04T22:07:22Z</updated>
    <published>2010-08-23T03:06:22Z</published>
    <title>Reliable Multicasting for Device-to-Device Radio Underlaying Cellular
  Networks</title>
    <summary>  This paper proposes Leader in Charge (LiC), a reliable multicast architecture
for device-to-device (D2D) radio underlaying cellular networks. The
multicast-requesting user equipments (UEs) in close proximity form a D2D
cluster to receive the multicast packets through cooperation. In addition to
receiving the multicast packets from the eNB, UEs share what they received from
the multicast on short-range links among UEs, namely the D2D links, to exploit
the wireless resources a more efficient way. Consequently, we show that
utilizing the D2D links in cellular networks increases the throughput of a
multicast session by means of simulation. We also discuss some practical issues
facing the integration of LiC into the current cellular networks. In
particular, we propose efficient delay control mechanism to reduce the average
and maximum delay experienced by LiC users, which is further confirmed by the
simulation results.
</summary>
    <author>
      <name>Wei Yang</name>
    </author>
    <author>
      <name>Wanlu Sun</name>
    </author>
    <author>
      <name>Lihua Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/1008.3741v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1008.3741v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.1170v1</id>
    <updated>2010-09-06T22:33:38Z</updated>
    <published>2010-09-06T22:33:38Z</published>
    <title>M-Learning: A New Paradigm of Learning Mathematics in Malaysia</title>
    <summary>  M-Learning is a new learning paradigm of the new social structure with mobile
and wireless technologies.Smart school is one of the four flagship applications
for Multimedia Super Corridor (MSC) under Malaysian government initiative to
improve education standard in the country. With the advances of mobile devices
technologies, mobile learning could help the government in realizing the
initiative. This paper discusses the prospect of implementing mobile learning
for primary school students. It indicates significant and challenges and
analysis of user perceptions on potential mobile applications through a survey
done in primary school context. The authors propose the m-Learning for
mathematics by allowing the extension of technology in the traditional
classroom in term of learning and teaching.
</summary>
    <author>
      <name>Saipunidzam Mahamad</name>
    </author>
    <author>
      <name>Mohammad Noor Ibrahim</name>
    </author>
    <author>
      <name>Shakirah Mohd Taib</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsit.2010.2407</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsit.2010.2407" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Wireless technology, teaching mathematics, flexible learning,
  m-Learning</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International journal of computer science &amp; information Technology
  (IJCSIT) Vol 2 No 4 (2010) pp 76-86</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.1170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.1170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.1882v1</id>
    <updated>2010-12-08T21:45:16Z</updated>
    <published>2010-12-08T21:45:16Z</published>
    <title>Evaluating Modelling Approaches for Medical Image Annotations</title>
    <summary>  Information system designers face many challenges w.r.t. selecting
appropriate semantic technologies and deciding on a modelling approach for
their system. However, there is no clear methodology yet to evaluate
"semantically enriched" information systems. In this paper we present a case
study on different modelling approaches for annotating medical images and
introduce a conceptual framework that can be used to analyse the fitness of
information systems and help designers to spot the strengths and weaknesses of
various modelling approaches as well as managing trade-offs between modelling
effort and their potential benefits.
</summary>
    <author>
      <name>Jasmin Opitz</name>
    </author>
    <author>
      <name>Bijan Parsia</name>
    </author>
    <author>
      <name>Ulrike Sattler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Adrian Paschke, Albert Burger, Andrea Splendiani, M. Scott
  Marshall, Paolo Romano: Proceedings of the 3rd International Workshop on
  Semantic Web Applications and Tools for the Life Sciences, Berlin,Germany,
  December 8-10, 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.1882v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.1882v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.2965v1</id>
    <updated>2010-12-14T08:50:29Z</updated>
    <published>2010-12-14T08:50:29Z</published>
    <title>Digital watermarking : An approach based on Hilbert transform</title>
    <summary>  Most of the well known algorithms for watermarking of digital images involve
transformation of the image data to Fourier or singular vector space. In this
paper, we introduce watermarking in Hilbert transform domain for digital media.
Generally, if the image is a matrix of order $m$ by $n$, then the transformed
space is also an image of the same order. However, with Hilbert transforms, the
transformed space is of order $2m$ by $2n$. This allows for more latitude in
storing the watermark in the host image. Based on this idea, we propose an
algorithm for embedding and extracting watermark in a host image and
analytically obtain a parameter related to this procedure. Using extensive
simulations, we show that the algorithm performs well even if the host image is
corrupted by various attacks.
</summary>
    <author>
      <name>Rashmi Agarwal</name>
    </author>
    <author>
      <name>R. Krishnan</name>
    </author>
    <author>
      <name>M. S. Santhanam</name>
    </author>
    <author>
      <name>K. Srinivas</name>
    </author>
    <author>
      <name>K. Venugopalan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 Pages, 52 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.2965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.2965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.5127v1</id>
    <updated>2011-01-13T15:01:12Z</updated>
    <published>2011-01-13T15:01:12Z</published>
    <title>A Color Image Digital Watermarking Scheme Based on SOFM</title>
    <summary>  Digital watermarking technique has been presented and widely researched to
solve some important issues in the digital world, such as copyright protection,
copy protection and content authentication. Several robust watermarking schemes
based on vector quantization (VQ) have been presented. In this paper, we
present a new digital image watermarking method based on SOFM vector quantizer
for color images. This method utilizes the codebook partition technique in
which the watermark bit is embedded into the selected VQ encoded block. The
main feature of this scheme is that the watermark exists both in VQ compressed
image and in the reconstructed image. The watermark extraction can be performed
without the original image. The watermark is hidden inside the compressed
image, so much transmission time and storage space can be saved when the
compressed data are transmitted over the Internet. Simulation results
demonstrate that the proposed method has robustness against various image
processing operations without sacrificing compression performance and the
computational speed.
</summary>
    <author>
      <name>J. Anitha</name>
    </author>
    <author>
      <name>S. Immanuel Alex Pandian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues online at
  http://www.ijcsi.org</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI, Volume 7, Issue 5, September 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1101.5127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.5127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.5699v1</id>
    <updated>2011-02-28T16:13:20Z</updated>
    <published>2011-02-28T16:13:20Z</published>
    <title>Ontology based approach for video transmission over the network</title>
    <summary>  With the increase in the bandwidth &amp; the transmission speed over the
internet, transmission of multimedia objects like video, audio, images has
become an easier work. In this paper we provide an approach that can be useful
for transmission of video objects over the internet without much fuzz. The
approach provides a ontology based framework that is used to establish an
automatic deployment of video transmission system. Further the video is
compressed using the structural flow mechanism that uses the wavelet principle
for compression of video frames. Finally the video transmission algorithm known
as RRDBFSF algorithm is provided that makes use of the concept of restrictive
flooding to avoid redundancy thereby increasing the efficiency.
</summary>
    <author>
      <name>Rachit Mohan Garg</name>
    </author>
    <author>
      <name>Yamini Sood</name>
    </author>
    <author>
      <name>Neha Tyagi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijma.2011.3106</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijma.2011.3106" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, 4 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The International journal of Multimedia &amp; Its Applications (IJMA)
  Vol.3, No.1, February 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1102.5699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.5699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.5769v1</id>
    <updated>2011-02-28T20:47:08Z</updated>
    <published>2011-02-28T20:47:08Z</published>
    <title>Multimedia Database Applications: Issues and Concerns for Classroom
  Teaching</title>
    <summary>  The abundance of multimedia data and information is challenging educators to
effectively search, browse, access, use, and store the data for their classroom
teaching. However, many educators could still be accustomed to teaching or
searching for information using conventional methods, but often the
conventional methods may not function well with multimedia data. Educators need
to efficiently interact and manage a variety of digital media files too. The
purpose of this study is to review current multimedia database applications in
teaching and learning, and further discuss some of the issues or concerns that
educators may have while incorporating multimedia data into their classrooms.
Some strategies and recommendations are also provided in order for educators to
be able to use multimedia data more effectively in their teaching environments.
</summary>
    <author>
      <name>Chien Yu</name>
    </author>
    <author>
      <name>Teri Brandenburg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages and 22 references</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The International Journal of Multimedia &amp; Its Applications, 3(1),
  2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1102.5769v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.5769v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.0065v1</id>
    <updated>2011-03-01T02:02:54Z</updated>
    <published>2011-03-01T02:02:54Z</published>
    <title>Interdisciplinary Collaboration through Designing 3D Simulation Case
  Studies</title>
    <summary>  Interdisciplinary collaboration is essential for the advance of research. As
domain subjects become more and more specialized, researchers need to cross
disciplines for insights from peers in other areas to have a broader and deeper
understand of a topic at micro- and macro-levels. We developed a 3D virtual
learning environment that served as a platform for faculty to plan curriculum,
share educational beliefs, and conduct cross-discipline research for effective
learning. Based upon the scripts designed by faculty from five disciplines,
virtual doctors, nurses, or patients interact in a 3D virtual hospital. The
teaching vignettes were then converted to video clips, allowing users to view,
pause, replay, or comment on the videos individually or in groups. Unlike many
existing platforms, we anticipated a value-added by adding a social networking
capacity to this virtual environment. The focus of this paper is on the
cost-efficiency and system design of the virtual learning environment.
</summary>
    <author>
      <name>Xin Bai</name>
    </author>
    <author>
      <name>Dana Fusco</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijma.2011.3109</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijma.2011.3109" rel="related"/>
    <link href="http://arxiv.org/abs/1103.0065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.0065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.0829v1</id>
    <updated>2011-03-04T05:38:59Z</updated>
    <published>2011-03-04T05:38:59Z</published>
    <title>Hiding Secret Information in Movie Clip: A Steganographic Approach</title>
    <summary>  Establishing hidden communication is an important subject of discussion that
has gained increasing importance nowadays with the development of the internet.
One of the key methods for establishing hidden communication is steganography.
Modern day steganography mainly deals with hiding information within files like
image, text, html, binary files etc. These file contains small irrelevant
information that can be substituted for small secret data. To store a high
capacity secret data these carrier files are not very supportive. To overcome
the problem of storing the high capacity secret data with the utmost security
fence, we have proposed a novel methodology for concealing a voluminous data
with high levels of security wall by using movie clip as a carrier file.
</summary>
    <author>
      <name>G. Sahoo</name>
    </author>
    <author>
      <name>Rajesh Kumar Tiwari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Steganography, Frame, Stego-Frame, Stego-key, and Carrier</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computing and Applications, Vol. 4, No.
  1, June 2009, pp. 87-94</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1103.0829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.0829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.0837v1</id>
    <updated>2011-03-04T07:40:44Z</updated>
    <published>2011-03-04T07:40:44Z</published>
    <title>Priority based Interface Selection for Overlaying Heterogeneous Networks</title>
    <summary>  Offering of different attractive opportunities by different wireless
technologies trends the convergence of heterogeneous networks for the future
wireless communication system. To make a seamless handover among the
heterogeneous networks, the optimization of the power consumption, and optimal
selection of interface are the challenging issues for convergence networks. The
access of multi interfaces simultaneously reduces the handover latency and data
loss in heterogeneous handover. The mobile node (MN) maintains one interface
connection while other interface is used for handover process. However, it
causes much battery power consumption. In this paper we propose an efficient
interface selection scheme including interface selection algorithms, interface
selection procedures considering battery power consumption and user mobility
with other existing parameters for overlaying networks. We also propose a
priority based network selection scheme according to the service types. MN's
battery power level, provision of QoS/QoE in the target network and our
proposed priority parameters are considered as more important parameters for
our interface selection algorithm. The performances of the proposed scheme are
verified using numerical analysis.
</summary>
    <author>
      <name>Mostafa Zaman Chowdhury</name>
    </author>
    <author>
      <name>Yeong Min Jang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The Journal of Korea Information and Communications Soceity
  (KICS,), Vol. 35, No. 7, July 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1103.0837v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.0837v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.3802v1</id>
    <updated>2011-03-19T18:43:30Z</updated>
    <published>2011-03-19T18:43:30Z</published>
    <title>Stage Staffing Scheme for Copyright Protection in Multimedia</title>
    <summary>  Copyright protection has become a need in today's world. To achieve a secure
copyright protection we embedded some information in images and videos and that
image or video is called copyright protected. The embedded information can't be
detected by human eye but some attacks and operations can tamper that
information to breach protection. So in order to find a secure technique of
copyright protection, we have analyzed image processing techniques i.e. Spatial
Domain (Least Significant Bit (LSB)), Transform Domain (Discrete Cosine
Transform (DCT)), Discrete Wavelet Transform (DWT) and there are numerous
algorithm for watermarking using them. After having a good understanding of the
same we have proposed a novel algorithm named as Stage Staffing Algorithm that
generates results with high effectiveness, additionally we can use self
extracted-watermark technique to increase the security and automate the process
of watermark image. The proposed algorithm provides protection in three stages.
We have implemented the algorithm and results of the simulations are shown. The
various factors affecting spatial domain watermarking are also discussed.
</summary>
    <author>
      <name>Sumit Kumar</name>
    </author>
    <author>
      <name>Santosh Kumar</name>
    </author>
    <author>
      <name>Sukumar Nandi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Network Security &amp; Its Applications
  (IJNSA), Vol.3, No.2, March 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1103.3802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.3802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.0809v1</id>
    <updated>2011-04-05T11:06:30Z</updated>
    <published>2011-04-05T11:06:30Z</published>
    <title>SLDs for Visualizing Multicolor Elevation Contour Lines in Geo-Spatial
  Web Applications</title>
    <summary>  This paper addresses the need for geospatial consumers (either humans or
machines) to visualize multicolored elevation contour poly lines with respect
their different contour intervals and control the visual portrayal of the data
with which they work. The current OpenGIS Web Map Service (WMS) specification
supports the ability for an information provider to specify very basic styling
options by advertising a preset collection of visual portrayals for each
available data set. However, while a WMS currently can provide the user with a
choice of style options, the WMS can only tell the user the name of each style.
It cannot tell the user what portrayal will look like on the map. More
importantly, the user has no way of defining their own styling rules. The
ability for a human or machine client to define these rules requires a styling
language that the client and server can both understand. Defining this
language, called the StyledLayerDescriptor (SLD), is the main focus of this
paper, and it can be used to portray the output of Web Map Servers, Web Feature
Servers and Web Coverage Servers.
</summary>
    <author>
      <name>B. G. Kodge</name>
    </author>
    <author>
      <name>P. S. Hiremath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">World of Computer Science and Information Technology Journal
  (WCSIT), ISSN: 2221-0741, Vol. 1, No. 2, 39-43, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1104.0809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.0809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.4451v1</id>
    <updated>2011-06-22T14:01:51Z</updated>
    <published>2011-06-22T14:01:51Z</published>
    <title>Activities of Daily Living Indexing by Hierarchical HMM for Dementia
  Diagnostics</title>
    <summary>  This paper presents a method for indexing human ac- tivities in videos
captured from a wearable camera being worn by patients, for studies of
progression of the dementia diseases. Our method aims to produce indexes to
facilitate the navigation throughout the individual video recordings, which
could help doctors search for early signs of the dis- ease in the activities of
daily living. The recorded videos have strong motion and sharp lighting
changes, inducing noise for the analysis. The proposed approach is based on a
two steps analysis. First, we propose a new approach to segment this type of
video, based on apparent motion. Each segment is characterized by two original
motion de- scriptors, as well as color, and audio descriptors. Second, a
Hidden-Markov Model formulation is used to merge the multimodal audio and video
features, and classify the test segments. Experiments show the good properties
of the ap- proach on real data.
</summary>
    <author>
      <name>Svebor Karaman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Jenny Benois-Pineau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-Fran√ßois Dartigues</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISPED</arxiv:affiliation>
    </author>
    <author>
      <name>Yann Ga√´stel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISPED</arxiv:affiliation>
    </author>
    <author>
      <name>R√©mi M√©gret</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMS</arxiv:affiliation>
    </author>
    <author>
      <name>Julien Pinquier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IRIT</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CBMI.2011.5972524</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CBMI.2011.5972524" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2011 9th International Workshop on Content-Based Multimedia Indexing
  (CBMI), Madrid : Spain (2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.4451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.4451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1107.2222v1</id>
    <updated>2011-07-12T09:21:37Z</updated>
    <published>2011-07-12T09:21:37Z</published>
    <title>Study of a Hybrid - Analog TV and Ethernet- Home Data Link using a
  Coaxial Cable</title>
    <summary>  The paper presents an implementation and compatibility tests of a simple home
network implemented in a nonconventional manner using a CATV coaxial cable.
Reusing the cable, normally designated to supply RF modulated TV signals from
cable TV networks, makes possible to add data services as well. A short
presentation of the technology is given with an investigation of the main
performances obtained using this technique. The measurements revealed that this
simple solution makes possible to have both TV and data services with
performances close to traditional home data services: cable modems or ADSL,
with minimal investments. This technology keeps also open the possibility for
future improvements of the network: DVB-C or Data via Cable Modems.
</summary>
    <author>
      <name>Radu Arsinte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 12 figures; Acta Technica Napocensis, Electronics and
  telecommunications, No.1/2007</arxiv:comment>
    <link href="http://arxiv.org/abs/1107.2222v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1107.2222v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.1145v1</id>
    <updated>2011-09-06T11:12:55Z</updated>
    <published>2011-09-06T11:12:55Z</published>
    <title>A Survey on Web Multimedia Mining</title>
    <summary>  Modern developments in digital media technologies has made transmitting and
storing large amounts of multi/rich media data (e.g. text, images, music, video
and their combination) more feasible and affordable than ever before. However,
the state of the art techniques to process, mining and manage those rich media
are still in their infancy. Advances developments in multimedia acquisition and
storage technology the rapid progress has led to the fast growing incredible
amount of data stored in databases. Useful information to users can be revealed
if these multimedia files are analyzed. Multimedia mining deals with the
extraction of implicit knowledge, multimedia data relationships, or other
patterns not explicitly stored in multimedia files. Also in retrieval, indexing
and classification of multimedia data with efficient information fusion of the
different modalities is essential for the system's overall performance. The
purpose of this paper is to provide a systematic overview of multimedia mining.
This article is also represents the issues in the application process component
for multimedia mining followed by the multimedia mining models.
</summary>
    <author>
      <name>Pravin M. Kamde</name>
    </author>
    <author>
      <name>Dr. Siddu. P. Algur</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijma</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijma" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 Pages; The International Journal of Multimedia &amp; Its Applications
  (IJMA) Vol.3, No.3, August 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.1145v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.1145v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.2325v1</id>
    <updated>2011-09-11T16:24:00Z</updated>
    <published>2011-09-11T16:24:00Z</published>
    <title>Secured color image watermarking technique in DWT-DCT domain</title>
    <summary>  The multilayer secured DWT-DCT and YIQ color space based image watermarking
technique with robustness and better correlation is presented here. The
security levels are increased by using multiple pn sequences, Arnold
scrambling, DWT domain, DCT domain and color space conversions. Peak signal to
noise ratio and Normalized correlations are used as measurement metrics. The
512x512 sized color images with different histograms are used for testing and
watermark of size 64x64 is embedded in HL region of DWT and 4x4 DCT is used.
'Haar' wavelet is used for decomposition and direct flexing factor is used. We
got PSNR value is 63.9988 for flexing factor k=1 for Lena image and the maximum
NC 0.9781 for flexing factor k=4 in Q color space. The comparative performance
in Y, I and Q color space is presented. The technique is robust for different
attacks like scaling, compression, rotation etc.
</summary>
    <author>
      <name>Baisa L. Gunjal</name>
    </author>
    <author>
      <name>Suresh N. Mali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages; International Journal of Computer Science, Engineering and
  Information Technology (IJCSEIT), Vol.1, No. 3, August 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.2325v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.2325v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.6895v1</id>
    <updated>2011-10-31T18:44:41Z</updated>
    <published>2011-10-31T18:44:41Z</published>
    <title>Multi-Layer Local Graph Words for Object Recognition</title>
    <summary>  In this paper, we propose a new multi-layer structural approach for the task
of object based image retrieval. In our work we tackle the problem of
structural organization of local features. The structural features we propose
are nested multi-layered local graphs built upon sets of SURF feature points
with Delaunay triangulation. A Bag-of-Visual-Words (BoVW) framework is applied
on these graphs, giving birth to a Bag-of-Graph-Words representation. The
multi-layer nature of the descriptors consists in scaling from trivial Delaunay
graphs - isolated feature points - by increasing the number of nodes layer by
layer up to graphs with maximal number of nodes. For each layer of graphs its
own visual dictionary is built. The experiments conducted on the SIVAL and
Caltech-101 data sets reveal that the graph features at different layers
exhibit complementary performances on the same content and perform better than
baseline BoVW approach. The combination of all existing layers, yields
significant improvement of the object recognition performance compared to
single level approaches.
</summary>
    <author>
      <name>Svebor Karaman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Jenny Benois-Pineau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>R√©mi M√©gret</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IMS</arxiv:affiliation>
    </author>
    <author>
      <name>Aur√©lie Bugeau</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-27355-1_6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-27355-1_6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on MultiMedia Modeling, Klagenfurt :
  Autriche (2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1110.6895v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.6895v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.0242v1</id>
    <updated>2011-11-01T16:52:10Z</updated>
    <published>2011-11-01T16:52:10Z</published>
    <title>Storage Balancing in Self-organizing Multimedia Delivery Systems</title>
    <summary>  Many of the current bio-inspired delivery networks set their focus on search,
e.g., by using artificial ants. If the network size and, therefore, the search
space gets too large, the users experience high delays until the requested
content can be consumed. In previous work, we proposed different replication
strategies to reduce the search space. In this report we further evaluate
measures for storage load balancing, because peers are most likely limited in
space. We periodically apply clean-ups if a certain storage level is reached.
For our evaluations we combine the already introduced replication measures with
least recently used (LRU), least frequently used (LFU) and a hormone-based
clean-up. The goal is to elaborate a combination that leads to low delays while
the replica utilization is high.
</summary>
    <author>
      <name>Anita Sobe</name>
    </author>
    <author>
      <name>Wilfried Elmenreich</name>
    </author>
    <author>
      <name>Laszlo B√∂sz√∂rmenyi</name>
    </author>
    <link href="http://arxiv.org/abs/1111.0242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.0242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.2993v1</id>
    <updated>2011-11-13T07:13:23Z</updated>
    <published>2011-11-13T07:13:23Z</published>
    <title>A Survey on Web-based AR Applications</title>
    <summary>  Due to the increase of interest in Augmented Reality (AR), the potential uses
of AR are increasing also. It can benefit the user in various fields such as
education, business, medicine, and other. Augmented Reality supports the real
environment with synthetic environment to give more details and meaning to the
objects in the real word. AR refers to a situation in which the goal is to
supplement a user's perception of the real-world through the addition of
virtual objects. This paper is an attempt to make a survey of web-based
Augmented Reality applications and make a comparison among them.
</summary>
    <author>
      <name>Behrang Parhizkar</name>
    </author>
    <author>
      <name>Ashraf Abbas M. Al-Modwahi</name>
    </author>
    <author>
      <name>Arash Habibi Lashkari</name>
    </author>
    <author>
      <name>Mohammad Mehdi Bartaripou</name>
    </author>
    <author>
      <name>Hossein Reza Babae</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues (IJCSI), Vol. 8,
  Issue 4, July 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.2993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.2993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.6727v1</id>
    <updated>2011-11-29T08:37:08Z</updated>
    <published>2011-11-29T08:37:08Z</published>
    <title>A New Digital Watermarking Algorithm Using Combination of Least
  Significant Bit (LSB) and Inverse Bit</title>
    <summary>  In this paper, we introduce a new digital watermarking algorithm using least
significant bit (LSB). LSB is used because of its little effect on the image.
This new algorithm is using LSB by inversing the binary values of the watermark
text and shifting the watermark according to the odd or even number of pixel
coordinates of image before embedding the watermark. The proposed algorithm is
flexible depending on the length of the watermark text. If the length of the
watermark text is more than ((MxN)/8)-2 the proposed algorithm will also embed
the extra of the watermark text in the second LSB. We compare our proposed
algorithm with the 1-LSB algorithm and Lee's algorithm using Peak
signal-to-noise ratio (PSNR). This new algorithm improved its quality of the
watermarked image. We also attack the watermarked image by using cropping and
adding noise and we got good results as well.
</summary>
    <author>
      <name>Abdullah Bamatraf</name>
    </author>
    <author>
      <name>Rosziati Ibrahim</name>
    </author>
    <author>
      <name>Mohd. Najib Mohd. Salleh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures and 4 tables; Journal of Computing, Volume 3,
  Issue 4, April 2011, ISSN 2151-9617</arxiv:comment>
    <link href="http://arxiv.org/abs/1111.6727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.6727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.0598v1</id>
    <updated>2012-01-03T09:19:58Z</updated>
    <published>2012-01-03T09:19:58Z</published>
    <title>Interactive multiview video system with non-complex navigation at the
  decoder</title>
    <summary>  Multiview video with interactive and smooth view switching at the receiver is
a challenging application with several issues in terms of effective use of
storage and bandwidth resources, reactivity of the system, quality of the
viewing experience and system complexity. The classical decoding system for
generating virtual views first projects a reference or encoded frame to a given
viewpoint and then fills in the holes due to potential occlusions. This last
step still constitutes a complex operation with specific software or hardware
at the receiver and requires a certain quantity of information from the
neighboring frames for insuring consistency between the virtual images. In this
work we propose a new approach that shifts most of the burden due to
interactivity from the decoder to the encoder, by anticipating the navigation
of the decoder and sending auxiliary information that guarantees temporal and
interview consistency. This leads to an additional cost in terms of
transmission rate and storage, which we minimize by using optimization
techniques based on the user behavior modeling. We show by experiments that the
proposed system represents a valid solution for interactive multiview systems
with classical decoders.
</summary>
    <author>
      <name>Thomas Maugey</name>
    </author>
    <author>
      <name>Pascal Frossard</name>
    </author>
    <link href="http://arxiv.org/abs/1201.0598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.0598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.1383v1</id>
    <updated>2012-01-06T10:10:54Z</updated>
    <published>2012-01-06T10:10:54Z</published>
    <title>Stereo image Transference &amp; Retrieval over SMS</title>
    <summary>  Paper presents the way of transferring stereo images using SMS over GSM
network. Generally, Stereo image is composed of two stereoscopic images in such
way that gives three dimensional affect when viewed. GSM have two short
messaging services, which can transfer images and sounds etc. Such services are
known as; MMS (Multimedia Messaging Service) and EMS (Extended Messaging
Service). EMS can send Predefined sounds, animation and images but have
limitation that it does not support widely. MMS can send much higher contents
than EMS but need 3G and other network capability in order to send large size
data up to 1000 bytes. Other limitations are Portability, content adaption etc.
Our major aim in this paper is to provide an alternative way of sending stereo
images over SMS which is widely supported than EMS. We develop an application
using J2ME Platform.
</summary>
    <author>
      <name>Muhammad Fahad Khan</name>
    </author>
    <author>
      <name>Saira Beg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages,3 figuers,Journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">JOURNAL OF COMPUTING, VOLUME 3, ISSUE 7, JULY 2011, ISSN 2151-9617
  WWW.JOURNALOFCOMPUTING.ORG</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1201.1383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.1383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.1668v1</id>
    <updated>2012-01-08T23:45:27Z</updated>
    <published>2012-01-08T23:45:27Z</published>
    <title>Identifying and Analysis of Scene Mining Methods Beased on Scenes
  Extracted Features</title>
    <summary>  Scene mining is a subset of image mining in which scenes are classified to a
distinct set of classes based on analysis of their content. In other word in
scene mining, a label is given to visual content of scene, for example,
mountain, beach. Scene mining is used in applications such as medicine, movie,
information retrieval, computer vision, recognition of traffic scene. Reviewing
of represented methods shows there are various methods in scene mining. Scene
mining applications extension and existence of various scenes, make comparison
of methods hard. Scene mining can be followed by identifying scene mining
components and representing a framework to analyzing and evaluating methods. In
this paper, at first, components of scene mining are introduced, then a
framework based on extracted features of scene is represented to classify scene
mining methods. Finally, these methods are analyzed and evaluated via a
proposal framework.
</summary>
    <author>
      <name>Ashraf Sadat Jabari</name>
    </author>
    <author>
      <name>Mohammadreza Keyvanpour</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Engineering Science and Technology, Vol.
  3 No. 9 September 2011, 7211-7217</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1201.1668v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.1668v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.3018v1</id>
    <updated>2012-01-14T14:22:54Z</updated>
    <published>2012-01-14T14:22:54Z</published>
    <title>Throughput Scaling Of Convolution For Error-Tolerant Multimedia
  Applications</title>
    <summary>  Convolution and cross-correlation are the basis of filtering and pattern or
template matching in multimedia signal processing. We propose two throughput
scaling options for any one-dimensional convolution kernel in programmable
processors by adjusting the imprecision (distortion) of computation. Our
approach is based on scalar quantization, followed by two forms of tight
packing in floating-point (one of which is proposed in this paper) that allow
for concurrent calculation of multiple results. We illustrate how our approach
can operate as an optional pre- and post-processing layer for off-the-shelf
optimized convolution routines. This is useful for multimedia applications that
are tolerant to processing imprecision and for cases where the input signals
are inherently noisy (error tolerant multimedia applications). Indicative
experimental results with a digital music matching system and an MPEG-7 audio
descriptor system demonstrate that the proposed approach offers up to 175%
increase in processing throughput against optimized (full-precision)
convolution with virtually no effect in the accuracy of the results. Based on
marginal statistics of the input data, it is also shown how the throughput and
distortion can be adjusted per input block of samples under constraints on the
signal-to-noise ratio against the full-precision convolution.
</summary>
    <author>
      <name>Mohammad Ashraful Anam</name>
    </author>
    <author>
      <name>Yiannis Andreopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Trans. on Multimedia, 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.3018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.3018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.5285v1</id>
    <updated>2012-01-25T14:39:39Z</updated>
    <published>2012-01-25T14:39:39Z</published>
    <title>An Authoring System for Editing Lessons in Phonetic English in SMIL3.0</title>
    <summary>  One of the difficulties of teaching English is the prosody, including the
stress. French learners have difficulties to encode this information about the
word because it is irrelevant for them. Therefore, they have difficulty to
produce this stress when they speak that language. Studies in this area have
concluded that the dual-coding approach (auditory and visual) of a phonetic
phenomenon helps a lot to improve its perception and memorization for novice
learners. The aim of our work is to provide English teachers with an authoring
named SaCoPh for editing multimedia courses that support this approach. This
course is based on a template that fits the educational aspects of phonetics,
exploiting the features of version 3.0 of the standard SMIL (Synchronized
Multimedia Integration Language) for the publication of this course on the web.
</summary>
    <author>
      <name>G. Merzougui</name>
    </author>
    <author>
      <name>M. Djoudi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures and 2 codes</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 6, No 3, November 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1201.5285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.5285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.1808v1</id>
    <updated>2012-02-08T20:26:26Z</updated>
    <published>2012-02-08T20:26:26Z</published>
    <title>Personalised product design using virtual interactive techniques</title>
    <summary>  Use of Virtual Interactive Techniques for personalized product design is
described in this paper. Usually products are designed and built by considering
general usage patterns and Prototyping is used to mimic the static or working
behaviour of an actual product before manufacturing the product. The user does
not have any control on the design of the product. Personalized design
postpones design to a later stage. It allows for personalized selection of
individual components by the user. This is implemented by displaying the
individual components over a physical model constructed using Cardboard or
Thermocol in the actual size and shape of the original product. The components
of the equipment or product such as screen, buttons etc. are then projected
using a projector connected to the computer into the physical model. Users can
interact with the prototype like the original working equipment and they can
select, shape, position the individual components displayed on the interaction
panel using simple hand gestures. Computer Vision techniques as well as sound
processing techniques are used to detect and recognize the user gestures
captured using a web camera and microphone.
</summary>
    <author>
      <name>Kurien Zacharia</name>
    </author>
    <author>
      <name>Eldo P. Elias</name>
    </author>
    <author>
      <name>Surekha Mariam Varghese</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcga.2012.2101</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcga.2012.2101" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages; International Journal of Computer Graphics &amp; Animation
  (IJCGA) Vol.2, No.1, January 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.1808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.1808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68u05, 68u20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.2; I.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.4743v1</id>
    <updated>2012-02-21T20:42:17Z</updated>
    <published>2012-02-21T20:42:17Z</published>
    <title>Real-time detection and tracking of multiple objects with partial
  decoding in H.264/AVC bitstream domain</title>
    <summary>  In this paper, we show that we can apply probabilistic spatiotemporal
macroblock filtering (PSMF) and partial decoding processes to effectively
detect and track multiple objects in real time in H.264|AVC bitstreams with
stationary background. Our contribution is that our method cannot only show
fast processing time but also handle multiple moving objects that are
articulated, changing in size or internally have monotonous color, even though
they contain a chaotic set of non-homogeneous motion vectors inside. In
addition, our partial decoding process for H.264|AVC bitstreams enables to
improve the accuracy of object trajectories and overcome long occlusion by
using extracted color information.
</summary>
    <author>
      <name>Wonsang You</name>
    </author>
    <author>
      <name>M. S. Houari Sabirin</name>
    </author>
    <author>
      <name>Munchurl Kim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.805596</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.805596" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SPIE Real-Time Image and Video Processing Conference 2009</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of SPIE 2009, Volume: 7244, Publisher: SPIE, Pages:
  72440D-72440D-12</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.4743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.4743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.4943v1</id>
    <updated>2012-02-22T15:43:56Z</updated>
    <published>2012-02-22T15:43:56Z</published>
    <title>A new hybrid jpeg image compression scheme using symbol reduction
  technique</title>
    <summary>  Lossy JPEG compression is a widely used compression technique. Normally the
JPEG standard technique uses three process mapping reduces interpixel
redundancy, quantization, which is lossy process and entropy encoding, which is
considered lossless process. In this paper, a new technique has been proposed
by combining the JPEG algorithm and Symbol Reduction Huffman technique for
achieving more compression ratio. The symbols reduction technique reduces the
number of symbols by combining together to form a new symbol. As a result of
this technique the number of Huffman code to be generated also reduced. It is
simple fast and easy to implement. The result shows that the performance of
standard JPEG method can be improved by proposed method. This hybrid approach
achieves about 20% more compression ratio than the Standard JPEG.
</summary>
    <author>
      <name>Bheshaj Kumar</name>
    </author>
    <author>
      <name>Kavita Thakur</name>
    </author>
    <author>
      <name>G. R. Sinha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/csit.2012.2101-10.5121-csit.2012.2141</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/csit.2012.2101-10.5121-csit.2012.2141" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages,9 figures, SIP 2012 held on 3-4 January 2012,at
  Bangalore,India. arXiv admin note: text overlap with standard references on
  JPEG without attribution</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.4943v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.4943v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.5289v2</id>
    <updated>2013-07-30T12:54:15Z</updated>
    <published>2012-02-23T20:39:38Z</published>
    <title>Development Trends in Steganography</title>
    <summary>  Steganography is a general term referring to all methods for the embedding of
additional secret content into some form of carrier, with the aim of
concealment of the introduced alterations. The choice of the carrier is nearly
unlimited, it may be an ancient piece of parchment, as well as a network
protocol header. Inspired by biological phenomena, adopted by man in the
ancient times, it has been developed over the ages. Present day steganographic
methods are far more sophisticated than their ancient predecessors, but the
main principles have remained unchanged. They typically rely on the utilization
of digital media files or network protocols as a carrier, in which secret data
is embedded. This paper presents the evolution of the hidden data carrier from
the ancient times till the present day and pinpoints the observed development
trends, with special emphasis on network steganography.
</summary>
    <author>
      <name>Elzbieta Zielinska</name>
    </author>
    <author>
      <name>Wojciech Mazurczyk</name>
    </author>
    <author>
      <name>Krzysztof Szczypiorski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 1 figure, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.5289v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.5289v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.2147v1</id>
    <updated>2012-03-09T17:43:28Z</updated>
    <published>2012-03-09T17:43:28Z</published>
    <title>A Hybrid Image Cryptosystem Based On OMFLIP Permutation Cipher</title>
    <summary>  The protection of confidential image data from unauthorized access is an
important area of research in network communication. This paper presents a
high-level security encryption scheme for gray scale images. The gray level
image is first decomposed into binary images using bit scale decomposition.
Each binary image is then compressed by selecting a good scanning path that
minimizes the total number of bits needed to encode the bit sequence along the
scanning path using two dimensional run encoding. The compressed bit string is
then scrambled iteratively using a pseudo-random number generator and finally
encrypted using a bit level permutation OMFLIP. The performance is tested,
illustrated and discussed.
</summary>
    <author>
      <name>G. Sudheer</name>
    </author>
    <author>
      <name>B. V. S. Renuka Devi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.3, No.1, February 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.2147v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.2147v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.4035v1</id>
    <updated>2012-03-19T05:09:50Z</updated>
    <published>2012-03-19T05:09:50Z</published>
    <title>Quantitative Multiscale Analysis using Different Wavelets in 1D Voice
  Signal and 2D Image</title>
    <summary>  Mutiscale analysis represents multiresolution scrutiny of a signal to improve
its signal quality. Multiresolution analysis of 1D voice signal and 2D image is
conducted using DCT, FFT and different wavelets such as Haar, Deubachies,
Morlet, Cauchy, Shannon, Biorthogonal, Symmlet and Coiflet deploying the
cascaded filter banks based decomposition and reconstruction. The outstanding
quantitative analysis of the specified wavelets is done to investigate the
signal quality, mean square error, entropy and peak-to-peak SNR at multiscale
stage-4 for both 1D voice signal and 2D image. In addition, the 2D image
compression performance is significantly found 93.00% in DB-4, 93.68% in
bior-4.4, 93.18% in Sym-4 and 92.20% in Coif-2 during the multiscale analysis.
</summary>
    <author>
      <name>Niraj Shakhakarmi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 37 figures, Full paper is available at
  http://sites.google.com/site/nirajskorg/Home/wavelets; IJCSI International
  Journal of Computer Science Issues, Vol. 9, Issue 1, No 2, January 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.4035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.4035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.5772v1</id>
    <updated>2012-03-26T19:44:26Z</updated>
    <published>2012-03-26T19:44:26Z</published>
    <title>Compressed Sensing for Moving Imagery in Medical Imaging</title>
    <summary>  Numerous applications in signal processing have benefited from the theory of
compressed sensing which shows that it is possible to reconstruct signals
sampled below the Nyquist rate when certain conditions are satisfied. One of
these conditions is that there exists a known transform that represents the
signal with a sufficiently small number of non-zero coefficients. However when
the signal to be reconstructed is composed of moving images or volumes, it is
challenging to form such regularization constraints with traditional transforms
such as wavelets. In this paper, we present a motion compensating prior for
such signals that is derived directly from the optical flow constraint and can
utilize the motion information during compressed sensing reconstruction.
Proposed regularization method can be used in a wide variety of applications
involving compressed sensing and images or volumes of moving and deforming
objects. It is also shown that it is possible to estimate the signal and the
motion jointly or separately. Practical examples from magnetic resonance
imaging has been presented to demonstrate the benefit of the proposed method.
</summary>
    <author>
      <name>Cagdas Bilen</name>
    </author>
    <author>
      <name>Yao Wang</name>
    </author>
    <author>
      <name>Ivan Selesnick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE Transactions on Image Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.5772v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.5772v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.0056v1</id>
    <updated>2012-03-31T02:06:00Z</updated>
    <published>2012-03-31T02:06:00Z</published>
    <title>I-SolFramework: An Integrated Solution Framework Six Layers Assessment
  on Multimedia Information Security Architecture Policy Compliance</title>
    <summary>  Multimedia Information security becomes a important part for the
organization's intangible assets. Level of confidence and stakeholder trusted
are performance indicator as successes organization, it is imperative for
organizations to use Information Security Management System (ISMS) to
effectively manage their multimedia information assets. The main objective of
this paper is to Provide a novel practical framework approach to the
development of ISMS, Called by the I-SolFramework, implemented in multimedia
information security architecture (MISA), it divides a problem into six object
domains or six layers, namely organization,stakeholders, tool &amp; technology,
policy, knowledge, and culture. In addition, this framework also introduced
novelty algorithm and mathematic models as measurement and assessment tools of
MISA parameters.
</summary>
    <author>
      <name>Heru Susanto</name>
    </author>
    <author>
      <name>Mohammad Nabil Almunawar</name>
    </author>
    <author>
      <name>Yong Chee Tuan</name>
    </author>
    <author>
      <name>Mehmet Sabih Aksoy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Electrical &amp; Computer Sciences IJECS-IJENS
  Vol: 12 No: 01 (126501-9494 IJECS-IJENS \c{opyright} February 2012 IJENS)</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.0056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.0056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.1868v1</id>
    <updated>2012-04-09T12:23:36Z</updated>
    <published>2012-04-09T12:23:36Z</published>
    <title>User-based key frame detection in social web video</title>
    <summary>  Video search results and suggested videos on web sites are represented with a
video thumbnail, which is manually selected by the video up-loader among three
randomly generated ones (e.g., YouTube). In contrast, we present a grounded
user-based approach for automatically detecting interesting key-frames within a
video through aggregated users' replay interactions with the video player.
Previous research has focused on content-based systems that have the benefit of
analyzing a video without user interactions, but they are monolithic, because
the resulting video thumbnails are the same regardless of the user preferences.
We constructed a user interest function, which is based on aggregate video
replays, and analyzed hundreds of user interactions. We found that the local
maximum of the replaying activity stands for the semantics of information rich
videos, such as lecture, and how-to. The concept of user-based key-frame
detection could be applied to any video on the web, in order to generate a
user-based and dynamic video thumbnail in search results.
</summary>
    <author>
      <name>Konstantinos Chorianopoulos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2382636.2382642</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2382636.2382642" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.1868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.1868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2214v3</id>
    <updated>2013-07-25T17:25:32Z</updated>
    <published>2012-04-10T16:41:33Z</published>
    <title>Simplification Resilient LDPC-Coded Sparse-QIM Watermarking for
  3D-Meshes</title>
    <summary>  We propose a blind watermarking scheme for 3-D meshes which combines sparse
quantization index modulation (QIM) with deletion correction codes. The QIM
operates on the vertices in rough concave regions of the surface thus ensuring
impeccability, while the deletion correction code recovers the data hidden in
the vertices which is removed by mesh optimization and/or simplification. The
proposed scheme offers two orders of magnitude better performance in terms of
recovered watermark bit error rate compared to the existing schemes of similar
payloads and fidelity constraints.
</summary>
    <author>
      <name>Bata Vasic</name>
    </author>
    <author>
      <name>Bane Vasic</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2013.2265673</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2013.2265673" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted, revised and Copyright transfered to IEEE Transactions on
  Multimedia, October 9th 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.2214v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2214v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2359v1</id>
    <updated>2012-04-11T07:16:35Z</updated>
    <published>2012-04-11T07:16:35Z</published>
    <title>An Overview of Video Allocation Algorithms for Flash-based SSD Storage
  Systems</title>
    <summary>  Despite the fact that Solid State Disk (SSD) data storage media had offered a
revolutionary property storages community, but the unavailability of a
comprehensive allocation strategy in SSDs storage media, leads to consuming the
available space, random writing processes, time-consuming reading processes,
and system resources consumption. In order to overcome these challenges, an
efficient allocation algorithm is a desirable option. In this paper, we had
executed an intensive investigation on the SSD-based allocation algorithms that
had been proposed by the knowledge community. An explanatory comparison had
been made between these algorithms. We reviewed these algorithms in order to
building advanced knowledge armature that would help in inventing new
allocation algorithms for this type of storage media.
</summary>
    <author>
      <name>Jaafer Al-Sabateen</name>
    </author>
    <author>
      <name>Saleh Ali Alomari</name>
    </author>
    <author>
      <name>Putra Sumari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figure, 3 algorithm, 3 table, (IJACSA) International
  Journal of Advanced Computer Science and Applications, Vol. 3, No. 3, 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.2359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2616v1</id>
    <updated>2012-04-12T04:43:08Z</updated>
    <published>2012-04-12T04:43:08Z</published>
    <title>Genetic Algorithm to Make Persistent Security and Quality of Image in
  Steganography from RS Analysis</title>
    <summary>  Retention of secrecy is one of the significant features during communication
activity. Steganography is one of the popular methods to achieve secret
communication between sender and receiver by hiding message in any form of
cover media such as an audio, video, text, images etc. Least significant bit
encoding is the simplest encoding method used by many steganography programs to
hide secret message in 24bit, 8bit colour images and grayscale images.
Steganalysis is a method of detecting secret message hidden in a cover media
using steganography. RS steganalysis is one of the most reliable steganalysis
which performs statistical analysis of the pixels to successfully detect the
hidden message in an image. However, existing steganography method protects the
information against RS steganalysis in grey scale images. This paper presents a
steganography method using genetic algorithm to protect against the RS attack
in colour images. Stego image is divided into number of blocks. Subsequently,
with the implementation of natural evolution on the stego image using genetic
algorithm enables to achieve optimized security and image quality.
</summary>
    <author>
      <name>T. R. Gopalakrishnan Nair</name>
    </author>
    <author>
      <name>Suma V</name>
    </author>
    <author>
      <name>Manas S</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 4 Figures, Swarm Evolutionary and Memetric Computing
  Conference (SEMCCO), Vishakhapatnam</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.2616v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2616v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.6321v1</id>
    <updated>2012-04-27T20:00:20Z</updated>
    <published>2012-04-27T20:00:20Z</published>
    <title>Efficient Video Indexing on the Web: A System that Leverages User
  Interactions with a Video Player</title>
    <summary>  In this paper, we propose a user-based video indexing method, that
automatically generates thumbnails of the most important scenes of an online
video stream, by analyzing users' interactions with a web video player. As a
test bench to verify our idea we have extended the YouTube video player into
the VideoSkip system. In addition, VideoSkip uses a web-database (Google
Application Engine) to keep a record of some important parameters, such as the
timing of basic user actions (play, pause, skip). Moreover, we implemented an
algorithm that selects representative thumbnails. Finally, we populated the
system with data from an experiment with nine users. We found that the
VideoSkip system indexes video content by leveraging implicit users
interactions, such as pause and thirty seconds skip. Our early findings point
toward improvements of the web video player and its thumbnail generation
technique. The VideSkip system could compliment content-based algorithms, in
order to achieve efficient video-indexing in difficult videos, such as lectures
or sports.
</summary>
    <author>
      <name>Ioannis Leftheriotis</name>
    </author>
    <author>
      <name>Chrysoula Gkonela</name>
    </author>
    <author>
      <name>Konstantinos Chorianopoulos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-35145-7_16</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-35145-7_16" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures, UCMedia 2010: 2nd International ICST Conference
  on User Centric Media</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.6321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.6321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.1365v1</id>
    <updated>2012-05-07T12:49:42Z</updated>
    <published>2012-05-07T12:49:42Z</published>
    <title>Image Enhancement with Statistical Estimation</title>
    <summary>  Contrast enhancement is an important area of research for the image analysis.
Over the decade, the researcher worked on this domain to develop an efficient
and adequate algorithm. The proposed method will enhance the contrast of image
using Binarization method with the help of Maximum Likelihood Estimation (MLE).
The paper aims to enhance the image contrast of bimodal and multi-modal images.
The proposed methodology use to collect mathematical information retrieves from
the image. In this paper, we are using binarization method that generates the
desired histogram by separating image nodes. It generates the enhanced image
using histogram specification with binarization method. The proposed method has
showed an improvement in the image contrast enhancement compare with the other
image.
</summary>
    <author>
      <name>Aroop Mukherjee</name>
    </author>
    <author>
      <name>Soumen Kanrar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijma.2012.4205</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijma.2012.4205" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages,6 figures; ISSN:0975-5578 (Online); 0975-5934 (Print)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The International Journal of Multimedia &amp; Its Applications (IJMA)
  April 2012, Volume 4, Number 2, page 59-67</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1205.1365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.1859v1</id>
    <updated>2012-05-09T02:50:16Z</updated>
    <published>2012-05-09T02:50:16Z</published>
    <title>Text Steganography using LSB insertion method along with Chaos Theory</title>
    <summary>  The art of information hiding has been around nearly as long as the need for
covert communication. Steganography, the concealing of information, arose early
on as an extremely useful method for covert information transmission.
Steganography is the art of hiding secret message within a larger image or
message such that the hidden message or an image is undetectable; this is in
contrast to cryptography, where the existence of the message itself is not
disguised, but the content is obscure. The goal of a steganographic method is
to minimize the visually apparent and statistical differences between the cover
data and a steganogram while maximizing the size of the payload. Current
digital image steganography presents the challenge of hiding message in a
digital image in a way that is robust to image manipulation and attack. This
paper explains about how a secret message can be hidden into an image using
least significant bit insertion method along with chaos.
</summary>
    <author>
      <name>Bhavana S.</name>
    </author>
    <author>
      <name>K. L. Sudha</name>
    </author>
    <link href="http://arxiv.org/abs/1205.1859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.1859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.0692v3</id>
    <updated>2012-09-17T23:11:17Z</updated>
    <published>2012-06-04T18:02:03Z</published>
    <title>Signal and Image Processing with Sinlets</title>
    <summary>  This paper presents a new family of localized orthonormal bases - sinlets -
which are well suited for both signal and image processing and analysis.
One-dimensional sinlets are related to specific solutions of the time-dependent
harmonic oscillator equation. By construction, each sinlet is infinitely
differentiable and has a well-defined and smooth instantaneous frequency known
in analytical form. For square-integrable transient signals with infinite
support, one-dimensional sinlet basis provides an advantageous alternative to
the Fourier transform by rendering accurate signal representation via a
countable set of real-valued coefficients. The properties of sinlets make them
suitable for analyzing many real-world signals whose frequency content changes
with time including radar and sonar waveforms, music, speech, biological
echolocation sounds, biomedical signals, seismic acoustic waves, and signals
employed in wireless communication systems. One-dimensional sinlet bases can be
used to construct two- and higher-dimensional bases with variety of potential
applications including image analysis and representation.
</summary>
    <author>
      <name>Alexander Y. Davydov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 21 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.0692v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.0692v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91A28" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.4326v1</id>
    <updated>2012-06-19T20:16:04Z</updated>
    <published>2012-06-19T20:16:04Z</published>
    <title>Joint Reconstruction of Multi-view Compressed Images</title>
    <summary>  The distributed representation of correlated multi-view images is an
important problem that arise in vision sensor networks. This paper concentrates
on the joint reconstruction problem where the distributively compressed
correlated images are jointly decoded in order to improve the reconstruction
quality of all the compressed images. We consider a scenario where the images
captured at different viewpoints are encoded independently using common coding
solutions (e.g., JPEG, H.264 intra) with a balanced rate distribution among
different cameras. A central decoder first estimates the underlying correlation
model from the independently compressed images which will be used for the joint
signal recovery. The joint reconstruction is then cast as a constrained convex
optimization problem that reconstructs total-variation (TV) smooth images that
comply with the estimated correlation model. At the same time, we add
constraints that force the reconstructed images to be consistent with their
compressed versions. We show by experiments that the proposed joint
reconstruction scheme outperforms independent reconstruction in terms of image
quality, for a given target bit rate. In addition, the decoding performance of
our proposed algorithm compares advantageously to state-of-the-art distributed
coding schemes based on disparity learning and on the DISCOVER.
</summary>
    <author>
      <name>Vijayaraghavan Thirumalai</name>
    </author>
    <author>
      <name>Pascal Frossard</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2013.2240006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2013.2240006" rel="related"/>
    <link href="http://arxiv.org/abs/1206.4326v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.4326v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.3718v1</id>
    <updated>2012-08-18T02:11:20Z</updated>
    <published>2012-08-18T02:11:20Z</published>
    <title>Exploiting Image Local And Nonlocal Consistency For Mixed
  Gaussian-Impulse Noise Removal</title>
    <summary>  Most existing image denoising algorithms can only deal with a single type of
noise, which violates the fact that the noisy observed images in practice are
often suffered from more than one type of noise during the process of
acquisition and transmission. In this paper, we propose a new variational
algorithm for mixed Gaussian-impulse noise removal by exploiting image local
consistency and nonlocal consistency simultaneously. Specifically, the local
consistency is measured by a hyper-Laplace prior, enforcing the local
smoothness of images, while the nonlocal consistency is measured by
three-dimensional sparsity of similar blocks, enforcing the nonlocal
self-similarity of natural images. Moreover, a Split-Bregman based technique is
developed to solve the above optimization problem efficiently. Extensive
experiments for mixed Gaussian plus impulse noise show that significant
performance improvements over the current state-of-the-art schemes have been
achieved, which substantiates the effectiveness of the proposed algorithm.
</summary>
    <author>
      <name>Jian Zhang</name>
    </author>
    <author>
      <name>Ruiqin Xiong</name>
    </author>
    <author>
      <name>Chen Zhao</name>
    </author>
    <author>
      <name>Siwei Ma</name>
    </author>
    <author>
      <name>Debin Zhao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICME.2012.109</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICME.2012.109" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, 3 tables, to be published at IEEE Int. Conf. on
  Multimedia &amp; Expo (ICME) 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.3718v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.3718v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.6389v1</id>
    <updated>2012-08-31T06:31:01Z</updated>
    <published>2012-08-31T06:31:01Z</published>
    <title>Behavioral Systel Level Power Consumption Modeling of Mobile Video
  Streaming applications</title>
    <summary>  Nowadays, the use of mobile applications and terminals faces fundamental
challenges related to energy constraint. This is due to the limited battery
lifetime as compared to the increasing hardware evolution. Video streaming is
one of the most energy consuming applications in a mobile system because of its
intensive use of bandwidth, memory and processing power. In this work, we aim
to propose a methodology for building and validating a high level global power
consumption model including a hardware and software elements. Our approach is
based on exploiting the interactions between power consumption sub-models of
standalone systems in the perspective to build more accurate global model. The
interactions are studied within the exclusive context of video streaming
applications that are one of the most used mobile applications.
</summary>
    <author>
      <name>Yahia Benmoussa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <author>
      <name>Jalil Boukhobza</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <author>
      <name>Yassine Hadjadj Aoul</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA - IRISA</arxiv:affiliation>
    </author>
    <author>
      <name>Lo√Øc Lagadec</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Lab-STICC</arxiv:affiliation>
    </author>
    <author>
      <name>Djamel Benazzouz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LMSS</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Colloque du GDR SoC SiP, Paris : France (2012)</arxiv:comment>
    <link href="http://arxiv.org/abs/1208.6389v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.6389v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.1399v1</id>
    <updated>2012-09-06T20:10:02Z</updated>
    <published>2012-09-06T20:10:02Z</published>
    <title>Video Chat with Multiple Cameras</title>
    <summary>  The dominant paradigm for video chat employs a single camera at each end of
the conversation, but some conversations can be greatly enhanced by using
multiple cameras at one or both ends. This paper provides the first rigorous
investigation of multi-camera video chat, concentrating especially on the
ability of users to switch between views at either end of the conversation. A
user study of 23 individuals analyzes the advantages and disadvantages of
permitting a user to switch between views at a remote location. Benchmark
experiments employing up to four webcams simultaneously demonstrate that
multi-camera video chat is feasible on consumer hardware. The paper also
presents the design of MultiCam, a software package permitting multi-camera
video chat. Some important trade-offs in the design of MultiCam are discussed,
and typical usage scenarios are analyzed.
</summary>
    <author>
      <name>John MacCormick</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">49 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.1399v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.1399v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1; C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.1673v1</id>
    <updated>2012-09-07T23:59:01Z</updated>
    <published>2012-09-07T23:59:01Z</published>
    <title>Recovering Missing Coefficients in DCT-Transformed Images</title>
    <summary>  A general method for recovering missing DCT coefficients in DCT-transformed
images is presented in this work. We model the DCT coefficients recovery
problem as an optimization problem and recover all missing DCT coefficients via
linear programming. The visual quality of the recovered image gradually
decreases as the number of missing DCT coefficients increases. For some images,
the quality is surprisingly good even when more than 10 most significant DCT
coefficients are missing. When only the DC coefficient is missing, the proposed
algorithm outperforms existing methods according to experimental results
conducted on 200 test images. The proposed recovery method can be used for
cryptanalysis of DCT based selective encryption schemes and other applications.
</summary>
    <author>
      <name>Shujun Li</name>
    </author>
    <author>
      <name>Andreas Karrenbauer</name>
    </author>
    <author>
      <name>Dietmar Saupe</name>
    </author>
    <author>
      <name>C. -C. Jay Kuo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICIP.2011.6115738</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICIP.2011.6115738" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 2011 18th IEEE International Conference on Image
  Processing (ICIP 2011), pages 1537-1540, IEEE, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.1673v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.1673v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.2067v2</id>
    <updated>2013-11-23T23:41:20Z</updated>
    <published>2012-09-10T17:19:45Z</published>
    <title>A Markov Decision Model for Adaptive Scheduling of Stored Scalable
  Videos</title>
    <summary>  We propose two scheduling algorithms that seek to optimize the quality of
scalably coded videos that have been stored at a video server before
transmission.} The first scheduling algorithm is derived from a Markov Decision
Process (MDP) formulation developed here. We model the dynamics of the channel
as a Markov chain and reduce the problem of dynamic video scheduling to a
tractable Markov decision problem over a finite state space. Based on the MDP
formulation, a near-optimal scheduling policy is computed that minimize the
mean square error. Using insights taken from the development of the optimal
MDP-based scheduling policy, the second proposed scheduling algorithm is an
online scheduling method that only requires easily measurable knowledge of the
channel dynamics, and is thus viable in practice. Simulation results show that
the performance of both scheduling algorithms is close to a performance upper
bound also derived in this paper.
</summary>
    <author>
      <name>Chao Chen</name>
    </author>
    <author>
      <name>Robert W. Heath Jr</name>
    </author>
    <author>
      <name>Alan C. Bovik</name>
    </author>
    <author>
      <name>Gustavo de Veciana</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Circuits and Systems for Video Technology,
  vol.23, no.6, pp.1081-1095, June 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.2067v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.2067v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.2699v1</id>
    <updated>2012-11-12T17:15:41Z</updated>
    <published>2012-11-12T17:15:41Z</published>
    <title>A Non-Blind Watermarking Scheme for Gray Scale Images in Discrete
  Wavelet Transform Domain using Two Subbands</title>
    <summary>  Digital watermarking is the process to hide digital pattern directly into a
digital content. Digital watermarking techniques are used to address digital
rights management, protect information and conceal secrets. An invisible
non-blind watermarking approach for gray scale images is proposed in this
paper. The host image is decomposed into 3-levels using Discrete Wavelet
Transform. Based on the parent-child relationship between the wavelet
coefficients the Set Partitioning in Hierarchical Trees (SPIHT) compression
algorithm is performed on the LH3, LH2, HL3 and HL2 subbands to find out the
significant coefficients. The most significant coefficients of LH2 and HL2
bands are selected to embed a binary watermark image. The selected significant
coefficients are modulated using Noise Visibility Function, which is considered
as the best strength to ensure better imperceptibility. The approach is tested
against various image processing attacks such as addition of noise, filtering,
cropping, JPEG compression, histogram equalization and contrast adjustment. The
experimental results reveal the high effectiveness of the method.
</summary>
    <author>
      <name>Abdur Shahid</name>
    </author>
    <author>
      <name>Shahriar Badsha</name>
    </author>
    <author>
      <name>Md. Rethwan Kabeer</name>
    </author>
    <author>
      <name>Junaid Ahsan</name>
    </author>
    <author>
      <name>Mufti Mahmud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 5, No 1, September 2012, page 101-109</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.2699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.2699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.4683v1</id>
    <updated>2012-11-20T08:22:56Z</updated>
    <published>2012-11-20T08:22:56Z</published>
    <title>Content based video retrieval</title>
    <summary>  Content based video retrieval is an approach for facilitating the searching
and browsing of large image collections over World Wide Web. In this approach,
video analysis is conducted on low level visual properties extracted from video
frame. We believed that in order to create an effective video retrieval system,
visual perception must be taken into account. We conjectured that a technique
which employs multiple features for indexing and retrieval would be more
effective in the discrimination and search tasks of videos. In order to
validate this claim, content based indexing and retrieval systems were
implemented using color histogram, various texture features and other
approaches. Videos were stored in Oracle 9i Database and a user study measured
correctness of response.
</summary>
    <author>
      <name>B. V. Patel</name>
    </author>
    <author>
      <name>B. B. Meshram</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijma.2012.4506</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijma.2012.4506" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The International Journal of Multimedia &amp; Its Applications (IJMA)
  Vol.4, No.5, October 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1211.4683v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.4683v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.6058v1</id>
    <updated>2012-12-25T14:28:52Z</updated>
    <published>2012-12-25T14:28:52Z</published>
    <title>High Quality Image Interpolation via Local Autoregressive and Nonlocal
  3-D Sparse Regularization</title>
    <summary>  In this paper, we propose a novel image interpolation algorithm, which is
formulated via combining both the local autoregressive (AR) model and the
nonlocal adaptive 3-D sparse model as regularized constraints under the
regularization framework. Estimating the high-resolution image by the local AR
regularization is different from these conventional AR models, which weighted
calculates the interpolation coefficients without considering the rough
structural similarity between the low-resolution (LR) and high-resolution (HR)
images. Then the nonlocal adaptive 3-D sparse model is formulated to regularize
the interpolated HR image, which provides a way to modify these pixels with the
problem of numerical stability caused by AR model. In addition, a new
Split-Bregman based iterative algorithm is developed to solve the above
optimization problem iteratively. Experiment results demonstrate that the
proposed algorithm achieves significant performance improvements over the
traditional algorithms in terms of both objective quality and visual perception
</summary>
    <author>
      <name>Xinwei Gao</name>
    </author>
    <author>
      <name>Jian Zhang</name>
    </author>
    <author>
      <name>Feng Jiang</name>
    </author>
    <author>
      <name>Xiaopeng Fan</name>
    </author>
    <author>
      <name>Siwei Ma</name>
    </author>
    <author>
      <name>Debin Zhao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/VCIP.2012.6410749</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/VCIP.2012.6410749" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures, 2 tables, to be published at IEEE Visual
  Communications and Image Processing (VCIP) 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.6058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.6058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.0344v1</id>
    <updated>2013-01-02T22:18:06Z</updated>
    <published>2013-01-02T22:18:06Z</published>
    <title>A Poisson Hidden Markov Model for Multiview Video Traffic</title>
    <summary>  Multiview video has recently emerged as a means to improve user experience in
novel multimedia services. We propose a new stochastic model to characterize
the traffic generated by a Multiview Video Coding (MVC) variable bit rate
source. To this aim, we resort to a Poisson Hidden Markov Model (P-HMM), in
which the first (hidden) layer represents the evolution of the video activity
and the second layer represents the frame sizes of the multiple encoded views.
We propose a method for estimating the model parameters in long MVC sequences.
We then present extensive numerical simulations assessing the model's ability
to produce traffic with realistic characteristics for a general class of MVC
sequences. We then extend our framework to network applications where we show
that our model is able to accurately describe the sender and receiver buffers
behavior in MVC transmission. Finally, we derive a model of user behavior for
interactive view selection, which, in conjunction with our traffic model, is
able to accurately predict actual network load in interactive multiview
services.
</summary>
    <author>
      <name>Lorenzo Rossi</name>
    </author>
    <author>
      <name>Jacob Chakareski</name>
    </author>
    <author>
      <name>Pascal Frossard</name>
    </author>
    <author>
      <name>Stefania Colonnese</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 21 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.0344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.1894v1</id>
    <updated>2013-01-09T15:36:40Z</updated>
    <published>2013-01-09T15:36:40Z</published>
    <title>An Extensive Analysis of Query by Singing/Humming System Through Query
  Proportion</title>
    <summary>  Query by Singing/Humming (QBSH) is a Music Information Retrieval (MIR) system
with small audio excerpt as query. The rising availability of digital music
stipulates effective music retrieval methods. Further, MIR systems support
content based searching for music and requires no musical acquaintance. Current
work on QBSH focuses mainly on melody features such as pitch, rhythm, note
etc., size of databases, response time, score matching and search algorithms.
Even though a variety of QBSH techniques are proposed, there is a dearth of
work to analyze QBSH through query excerption. Here, we present an analysis
that works on QBSH through query excerpt. To substantiate a series of
experiments are conducted with the help of Mel-Frequency Cepstral Coefficients
(MFCC), Linear Predictive Coefficients (LPC) and Linear Predictive Cepstral
Coefficients (LPCC) to portray the robustness of the knowledge representation.
Proposed experiments attempt to reveal that retrieval performance as well as
precision diminishes in the snail phase with the growing database size.
</summary>
    <author>
      <name>Trisiladevi C. Nagavi</name>
    </author>
    <author>
      <name>Nagappa U. Bhajantri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijma.2012.4606</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijma.2012.4606" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages,11 figures; The International Journal of Multimedia &amp; Its
  Applications (IJMA) Vol.4, No.6, December 2012. arXiv admin note: text
  overlap with arXiv:1003.4083 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.1894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.1894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.2173v1</id>
    <updated>2013-01-10T16:29:11Z</updated>
    <published>2013-01-10T16:29:11Z</published>
    <title>AViTExt: Automatic Video Text Extraction, A new Approach for video
  content indexing Application</title>
    <summary>  In this paper, we propose a spatial temporal video-text detection technique
which proceed in two principal steps:potential text region detection and a
filtering process. In the first step we divide dynamically each pair of
consecutive video frames into sub block in order to detect change. A
significant difference between homologous blocks implies the appearance of an
important object which may be a text region. The temporal redundancy is then
used to filter these regions and forms an effective text region. The
experimentation driven on a variety of video sequences shows the effectiveness
of our approach by obtaining a 89,39% as precision rate and 90,19 as recall.
</summary>
    <author>
      <name>Baseem Bouaziz</name>
    </author>
    <author>
      <name>Tarek Zlitni</name>
    </author>
    <author>
      <name>Walid Mahdi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, 3rd International Conference on Information and
  Communication Technologies: From Theory to Applications(ICTTA 2008)</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.2173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.2173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.2200v1</id>
    <updated>2013-01-10T17:56:02Z</updated>
    <published>2013-01-10T17:56:02Z</published>
    <title>A Visual Grammar Approach for TV Program Identification</title>
    <summary>  Automatic identification of TV programs within TV streams is an important
task for archive exploitation. This paper proposes a new spatial-temporal
approach to identify programs in TV streams in two main steps: First, a
reference catalogue for video grammars of visual jingles is constructed. We
exploit visual grammars characterizing instances of the same program type in
order to identify the various program types in the TV stream. The role of video
grammar is to represent the visual invariants for each visual jingle using a
set of descriptors appropriate for each TV program. Secondly, programs in TV
streams are identified by examining the similarity of the video signal to the
visual grammars in the catalogue. The main idea of identification process
consists in comparing the visual similarity of the video signal signature in TV
stream to the catalogue elements. After presenting the proposed approach, the
paper overviews the encouraging experimental results on several streams
extracted from different channels and composed of several programs.
</summary>
    <author>
      <name>Tarek Zlitni</name>
    </author>
    <author>
      <name>Walid Mahdi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures, (IJCNS) International Journal of Computer and
  Network Security, Vol. 2, No. 9, September 2010</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.2200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.2200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.5793v1</id>
    <updated>2013-01-24T14:31:10Z</updated>
    <published>2013-01-24T14:31:10Z</published>
    <title>Video Tester -- A multiple-metric framework for video quality assessment
  over IP networks</title>
    <summary>  This paper presents an extensible and reusable framework which addresses the
problem of video quality assessment over IP networks. The proposed tool
(referred to as Video-Tester) supports raw uncompressed video encoding and
decoding. It also includes different video over IP transmission methods (i.e.:
RTP over UDP unicast and multicast, as well as RTP over TCP). In addition, it
is furnished with a rich set of offline analysis capabilities. Video-Tester
analysis includes QoS and bitstream parameters estimation (i.e.: bandwidth,
packet inter-arrival time, jitter and loss rate, as well as GOP size and
I-frame loss rate). Our design facilitates the integration of virtually any
existing video quality metric thanks to the adopted Python-based modular
approach. Video-Tester currently provides PSNR, SSIM, ITU-T G.1070 video
quality metric, DIV and PSNR-based MOS estimations. In order to promote its use
and extension, Video-Tester is open and publicly available.
</summary>
    <author>
      <name>I√±aki Ucar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universidad P√∫blica de Navarra</arxiv:affiliation>
    </author>
    <author>
      <name>Jorge Navarro-Ortiz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universidad de Granada</arxiv:affiliation>
    </author>
    <author>
      <name>Pablo Ameigeiras</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universidad de Granada</arxiv:affiliation>
    </author>
    <author>
      <name>Juan M. Lopez-Soler</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Universidad de Granada</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/BMSB.2012.6264243</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/BMSB.2012.6264243" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures. For the Google Code project, see
  http://video-tester.googlecode.com/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE international Symposium on Broadband Multimedia Systems and
  Broadcasting, Seoul, 2012, pp. 1-5</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1301.5793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.5793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.3; C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.1697v1</id>
    <updated>2013-03-07T14:17:23Z</updated>
    <published>2013-03-07T14:17:23Z</published>
    <title>Secure Video Streaming Plug-In</title>
    <summary>  Video sharing sites like YouTube, Metacafe, Dailymotion, Vimeo, etc. provide
a platform for media content sharing among its users. Some of these videos are
copyright protected and restricted from being downloaded and saved. But users
can use various download managers or application programs to download and save
these videos. This affects the incoming traffic on these websites reducing
their hit rate and consequently reducing their revenue. Adobe Flash Player is
the most commonly used player for watching online videos. It uses RTMP (Real
Time Messaging Protocol) to stream audio, video and data over the Internet,
between a Flash Player and Adobe Flash Media Server.Here, we propose a plug-in
that enables the site owner control over downloading of videos from such
website. The plug-in will be installed at the client side with the consent of
the user. When the video is being played this plug-in will send unique keys to
the media server. The server will continue streaming the video after verifying
the keys. Download managers or application programs will not be able to
download the videos as they wont be able to create the unique keys that need to
be sent to the server.
</summary>
    <author>
      <name>Avinash Bhujbal</name>
    </author>
    <author>
      <name>Ashish Jagtap</name>
    </author>
    <author>
      <name>Devendra Gurav</name>
    </author>
    <author>
      <name>Tino Jameskutty</name>
    </author>
    <link href="http://arxiv.org/abs/1303.1697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.1697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.0793v1</id>
    <updated>2013-04-02T20:35:11Z</updated>
    <published>2013-04-02T20:35:11Z</published>
    <title>A local fingerprinting approach for audio copy detection</title>
    <summary>  This study proposes an audio copy detection system that is robust to various
attacks. These include the severe pitch shift and tempo change attacks which
existing systems fail to detect. First, we propose a novel two dimensional
representation for audio signals called the time-chroma image. This image is
based on a modification of the concept of chroma in the music literature and is
shown to achieve better performance in song identification. Then, we propose a
novel fingerprinting algorithm that extracts local fingerprints from the
time-chroma image. The proposed local fingerprinting algorithm is invariant to
time/frequency scale changes in audio signals. It also outperforms existing
methods like SIFT by a great extent. Finally, we introduce a song
identification algorithm that uses the proposed fingerprints. The resulting
copy detection system is shown to significantly outperform existing methods.
Besides being able to detect whether a song (or a part of it) has been copied,
the proposed system can accurately estimate the amount of pitch shift and/or
tempo change that might have been applied to a song.
</summary>
    <author>
      <name>Mani Malekesmaeili</name>
    </author>
    <author>
      <name>Rabab K. Ward</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.0793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.0793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.1571v1</id>
    <updated>2013-04-04T22:17:47Z</updated>
    <published>2013-04-04T22:17:47Z</published>
    <title>Hiding Image in Image by Five Modulus Method for Image Steganography</title>
    <summary>  This paper is to create a practical steganographic implementation to hide
color image (stego) inside another color image (cover). The proposed technique
uses Five Modulus Method to convert the whole pixels within both the cover and
the stego images into multiples of five. Since each pixels inside the stego
image is divisible by five then the whole stego image could be divided by five
to get new range of pixels 0..51. Basically, the reminder of each number that
is not divisible by five is either 1,2,3 or 4 when divided by 5. Subsequently,
then a 4-by-4 window size has been implemented to accommodate the proposed
technique. For each 4-by-4 window inside the cover image, a number from 1 to 4
could be embedded secretly from the stego image. The previous discussion must
be applied separately for each of the R, G, and B arrays. Moreover, a stego-key
could be combined with the proposed algorithm to make it difficult for any
adversary to extract the secret image from the cover image. Based on the PSNR
value, the extracted stego image has high PSNR value. Hence this new
steganography algorithm is very efficient to hide color images.
</summary>
    <author>
      <name>Firas A. Jassim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 tables, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of computing, volume 5, issue 2, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.1571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.1571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.2671v1</id>
    <updated>2013-04-09T17:29:56Z</updated>
    <published>2013-04-09T17:29:56Z</published>
    <title>Genetic Soundtracks: Creative Matching of Audio to Video</title>
    <summary>  The matching of the soundtrack in a movie or a video can have an enormous
influence in the message being conveyed and its impact, in the sense of
involvement and engagement, and ultimately in their aesthetic and entertainment
qualities. Art is often associated with creativity, implying the presence of
inspiration, originality and appropriateness. Evolutionary systems provides us
with the novelty, showing us new and subtly different solutions in every
generation, possibly stimulating the creativity of the human using the system.
In this paper, we present Genetic Soundtracks, an evolutionary approach to the
creative matching of audio to a video. It analyzes both media to extract
features based on their content, and adopts genetic algorithms, with the
purpose of truncating, combining and adjusting audio clips, to align and match
them with the video scenes.
</summary>
    <author>
      <name>Jorge Gomes</name>
    </author>
    <author>
      <name>Fernando Silva</name>
    </author>
    <author>
      <name>Teresa Chambel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Artech'2012 Crossing Digital Boundaries, 6th International Conference
  on Digital Arts. Faro, Portugal. Nov 7-9, 2012</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">6th International Conference on Digital Arts (Artech), pp.349-352,
  2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.2671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.2671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3056v1</id>
    <updated>2013-04-10T19:12:38Z</updated>
    <published>2013-04-10T19:12:38Z</published>
    <title>Anticipatory Buffer Control and Resource Allocation for Wireless Video
  Streaming</title>
    <summary>  This paper describes a new approach for allocating resources to video
streaming traffic. Assuming that the future channel state can be predicted for
a certain time, we minimize the fraction of the bandwidth consumed for smooth
streaming by jointly allocating wireless channel resources and play-out buffer
size. To formalize this idea, we introduce a new model to capture the dynamic
of a video streaming buffer and the allocated spectrum in an optimization
problem. The result is a Linear Program that allows to trade off buffer size
and allocated bandwidth. Based on this tractable model, our simulation results
show that anticipating poor channel states and pre-loading the buffer
accordingly allows to serve more users at perfect video quality.
</summary>
    <author>
      <name>Sanam Sadr</name>
    </author>
    <author>
      <name>Stefan Valentin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, submitted to IEEE Globecom 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.3056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3629v1</id>
    <updated>2013-04-11T14:45:51Z</updated>
    <published>2013-04-11T14:45:51Z</published>
    <title>A Secure And High Capacity Image Steganography Technique</title>
    <summary>  Steganography is the science of invisible communication. The purpose of
Steganography is to maintain secret communication between two parties. The
secret information can be concealed in content such as image, audio, or video.
This paper provides a novel image steganography technique to hide multiple
secret images and keys in color cover image using Integer Wavelet Transform
(IWT). There is no visual difference between the stego image and the cover
image. The extracted secret images are also similar to the original secret
images. Very good PSNR (Peak Signal to Noise Ratio) values are obtained for
both stego and extracted secret images. The results are compared with the
results of other techniques, where single image is hidden and it is found that
the proposed technique is simple and gives better PSNR values than others.
</summary>
    <author>
      <name>Hemalatha S</name>
    </author>
    <author>
      <name>U Dinesh Acharya</name>
    </author>
    <author>
      <name>Renuka A</name>
    </author>
    <author>
      <name>Priya R. Kamath</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/sipij.2013.4108</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/sipij.2013.4108" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.4, No.1, February 2013. arXiv admin note: substantial text overlap with
  arXiv:1304.3313</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.3629v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3629v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3758v1</id>
    <updated>2013-04-13T01:59:01Z</updated>
    <published>2013-04-13T01:59:01Z</published>
    <title>Metrics for Video Quality Assessment in Mobile Scenarios</title>
    <summary>  With exponential increase in the volumes of video traffic in cellular
net-works, there is an increasing need for optimizing the quality of video
delivery. 4G networks (Long Term Evolution Advanced or LTE A) are being
introduced in many countries worldwide, which allow a downlink speed of upto 1
Gbps and uplink of 100 Mbps over a single base station. This makes a strong
push towards video broadcasting over LTE networks, characterizing its
performance and developing metrics which can be deployed to provide user
feedback of video quality and feed-back them to network operators to fine tune
the network. In this paper, we characterize the performance of video
transmission over LTE A physical layer using popular video quality metrics such
as SSIM, Blocking, Blurring, NIQE and BRISQUE. We conduct experiments to find a
suitable no-reference metrics for mobile scenario and find that Blocking
Metrics is most promising in case of channel or modulation variations but it
does not perform well to quantize variations in compression ratios. The metrics
BRISQUE is very efficient in quantizing this distortion and performs well in
case of network variations also.
</summary>
    <author>
      <name>Gaurav Pande</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to International Journal of Digital Multimedia Broadcasting</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.3758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.5068v1</id>
    <updated>2013-04-18T10:00:38Z</updated>
    <published>2013-04-18T10:00:38Z</published>
    <title>Joint On-the-Fly Network Coding/Video Quality Adaptation for Real-Time
  Delivery</title>
    <summary>  This paper introduces a redundancy adaptation algorithm for an on-the-fly
erasure network coding scheme called Tetrys in the context of real-time video
transmission. The algorithm exploits the relationship between the redundancy
ratio used by Tetrys and the gain or loss in encoding bit rate from changing a
video quality parameter called the Quantization Parameter (QP). Our evaluations
show that with equal or less bandwidth occupation, the video protected by
Tetrys with redundancy adaptation algorithm obtains a PSNR gain up to or more 4
dB compared to the video without Tetrys protection. We demonstrate that the
Tetrys redundancy adaptation algorithm performs well with the variations of
both loss pattern and delay induced by the networks. We also show that Tetrys
with the redundancy adaptation algorithm outperforms FEC with and without
redundancy adaptation.
</summary>
    <author>
      <name>Tuan Tran Thai</name>
    </author>
    <author>
      <name>J√©r√¥me Lacan</name>
    </author>
    <author>
      <name>Emmanuel Lochin</name>
    </author>
    <link href="http://arxiv.org/abs/1304.5068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.5068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.6872v1</id>
    <updated>2013-04-25T10:53:39Z</updated>
    <published>2013-04-25T10:53:39Z</published>
    <title>Security Issues In Speech Watermarking for Information Transmission</title>
    <summary>  The secure transmission of speech information is a significant issue faced by
many security professionals and individuals. By applying voice-encryption
technique any kind of encrypted sensitive speech data such as password can be
transmitted. But this has the serious disadvantage that by means of
cryptanalysis attack encrypted data can be compromised. Increasing the strength
of encryption/decryption results in an associated increased in the cost.
Additional techniques like stenography and digital watermarking can be used to
conceal information in an undetectable way in audio data. However this
watermarked audio data has to be send through unreliable media and an
eavesdropper might get hold of secret message and can also determine the
identity of a speaker who is sending the information since human voice contains
information based on its characteristics such as frequency, pitch, and energy.
This paper proposes Normalized Speech Watermarking technique. Speech signal is
normalized to hide the identity of the speaker who is sending the information
and then speech watermarking technique is applied on this normalized signal
that contains the message (password) so that what information is transmitted
should not be unauthorizedly revealed.
</summary>
    <author>
      <name>Rupa Patel</name>
    </author>
    <author>
      <name>Urmila Shrawankar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pages: 10 Figures: 5, Conference Procedings, AMOC 2011, Advances in
  Modeling, Optimization and Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.6872v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.6872v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.8080v1</id>
    <updated>2013-04-25T10:42:04Z</updated>
    <published>2013-04-25T10:42:04Z</published>
    <title>Secure Transmission of Password Using Speech Watermarking</title>
    <summary>  Internet is one of the most valuable resources for information communication
and retrievals. Most multimedia signals today are in digital formats. The
digital data can be duplicated and edited with great ease which has led to a
need for data integrity and protection of digital data. The security
requirements such as integrity or data authentication can be met by
implementing security measures using digital watermarking techniques. In this
paper a blind speech watermarking algorithm that embeds the watermark signal
data in the musical (sequence) host signal by using frequency masking is used.
A different logarithmic approach is proposed. In this regard a logarithmic
function is first applied to watermark data. Then the transformed signal is
embedded to the converted version of host signal which is obtained by applying
Fast Fourier transform method. Finally using inverse Fast Fourier Transform and
antilogarithmic function watermark signal is retrieved.
</summary>
    <author>
      <name>Rupa Patel</name>
    </author>
    <author>
      <name>Urmila Shrawankar</name>
    </author>
    <author>
      <name>V. M Thakare</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Pages: 4 Figures: 7, International Journal of Computer Science and
  Technology (IJCST) Vol.2, Issue 3, September 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1304.8080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.8080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.1887v1</id>
    <updated>2013-05-08T17:19:50Z</updated>
    <published>2013-05-08T17:19:50Z</published>
    <title>Performance Evaluation of Video Communications over 4G Network</title>
    <summary>  With exponential increase in the volumes of video traffic in cellular
net-works, there is an increasing need for optimizing the quality of video
delivery. 4G networks (Long Term Evolution Advanced or LTE A) are being
introduced in many countries worldwide, which allow a downlink speed of upto 1
Gbps and uplink of 100 Mbps over a single base station. In this paper, we
characterize the performance of LTE A physical layer in terms of transmitted
video quality when the channel condi-tions and LTE settings are varied. We test
the performance achieved as the channel quality is changed and HARQ features
are enabled in physical layer. Blocking and blurring metrics were used to model
image quality.
</summary>
    <author>
      <name>Gaurav Pande</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in ICACNI 2013. arXiv admin note: substantial text overlap
  with arXiv:1304.3758</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.1887v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.1887v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.2251v4</id>
    <updated>2015-07-17T10:29:44Z</updated>
    <published>2013-05-10T02:53:07Z</published>
    <title>Quantum Image Representation Through Two-Dimensional Quantum States and
  Normalized Amplitude</title>
    <summary>  We propose a novel method for image representation in quantum computers,
which uses the two-dimensional (2-D) quantum states to locate each pixel in an
image through row-location and column-location vectors for identifying each
pixel location. The quantum state of an image is the linear superposition of
the tensor product of the m-qubits row-location vector and the n-qubits
column-location vector of each pixel. It enables the natural quantum
representation of rectangular images that other methods lack. The
amplitude/intensity of each pixel is incorporated into the coefficient values
of the pixel's quantum state, without using any qubits. Due to the fact that
linear superposition, tensor product and qubits form the fundamental basis of
quantum computing, the proposed method presents the machine level
representation of images on quantum computers. Unlike other methods, this
method is a pure quantum representation without any classical components.
</summary>
    <author>
      <name>Madhur Srivastava</name>
    </author>
    <author>
      <name>Subhayan R. Moulick</name>
    </author>
    <author>
      <name>Prasanta K. Panigrahi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.2251v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.2251v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.3021v2</id>
    <updated>2015-08-07T13:06:09Z</updated>
    <published>2013-05-14T05:27:52Z</published>
    <title>Wave Atom Based Watermarking</title>
    <summary>  Watermarking helps in ensuring originality, ownership and copyrights of a
digital image. This paper aims at embedding a Watermark in an image using Wave
Atom Transform. Preference of Wave Atoms on other transformations has been due
to its sparser expansion, adaptability to the direction of local pattern, and
sharp frequency localization. In this scheme, we had tried to spread the
watermark in an image so that the information at one place is very small and
undetectable. In order to extract the watermark and verify ownership of an
image, one would have the advantage of prior knowledge of embedded locations. A
noise of high amplitude will be needed to be added to the image for watermark
distortion. Furthermore, the information spread will ensure the robustness of
the watermark data. The proposed scheme has the ability to withstand malicious
operations and attacks.
</summary>
    <author>
      <name>Ijaz Bukhari</name>
    </author>
    <author>
      <name> Nuhman-ul-Haq</name>
    </author>
    <author>
      <name>Khizar Hyat</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">I want to withdraw the paper due to serious error</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.3021v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.3021v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.4999v2</id>
    <updated>2013-12-08T10:24:01Z</updated>
    <published>2013-05-22T01:25:02Z</published>
    <title>Optimal Frame Transmission for Scalable Video with Hierarchical
  Prediction Structure</title>
    <summary>  An optimal frame transmission scheme is presented for streaming scalable
video over a link with limited capacity. The objective is to select a
transmission sequence of frames and their transmission schedule such that the
overall video quality is maximized. The problem is solved for two general
classes of hierarchical prediction structures, which include as a special case
the popular dyadic structure. Based on a new characterization of the
interdependence among frames in terms of trees, structural properties of an
optimal transmission schedule are derived. These properties lead to the
development of a jointly optimal frame selection and scheduling algorithm,
which has computational complexity that is quadratic in the number of frames.
Simulation results show that the optimal scheme substantially outperforms three
existing alternatives.
</summary>
    <author>
      <name>Saied Mehdian</name>
    </author>
    <author>
      <name>Ben Liang</name>
    </author>
    <link href="http://arxiv.org/abs/1305.4999v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.4999v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.0642v1</id>
    <updated>2013-07-02T09:33:43Z</updated>
    <published>2013-07-02T09:33:43Z</published>
    <title>A Novel Steganography Algorithm for Hiding Text in Image using Five
  Modulus Method</title>
    <summary>  The needs for steganographic techniques for hiding secret message inside
images have been arise. This paper is to create a practical steganographic
implementation to hide text inside grey scale images. The secret message is
hidden inside the cover image using Five Modulus Method. The novel algorithm is
called (ST-FMM. FMM which consists of transforming all the pixels within the
5X5 window size into its corresponding multiples of 5. After that, the secret
message is hidden inside the 5X5 window as a non-multiples of 5. Since the
modulus of non-multiples of 5 are 1,2,3 and 4, therefore; if the reminder is
one of these, then this pixel represents a secret character. The secret key
that has to be sent is the window size. The main advantage of this novel
algorithm is to keep the size of the cover image constant while the secret
message increased in size. Peak signal-to-noise ratio is captured for each of
the images tested. Based on the PSNR value of each images, the stego image has
high PSNR value. Hence this new steganography algorithm is very efficient to
hide the data inside the image.
</summary>
    <author>
      <name>Firas A. Jassim</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications, Vol.72, No.17, pp.
  39-44, June 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.0642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.0642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.2818v1</id>
    <updated>2013-07-10T15:07:26Z</updated>
    <published>2013-07-10T15:07:26Z</published>
    <title>Anisotropic Diffusion for Details Enhancement in Multi-Exposure Image
  Fusion</title>
    <summary>  We develop a multiexposure image fusion method based on texture features,
which exploits the edge preserving and intraregion smoothing property of
nonlinear diffusion filters based on partial differential equations (PDE). With
the captured multiexposure image series, we first decompose images into base
layers and detail layers to extract sharp details and fine details,
respectively. The magnitude of the gradient of the image intensity is utilized
to encourage smoothness at homogeneous regions in preference to inhomogeneous
regions. Then, we have considered texture features of the base layer to
generate a mask (i.e., decision mask) that guides the fusion of base layers in
multiresolution fashion. Finally, well-exposed fused image is obtained that
combines fused base layer and the detail layers at each scale across all the
input exposures. Proposed algorithm skipping complex High Dynamic Range Image
(HDRI) generation and tone mapping steps to produce detail preserving image for
display on standard dynamic range display devices. Moreover, our technique is
effective for blending flash/no-flash image pair and multifocus images, that
is, images focused on different targets.
</summary>
    <author>
      <name>Harbinder Singh</name>
    </author>
    <author>
      <name>Vinay Kumar</name>
    </author>
    <author>
      <name>Sunil Bhooshan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1155/2013/928971</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1155/2013/928971" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ISRN Signal Processing, vol. 2013, Article ID 928971, 18 pages,
  2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.2818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.2818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3026v1</id>
    <updated>2013-07-11T09:26:21Z</updated>
    <published>2013-07-11T09:26:21Z</published>
    <title>Comparison of secure and high capacity color image steganography
  techniques in RGB and YCbCr domains</title>
    <summary>  Steganography is one of the methods used for secret communication.
Steganography attempts to hide the existence of the information. The object
used to hide the secret information is called as cover object. Images are the
most popular cover objects used for steganography. Different techniques have to
be used for color image steganography and grey scale image steganography since
they are stored in different ways. Color image are normally stored with 24 bit
depth and grey scale images are stored with 8 bit depth. Color images can hold
large amount of secret information since they have three color components.
Different color spaces namely RGB (Red Green Blue), HSV (Hue, Saturation,
Value), YUV, YIQ, YCbCr (Luminance, Chrominance) etc. are used to represent
color images. Color image steganography can be done in any color space domain.
In this paper color image steganography in RGB and YCbCr domain are compared.
The secret information considered is grey scale image. Since RGB is the common
method of representation, hiding secret information in this format is not
secure.
</summary>
    <author>
      <name>S. Hemalatha</name>
    </author>
    <author>
      <name>U. Dinesh Acharya</name>
    </author>
    <author>
      <name>A. Renuka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Journal, 9 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Information Technology (IJAIT)
  Vol. 3, No. 3, June 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.3026v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3026v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3294v1</id>
    <updated>2013-07-11T23:35:44Z</updated>
    <published>2013-07-11T23:35:44Z</published>
    <title>A dwt, dct and svd based watermarking technique to protect the image
  piracy</title>
    <summary>  With the rapid development of information technology and multimedia, the use
of digital data is increasing day by day. So it becomes very essential to
protect multimedia information from piracy and also it is challenging. A great
deal of Copyright owners is worried about protecting any kind of illegal
repetition of their information. Hence, facing all these kinds of problems
development of the techniques is very important. Digital watermarking
considered as a solution to prevent the multimedia data. In this paper, an idea
of watermarking is proposed and implemented. In proposed watermarking method,
the original image is rearranged using zigzag sequence and DWT is applied on
rearranged image. Then DCT and SVD are applied on all high bands LH, HL and HH.
Watermark is then embedded by modifying the singular values of these bands.
Extraction of watermark is performed by the inversion of watermark embedding
process. For choosing of these three bands it gives facility of mid-band and
pure high band that ensures good imperceptibility and more robustness against
different kinds of attacks.
</summary>
    <author>
      <name>Md. Maklachur Rahman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijmpict.2013.4203</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijmpict.2013.4203" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 12 figures and 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Managing Public Sector Information and
  Communication Technologies (IJMPICT) Vol. 4, No. 2, June 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.3294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3811v1</id>
    <updated>2013-07-15T03:14:05Z</updated>
    <published>2013-07-15T03:14:05Z</published>
    <title>Multiview Hessian Discriminative Sparse Coding for Image Annotation</title>
    <summary>  Sparse coding represents a signal sparsely by using an overcomplete
dictionary, and obtains promising performance in practical computer vision
applications, especially for signal restoration tasks such as image denoising
and image inpainting. In recent years, many discriminative sparse coding
algorithms have been developed for classification problems, but they cannot
naturally handle visual data represented by multiview features. In addition,
existing sparse coding algorithms use graph Laplacian to model the local
geometry of the data distribution. It has been identified that Laplacian
regularization biases the solution towards a constant function which possibly
leads to poor extrapolating power. In this paper, we present multiview Hessian
discriminative sparse coding (mHDSC) which seamlessly integrates Hessian
regularization with discriminative sparse coding for multiview learning
problems. In particular, mHDSC exploits Hessian regularization to steer the
solution which varies smoothly along geodesics in the manifold, and treats the
label information as an additional view of feature for incorporating the
discriminative power for image annotation. We conduct extensive experiments on
PASCAL VOC'07 dataset and demonstrate the effectiveness of mHDSC for image
annotation.
</summary>
    <author>
      <name>Weifeng Liu</name>
    </author>
    <author>
      <name>Dacheng Tao</name>
    </author>
    <author>
      <name>Jun Cheng</name>
    </author>
    <author>
      <name>Yuanyan Tang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer vision and image understanding,118(2014) 50-60</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1307.3811v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3811v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.4581v4</id>
    <updated>2014-04-10T00:51:45Z</updated>
    <published>2013-07-17T11:23:09Z</published>
    <title>Smart Streaming for Online Video Services</title>
    <summary>  Bandwidth consumption is a significant concern for online video service
providers. Practical video streaming systems usually use some form of HTTP
streaming (progressive download) to let users download the video at a faster
rate than the video bitrate. Since users may quit before viewing the complete
video, however, much of the downloaded video will be "wasted". To the extent
that users' departure behavior can be predicted, we develop smart streaming
that can be used to improve user QoE with limited server bandwidth or save
bandwidth cost with unlimited server bandwidth. Through measurement, we extract
certain user behavior properties for implementing such smart streaming, and
demonstrate its advantage using prototype implementation as well as
simulations.
</summary>
    <author>
      <name>Liang Chen</name>
    </author>
    <author>
      <name>Yipeng Zhou</name>
    </author>
    <author>
      <name>Dah Ming Chiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been updated after checking the possible issues</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.4581v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.4581v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.8385v1</id>
    <updated>2013-07-31T16:55:35Z</updated>
    <published>2013-07-31T16:55:35Z</published>
    <title>A simple technique for steganography</title>
    <summary>  A new technique for data hiding in digital image is proposed in this paper.
Steganography is a well known technique for hiding data in an image, but
generally the format of image plays a pivotal role in it, and the scheme is
format dependent. In this paper we will discuss a new technique where
irrespective of the format of image, we can easily hide a large amount of data
without deteriorating the quality of the image. The data to be hidden is
enciphered with the help of a secret key. This enciphered data is then embedded
at the end of the image. The enciphered data bits are extracted and then
deciphered with the help of same key used for encryption. Simulation results
show that Image Quality Measures of this proposed scheme are better than the
conventional LSB replacing technique. The proposed method is simple and is easy
to implement.
</summary>
    <author>
      <name>Adity Sharma</name>
    </author>
    <author>
      <name>Anoo Agarwal</name>
    </author>
    <author>
      <name>Vinay Kumar</name>
    </author>
    <link href="http://arxiv.org/abs/1307.8385v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.8385v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.0315v1</id>
    <updated>2013-08-01T19:45:23Z</updated>
    <published>2013-08-01T19:45:23Z</published>
    <title>MAS for video objects segmentation and tracking based on active contours
  and SURF descriptor</title>
    <summary>  In computer vision, video segmentation and tracking is an important
challenging issue. In this paper, we describe a new video sequences
segmentation and tracking algorithm based on MAS "multi-agent systems" and SURF
"Speeded Up Robust Features". Our approach consists in modelling a multi-agent
system for segmenting the first image from a video sequence and tracking
objects in the video sequences. The used agents are supervisor and explorator
agents, they are communicating between them and they inspire in their behavior
from active contours approaches. The tracking of objects is based on SURF
descriptors "Speed Up Robust Features". We used the DIMA platform and "API
Ateji PX" (an extension of the Java language to facilitate parallel programming
on heterogeneous architectures) to implement this algorithm. The experimental
results indicate that the proposed algorithm is more robust and faster than
previous approaches.
</summary>
    <author>
      <name>Mohamed Chakroun</name>
    </author>
    <author>
      <name>Ali Wali</name>
    </author>
    <author>
      <name>Adel M. Alimi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 10,
  Issue 2, No 3, March 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1308.0315v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.0315v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.0435v1</id>
    <updated>2013-08-02T08:20:13Z</updated>
    <published>2013-08-02T08:20:13Z</published>
    <title>Improved Watermarking Scheme Using Discrete Cosine Transform and Schur
  Decomposition</title>
    <summary>  Watermarking is a technique which consists in introducing a brand, the name
or the logo of the author, in an image in order to protect it against illegal
copy. The capacity of the existing watermark channel is often limited. We
propose in this paper a new robust method which consists in adding the
triangular matrix of the mark obtained after the Schur decomposition to the DCT
transform of the host image. The unitary matrix acts as secret key for the
extraction of the mark. Unlike most watermarking algorithms, the host image and
the mark have the same size. The results show that our method is robust against
attack techniques as : JPEG compression, colors reducing, adding noise,
filtering, cropping, low rotations, and histogram spreading.
</summary>
    <author>
      <name>Henri Bruno Razafindradina</name>
    </author>
    <author>
      <name>Nicolas Raft Razafindrakoto</name>
    </author>
    <author>
      <name>Paul Auguste Randriamitantsoa</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSN International Journal of Computer Science and Network,
  Volume 2, Issue 4, August 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1308.0435v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.0435v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.1150v1</id>
    <updated>2013-08-06T01:21:35Z</updated>
    <published>2013-08-06T01:21:35Z</published>
    <title>Multimodal Approach for Video Surveillance Indexing and Retrieval</title>
    <summary>  In this paper, we present an overview of a multimodal system to indexing and
searching video sequence by the content that has been developed within the
REGIMVid project. A large part of our system has been developed as part of
TRECVideo evaluation. The MAVSIR platform provides High-level feature
extraction from audio-visual content and concept/event-based video retrieval.
We illustrate the architecture of the system as well as provide an overview of
the descriptors supported to date. Then we demonstrate the usefulness of the
toolbox in the context of feature extraction, concepts/events learning and
retrieval in large collections of video surveillance dataset. The results are
encouraging as we are able to get good results on several event categories,
while for all events we have gained valuable insights and experience.
</summary>
    <author>
      <name>Ali Wali</name>
    </author>
    <author>
      <name>Adel M. Alimi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Intelligent Computing, Volume: 1, Issue: 4 (December
  2010), Page: 165-175</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1308.1150v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.1150v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.2393v1</id>
    <updated>2013-08-11T13:29:06Z</updated>
    <published>2013-08-11T13:29:06Z</published>
    <title>An Efficient Transport Protocol for delivery of Multimedia An Efficient
  Transport Protocol for delivery of Multimedia Content in Wireless Grids</title>
    <summary>  A grid computing system is designed for solving complicated scientific and
commercial problems effectively,whereas mobile computing is a traditional
distributed system having computing capability with mobility and adopting
wireless communications. Media and Entertainment fields can take advantage from
both paradigms by applying its usage in gaming applications and multimedia data
management. Multimedia data has to be stored and retrieved in an efficient and
effective manner to put it in use. In this paper, we proposed an application
layer protocol for delivery of multimedia data in wireless girds i.e.
multimedia grid protocol (MMGP). To make streaming efficient a new video
compression algorithm called dWave is designed and embedded in the proposed
protocol. This protocol will provide faster, reliable access and render an
imperceptible QoS in delivering multimedia in wireless grid environment and
tackles the challenging issues such as i) intermittent connectivity, ii) device
heterogeneity, iii) weak security and iv) device mobility.
</summary>
    <author>
      <name>Suresh Jaganathan</name>
    </author>
    <author>
      <name>Srinivasan Arulanadam</name>
    </author>
    <author>
      <name>Damodaram Avula</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 15 figures, Peer Reviewed Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.2393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.2393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.3225v1</id>
    <updated>2013-08-14T19:54:11Z</updated>
    <published>2013-08-14T19:54:11Z</published>
    <title>An interactive engine for multilingual video browsing using semantic
  content</title>
    <summary>  The amount of audio-visual information has increased dramatically with the
advent of High Speed Internet. Furthermore, technological advances in recent
years in the field of information technology, have simplified the use of video
data in various fields by the general public. This made it possible to store
large collections of video documents into computer systems. To enable efficient
use of these collections, it is necessary to develop tools to facilitate access
to these documents and handling them. In this paper we propose a method for
indexing and retrieval of video sequences in a video database of large
dimension, based on a weighting technique to calculate the degree of membership
of a concept in a video also a structuring of the data of the audio-visual
(context / concept / video) and a relevance feedback mechanism.
</summary>
    <author>
      <name>M. Ben Halima</name>
    </author>
    <author>
      <name>M. Hamroun</name>
    </author>
    <author>
      <name>S. Ben Moussa</name>
    </author>
    <author>
      <name>A. M. Alimi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, IGS 2013 Conference; IGS 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.3225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.3225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.3243v1</id>
    <updated>2013-08-14T20:15:44Z</updated>
    <published>2013-08-14T20:15:44Z</published>
    <title>Arabic Text Recognition in Video Sequences</title>
    <summary>  In this paper, we propose a robust approach for text extraction and
recognition from Arabic news video sequence. The text included in video
sequences is an important needful for indexing and searching system. However,
this text is difficult to detect and recognize because of the variability of
its size, their low resolution characters and the complexity of the
backgrounds. To solve these problems, we propose a system performing in two
main tasks: extraction and recognition of text. Our system is tested on a
varied database composed of different Arabic news programs and the obtained
results are encouraging and show the merits of our approach.
</summary>
    <author>
      <name>M. Ben Halima</name>
    </author>
    <author>
      <name>H. Karray</name>
    </author>
    <author>
      <name>A. M. Alimi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages - International Journal of Computational Linguistics
  Research. arXiv admin note: substantial text overlap with arXiv:1211.2150</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.3243v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.3243v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.4458v1</id>
    <updated>2013-08-21T01:13:46Z</updated>
    <published>2013-08-21T01:13:46Z</published>
    <title>Coded Acquisition of High Frame Rate Video</title>
    <summary>  High frame video (HFV) is an important investigational tool in sciences,
engineering and military. In ultra-high speed imaging, the obtainable temporal,
spatial and spectral resolutions are limited by the sustainable throughput of
in-camera mass memory, the lower bound of exposure time, and illumination
conditions. In order to break these bottlenecks, we propose a new coded video
acquisition framework that employs K &gt; 2 conventional cameras, each of which
makes random measurements of the 3D video signal in both temporal and spatial
domains. For each of the K cameras, this multi-camera strategy greatly relaxes
the stringent requirements in memory speed, shutter speed, and illumination
strength. The recovery of HFV from these random measurements is posed and
solved as a large scale l1 minimization problem by exploiting joint temporal
and spatial sparsities of the 3D signal. Three coded video acquisition
techniques of varied trade offs between performance and hardware complexity are
developed: frame-wise coded acquisition, pixel-wise coded acquisition, and
column-row-wise coded acquisition. The performances of these techniques are
analyzed in relation to the sparsity of the underlying video signal.
Simulations of these new HFV capture techniques are carried out and
experimental results are reported.
</summary>
    <author>
      <name>Reza Pournaghi</name>
    </author>
    <author>
      <name>Xiaolin Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2014.2368359</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2014.2368359" rel="related"/>
    <link href="http://arxiv.org/abs/1308.4458v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.4458v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.2359v1</id>
    <updated>2013-09-10T01:57:14Z</updated>
    <published>2013-09-10T01:57:14Z</published>
    <title>Speech Enhancement using Kernel and Normalized Kernel Affine Projection
  Algorithm</title>
    <summary>  The goal of this paper is to investigate the speech signal enhancement using
Kernel Affine Projection Algorithm (KAPA) and Normalized KAPA. The removal of
background noise is very important in many applications like speech
recognition, telephone conversations, hearing aids, forensic, etc. Kernel
adaptive filters shown good performance for removal of noise. If the evaluation
of background noise is more slowly than the speech, i.e., noise signal is more
stationary than the speech, we can easily estimate the noise during the pauses
in speech. Otherwise it is more difficult to estimate the noise which results
in degradation of speech. In order to improve the quality and intelligibility
of speech, unlike time and frequency domains, we can process the signal in new
domain like Reproducing Kernel Hilbert Space (RKHS) for high dimensional to
yield more powerful nonlinear extensions. For experiments, we have used the
database of noisy speech corpus (NOIZEUS). From the results, we observed the
removal noise in RKHS has great performance in signal to noise ratio values in
comparison with conventional adaptive filters.
</summary>
    <author>
      <name>Bolimera Ravi</name>
    </author>
    <author>
      <name>T. Kishore Kumar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/sipij.2013.4411</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/sipij.2013.4411" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.4, No.4, August 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1309.2359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.2359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.2423v7</id>
    <updated>2013-09-27T03:08:12Z</updated>
    <published>2013-09-10T09:18:27Z</published>
    <title>Robust watermarking based on DWT SVD</title>
    <summary>  Digital information revolution has brought about many advantages and new
issues. The protection of ownership and the prevention of unauthorized
manipulation of digital audio, image, and video materials has become an
important concern due to the ease of editing and perfect reproduction.
Watermarking is identified as a major means to achieve copyright protection. It
is a branch of information hiding which is used to hide proprietary information
in digital media like photographs, digital music, digital video etc. In this
paper, a new image watermarking algorithm that is robust against various
attacks is presented. DWT (Discrete Wavelet Transform) and SVD (Singular Value
Decomposition) have been used to embed two watermarks in the HL and LH bands of
the host image. Simulation evaluation demonstrates that the proposed technique
withstand various attacks.
</summary>
    <author>
      <name>Anumol Joseph</name>
    </author>
    <author>
      <name>K. Anusudha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">paper has bee withdrawn by the author due to error in equation</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.2423v7" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.2423v7" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.7640v1</id>
    <updated>2013-09-29T19:17:18Z</updated>
    <published>2013-09-29T19:17:18Z</published>
    <title>An Efficient Authorship Protection Scheme for Shared Multimedia Content</title>
    <summary>  Many electronic content providers today like Flickr and Google, offer space
to users to publish their electronic media (e.g. photos and videos) in their
cloud infrastructures, so that they can be publicly accessed. Features like
including other information, such as keywords or owner information into the
digital material is already offered by existing providers. Despite the useful
features made available to users by such infrastructures, the authorship of the
published content is not protected against various attacks such as compression.
In this paper we propose a robust scheme that uses digital invisible
watermarking and hashing to protect the authorship of the digital content and
provide resistance against malicious manipulation of multimedia content. The
scheme is enhanced by an algorithm called MMBEC, that is an extension of an
established scheme MBEC, towards higher resistance.
</summary>
    <author>
      <name>Mohamed El-Hadedy</name>
    </author>
    <author>
      <name>Georgios Pitsilis</name>
    </author>
    <author>
      <name>Svein J. Knapskog</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICIG.2011.183</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICIG.2011.183" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extensive technical report of paper published in Sixth International
  Conference on Image and Graphics (ICIG), pp.914-919, Hefei, Anhui, China,
  August 12-15, 2011. ISBN: 978-0-7695-4541-7</arxiv:comment>
    <link href="http://arxiv.org/abs/1309.7640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.7640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.0524v1</id>
    <updated>2013-09-27T12:00:29Z</updated>
    <published>2013-09-27T12:00:29Z</published>
    <title>Steganography using the Extensible Messaging and Presence Protocol
  (XMPP)</title>
    <summary>  We present here the first work to propose different mechanisms for hiding
data in the Extensible Messaging and Presence Protocol (XMPP). This is a very
popular instant messaging protocol used by many messaging platforms such as
Google Talk, Cisco, LiveJournal and many others. Our paper describes how to
send a secret message from one XMPP client to another, without raising the
suspicion of any intermediaries. The methods described primarily focus on using
the underlying protocol as a means for steganography, unlike other related
works that try to hide data in the content of instant messages. In doing so, we
provide a more robust means of data hiding and additionally offer some
preliminary analysis of its general security, in particular against
entropic-based steganalysis.
</summary>
    <author>
      <name>Reshad Patuck</name>
    </author>
    <author>
      <name>Julio Hernandez-Castro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 3 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1310.0524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.0524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1310.6377v1</id>
    <updated>2013-10-22T18:41:45Z</updated>
    <published>2013-10-22T18:41:45Z</published>
    <title>Multiview Navigation based on Extended Layered Depth Image
  Representation</title>
    <summary>  Emerging applications in multiview streaming look for providing interactive
navigation services to video players. The user can ask for information from any
viewpoint with a minimum transmission delay. The purpose is to provide user
with as much information as possible with least number of redundancies. The
recent concept of navigation segment representation consists of regrouping a
given number of viewpoints in one signal and transmitting them to the users
according to their navigation path. The question of the best description
strategy of these navigation segments is however still open. In this paper, we
propose to represent and code navigation segments by a method that extends the
recent layered depth image (LDI) format. It consists of describing the scene
from a viewpoint with multiple images organized in layers corresponding to the
different levels of occluded objects. The notion of extended LDI comes from the
fact that the size of this image is adapted to take into account the sides of
the scene also, in contrary to classical LDI. The obtained results show a
significant rate-distortion gain compared to classical multiview compression
approaches in navigation scenario.
</summary>
    <author>
      <name>Uday Takyar</name>
    </author>
    <author>
      <name>Thomas Maugey</name>
    </author>
    <author>
      <name>Pascal Frossard</name>
    </author>
    <link href="http://arxiv.org/abs/1310.6377v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1310.6377v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.1083v1</id>
    <updated>2013-09-26T12:01:18Z</updated>
    <published>2013-09-26T12:01:18Z</published>
    <title>An Efficient Method for Image and Audio Steganography using Least
  Significant Bit (LSB) Substitution</title>
    <summary>  In order to improve the data hiding in all types of multimedia data formats
such as image and audio and to make hidden message imperceptible, a novel
method for steganography is introduced in this paper. It is based on Least
Significant Bit (LSB) manipulation and inclusion of redundant noise as secret
key in the message. This method is applied to data hiding in images. For data
hiding in audio, Discrete Cosine Transform (DCT) and Discrete Wavelet Transform
(DWT) both are used. All the results displayed prove to be time-efficient and
effective. Also the algorithm is tested for various numbers of bits. For those
values of bits, Mean Square Error (MSE) and Peak-Signal-to-Noise-Ratio (PSNR)
are calculated and plotted. Experimental results show that the stego-image is
visually indistinguishable from the original cover-image when n&lt;=4, because of
better PSNR which is achieved by this technique. The final results obtained
after steganography process does not reveal presence of any hidden message,
thus qualifying the criteria of imperceptible message.
</summary>
    <author>
      <name>Ankit Chadha</name>
    </author>
    <author>
      <name>Neha Satam</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/13547-1342</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/13547-1342" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications 77(13):37-45,
  September 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1311.1083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.1083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.1419v1</id>
    <updated>2013-11-06T15:24:17Z</updated>
    <published>2013-11-06T15:24:17Z</published>
    <title>Increasing Compression Ratio of Low Complexity Compressive Sensing Video
  Encoder with Application-Aware Configurable Mechanism</title>
    <summary>  With the development of embedded video acquisition nodes and wireless video
surveillance systems, traditional video coding methods could not meet the needs
of less computing complexity any more, as well as the urgent power consumption.
So, a low-complexity compressive sensing video encoder framework with
application-aware configurable mechanism is proposed in this paper, where novel
encoding methods are exploited based on the practical purposes of the real
applications to reduce the coding complexity effectively and improve the
compression ratio (CR). Moreover, the group of processing (GOP) size and the
measurement matrix size can be configured on the encoder side according to the
post-analysis requirements of an application example of object tracking to
increase the CR of encoder as best as possible. Simulations show the proposed
framework of encoder could achieve 60X of CR when the tracking successful rate
(SR) is still keeping above 90%.
</summary>
    <author>
      <name>Shuang Yu</name>
    </author>
    <author>
      <name>Fei Qiao</name>
    </author>
    <author>
      <name>Li Luo</name>
    </author>
    <author>
      <name>Huazhong Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages with 6figures and 1 table,conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.1419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.1419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.1700v1</id>
    <updated>2013-11-07T14:45:30Z</updated>
    <published>2013-11-07T14:45:30Z</published>
    <title>Image Steganography using Karhunen-Loeve Transform and Least Bit
  Substitution</title>
    <summary>  As communication channels are increasing in number, reliability of faithful
communication is reducing. Hacking and tempering of data are two major issues
for which security should be provided by channel. This raises the importance of
steganography. In this paper, a novel method to encode the message information
inside a carrier image has been described. It uses Karhunen-Lo\`eve Transform
for compression of data and Least Bit Substitution for data encryption.
Compression removes redundancy and thus also provides encoding to a level. It
is taken further by means of Least Bit Substitution. The algorithm used for
this purpose uses pixel matrix which serves as a best tool to work on. Three
different sets of images were used with three different numbers of bits to be
substituted by message information. The experimental results show that
algorithm is time efficient and provides high data capacity. Further, it can
decrypt the original data effectively. Parameters such as carrier error and
message error were calculated for each set and were compared for performance
analysis.
</summary>
    <author>
      <name>Ankit Chadha</name>
    </author>
    <author>
      <name>Neha Satam</name>
    </author>
    <author>
      <name>Rakshak Sood</name>
    </author>
    <author>
      <name>Dattatray Bade</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/13771-1628 10.5120/13771-1628 10.5120/13771-1628</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/13771-1628" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.5120/13771-1628" rel="related"/>
    <link title="doi" href="http://dx.doi.org/10.5120/13771-1628" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications 79(9):31-37,
  October 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1311.1700v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.1700v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.3220v1</id>
    <updated>2013-11-13T17:21:42Z</updated>
    <published>2013-11-13T17:21:42Z</published>
    <title>Chaotic Arithmetic Coding for Secure Video Multicast</title>
    <summary>  Arithmetic Coding (AC) is widely used for the entropy coding of text and
video data. It involves recursive partitioning of the range [0,1) in accordance
with the relative probabilities of occurrence of the input symbols. A data
(image or video) encryption scheme based on arithmetic coding called as Chaotic
Arithmetic Coding (CAC) has been presented in previous works. In CAC, a large
number of chaotic maps can be used to perform coding, each achieving Shannon
optimal compression performance. The exact choice of map is governed by a key.
CAC has the effect of scrambling the intervals without making any changes to
the width of interval in which the codeword must lie, thereby allowing
encryption without sacrificing any coding efficiency. In this paper, we use a
redundancy in CAC procedure for secure multicast of videos where multiple users
are distributed with different keys to decode same encrypted file. By
encrypting once, we can generate multiple keys, either of which can be used to
decrypt the encoded file. This is very suitable for video distribution over
Internet where a single video can be distributed to multiple clients in a
privacy preserving manner.
</summary>
    <author>
      <name>Gaurav Pande</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to SPIN 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.3220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.3220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.5834v1</id>
    <updated>2013-11-22T18:09:25Z</updated>
    <published>2013-11-22T18:09:25Z</published>
    <title>Traffic and Statistical Multiplexing Characterization of 3D Video
  Representation Formats (Extended Version)</title>
    <summary>  The network transport of 3D video, which contains two views of a video scene,
poses significant challenges due to the increased video data compared to
conventional single-view video. Addressing these challenges requires a thorough
understanding of the traffic and multiplexing characteristics of the different
representation formats of 3D video. We examine the average bitrate-distortion
(RD) and bitrate variability-distortion (VD) characteristics of three main
representation formats. Specifically, we compare multiview video (MV)
representation and encoding, frame sequential (FS) representation, and
side-by-side (SBS) representation, whereby conventional single-view encoding is
employed for the FS and SBS representations. Our results for long 3D videos in
full HD format indicate that the MV representation and encoding achieves the
highest RD efficiency, while exhibiting the highest bitrate variabilities. We
examine the impact of these bitrate variabilities on network transport through
extensive statistical multiplexing simulations. We find that when multiplexing
a small number of streams, the MV and FS representations require the same
bandwidth. However, when multiplexing a large number of streams or smoothing
traffic, the MV representation and encoding reduces the bandwidth requirement
relative to the FS representation.
</summary>
    <author>
      <name>Akshay Pulipaka</name>
    </author>
    <author>
      <name>Patrick Seeling</name>
    </author>
    <author>
      <name>Martin Reisslein</name>
    </author>
    <author>
      <name>Lina J. Karam</name>
    </author>
    <link href="http://arxiv.org/abs/1311.5834v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.5834v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.6355v1</id>
    <updated>2013-11-06T12:20:35Z</updated>
    <published>2013-11-06T12:20:35Z</published>
    <title>Exploration in Interactive Personalized Music Recommendation: A
  Reinforcement Learning Approach</title>
    <summary>  Current music recommender systems typically act in a greedy fashion by
recommending songs with the highest user ratings. Greedy recommendation,
however, is suboptimal over the long term: it does not actively gather
information on user preferences and fails to recommend novel songs that are
potentially interesting. A successful recommender system must balance the needs
to explore user preferences and to exploit this information for recommendation.
This paper presents a new approach to music recommendation by formulating this
exploration-exploitation trade-off as a reinforcement learning task called the
multi-armed bandit. To learn user preferences, it uses a Bayesian model, which
accounts for both audio content and the novelty of recommendations. A
piecewise-linear approximation to the model and a variational inference
algorithm are employed to speed up Bayesian inference. One additional benefit
of our approach is a single unified model for both music recommendation and
playlist generation. Both simulation results and a user study indicate strong
potential for the new approach.
</summary>
    <author>
      <name>Xinxi Wang</name>
    </author>
    <author>
      <name>Yi Wang</name>
    </author>
    <author>
      <name>David Hsu</name>
    </author>
    <author>
      <name>Ye Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1311.6355v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.6355v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.6441v1</id>
    <updated>2013-11-25T20:31:44Z</updated>
    <published>2013-11-25T20:31:44Z</published>
    <title>Modeling the Time-varying Subjective Quality of HTTP Video Streams with
  Rate Adaptations</title>
    <summary>  Newly developed HTTP-based video streaming technologies enable flexible
rate-adaptation under varying channel conditions. Accurately predicting the
users' Quality of Experience (QoE) for rate-adaptive HTTP video streams is thus
critical to achieve efficiency. An important aspect of understanding and
modeling QoE is predicting the up-to-the-moment subjective quality of a video
as it is played, which is difficult due to hysteresis effects and
nonlinearities in human behavioral responses. This paper presents a
Hammerstein-Wiener model for predicting the time-varying subjective quality
(TVSQ) of rate-adaptive videos. To collect data for model parameterization and
validation, a database of longer-duration videos with time-varying distortions
was built and the TVSQs of the videos were measured in a large-scale subjective
study. The proposed method is able to reliably predict the TVSQ of rate
adaptive videos. Since the Hammerstein-Wiener model has a very simple
structure, the proposed method is suitable for on-line TVSQ prediction in HTTP
based streaming.
</summary>
    <author>
      <name>Chao Chen</name>
    </author>
    <author>
      <name>Lark Kwon Choi</name>
    </author>
    <author>
      <name>Gustavo de Veciana</name>
    </author>
    <author>
      <name>Constantine Caramanis</name>
    </author>
    <author>
      <name>Robert W. Heath Jr.</name>
    </author>
    <author>
      <name>Alan C. Bovik</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2014.2312613</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2014.2312613" rel="related"/>
    <link href="http://arxiv.org/abs/1311.6441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.6441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.6453v1</id>
    <updated>2013-11-25T20:42:33Z</updated>
    <published>2013-11-25T20:42:33Z</published>
    <title>Rate Adaptation and Admission Control for Video Transmission with
  Subjective Quality Constraints</title>
    <summary>  Adapting video data rate during streaming can effectively reduce the risk of
playback interruptions caused by channel throughput fluctuations. The
variations in rate, however, also introduce video quality fluctuations and thus
potentially affects viewers' Quality of Experience (QoE). We show how the QoE
of video users can be improved by rate adaptation and admission control. We
conducted a subjective study wherein we found that viewers' QoE was strongly
correlated with the empirical cumulative distribution function (eCDF) of the
predicted video quality. Based on this observation, we propose a
rate-adaptation algorithm that can incorporate QoE constraints on the empirical
cumulative quality distribution per user. We then propose a threshold-based
admission control policy to block users whose empirical cumulative quality
distribution is not likely to satisfy their QoE constraint. We further devise
an online adaptation algorithm to automatically optimize the threshold.
Extensive simulation results show that the proposed scheme can reduce network
resource consumption by $40\%$ over conventional average-quality maximized
rate-adaptation algorithms.
</summary>
    <author>
      <name>Chao Chen</name>
    </author>
    <author>
      <name>Xiaoqing Zhu</name>
    </author>
    <author>
      <name>Gustavo de Veciana</name>
    </author>
    <author>
      <name>Alan C. Bovik</name>
    </author>
    <author>
      <name>Robert W. Heath Jr</name>
    </author>
    <link href="http://arxiv.org/abs/1311.6453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.6453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6090v1</id>
    <updated>2013-12-20T19:55:33Z</updated>
    <published>2013-12-20T19:55:33Z</published>
    <title>Graph-based representation for multiview image coding</title>
    <summary>  In this paper, we propose a new representation for multiview image sets. Our
approach relies on graphs to describe geometry information in a compact and
controllable way. The links of the graph connect pixels in different images and
describe the proximity between pixels in the 3D space. These connections are
dependent on the geometry of the scene and provide the right amount of
information that is necessary for coding and reconstructing multiple views.
This multiview image representation is very compact and adapts the transmitted
geometry information as a function of the complexity of the prediction
performed at the decoder side. To achieve this, our GBR adapts the accuracy of
the geometry representation, in contrast with depth coding, which directly
compresses with losses the original geometry signal. We present the principles
of this graph-based representation (GBR) and we build a complete prototype
coding scheme for multiview images. Experimental results demonstrate the
potential of this new representation as compared to a depth-based approach. GBR
can achieve a gain of 2 dB in reconstructed quality over depth-based schemes
operating at similar rates.
</summary>
    <author>
      <name>Thomas Maugey</name>
    </author>
    <author>
      <name>Antonio Ortega</name>
    </author>
    <author>
      <name>Pascal Frossard</name>
    </author>
    <link href="http://arxiv.org/abs/1312.6090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.6497v1</id>
    <updated>2013-12-23T09:43:09Z</updated>
    <published>2013-12-23T09:43:09Z</published>
    <title>State-of-the Art Motion Estimation in the Context of 3D TV</title>
    <summary>  Progress in image sensors and computation power has fueled studies to improve
acquisition, processing, and analysis of 3D streams along with 3D
scenes/objects reconstruction. The role of motion compensation/motion
estimation (MCME) in 3D TV from end-to-end user is investigated in this
chapter. Motion vectors (MVs) are closely related to the concept of
disparities, and they can help improving dynamic scene acquisition, content
creation, 2D to 3D conversion, compression coding, decompression/decoding,
scene rendering, error concealment, virtual/augmented reality handling,
intelligent content retrieval, and displaying. Although there are different 3D
shape extraction methods, this chapter focuses mostly on shape-from-motion
(SfM) techniques due to their relevance to 3D TV. SfM extraction can restore 3D
shape information from a single camera data.
</summary>
    <author>
      <name>Vania V. Estrela</name>
    </author>
    <author>
      <name>Alessandra M. Coelho</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4018/978-1-4666-2660-7.ch006</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4018/978-1-4666-2660-7.ch006" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Multimedia Networking and Coding. IGI Global, 2013. 148-173. Web.
  23 Dec. 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1312.6497v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.6497v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.7442v1</id>
    <updated>2013-12-28T15:19:09Z</updated>
    <published>2013-12-28T15:19:09Z</published>
    <title>Evaluating the Performance of IPTV over Fixed WiMAX</title>
    <summary>  IEEE specifies different modulation techniques for WiMAX; namely, BPSK, QPSK,
16 QAM and 64 QAM. This paper studies the performance of Internet Protocol
Television (IPTV) over Fixed WiMAX system considering different combinations of
digital modulation. The performance is studied taking into account a number of
key system parameters which include the variation in the video coding,
path-loss, scheduling service classes different rated codes in FEC channel
coding. The performance study was conducted using OPNET simulation. The
performance is studied in terms of packet lost, packet jitter delay, end-to-end
delay, and network throughput. Simulation results show that higher order
modulation and coding schemes (namely, 16 QAM and 64 QAM) yield better
performance than that of QPSK.
</summary>
    <author>
      <name>Jamil Hamodi</name>
    </author>
    <author>
      <name>Khaled Salah</name>
    </author>
    <author>
      <name>Ravindra Thool</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/14582-2812</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/14582-2812" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages, 9 Figures. arXiv admin note: substantial text overlap with
  other internet sources by other authors</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications 84(6):35-43,
  December 2013. Published by Foundation of Computer Science, New York, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1312.7442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.7442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.2482v1</id>
    <updated>2014-01-10T23:36:51Z</updated>
    <published>2014-01-10T23:36:51Z</published>
    <title>STIMONT: A core ontology for multimedia stimuli description</title>
    <summary>  Affective multimedia documents such as images, sounds or videos elicit
emotional responses in exposed human subjects. These stimuli are stored in
affective multimedia databases and successfully used for a wide variety of
research in psychology and neuroscience in areas related to attention and
emotion processing. Although important all affective multimedia databases have
numerous deficiencies which impair their applicability. These problems, which
are brought forward in the paper, result in low recall and precision of
multimedia stimuli retrieval which makes creating emotion elicitation
procedures difficult and labor-intensive. To address these issues a new core
ontology STIMONT is introduced. The STIMONT is written in OWL-DL formalism and
extends W3C EmotionML format with an expressive and formal representation of
affective concepts, high-level semantics, stimuli document metadata and the
elicited physiology. The advantages of ontology in description of affective
multimedia stimuli are demonstrated in a document retrieval experiment and
compared against contemporary keyword-based querying methods. Also, a software
tool Intelligent Stimulus Generator for retrieval of affective multimedia and
construction of stimuli sequences is presented.
</summary>
    <author>
      <name>Marko Horvat</name>
    </author>
    <author>
      <name>Nikola Bogunoviƒá</name>
    </author>
    <author>
      <name>Kre≈°imir ƒÜosiƒá</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11042-013-1624-4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11042-013-1624-4" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">27 pages, 13 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Multimedia tools and applications, 11042, July 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1401.2482v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.2482v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.5966v2</id>
    <updated>2016-08-27T00:50:13Z</updated>
    <published>2014-01-23T13:23:27Z</published>
    <title>Image Block Loss Restoration Using Sparsity Pattern as Side Information</title>
    <summary>  In this paper, we propose a method for image block loss restoration based on
the notion of sparse representation. We use the sparsity pattern as side
information to efficiently restore block losses by iteratively imposing the
constraints of spatial and transform domains on the corrupted image. Two novel
features, including a pre-interpolation and a criterion for stopping the
iterations, are proposed to improve the performance. Also, to deal with
practical applications, we develop a technique to transmit the side information
along with the image. In this technique, we first compress the side information
and then embed its LDPC coded version in the least significant bits of the
image pixels. This technique ensures the error-free transmission of the side
information, while causing only a small perturbation on the transmitted image.
Mathematical analysis and extensive simulations are performed to validate the
method and investigate the efficiency of the proposed techniques. The results
verify that the proposed method outperforms its counterparts for image block
loss restoration.
</summary>
    <author>
      <name>Hossein Hosseini</name>
    </author>
    <author>
      <name>Ali Goli</name>
    </author>
    <author>
      <name>Neda Barzegar Marvasti</name>
    </author>
    <author>
      <name>Masoume Azghani</name>
    </author>
    <author>
      <name>Farokh Marvasti</name>
    </author>
    <link href="http://arxiv.org/abs/1401.5966v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.5966v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.0812v1</id>
    <updated>2014-02-04T18:05:37Z</updated>
    <published>2014-02-04T18:05:37Z</published>
    <title>A Study on the Optimal Implementation of Statistical Multiplexing in DVB
  Distribution Systems</title>
    <summary>  The paper presents an overview of the main methods used to improve the
efficiency of DVB systems, based on multiplexing, through a study on the impact
of the multiplexing methods used in DVB, having as a final goal a better usage
of the data capacity and the possibility to insert new services into the
original DVB Transport Stream. This study revealed that not all DVB providers
are using statistical multiplexing. Based on this study, we were able to
propose a method to improve the original DVB stream, originated from DVB-S or
DVB-T providers. This method is proposing the detection of null packets,
removal and reinserting a new service, with a VBR content. The method developed
in this research can be implemented even in optimized statistical multiplexing
systems, due to a residual use of null packets for data rate adjustment. There
is no need to have access in the original stream multiplexer, since the method
allows the implementation on the fly, near to the end user. The proposed method
is proposed to be applied in DVB-S to DVB-C translation, using the computing
power of a PC or in a FPGA implementation.
</summary>
    <author>
      <name>Alexandru Florin Antone</name>
    </author>
    <author>
      <name>Radu Arsinte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 17 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Informatics and IT Today, Volume 1, July 2013, pp.19-27</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.0812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.0812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.5979v2</id>
    <updated>2016-12-11T19:23:57Z</updated>
    <published>2014-02-24T21:04:41Z</published>
    <title>A Multiplierless Pruned DCT-like Transformation for Image and Video
  Compression that Requires 10 Additions Only</title>
    <summary>  A multiplierless pruned approximate 8-point discrete cosine transform (DCT)
requiring only 10 additions is introduced. The proposed algorithm was assessed
in image and video compression, showing competitive performance with
state-of-the-art methods. Digital implementation in 45 nm CMOS technology up to
place-and-route level indicates clock speed of 288 MHz at a 1.1 V supply. The
8x8 block rate is 36 MHz.The DCT approximation was embedded into HEVC reference
software; resulting video frames, at up to 327 Hz for 8-bit RGB HEVC, presented
negligible image degradation.
</summary>
    <author>
      <name>V. A. Coutinho</name>
    </author>
    <author>
      <name>R. J. Cintra</name>
    </author>
    <author>
      <name>F. M. Bayer</name>
    </author>
    <author>
      <name>S. Kulasekera</name>
    </author>
    <author>
      <name>A. Madanayake</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11554-015-0492-8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11554-015-0492-8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 4 figures, 5 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Real-Time Image Processing, August 2016, Volume 12,
  Issue 2, pp 247-255</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.5979v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.5979v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.3710v1</id>
    <updated>2014-03-14T21:08:28Z</updated>
    <published>2014-03-14T21:08:28Z</published>
    <title>Saving Energy in Mobile Devices for On-Demand Multimedia Streaming -- A
  Cross-Layer Approach</title>
    <summary>  This paper proposes a novel energy-efficient multimedia delivery system
called EStreamer. First, we study the relationship between buffer size at the
client, burst-shaped TCP-based multimedia traffic, and energy consumption of
wireless network interfaces in smartphones. Based on the study, we design and
implement EStreamer for constant bit rate and rate-adaptive streaming.
EStreamer can improve battery lifetime by 3x, 1.5x and 2x while streaming over
Wi-Fi, 3G and 4G respectively.
</summary>
    <author>
      <name>Mohammad Ashraful Hoque</name>
    </author>
    <author>
      <name>Matti Siekkinen</name>
    </author>
    <author>
      <name>Jukka K. Nurminen</name>
    </author>
    <author>
      <name>Sasu Tarkoma</name>
    </author>
    <author>
      <name>Mika Aalto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2556942</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2556942" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in ACM Transactions on Multimedia Computing, Communications
  and Applications (ACM TOMCCAP), November 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.3710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.3710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2.1; C.2.4; C.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.4169v1</id>
    <updated>2014-03-17T17:03:01Z</updated>
    <published>2014-03-17T17:03:01Z</published>
    <title>Pervasive Image Computation: A Mobile Phone Application for getting
  Information of the Images</title>
    <summary>  Although many of the information processing systems are text-based, much of
the information in the real life is generally multimedia objects, so there is a
need to define and standardize the frame works for multimedia-based information
processing systems. In this paper we consider the application of such a system
namely pervasive image computation system, in which the user uses the cellphone
for taking the picture of the objects, and he wants to get some information
about them. We have implemented two architectures, the first one, called online
architecture, which the user sends the picture to the server and server sends
the picture information directly back to him. In the second one, which is
called offline architecture, the user uploads the image in one public image
database such as Flickr and sends the ID of the image in this database to the
server. The server processes the image and adds the information of the image in
the database, and finally the user can connect to the database and download the
image information. The implementation results show that these architectures are
very flexible and could be easily extended to be used in more complicated
pervasive multimedia systems.
</summary>
    <author>
      <name>Reza Rahimi</name>
    </author>
    <author>
      <name>J Hengmeechai</name>
    </author>
    <link href="http://arxiv.org/abs/1403.4169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.4169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.1314v1</id>
    <updated>2014-03-21T06:10:27Z</updated>
    <published>2014-03-21T06:10:27Z</published>
    <title>Robust Video Watermarking Schemes in Phase domain Using Binary Phase
  Shift Keying</title>
    <summary>  This paper presents a robust video watermarking scheme in Discrete Fourier
Transform (DFT) and Sequencyordered Complex Hadamard Transform (SCHT). The DFT
and SCHT coefficients are complex and consist of both magnitude and phase and
are well suited to adopt phase shift keying techniques to embed the watermark.
In the proposed schemes, the phases of DFT and SCHT coefficients are modified
to convey watermark information using binary phase shift keying in cover video.
Low amplitude block selection (LABS) is used to improve transparency, amplitude
boost to improve the resistance of watermark from signal processing and
compression attacks and spread spectrum technique is used for encrypting
watermark in order to protect it from third party. It is observed that both
algorithms showing more or less same robustness but SCHT offers high
transparency, simple implementation and less computational cost than DFT.
</summary>
    <author>
      <name>K. Meenakshi</name>
    </author>
    <author>
      <name>Ch. Srinivasa Rao</name>
    </author>
    <author>
      <name>K. Satya Prasad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.1314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.1314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.2344v1</id>
    <updated>2014-04-09T01:23:31Z</updated>
    <published>2014-04-09T01:23:31Z</published>
    <title>Analysis of Computer Hardware Affecting Video Transmission via IEEE
  1394a connection</title>
    <summary>  When 60 de-interlaced fields per second digital uncompressed video is
streamed to a computer, some video fields are lost and not able to be stored on
a computer s hard drive successfully. Additionally, this problem amplifies once
multiple video sources are deployed. If it is possible to stream digital
uncompressed video without dropped video fields, then a sophisticated computer
analysis of the transmitted via IEEE 1394a connection video is possible. Such
process is used in biomechanics when it is important to analyze athletes
performance via streaming digital uncompressed video to a computer and then
analyzing it. If a loss of video fields occurs, then a quality analysis of
video is not possible.
</summary>
    <author>
      <name>Dr. Timur Mirzoev</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information systems. Problems, perspectives, innovation
  approaches. Volume 2. 2007. Saint Petersburg State University of Aerospace
  Instrumentation. ISBN 978-5-80888-0244-5</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.2344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.2344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.2952v1</id>
    <updated>2014-04-10T20:56:21Z</updated>
    <published>2014-04-10T20:56:21Z</published>
    <title>A blind robust watermarking scheme based on svd and circulant matrices</title>
    <summary>  Multimedia security has been the aim point of considerable research activity
because of its wide application area. The major technology to achieve copyright
protection, content authentication, access control and multimedia security is
watermarking which is the process of embedding data into a multimedia element
such as image or audio, this embedded data can later be extracted from, or
detected in the embedded element for different purposes. In this work, a blind
watermarking algorithm based on SVD and circulant matrices has been presented.
Every circulant matrix is associated with a matrix for which the SVD
decomposition coincides with the spectral decomposition. This leads to improve
the Chandra algorithm [1], our presentation will include a discussion on the
data hiding capacity, watermark transparency and robustness against a wide
range of common image processing attacks.
</summary>
    <author>
      <name>Noui Oussama</name>
    </author>
    <author>
      <name>Noui Lemnouar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/csit.2014.4406</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/csit.2014.4406" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures, 5 tables, Second International Conference on
  Computational Science &amp; Engineering (CSE - 2014)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Second International Conference on Computational Science and
  Engineering (CSE-2014) Dubai, UAE, April 04 ~ 05 - 2014 Volume Editors:
  Dhinaharan Nagamalai, Sundarapandian Vaidyanathan ISBN : 978-1-921987-30-4 pp
  65 - 77</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.2952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.2952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.3018v1</id>
    <updated>2014-04-11T07:13:22Z</updated>
    <published>2014-04-11T07:13:22Z</published>
    <title>Enhancing User Experience for Multi-Screen Social TV Streaming over
  Wireless Networks</title>
    <summary>  Recently, multi-screen cloud social TV is invented to transform TV into
social experience. People watching the same content on social TV may come from
different locations, while freely interact with each other through text, image,
audio and video. This crucial virtual living-room experience adds social
aspects into existing performance metrics. In this paper, we parse social TV
user experience into three elements (i.e., inter-user delay, video quality of
experience (QoE), and resource efficiency), and provide a joint analytical
framework to enhance user experience. Specifically, we propose a cloud-based
optimal playback rate allocation scheme to maximize the overall QoE while upper
bounding inter-user delay. Experiment results show that our algorithm achieves
near-optimal tradeoff between inter-user delay and video quality, and
demonstrates resilient performance even under very fast wireless channel
fading.
</summary>
    <author>
      <name>Huazi Zhang</name>
    </author>
    <author>
      <name>Yichao Jin</name>
    </author>
    <author>
      <name>Weiwen Zhang</name>
    </author>
    <author>
      <name>Yonggang Wen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/GLOCOMW.2014.7063414</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/GLOCOMW.2014.7063414" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to IEEE GLOBECOM 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.3018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.3018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.4026v2</id>
    <updated>2015-04-24T10:24:57Z</updated>
    <published>2014-04-15T19:26:13Z</published>
    <title>Improving Low Bit-Rate Video Coding using Spatio-Temporal Down-Scaling</title>
    <summary>  Good quality video coding for low bit-rate applications is important for
transmission over narrow-bandwidth channels and for storage with limited memory
capacity. In this work, we develop a previous analysis for image compression at
low bit-rates to adapt it to video signals. Improving compression using
down-scaling in the spatial and temporal dimensions is examined. We show, both
theoretically and experimentally, that at low bit-rates, we benefit from
applying spatio-temporal scaling. The proposed method includes down-scaling
before the compression and a corresponding up-scaling afterwards, while the
codec itself is left unmodified. We propose analytic models for low bit-rate
compression and spatio-temporal scaling operations. Specifically, we use
theoretic models of motion-compensated prediction of available and absent
frames as in coding and frame-rate up-conversion (FRUC) applications,
respectively. The proposed models are designed for multi-resolution analysis.
In addition, we formulate a bit-allocation procedure and propose a method for
estimating good down-scaling factors of a given video based on its second-order
statistics and the given bit-budget. We validate our model with experimental
results of H.264 compression.
</summary>
    <author>
      <name>Yehuda Dar</name>
    </author>
    <author>
      <name>Alfred M. Bruckstein</name>
    </author>
    <link href="http://arxiv.org/abs/1404.4026v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.4026v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.4543v1</id>
    <updated>2014-04-17T14:42:03Z</updated>
    <published>2014-04-17T14:42:03Z</published>
    <title>A Novel Approach for Video Temporal Annotation</title>
    <summary>  Recent advances in computing, communication, and data storage have led to an
increasing number of large digital libraries publicly available on the
Internet. Main problem of content-based video retrieval is inferring semantics
from raw video data. Video data play an important role in these libraries.
Instead of words, a video retrieval system deals with collections of video
records. Therefore, the system is confronted with the problem of video
understanding. Because machine understanding of the video data is still an
unsolved research problem, text annotations are usually used to describe the
content of video data according to the annotator's understanding and the
purpose of that video data. Most of proposed systems for video annotation are
domain dependent. In addition, in many of these systems, an important feature
of video data, temporality, is disregarded. In this paper, we proposed a
framework for video temporal annotation. The proposed system uses domain
knowledge and a time ontology to perform temporal annotation of input video.
</summary>
    <author>
      <name>Hadi Restgou Haghi</name>
    </author>
    <author>
      <name>Mohammadreza Kangavari</name>
    </author>
    <author>
      <name>Behrang QasemiZadeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in a Local Confrence, 2006</arxiv:comment>
    <link href="http://arxiv.org/abs/1404.4543v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.4543v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Uxx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.7237v1</id>
    <updated>2014-04-29T05:20:25Z</updated>
    <published>2014-04-29T05:20:25Z</published>
    <title>A Smart Intelligent Way of Video Authentication Using Classification and
  Decomposition of Watermarking Methods</title>
    <summary>  Video Watermarking serves as a new technology mainly used to provide security
to the illegal distribution of digital video over the web. The purpose of any
video watermarking scheme is to embed extra information into video in such a
way that must be perceptually undetectable while still holding enough
information in order to extract the watermark beginning with the resultant
video. Information which is embedded within the original image is a Digital
Watermark, which could be visible or invisible. To improved more security,
embedding and extraction Watermark process should be complex against attackers.
Recent research indicates SVD (Singular Value Decomposition) algorithms are
employed owing to their simple scheme with mathematical function. In this
proposed work an advanced SVD transformation algorithm is used for embedding
and extraction process. Experimental results show proposed watermarking process
is more secured than existing SVD approach.
</summary>
    <author>
      <name>T. Srinivasa Rao</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">InformationTechnology, UshaRama College of Engineering &amp; Technology, India</arxiv:affiliation>
    </author>
    <author>
      <name>Rajasekhar R. Kurra</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Principal, Sri Prakash College of Engineering &amp; Technology, India</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V10P123</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V10P123" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Pages, 4 Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Trends and Technology (IJCTT)
  V10(3):136-142, Apr 2014. ISSN:2231-2803. www.ijcttjournal.org. Published by
  Seventh Sense Research Group</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.7237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.7237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.3173v1</id>
    <updated>2014-05-11T08:45:07Z</updated>
    <published>2014-05-11T08:45:07Z</published>
    <title>Image Restoration Using Joint Statistical Modeling in Space-Transform
  Domain</title>
    <summary>  This paper presents a novel strategy for high-fidelity image restoration by
characterizing both local smoothness and nonlocal self-similarity of natural
images in a unified statistical manner. The main contributions are three-folds.
First, from the perspective of image statistics, a joint statistical modeling
(JSM) in an adaptive hybrid space-transform domain is established, which offers
a powerful mechanism of combining local smoothness and nonlocal self-similarity
simultaneously to ensure a more reliable and robust estimation. Second, a new
form of minimization functional for solving image inverse problem is formulated
using JSM under regularization-based framework. Finally, in order to make JSM
tractable and robust, a new Split-Bregman based algorithm is developed to
efficiently solve the above severely underdetermined inverse problem associated
with theoretical proof of convergence. Extensive experiments on image
inpainting, image deblurring and mixed Gaussian plus salt-and-pepper noise
removal applications verify the effectiveness of the proposed algorithm.
</summary>
    <author>
      <name>Jian Zhang</name>
    </author>
    <author>
      <name>Debin Zhao</name>
    </author>
    <author>
      <name>Ruiqin Xiong</name>
    </author>
    <author>
      <name>Siwei Ma</name>
    </author>
    <author>
      <name>Wen Gao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCSVT.2014.2302380</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCSVT.2014.2302380" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 18 figures, 7 Tables, to be published in IEEE Transactions
  on Circuits System and Video Technology (TCSVT). High resolution pdf version
  and Code can be found at: http://idm.pku.edu.cn/staff/zhangjian/IRJSM/</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.3173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.3173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.4709v1</id>
    <updated>2014-05-19T13:10:09Z</updated>
    <published>2014-05-19T13:10:09Z</published>
    <title>YouTube QoE Evaluation Tool for Android Wireless Terminals</title>
    <summary>  In this paper, we present an Android application which is able to evaluate
and analyze the perceived Quality of Experience (QoE) for YouTube service in
wireless terminals. To achieve this goal, the application carries out
measurements of objective Quality of Service (QoS) parameters, which are then
mapped onto subjective QoE (in terms of Mean Opinion Score, MOS) by means of a
utility function. Our application also informs the user about potential causes
that lead to a low MOS as well as provides some hints to improve it. After each
YouTube session, the users may optionally qualify the session through an online
opinion survey. This information has been used in a pilot experience to
correlate the theoretical QoE model with real user feedback. Results from such
an experience have shown that the theoretical model (taken from the literature)
provides slightly more pessimistic results compared to user feedback. Users
seem to be more indulgent with wireless connections, increasing the MOS from
the opinion survey in about 20% compared to the theoretical model, which was
obtained from wired scenarios.
</summary>
    <author>
      <name>Gerardo Gomez</name>
    </author>
    <author>
      <name>Lorenzo Hortiguela</name>
    </author>
    <author>
      <name>Quiliano Perez</name>
    </author>
    <author>
      <name>Javier Lorca</name>
    </author>
    <author>
      <name>Raquel Garcia</name>
    </author>
    <author>
      <name>Mari Carmen Aguayo-Torres</name>
    </author>
    <link href="http://arxiv.org/abs/1405.4709v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.4709v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.5340v1</id>
    <updated>2014-05-21T09:01:32Z</updated>
    <published>2014-05-21T09:01:32Z</published>
    <title>A hybrid video quality metric for analyzing quality degradation due to
  frame drop</title>
    <summary>  In last decade, ever growing internet technologies provided platform to share
the multimedia data among different communities. As the ultimate users are
human subjects who are concerned about quality of visual information, it is
often desired to have good resumed perceptual quality of videos, thus arises
the need of quality assessment. This paper presents a full reference hybrid
video quality metric which is capable to analyse the video quality for
spatially or temporally (frame drop) or spatio-temporally distorted video
sequences. Simulated results show that the metric efficiently analyses the
quality degradation and more closer to the developed human visual system
</summary>
    <author>
      <name>Manish K Thakur</name>
    </author>
    <author>
      <name>Vikas Saxena</name>
    </author>
    <author>
      <name>J P Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 6, No 1, November 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1405.5340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.5340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.6661v1</id>
    <updated>2014-05-08T07:16:46Z</updated>
    <published>2014-05-08T07:16:46Z</published>
    <title>A scenario based approach for dealing with challenges in a pervasive
  computing environment</title>
    <summary>  With the surge in modern research focus towards Pervasive Computing, lot of
techniques and challenges needs to be addressed so as to effectively create
smart spaces and achieve miniaturization. In the process of scaling down to
compact devices, the real things to ponder upon are the Information Retrieval
challenges. In this work, we discuss the aspects of multimedia which makes
information access challenging. An Example Pattern Recognition scenario is
presented and the mathematical techniques that can be used to model uncertainty
are also presented for developing a system that can sense, compute and
communicate in a way that can make human life easy with smart objects assisting
from around his surroundings.
</summary>
    <author>
      <name>Divyajyothi M G</name>
    </author>
    <author>
      <name> Rachappa</name>
    </author>
    <author>
      <name>D H Rao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsa.2014.4204</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsa.2014.4204" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, IJCSA, Vol 4, No.2,April 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.6661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.6661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.7102v2</id>
    <updated>2014-06-14T20:17:48Z</updated>
    <published>2014-05-28T02:07:29Z</published>
    <title>Detection Bank: An Object Detection Based Video Representation for
  Multimedia Event Recognition</title>
    <summary>  While low-level image features have proven to be effective representations
for visual recognition tasks such as object recognition and scene
classification, they are inadequate to capture complex semantic meaning
required to solve high-level visual tasks such as multimedia event detection
and recognition. Recognition or retrieval of events and activities can be
improved if specific discriminative objects are detected in a video sequence.
In this paper, we propose an image representation, called Detection Bank, based
on the detection images from a large number of windowed object detectors where
an image is represented by different statistics derived from these detections.
This representation is extended to video by aggregating the key frame level
image representations through mean and max pooling. We empirically show that it
captures complementary information to state-of-the-art representations such as
Spatial Pyramid Matching and Object Bank. These descriptors combined with our
Detection Bank representation significantly outperforms any of the
representations alone on TRECVID MED 2011 data.
</summary>
    <author>
      <name>Tim Althoff</name>
    </author>
    <author>
      <name>Hyun Oh Song</name>
    </author>
    <author>
      <name>Trevor Darrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Multimedia 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.7102v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.7102v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.7571v1</id>
    <updated>2014-05-29T14:43:55Z</updated>
    <published>2014-05-29T14:43:55Z</published>
    <title>JPEG Noises beyond the First Compression Cycle</title>
    <summary>  This paper focuses on the JPEG noises, which include the quantization noise
and the rounding noise, during a JPEG compression cycle. The JPEG noises in the
first compression cycle have been well studied; however, so far less attention
has been paid on the JPEG noises in higher compression cycles. In this work, we
present a statistical analysis on JPEG noises beyond the first compression
cycle. To our knowledge, this is the first work on this topic. We find that the
noise distributions in higher compression cycles are different from those in
the first compression cycle, and they are dependent on the quantization
parameters used between two successive cycles. To demonstrate the benefits from
the statistical analysis, we provide two applications that can employ the
derived noise distributions to uncover JPEG compression history with
state-of-the-art performance.
</summary>
    <author>
      <name>Bin Li</name>
    </author>
    <author>
      <name>Tian-Tsong Ng</name>
    </author>
    <author>
      <name>Xiaolong Li</name>
    </author>
    <author>
      <name>Shunquan Tan</name>
    </author>
    <author>
      <name>Jiwu Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1405.7571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.7571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.7629v1</id>
    <updated>2014-05-28T17:15:51Z</updated>
    <published>2014-05-28T17:15:51Z</published>
    <title>QoE assessment for SVC streaming in ENVISION</title>
    <summary>  Scalable video coding has drawn great interest in content delivery in many
multimedia services thanks to its capability to handle terminal heterogeneity
and network conditions variation. In our previous work, and under the umbrella
of ENVISION, we have proposed a playout smoothing mechanism to ensure the
uniform delivery of the layered stream, by reducing the quality changes that
the stream undergoes when adapting to changing network conditions. In this
paper we study the resulting video quality, from the final user perception
under different network conditions of loss and delays. For that we have adopted
the Double Stimulus Impairment Scale (DSIS) method. The results show that the
Mean Opinion Score for the smoothed video clips was higher under different
network configuration. This confirms the effectiveness of the proposed
smoothing mechanism.
</summary>
    <author>
      <name>Abbas Bradai</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Toufik Ahmed</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <author>
      <name>Samir Medjiah</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LaBRI</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE 20th International Conference on Electronics, Circuits, and
  Systems (IEEE ICECS 2013), Abu Dhabi : United Arab Emirates (2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.7629v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.7629v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.0912v3</id>
    <updated>2015-03-04T14:49:04Z</updated>
    <published>2014-06-04T00:14:06Z</published>
    <title>Towards Quality of Experience Determination for Video in Augmented
  Binocular Vision Scenarios</title>
    <summary>  With the continuous growth in the consumer markets of mobile smartphones and
increasingly in augmented reality wearable devices, several avenues of research
investigate the relationships between the quality perceived by mobile users and
the delivery mechanisms at play to support a high quality of experience for
mobile users. In this paper, we present the first study that evaluates the
relationships of mobile movie quality and the viewer-perceived quality thereof
in an augmented reality setting with see-through devices. We find that
participants tend to overestimate the video quality and exhibit a significant
variation of accuracy that leans onto the movie content and its dynamics. Our
findings, thus, can broadly impact future media adaptation and delivery
mechanisms for this new display format of mobile multimedia.
</summary>
    <author>
      <name>Patrick Seeling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Signal Processing: Image Communication</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.0912v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.0912v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2519v1</id>
    <updated>2014-06-10T12:07:22Z</updated>
    <published>2014-06-10T12:07:22Z</published>
    <title>On Importance of Steganographic Cost For Network Steganography</title>
    <summary>  Network steganography encompasses the information hiding techniques that can
be applied in communication network environments and that utilize hidden data
carriers for this purpose. In this paper we introduce a characteristic called
steganographic cost which is an indicator for the degradation or distortion of
the carrier caused by the application of the steganographic method. Based on
exemplary cases for single- and multi-method steganographic cost analyses we
observe that it can be an important characteristic that allows to express
hidden data carrier degradation - similarly as MSE (Mean-Square Error) or PSNR
(Peak Signal-to-Noise Ratio) are utilized for digital media steganography.
Steganographic cost can moreover be helpful to analyse the relationships
between two or more steganographic methods applied to the same hidden data
carrier.
</summary>
    <author>
      <name>Wojciech Mazurczyk</name>
    </author>
    <author>
      <name>Steffen Wendzel</name>
    </author>
    <author>
      <name>Ignacio Azagra Villares</name>
    </author>
    <author>
      <name>Krzysztof Szczypiorski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 14 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.6473v1</id>
    <updated>2014-06-25T06:45:02Z</updated>
    <published>2014-06-25T06:45:02Z</published>
    <title>Performance Comparison of Linear Prediction based Vocoders in Linux
  Platform</title>
    <summary>  Linear predictive coders form an important class of speech coders. This paper
describes the software level implementation of linear prediction based
vocoders, viz. Code Excited Linear Prediction (CELP), Low-Delay CELP (LD-CELP)
and Mixed Excitation Linear Prediction (MELP) at bit rates of 4.8 kb/s, 16 kb/s
and 2.4 kb/s respectively. The C programs of the vocoders have been compiled
and executed in Linux platform. Subjective testing with the help of Mean
Opinion Score test has been performed. Waveform analysis has been done using
Praat and Adobe Audition software. The results show that MELP and CELP produce
comparable quality while the quality of LD-CELP coder is much higher, at the
expense of higher bit rate.
</summary>
    <author>
      <name>Lani Rachel Mathew</name>
    </author>
    <author>
      <name>Ancy S. Anselam</name>
    </author>
    <author>
      <name>Sakuntala S. Pillai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22315381/IJETT-V10P310</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22315381/IJETT-V10P310" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, Published with International Journal of
  Engineering Trends and Technology (IJETT)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Engineering Trends and Technology
  (IJETT),V10(11),554-558 April 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.6473v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.6473v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.7226v1</id>
    <updated>2014-06-26T18:27:36Z</updated>
    <published>2014-06-26T18:27:36Z</published>
    <title>Securing Medical Images by Watermarking Using DWT DCT and SVD</title>
    <summary>  Telemedicine is well known application where enormous amount of medical data
need to be transferred securely over network and manipulate effectively.
Security of digital data, especially medical images, becomes important for many
reasons such as confidentiality, authentication and integrity. Digital
watermarking has emerged as a advanced technology to enhance the security of
digital images. The insertion of watermark in medical images can authenticate
it and guarantee its integrity. The watermark must be generally hidden does not
affect the quality of the medical image. In this paper, we propose blind
watermarking based on Discrete Wavelet Transform (DWT), Discrete Cosine
Transform (DCT) and Singular Value Decomposition (SVD), we compare the
performance of this technique with watermarking based DWT and SVD. The proposed
method DWT, DCT and SVD comparatively better than DWT and SVD method.
</summary>
    <author>
      <name>Nilesh Rathi</name>
    </author>
    <author>
      <name>Ganga Holi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V12P113</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V12P113" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 14 figures, 4 tables, Published with International Journal
  of Computer Trends and Technology (IJCTT)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Trends and Technology (IJCTT)
  V12(2):67-74, June 2014. ISSN:2231-2803. Published by Seventh Sense Research
  Group</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1406.7226v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.7226v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.6877v1</id>
    <updated>2014-07-25T12:58:23Z</updated>
    <published>2014-07-25T12:58:23Z</published>
    <title>An Easy yet Effective Method for Detecting Spatial Domain LSB
  Steganography</title>
    <summary>  Digitization of image was a revolutionary step for the fields of photography
and Image processing as this made the editing of images much effortless and
easier. Image editing was not an issue until it was limited to corrective
editing procedures used to enhance the quality of an image such as, contrast
stretching, noise filtering, sharpening etc. But, it became a headache for many
fields when image editing became manipulative. Digital images have become an
easier source of tampering and forgery during last few decades. Today users and
editing specialists, equipped with easily available image editing software,
manipulate digital images with varied goals. Photo journalists often tamper
photographs to give dramatic effect to their stories. Scientists and
researchers use this trick to get theirs works published. Patients' diagnoses
are misrepresented by manipulating medical imageries. Lawyers and Politicians
use tampered images to direct the opinion of people or court to their favor.
Terrorists, anti-social groups use manipulated Stego images for secret
communication. In this paper we present an effective method for detecting
spatial domain Steganography.
</summary>
    <author>
      <name>Minati Mishra</name>
    </author>
    <author>
      <name>M. C. Adhikary</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages; International Journal of Computer Science and Business
  Informatics, Dec 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.6877v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.6877v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.6879v1</id>
    <updated>2014-07-25T13:00:50Z</updated>
    <published>2014-07-25T13:00:50Z</published>
    <title>Detection of Clones in Digital Images</title>
    <summary>  During the recent years, tampering of digital images has become a general
habit among people and professionals. As a result, establishment of image
authenticity has become a key issue in fields those make use of digital images.
Authentication of an image involves separation of original camera outputs from
their tampered or Stego counterparts. Digital image cloning being a popular
type of image tampering, in this paper we have experimentally analyzed seven
different algorithms of cloning detection such as the simple overlapped block
matching with lexicographic sorting (SOBMwLS) algorithm, block matching with
discrete cosine transformation, principal component analysis, discrete wavelet
transformation and singular value decomposition performed on the blocks (DCT,
DWT, PCA, SVD), two combination models where, DCT and DWT are combined with
singular value decomposition (DCTSVD and DWTSVD. A comparative study of all
these techniques with respect to their time complexities and robustness of
detection against various post processing operations such as cropping,
brightness and contrast adjustments are presented in the paper.
</summary>
    <author>
      <name>Minati Mishra</name>
    </author>
    <author>
      <name>M. C. Adhikary</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages, International Journal of Computer Science and Business
  Informatics, Jan 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.6879v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.6879v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.7667v2</id>
    <updated>2014-08-01T09:25:38Z</updated>
    <published>2014-07-29T09:10:05Z</published>
    <title>Impact of video quality and wireless network interface on power
  consumption of mobile devices</title>
    <summary>  During the last years, many improvements were made to the hardware capability
of mobile devices. As mobile software also became more interactive and data
processing intensive, the increased power demand could not be compensated by
the improvements on battery technology. Adaptive systems can help to balance
the demand of applications with the limitations of battery resources. For
effective systems, the influence of multimedia quality on power consumption of
the components of mobile devices needs to be better understood. In this paper,
we analyze the impact of video quality and wireless network type on the energy
consumption of a mobile device. We have found that the additional power
consumption is up to 38% higher when a movie is played over a WiFi network
instead from internal memory and 64% higher in case of a mobile network (3G).
We have also discovered that a higher movie quality not only affects the power
consumption of the CPU but also the power consumption of the WiFi unit by up to
58% and up to 72% respectively on mobile networks.
</summary>
    <author>
      <name>Norbert Zsak</name>
    </author>
    <author>
      <name>Christian Wolff</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure, unpublished short paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.7667v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.7667v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.3564v1</id>
    <updated>2014-08-15T15:33:40Z</updated>
    <published>2014-08-15T15:33:40Z</published>
    <title>Digital Image Data Hiding Techniques: A Comparative Study</title>
    <summary>  With the advancements in the field of digital image processing during the
last decade, digital image data hiding techniques such as watermarking,
Steganography have gained wide popularity. Digital image watermarking
techniques hide a small amount of data into a digital image which, later can be
retrieved using some specific retrieval algorithms to prove the copyright of a
piece of digital information whereas, Steganographic techniques are used to
hide a large amount of data secretly into some innocuous looking digital
medium. In this paper we are providing an up-to-date review of these data
hiding techniques.
</summary>
    <author>
      <name>Minati Mishra</name>
    </author>
    <author>
      <name>Priyadarsini Mishra</name>
    </author>
    <author>
      <name>M. C. Adhikary</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, ANVESA - The Journal of F.M. University, ISSN-0974-715X.
  arXiv admin note: text overlap with
  http://dx.doi.org/10.1016/j.sigpro.2009.08.010 by other authors</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ANVESA,7(2), 105-115, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1408.3564v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.3564v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.5777v1</id>
    <updated>2014-08-07T16:38:25Z</updated>
    <published>2014-08-07T16:38:25Z</published>
    <title>Characterizing Internet Video for Large-scale Active Measurements</title>
    <summary>  The availability of high definition video content on the web has brought
about a significant change in the characteristics of Internet video, but not
many studies on characterizing video have been done after this change. Video
characteristics such as video length, format, target bit rate, and resolution
provide valuable input to design Adaptive Bit Rate (ABR) algorithms, sizing
playout buffers in Dynamic Adaptive HTTP streaming (DASH) players, model the
variability in video frame sizes, etc. This paper presents datasets collected
in 2013 and 2014 that contains over 130,000 videos from YouTube's most viewed
(or most popular) video charts in 58 countries. We describe the basic
characteristics of the videos on YouTube for each category, format, video
length, file size, and data rate variation, observing that video length and
file size fit a log normal distribution. We show that three minutes of a video
suffice to represent its instant data rate fluctuation and that we can infer
data rate characteristics of different video resolutions from a single given
one. Based on our findings, we design active measurements for measuring the
performance of Internet video.
</summary>
    <author>
      <name>Saba Ahsan</name>
    </author>
    <author>
      <name>Varun Singh</name>
    </author>
    <author>
      <name>J√∂rg Ott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 18 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.5777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.5777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.1148v1</id>
    <updated>2014-09-03T16:28:36Z</updated>
    <published>2014-09-03T16:28:36Z</published>
    <title>Toward Green Media Delivery: Location-Aware Opportunities and Approaches</title>
    <summary>  Mobile media has undoubtedly become the predominant source of traffic in
wireless networks. The result is not only congestion and poor
Quality-of-Experience, but also an unprecedented energy drain at both the
network and user devices. In order to sustain this continued growth, novel
disruptive paradigms of media delivery are urgently needed. We envision that
two key contemporary advancements can be leveraged to develop greener media
delivery platforms: 1) the proliferation of navigation hardware and software in
mobile devices has created an era of location-awareness, where both the current
and future user locations can be predicted; and 2) the rise of context-aware
network architectures and self-organizing functionalities is enabling context
signaling and in-network adaptation. With these developments in mind, this
article investigates the opportunities of exploiting location-awareness to
enable green end-to-end media delivery. In particular, we discuss and propose
approaches for location-based adaptive video quality planning, in-network
caching, content prefetching, and long-term radio resource management. To
provide insights on the energy savings, we then present a cross-layer framework
that jointly optimizes resource allocation and multi-user video quality using
location predictions. Finally, we highlight some of the future research
directions for location-aware media delivery in the conclusion.
</summary>
    <author>
      <name>Hatem Abou-zeid</name>
    </author>
    <author>
      <name>Hosssam S. Hassenein</name>
    </author>
    <link href="http://arxiv.org/abs/1409.1148v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.1148v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.4587v1</id>
    <updated>2014-09-16T11:27:38Z</updated>
    <published>2014-09-16T11:27:38Z</published>
    <title>A new Watermarking Technique for Medical Image using Hierarchical
  Encryption</title>
    <summary>  In recent years, characterized by the innovation of technology and the
digital revolution, the field of media has become important. The transfer and
exchange of multimedia data and duplication have become major concerns of
researchers. Consequently, protecting copyrights and ensuring service safety is
needed. Cryptography has a specific role, is to protect secret files against
unauthorized access. In this paper, a hierarchical cryptosystem algorithm based
on Logistic Map chaotic systems is proposed. The results show that the proposed
method improves the security of the image. Experimental results on a database
of 200 medical images show that the proposed method significantly gives better
results.
</summary>
    <author>
      <name>Med Karim Abdmouleh</name>
    </author>
    <author>
      <name>Ali Khalfallah</name>
    </author>
    <author>
      <name>Med Salim Bouhlel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 9 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues 11(4)
  (2014) 27-32</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1409.4587v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.4587v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.1474v1</id>
    <updated>2014-10-06T17:48:39Z</updated>
    <published>2014-10-06T17:48:39Z</published>
    <title>An adaptive quasi harmonic broadcasting scheme with optimal bandwidth
  requirement</title>
    <summary>  The aim of Harmonic Broadcasting protocol is to reduce the bandwidth usage in
video-on-demand service where a video is divided into some equal sized segments
and every segment is repeatedly transmitted over a number of channels that
follows harmonic series for channel bandwidth assignment. As the bandwidth of
channels differs from each other and users can join at any time to these
multicast channels, they may experience a synchronization problem between
download and playback. To deal with this issue, some schemes have been
proposed, however, at the cost of additional or wastage of bandwidth or sudden
extreme bandwidth requirement. In this paper we present an adaptive quasi
harmonic broadcasting scheme (AQHB) which delivers all data segment on time
that is the download and playback synchronization problem is eliminated while
keeping the bandwidth consumption as same as traditional harmonic broadcasting
scheme without cost of any additional or wastage of bandwidth. It also ensures
the video server not to increase the channel bandwidth suddenly that is, also
eliminates the sudden buffer requirement at the client side. We present several
analytical results to exhibit the efficiency of our proposed broadcasting
scheme over the existing ones.
</summary>
    <author>
      <name>Farzana Afrin</name>
    </author>
    <author>
      <name>Mohammad Saiedur Rahaman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICIEV.2013.6572684</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICIEV.2013.6572684" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Informatics, Electronics &amp; Vision
  (ICIEV), 2013, 6pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.1474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.1474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.3117v1</id>
    <updated>2014-10-12T16:47:55Z</updated>
    <published>2014-10-12T16:47:55Z</published>
    <title>An Efficient Bit Plane X-OR Algorithm for Irreversible Image
  Steganography</title>
    <summary>  The science of hiding secret information in another message is known as
Steganography; hence the presence of secret information is concealed. It is the
method of hiding cognitive content in same or another media to avoid
recognition by the intruders. This paper introduces new method wherein
irreversible steganography is used to hide an image in the same medium so that
the secret data is masked. The secret image is known as payload and the carrier
is known as cover image. X-OR operation is used amongst mid level bit planes of
carrier image and high level bit planes of data image to generate new low level
bit planes of the stego image. Recovery process includes the X-ORing of low
level bit planes and mid level bit planes of the stego image. Based on the
result of the recovery, subsequent data image is generated. A RGB color image
is used as carrier and the data image is a grayscale image of dimensions less
than or equal to the dimensions of the carrier image. The proposed method
greatly increases the embedding capacity without significantly decreasing the
PSNR value.
</summary>
    <author>
      <name>Soumendu Chakraborty</name>
    </author>
    <author>
      <name>Anand Singh Jalal</name>
    </author>
    <author>
      <name>Charul Bhatnagar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1504/IJTMCC.2013.053263</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1504/IJTMCC.2013.053263" rel="related"/>
    <link href="http://arxiv.org/abs/1410.3117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.3117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.3977v1</id>
    <updated>2014-10-15T09:09:10Z</updated>
    <published>2014-10-15T09:09:10Z</published>
    <title>Multi-View 3D Video Multicast for Broadband IP Networks</title>
    <summary>  With the recent emergence of 3D-supported TVs, video service providers now
face an opportunity to provide high resolution multi-view 3D videos over IP
networks. One simple way to support efficient communications between a video
server and multiple clients is to deliver each desired view in a multicast
stream. Nevertheless, it is expected that significantly increased bandwidth
will be required to support the transmission of all views in multi-view 3D
videos. However, the recent emergence of a new video synthesis technique called
Depth-Image-Based Rendering (DIBR) suggests that multi-view 3D video does not
necessarily require the transmission of all views. Therefore, we formulate a
new problem, named Multi-view and Multicast Delivery Selection Problem (MMDS),
and design an algorithm, called MMDEA, to find the optimal solution. Simulation
results manifest that using DIBR can effectively reduce bandwidth consumption
by 35% compared to the original multicast delivery scheme.
</summary>
    <author>
      <name>Ting-Yu Ho</name>
    </author>
    <author>
      <name>Yi-Nung Yeh</name>
    </author>
    <author>
      <name>De-Nian Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 10 figures, IEEE ICC 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.3977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.3977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.4730v1</id>
    <updated>2014-10-17T14:16:08Z</updated>
    <published>2014-10-17T14:16:08Z</published>
    <title>Human Motion Capture Data Tailored Transform Coding</title>
    <summary>  Human motion capture (mocap) is a widely used technique for digitalizing
human movements. With growing usage, compressing mocap data has received
increasing attention, since compact data size enables efficient storage and
transmission. Our analysis shows that mocap data have some unique
characteristics that distinguish themselves from images and videos. Therefore,
directly borrowing image or video compression techniques, such as discrete
cosine transform, does not work well. In this paper, we propose a novel
mocap-tailored transform coding algorithm that takes advantage of these
features. Our algorithm segments the input mocap sequences into clips, which
are represented in 2D matrices. Then it computes a set of data-dependent
orthogonal bases to transform the matrices to frequency domain, in which the
transform coefficients have significantly less dependency. Finally, the
compression is obtained by entropy coding of the quantized coefficients and the
bases. Our method has low computational cost and can be easily extended to
compress mocap databases. It also requires neither training nor complicated
parameter setting. Experimental results demonstrate that the proposed scheme
significantly outperforms state-of-the-art algorithms in terms of compression
performance and speed.
</summary>
    <author>
      <name>Junhui Hou</name>
    </author>
    <author>
      <name>Lap-Pui Chau</name>
    </author>
    <author>
      <name>Nadia Magnenat-Thalmann</name>
    </author>
    <author>
      <name>Ying He</name>
    </author>
    <link href="http://arxiv.org/abs/1410.4730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.4730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.6592v1</id>
    <updated>2014-10-24T06:36:48Z</updated>
    <published>2014-10-24T06:36:48Z</published>
    <title>Hiding Sound in Image by K-LSB Mutation</title>
    <summary>  In this paper a novel approach to hide sound files in a digital image is
proposed and implemented such that it becomes difficult to conclude about the
existence of the hidden data inside the image. In this approach, we utilize the
rightmost k-LSB of pixels in an image to embed MP3 sound bits into a pixel. The
pixels are so chosen that the distortion in image would be minimized due to
embedding. This requires comparing all the possible permutations of pixel
values, which may would lead to exponential time computation. To speed up this,
Cuckoo Search (CS) could be used to find the most optimal solution. The
advantage of using proposed CS is that it is easy to implement and is very
effective at converging in relatively less iterations/generations.
</summary>
    <author>
      <name>Ankur Gupta</name>
    </author>
    <author>
      <name>Ankit Chaudhary</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">appears in ISCBI 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.6592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.6592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.6796v1</id>
    <updated>2014-08-27T08:46:05Z</updated>
    <published>2014-08-27T08:46:05Z</published>
    <title>Steganography in Modern Smartphones and Mitigation Techniques</title>
    <summary>  By offering sophisticated services and centralizing a huge volume of personal
data, modern smartphones changed the way we socialize, entertain and work. To
this aim, they rely upon complex hardware/software frameworks leading to a
number of vulnerabilities, attacks and hazards to profile individuals or gather
sensitive information. However, the majority of works evaluating the security
degree of smartphones neglects steganography, which can be mainly used to: i)
exfiltrate confidential data via camouflage methods, and ii) conceal valuable
or personal information into innocent looking carriers.
  Therefore, this paper surveys the state of the art of steganographic
techniques for smartphones, with emphasis on methods developed over the period
2005 to the second quarter of 2014. The different approaches are grouped
according to the portion of the device used to hide information, leading to
three different covert channels, i.e., local, object and network. Also, it
reviews the relevant approaches used to detect and mitigate steganographic
attacks or threats. Lastly, it showcases the most popular software applications
to embed secret data into carriers, as well as possible future directions.
</summary>
    <author>
      <name>Wojciech Mazurczyk</name>
    </author>
    <author>
      <name>Luca Caviglione</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, 8 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.6796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.6796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.1705v1</id>
    <updated>2014-11-05T16:29:30Z</updated>
    <published>2014-11-05T16:29:30Z</published>
    <title>A Novel No-reference Video Quality Metric for Evaluating Temporal
  Jerkiness due to Frame Freezing</title>
    <summary>  In this work, we propose a novel no-reference (NR) video quality metric that
evaluates the impact of frame freezing due to either packet loss or late
arrival. Our metric uses a trained neural network acting on features that are
chosen to capture the impact of frame freezing on the perceived quality. The
considered features include the number of freezes, freeze duration statistics,
inter-freeze distance statistics, frame difference before and after the freeze,
normal frame difference, and the ratio of them. We use the neural network to
find the mapping between features and subjective test scores. We optimize the
network structure and the feature selection through a cross validation
procedure, using training samples extracted from both VQEG and LIVE video
databases. The resulting feature set and network structure yields accurate
quality prediction for both the training data containing 54 test videos and a
separate testing dataset including 14 videos, with Pearson Correlation
Coefficients greater than 0.9 and 0.8 for the training set and the testing set,
respectively. Our proposed metric has low complexity and could be utilized in a
system with realtime processing constraint.
</summary>
    <author>
      <name>Yuanyi Xue</name>
    </author>
    <author>
      <name>Beril Erkin</name>
    </author>
    <author>
      <name>Yao Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2014.2368272</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2014.2368272" rel="related"/>
    <link href="http://arxiv.org/abs/1411.1705v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.1705v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.4080v1</id>
    <updated>2014-11-14T23:29:18Z</updated>
    <published>2014-11-14T23:29:18Z</published>
    <title>6 Seconds of Sound and Vision: Creativity in Micro-Videos</title>
    <summary>  The notion of creativity, as opposed to related concepts such as beauty or
interestingness, has not been studied from the perspective of automatic
analysis of multimedia content. Meanwhile, short online videos shared on social
media platforms, or micro-videos, have arisen as a new medium for creative
expression. In this paper we study creative micro-videos in an effort to
understand the features that make a video creative, and to address the problem
of automatic detection of creative content. Defining creative videos as those
that are novel and have aesthetic value, we conduct a crowdsourcing experiment
to create a dataset of over 3,800 micro-videos labelled as creative and
non-creative. We propose a set of computational features that we map to the
components of our definition of creativity, and conduct an analysis to
determine which of these features correlate most with creative video. Finally,
we evaluate a supervised approach to automatically detect creative video, with
promising results, showing that it is necessary to model both aesthetic value
and novelty to achieve optimal classification accuracy.
</summary>
    <author>
      <name>Miriam Redi</name>
    </author>
    <author>
      <name>Neil O Hare</name>
    </author>
    <author>
      <name>Rossano Schifanella</name>
    </author>
    <author>
      <name>Michele Trevisiol</name>
    </author>
    <author>
      <name>Alejandro Jaimes</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CVPR.2014.544</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CVPR.2014.544" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figures, conference IEEE CVPR 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.4080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.4080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.4290v1</id>
    <updated>2014-11-16T18:52:54Z</updated>
    <published>2014-11-16T18:52:54Z</published>
    <title>Maximizing compression efficiency through block rotation</title>
    <summary>  The Discrete Cosine Transform (DCT) is widely used in lossy image and video
compression schemes, e.g., JPEG and MPEG. In this paper, we show that the
compression efficiency of the DCT is dependent on the edge directions within a
block. In particular, higher compression ratios are achieved when edges are
aligned with the image axes. To maximize compression for general images, we
propose a rotated block DCT method. It consists of rotating each block, before
applying the DCT, by an angle that aligns the edges, and rotating back the
block in the decompression stage. We show how to compute the rotation angle and
analyze two alternative block rotation approaches. Our experiments show that
our method enables both a perceptual improvement and a PSNR increase of up to
2dB, compared with the standard DCT, for low and medium bit rates.
</summary>
    <author>
      <name>Rui F. C. Guerreiro</name>
    </author>
    <author>
      <name>Pedro M. Q. Aguiar</name>
    </author>
    <link href="http://arxiv.org/abs/1411.4290v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.4290v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.4790v1</id>
    <updated>2014-11-18T10:05:23Z</updated>
    <published>2014-11-18T10:05:23Z</published>
    <title>The Art of Data Hiding with Reed-Solomon Error Correcting Codes</title>
    <summary>  With the tremendous advancements in technology and the Internet, data
security has become a major issue around the globe. To guarantee that data is
protected and does not go to an unintended endpoint, the art of data hiding
(steganography) emerged. Steganography is the art of hiding information such
that it is not detectable to the naked eye. Various techniques have been
proposed for hiding a secret message in a carrier document. In this paper, we
present a novel design that applies Reed-Solomon (RS) error correcting codes in
steganographic applications. The model works by substituting the redundant RS
codes with the steganographic message. The experimental results show that the
proposed design is satisfactory with the percentage of decoded information 100%
and percentage of decoded secret message 97. 36%. The proposed model proved
that it could be applied in various steganographic applications.
</summary>
    <author>
      <name>Fredrick R. Ishengoma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/18590-9902</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/18590-9902" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications 106(14):28-31,
  November 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1411.4790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.4790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6143v1</id>
    <updated>2014-11-17T16:58:54Z</updated>
    <published>2014-11-17T16:58:54Z</published>
    <title>Block Based Medical Image Watermarking Technique for Tamper Detection
  and Recovery</title>
    <summary>  In this paper, we propose a novel fragile block based medical image
watermarking technique for embedding data of patient into medical image,
verifying the integrity of ROI (Region of Interest), detecting the tampered
blocks inside ROI and recovering original ROI with less size authentication and
recovery data and with simple mathematical calculations. In the proposed
method, the medical image is divided into three regions called ROI, RONI
(Region of Non Interest) and border pixels. Later, authentication data of ROI
and Electronic Patient Record (EPR) are compressed using Run Length Encoding
(RLE) technique and then embedded into ROI. Recovery information of ROI is
embedded inside RONI and information of ROI is embedded inside border pixels.
Results of experiments conducted on several medical images reveal that proposed
method produces high quality watermarked medical images, identifies tampered
areas inside ROI of watermarked medical images and recovers the original ROI.
</summary>
    <author>
      <name>Eswaraiah Rayachoti</name>
    </author>
    <author>
      <name>Sreenivasa Reddy Edara</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ijcsi.org/papers/IJCSI-11-5-1-31-40.pdf 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1412.6143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.01576v3</id>
    <updated>2015-09-16T17:17:32Z</updated>
    <published>2015-01-07T17:59:15Z</published>
    <title>Improving image watermarking based on Tabu search by Chaos</title>
    <summary>  With the fast development of communication and multimedia technology, the
rights of the owners of multimedia products is vulnerable to the unauthorized
copies and watermarking is one of the best known methods for proving the
ownership of a product. In this paper we prosper the previous watermarking
method which was based on Tabu search by Chaos. The modification applied in the
permutation step of watermarking and the initial population generation of the
Tabu search. We analyze our method on some well known images and experimental
results shows the improvement in the quality and speed of the proposed
watermarking method.
</summary>
    <author>
      <name>Mohammad Tafaghodi</name>
    </author>
    <author>
      <name>Meysam Ghaffari</name>
    </author>
    <author>
      <name>Alimohammad Latif</name>
    </author>
    <author>
      <name>Seyed Rasoul Mousavi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by arXiv. arXiv admin note: author list
  truncated due to disputed authorship and content</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.01576v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.01576v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.01755v1</id>
    <updated>2015-01-08T08:02:01Z</updated>
    <published>2015-01-08T08:02:01Z</published>
    <title>Minimization of image watermarking side effects through subjective
  optimization</title>
    <summary>  This paper investigates the use of Structural Similaritys (SSIM) index on the
minimized side effect to image watermarking. For fast implementation and more
compatibility with the standard DCT based codecs, watermark insertion is
carried out on the DCT coefficients and hence a SSIM model for DCT based
watermarking is developed. For faster implementation, the SSIM index is
maximized over independent 4x4 non-overlapped blocks but the disparity between
the adjacent blocks reduces the overall image quality. This problem is resolved
through optimization of overlapped blocks, but, the higher image quality is
achieved at a cost of high computational complexity. To reduce the
computational complexity while preserving the good quality, optimization of
semi-overlapped blocks is introduced. We show that while SSIM-based
optimization over overlapped blocks has as high as 64 times the complexity of
the 4x4 non-overlapped method, with semi-overlapped optimization the high
quality of overlapped method is preserved only at a cost of less than 8 times
the non-overlapped method.
</summary>
    <author>
      <name>Hossein Bakhshi Golestani</name>
    </author>
    <author>
      <name>Mohammed Ghanbari</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1049/iet-ipr.2013.0086</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1049/iet-ipr.2013.0086" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages,11 figures, IET Image Processing Journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IET Image Processing, vol. 7, no. 8, pp. 733-741, 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1501.01755v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.01755v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.02528v1</id>
    <updated>2015-01-12T03:13:34Z</updated>
    <published>2015-01-12T03:13:34Z</published>
    <title>A Systematic Scheme for Measuring the Performance of the Display-Camera
  Channel</title>
    <summary>  Display-camera communication has become a promising direction in both
computer vision and wireless communication communities. However, the
consistency of the channel measurement is an open issue since precise
calibration of the experimental setting has not been fully studied in the
literatures. This paper focuses on establishing a scheme for precise
calibration of the display-camera channel performance. To guarantee high
consistency of the experiment, we propose an accurate measurement scheme for
the geometric parameters, and identify some unstable channel factors, e.g.,
Moire effect, rolling shutter effect, blocking artifacts, inconsistency in
auto-focus, trembling and vibration. In the experiment, we first define the
consistency criteria according to the error-prone region in bit error rate
(BER) plots of the channel measurements. It is demonstrated that the
consistency of the experimental result can be improved by the proposed precise
calibration scheme.
</summary>
    <author>
      <name>Changsheng Chen</name>
    </author>
    <author>
      <name>Wai Ho Mow</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1177/1550147717738193</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1177/1550147717738193" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, preliminary conference version</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.02528v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.02528v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00296v1</id>
    <updated>2015-02-01T18:43:29Z</updated>
    <published>2015-02-01T18:43:29Z</published>
    <title>Fragile Watermarking Using Finite Field Trigonometrical Transforms</title>
    <summary>  Fragile digital watermarking has been applied for authentication and
alteration detection in images. Utilizing the cosine and Hartley transforms
over finite fields, a new transform domain fragile watermarking scheme is
introduced. A watermark is embedded into a host image via a blockwise
application of two-dimensional finite field cosine or Hartley transforms.
Additionally, the considered finite field transforms are adjusted to be number
theoretic transforms, appropriate for error-free calculation. The employed
technique can provide invisible fragile watermarking for authentication systems
with tamper location capability. It is shown that the choice of the finite
field characteristic is pivotal to obtain perceptually invisible watermarked
images. It is also shown that the generated watermarked images can be used as
publicly available signature data for authentication purposes.
</summary>
    <author>
      <name>R. J. Cintra</name>
    </author>
    <author>
      <name>V. S. Dimitrov</name>
    </author>
    <author>
      <name>H. M. de Oliveira</name>
    </author>
    <author>
      <name>R. M. Campello de Souza</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.image.2009.04.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.image.2009.04.003" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Image Communication, Volume 24, Issue 7, August, 2009, pp. 587-597</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1502.00296v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00296v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.01996v1</id>
    <updated>2015-02-06T19:16:45Z</updated>
    <published>2015-02-06T19:16:45Z</published>
    <title>Wavelet based Watermarking approach in the Compressive Sensing Scenario</title>
    <summary>  Due to the wide distribution and usage of digital media, an important issue
is protection of the digital content. There is a number of algorithms and
techniques developed for the digital watermarking.In this paper, the invisible
image watermark procedure is considered. Watermark is created as a pseudo
random sequence, embedded in the certain region of the image, obtained using
Haar wavelet decomposition. Generally, the watermarking procedure should be
robust to the various attacks-filtering, noise etc. Here we assume the
Compressive sensing scenario as a new signal processing technique that may
influence the robustness. The focus of this paper was the possibility of the
watermark detection under Compressive Sensing attack with different number of
available image coefficients. The quality of the reconstructed images has been
evaluated using Peak Signal to Noise Ratio (PSNR).The theory is supported with
experimental results.
</summary>
    <author>
      <name>Jelena Music</name>
    </author>
    <author>
      <name>Ivan Knezevic</name>
    </author>
    <author>
      <name>Edis Franca</name>
    </author>
    <link href="http://arxiv.org/abs/1502.01996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.01996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02969v1</id>
    <updated>2015-02-10T16:23:07Z</updated>
    <published>2015-02-10T16:23:07Z</published>
    <title>A DCT And SVD based Watermarking Technique To Identify Tag</title>
    <summary>  With the rapid development of the multimedia,the secure of the multimedia is
get more concerned. as far as we know , Digital watermarking is an effective
way to protect copyright. The watermark must be generally hidden does not
affect the quality of the original image. In this paper,a novel way based on
discrete cosine transform(DCT) and singular value decomposition(SVD) .In the
proposed way,we decomposition the image into 8*8 blocks, next we use the DCT to
get the transformed block,then we choose the diagonal to embed the information,
after we do this, we recover the image and then we decomposition the image to
8*8 blocks,we use the SVD way to get the diagonal matrix and embed the
information in the matrix. next we extract the information use both inverse of
DCT and SVD, as we all know,after we embed the information seconded time , the
information we first information we embed must be changed, we choose a measure
way called Peak Signal to Noise Ratio(PSNR) to estimate the similarity of the
two image, and set a threshold to ensure whether the information is same or
not.
</summary>
    <author>
      <name>Ke Ji</name>
    </author>
    <author>
      <name>Jianbiao Lin</name>
    </author>
    <author>
      <name>Hui Li</name>
    </author>
    <author>
      <name>Ao Wang</name>
    </author>
    <author>
      <name>Tianjing Tang</name>
    </author>
    <link href="http://arxiv.org/abs/1502.02969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.03802v1</id>
    <updated>2015-02-12T20:41:15Z</updated>
    <published>2015-02-12T20:41:15Z</published>
    <title>A two-stage video coding framework with both self-adaptive redundant
  dictionary and adaptively orthonormalized DCT basis</title>
    <summary>  In this work, we propose a two-stage video coding framework, as an extension
of our previous one-stage framework in [1]. The two-stage frameworks consists
two different dictionaries. Specifically, the first stage directly finds the
sparse representation of a block with a self-adaptive dictionary consisting of
all possible inter-prediction candidates by solving an L0-norm minimization
problem using an improved orthogonal matching pursuit with embedded
orthonormalization (eOMP) algorithm, and the second stage codes the residual
using DCT dictionary adaptively orthonormalized to the subspace spanned by the
first stage atoms. The transition of the first stage and the second stage is
determined based on both stages' quantization stepsizes and a threshold. We
further propose a complete context adaptive entropy coder to efficiently code
the locations and the coefficients of chosen first stage atoms. Simulation
results show that the proposed coder significantly improves the RD performance
over our previous one-stage coder. More importantly, the two-stage coder, using
a fixed block size and inter-prediction only, outperforms the H.264 coder
(x264) and is competitive with the HEVC reference coder (HM) over a large rate
range.
</summary>
    <author>
      <name>Yuanyi Xue</name>
    </author>
    <author>
      <name>Yi Zhou</name>
    </author>
    <author>
      <name>Yao Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1502.03802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.06078v1</id>
    <updated>2015-02-21T07:26:33Z</updated>
    <published>2015-02-21T07:26:33Z</published>
    <title>Evaluating QoS Parameters for IPTV Distribution in Heterogeneous
  Networks</title>
    <summary>  The present work presents an architecture developed to evaluate the QoS
parameters for the IPTV heterogeneous network. At its very basic level lie two
software technologies: Video LAN and Windows Media Services with two operating
systems: Windows and Linux. Three types of streams are analyzed, which will be
transmitted to a Linux VLC client through means of the aggregation and access
servers. The first stream is generated in real time by a capture camera,
processed by the encapsulated VC-1 encoder and sent to the Media Server, while
the second one is of VoD(Video on Demand) type and the third one will be
handled by DVBViewer through the MPEG TS form. The first stream is transcoded
in H.264-AAC such that the Linux stations will recognize its format. Through
the simultaneous transmission of the three streams, we are analyzing their
performance from a QoS parameters point of view by means of an application
implemented in C programming language. The stream transporting the DVB-S
television content was proven to ensure the best performance regarding loss of
packets, delays and jitter.
</summary>
    <author>
      <name>Ioan Sorin Comsa</name>
    </author>
    <author>
      <name>Radu Arsinte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 19 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.06078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.06078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.06314v1</id>
    <updated>2015-02-23T04:44:02Z</updated>
    <published>2015-02-23T04:44:02Z</published>
    <title>Crowdsourced Live Streaming over the Cloud</title>
    <summary>  Empowered by today's rich tools for media generation and distribution, and
the convenient Internet access, crowdsourced streaming generalizes the
single-source streaming paradigm by including massive contributors for a video
channel. It calls a joint optimization along the path from crowdsourcers,
through streaming servers, to the end-users to minimize the overall latency.
The dynamics of the video sources, together with the globalized request demands
and the high computation demand from each sourcer, make crowdsourced live
streaming challenging even with powerful support from modern cloud computing.
In this paper, we present a generic framework that facilitates a cost-effective
cloud service for crowdsourced live streaming. Through adaptively leasing, the
cloud servers can be provisioned in a fine granularity to accommodate
geo-distributed video crowdsourcers. We present an optimal solution to deal
with service migration among cloud instances of diverse lease prices. It also
addresses the location impact to the streaming quality. To understand the
performance of the proposed strategies in the realworld, we have built a
prototype system running over the planetlab and the Amazon/Microsoft Cloud. Our
extensive experiments demonstrate that the effectiveness of our solution in
terms of deployment cost and streaming quality.
</summary>
    <author>
      <name>Fei Chen</name>
    </author>
    <author>
      <name>Cong Zhang</name>
    </author>
    <author>
      <name>Feng Wang</name>
    </author>
    <author>
      <name>Jiangchuan Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1502.06314v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.06314v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.07808v1</id>
    <updated>2015-02-27T03:05:43Z</updated>
    <published>2015-02-27T03:05:43Z</published>
    <title>A Secure Cyclic Steganographic Technique for Color Images using
  Randomization</title>
    <summary>  Information Security is a major concern in today's modern era. Almost all the
communicating bodies want the security, confidentiality and integrity of their
personal data. But this security goal cannot be achieved easily when we are
using an open network like Internet. Steganography provides one of the best
solutions to this problem. This paper represents a new Cyclic Steganographic T
echnique (CST) based on Least Significant Bit (LSB) for true color (RGB)
images. The proposed method hides the secret data in the LSBs of cover image
pixels in a randomized cyclic manner. The proposed technique is evaluated using
both subjective and objective analysis using histograms changeability, Peak
Signal-to-Noise Ratio (PSNR) and Mean Square Error (MSE). Experimentally it is
found that the proposed method gives promising results in terms of security,
imperceptibility and robustness as compared to some existent methods and
vindicates this new algorithm.
</summary>
    <author>
      <name>Khan Muhammad</name>
    </author>
    <author>
      <name>Jamil Ahmad</name>
    </author>
    <author>
      <name>Naeem Ur Rehman</name>
    </author>
    <author>
      <name>Zahoor Jan</name>
    </author>
    <author>
      <name>Rashid Jalal Qureshi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Journal, University of Engineering and Technology
  Taxila, Pakistan, vol. 19, pp. 57-64, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1502.07808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.07808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.06786v2</id>
    <updated>2015-05-06T19:37:36Z</updated>
    <published>2015-04-26T05:06:33Z</published>
    <title>Deviation Based Pooling Strategies For Full Reference Image Quality
  Assessment</title>
    <summary>  The state-of-the-art pooling strategies for perceptual image quality
assessment (IQA) are based on the mean and the weighted mean. They are robust
pooling strategies which usually provide a moderate to high performance for
different IQAs. Recently, standard deviation (SD) pooling was also proposed.
Although, this deviation pooling provides a very high performance for a few
IQAs, its performance is lower than mean poolings for many other IQAs. In this
paper, we propose to use the mean absolute deviation (MAD) and show that it is
a more robust and accurate pooling strategy for a wider range of IQAs. In fact,
MAD pooling has the advantages of both mean pooling and SD pooling. The joint
computation and use of the MAD and SD pooling strategies is also considered in
this paper. Experimental results provide useful information on the choice of
the proper deviation pooling strategy for different IQA models.
</summary>
    <author>
      <name>Hossein Ziaei Nafchi</name>
    </author>
    <author>
      <name>Rachid Hedjam</name>
    </author>
    <author>
      <name>Atena Shahkolaei</name>
    </author>
    <author>
      <name>Mohamed Cheriet</name>
    </author>
    <link href="http://arxiv.org/abs/1504.06786v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.06786v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.07004v1</id>
    <updated>2015-04-27T09:44:30Z</updated>
    <published>2015-04-27T09:44:30Z</published>
    <title>An Active Learning Based Approach For Effective Video Annotation And
  Retrieval</title>
    <summary>  Conventional multimedia annotation/retrieval systems such as Normalized
Continuous Relevance Model (NormCRM) [16] require a fully labeled training data
for a good performance. Active Learning, by determining an order for labeling
the training data, allows for a good performance even before the training data
is fully annotated. In this work we propose an active learning algorithm, which
combines a novel measure of sample uncertainty with a novel clustering-based
approach for determining sample density and diversity and integrate it with
NormCRM. The clusters are also iteratively refined to ensure both feature and
label-level agreement among samples. We show that our approach outperforms
multiple baselines both on a recent, open character animation dataset and on
the popular TRECVID corpus at both the tasks of annotation and text-based
retrieval of videos.
</summary>
    <author>
      <name>Moitreya Chatterjee</name>
    </author>
    <author>
      <name>Anton Leuski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, Compressed version published at ACM ICMR 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.07004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.07004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.00765v1</id>
    <updated>2015-06-02T06:31:57Z</updated>
    <published>2015-06-02T06:31:57Z</published>
    <title>Video (GIF) Sentiment Analysis using Large-Scale Mid-Level Ontology</title>
    <summary>  With faster connection speed, Internet users are now making social network a
huge reservoir of texts, images and video clips (GIF). Sentiment analysis for
such online platform can be used to predict political elections, evaluates
economic indicators and so on. However, GIF sentiment analysis is quite
challenging, not only because it hinges on spatio-temporal visual
contentabstraction, but also for the relationship between such abstraction and
final sentiment remains unknown.In this paper, we dedicated to find out such
relationship.We proposed a SentiPairSequence basedspatiotemporal visual
sentiment ontology, which forms the midlevel representations for GIFsentiment.
The establishment process of SentiPair contains two steps. First, we construct
the Synset Forest to define the semantic tree structure of visual sentiment
label elements. Then, through theSynset Forest, we organically select and
combine sentiment label elements to form a mid-level visual sentiment
representation. Our experiments indicate that SentiPair outperforms other
competing mid-level attributes. Using SentiPair, our analysis frameworkcan
achieve satisfying prediction accuracy (72.6%). We also opened ourdataset
(GSO-2015) to the research community. GSO-2015 contains more than 6,000
manually annotated GIFs out of more than 40,000 candidates. Each is labeled
with both sentiment and SentiPair Sequence.
</summary>
    <author>
      <name>Zheng Cai</name>
    </author>
    <author>
      <name>Donglin Cao</name>
    </author>
    <author>
      <name>Rongrong Ji</name>
    </author>
    <link href="http://arxiv.org/abs/1506.00765v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.00765v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02071v1</id>
    <updated>2015-06-05T21:16:22Z</updated>
    <published>2015-06-05T21:16:22Z</published>
    <title>Using Facebook for Image Steganography</title>
    <summary>  Because Facebook is available on hundreds of millions of desktop and mobile
computing platforms around the world and because it is available on many
different kinds of platforms (from desktops and laptops running Windows, Unix,
or OS X to hand held devices running iOS, Android, or Windows Phone), it would
seem to be the perfect place to conduct steganography. On Facebook, information
hidden in image files will be further obscured within the millions of pictures
and other images posted and transmitted daily. Facebook is known to alter and
compress uploaded images so they use minimum space and bandwidth when displayed
on Facebook pages. The compression process generally disrupts attempts to use
Facebook for image steganography. This paper explores a method to minimize the
disruption so JPEG images can be used as steganography carriers on Facebook.
</summary>
    <author>
      <name>Jason Hiney</name>
    </author>
    <author>
      <name>Tejas Dakve</name>
    </author>
    <author>
      <name>Krzysztof Szczypiorski</name>
    </author>
    <author>
      <name>Kris Gaj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, 2 tables. Accepted to Fourth International
  Workshop on Cyber Crime (IWCC 2015), co-located with 10th International
  Conference on Availability, Reliability and Security (ARES 2015), Toulouse,
  France, 24-28 August 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.02071v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02071v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02311v1</id>
    <updated>2015-06-07T20:46:27Z</updated>
    <published>2015-06-07T20:46:27Z</published>
    <title>StegBlocks: ensuring perfect undetectability of network steganography</title>
    <summary>  The paper presents StegBlocks, which defines a new concept for performing
undetectable hidden communication. StegBlocks is a general approach for
constructing methods of network steganography. In StegBlocks, one has to
determine objects with defined properties which will be used to transfer hidden
messages. The objects are dependent on a specific network protocol (or
application) used as a carrier for a given network steganography method.
Moreover, the paper presents the approach to perfect undetectability of network
steganography, which was developed based on the rules of undetectability for
general steganography. The approach to undetectability of network steganography
was used to show the possibility of developing perfectly undetectable network
steganography methods using the StegBlocks concept.
</summary>
    <author>
      <name>Wojciech Fraczek</name>
    </author>
    <author>
      <name>Krzysztof Szczypiorski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, Accepted to Fourth International Workshop on Cyber
  Crime (IWCC 2015), co-located with 10th International Conference on
  Availability, Reliability and Security (ARES 2015), Toulouse, France, 24-28
  August 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.02311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.07823v1</id>
    <updated>2015-06-25T17:24:13Z</updated>
    <published>2015-06-25T17:24:13Z</published>
    <title>Optimal Layered Representation for Adaptive Interactive Multiview Video
  Streaming</title>
    <summary>  We consider an interactive multiview video streaming (IMVS) system where
clients select their preferred viewpoint in a given navigation window. To
provide high quality IMVS, many high quality views should be transmitted to the
clients. However, this is not always possible due to the limited and
heterogeneous capabilities of the clients. In this paper, we propose a novel
adaptive IMVS solution based on a layered multiview representation where camera
views are organized into layered subsets to match the different clients
constraints. We formulate an optimization problem for the joint selection of
the views subsets and their encoding rates. Then, we propose an optimal and a
reduced computational complexity greedy algorithms, both based on
dynamic-programming. Simulation results show the good performance of our novel
algorithms compared to a baseline algorithm, proving that an effective IMVS
adaptive solution should consider the scene content and the client capabilities
and their preferences in navigation.
</summary>
    <author>
      <name>Ana De Abreu</name>
    </author>
    <author>
      <name>Laura Toni</name>
    </author>
    <author>
      <name>Nikolaos Thomos</name>
    </author>
    <author>
      <name>Thomas Maugey</name>
    </author>
    <author>
      <name>Fernando Pereira</name>
    </author>
    <author>
      <name>Pascal Frossard</name>
    </author>
    <link href="http://arxiv.org/abs/1506.07823v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.07823v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.08125v1</id>
    <updated>2015-06-26T15:46:24Z</updated>
    <published>2015-06-26T15:46:24Z</published>
    <title>Data-driven Approaches for Social Video Distribution</title>
    <summary>  The Internet has recently witnessed the convergence of online social network
services and online video services: users import videos from content sharing
sites, and propagate them along the social connections by re-sharing them. Such
social behaviors have dramatically reshaped how videos are disseminated, and
the users are now actively engaged to be part of the social ecosystem, rather
than being passively consumers. Despite the increasingly abundant bandwidth and
computation resources, the ever increasing data volume of user generated video
content and the boundless coverage of socialized sharing have presented
unprecedented challenges. In this paper, we first presents the challenges in
social-aware video delivery. Then, we present a principal framework for
data-driven social video delivery approaches. Moreover, we identify the unique
characteristics of social-aware video access and the social content
propagation, and closely reveal the design of individual modules and their
integration towards enhancing users' experience in the social network context.
</summary>
    <author>
      <name>Zhi Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1506.08125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.08898v3</id>
    <updated>2016-02-19T02:53:43Z</updated>
    <published>2015-06-29T23:47:00Z</published>
    <title>Low-latency compression of mocap data using learned spatial
  decorrelation transform</title>
    <summary>  Due to the growing needs of human motion capture (mocap) in movie, video
games, sports, etc., it is highly desired to compress mocap data for efficient
storage and transmission. This paper presents two efficient frameworks for
compressing human mocap data with low latency. The first framework processes
the data in a frame-by-frame manner so that it is ideal for mocap data
streaming and time critical applications. The second one is clip-based and
provides a flexible tradeoff between latency and compression performance. Since
mocap data exhibits some unique spatial characteristics, we propose a very
effective transform, namely learned orthogonal transform (LOT), for reducing
the spatial redundancy. The LOT problem is formulated as minimizing square
error regularized by orthogonality and sparsity and solved via alternating
iteration. We also adopt a predictive coding and temporal DCT for temporal
decorrelation in the frame- and clip-based frameworks, respectively.
Experimental results show that the proposed frameworks can produce higher
compression performance at lower computational cost and latency than the
state-of-the-art methods.
</summary>
    <author>
      <name>Junhui Hou</name>
    </author>
    <author>
      <name>Lap-Pui Chau</name>
    </author>
    <author>
      <name>Nadia Magnenat-Thalmann</name>
    </author>
    <author>
      <name>Ying He</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.08898v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08898v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05150v2</id>
    <updated>2015-11-20T19:56:36Z</updated>
    <published>2015-07-18T05:55:37Z</published>
    <title>Towards Understanding User Preferences from User Tagging Behavior for
  Personalization</title>
    <summary>  Personalizing image tags is a relatively new and growing area of research,
and in order to advance this research community, we must review and challenge
the de-facto standard of defining tag importance. We believe that for greater
progress to be made, we must go beyond tags that merely describe objects that
are visually represented in the image, towards more user-centric and subjective
notions such as emotion, sentiment, and preferences.
  We focus on the notion of user preferences and show that the order that users
list tags on images is correlated to the order of preference over the tags that
they provided for the image. While this observation is not completely
surprising, to our knowledge, we are the first to explore this aspect of user
tagging behavior systematically and report empirical results to support this
observation. We argue that this observation can be exploited to help advance
the image tagging (and related) communities.
  Our contributions include: 1.) conducting a user study demonstrating this
observation, 2.) collecting a dataset with user tag preferences explicitly
collected.
</summary>
    <author>
      <name>Amandianeze O. Nwana</name>
    </author>
    <author>
      <name>Tshuan Chen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISM.2015.79</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISM.2015.79" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.05150v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05150v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05174v1</id>
    <updated>2015-07-18T11:45:44Z</updated>
    <published>2015-07-18T11:45:44Z</published>
    <title>Joint Data Scheduling and FEC Coding for Multihomed Wireless Video
  Delivery</title>
    <summary>  This paper studies the problem of mobile video delivery in heterogenous
wireless networks from a server to multihomed device. Most existing works only
consider delivering video streaming on single path which bandwidth is limited
causing ultimate video transmission rate. To solve this live video streaming
transmission bottleneck problem, we propose a novel solution named Joint Data
Allocation and Fountain Coding (JDAFC) method that contain below characters:
(1) path selection, (2) dynamic data allocation, and (3) fountain coding. We
evaluate the performance of JDAFC by simulation experiments using Exata and
JVSM and compare it with some reference solutions. Experimental results
represent that JDAFC outperforms the competing solutions in improving the video
peak signal-to-noise ratio as well as reducing the end-to-end delay.
</summary>
    <author>
      <name>Jasmin Fantel</name>
    </author>
    <author>
      <name>Yan Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.05174v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05174v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05242v1</id>
    <updated>2015-07-19T03:05:26Z</updated>
    <published>2015-07-19T03:05:26Z</published>
    <title>Data Hiding in Video using Triangularization LSB Technique</title>
    <summary>  The importance of data hiding in the field of Information Technology is a
widely accepted. The challenge is to be able to pass information in a manner
that the very existence of the message is unknown in order to repel attention
of the potential attacker. Steganography is a technique that has been widely
used to achieve this objective. However Steganography is often found to be
lacking when it comes to hiding bulk data. Attempting to hide data in a video
overcomes this problem because of the large sized cover object (video) as
compared to an image in the case of steganography. This paper attempts to
propose a scheme using which data can be hidden in a video. We focus on the
Triangularization method and make use of the Least Significant Bit (LSB)
technique in hiding messages in a video.
</summary>
    <author>
      <name>Subhashri Acharya</name>
    </author>
    <author>
      <name>Pramita Srimany</name>
    </author>
    <author>
      <name>Sanchari Kundu</name>
    </author>
    <author>
      <name>JayatiGhosh Dastidar</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Trends in Computer Science and
  Engineering, Volume 4, No.3, May - June 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1507.05242v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05242v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08075v1</id>
    <updated>2015-07-29T09:25:44Z</updated>
    <published>2015-07-29T09:25:44Z</published>
    <title>Low Bit-Rate and High Fidelity Reversible Data Hiding</title>
    <summary>  An accurate predictor is crucial for histogram-shifting (HS) based reversible
data hiding methods. The embedding capacity is increased and the embedding
distortion is decreased simultaneously if the predictor can generate accurate
predictions. In this paper, we propose an accurate linear predictor based on
weighted least squares (WLS) estimation. The robustness of WLS helps the
proposed predictor generate accurate predictions, especially in complex texture
areas of an image, where other predictors usually fail. To further reduce the
embedding distortion, we propose a new embedding method called dynamic
histogram shifting with pixel selection (DHS-PS) that selects not only the
proper histogram bins but also the proper pixel locations to embed the given
data. As a result, the proposed method can obtain very high fidelity marked
images with low bit-rate data embedded. The experimental results show that the
proposed method outperforms the state-of-the-art low bit-rate reversible data
hiding method.
</summary>
    <author>
      <name>Xiaochao Qu</name>
    </author>
    <author>
      <name>Suah Kim</name>
    </author>
    <author>
      <name>Run Cui</name>
    </author>
    <author>
      <name>Hyoung Joong Kim</name>
    </author>
    <link href="http://arxiv.org/abs/1507.08075v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08075v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08861v2</id>
    <updated>2018-04-30T14:23:01Z</updated>
    <published>2015-07-31T13:02:23Z</published>
    <title>Mobile Multi-View Object Image Search</title>
    <summary>  High user interaction capability of mobile devices can help improve the
accuracy of mobile visual search systems. At query time, it is possible to
capture multiple views of an object from different viewing angles and at
different scales with the mobile device camera to obtain richer information
about the object compared to a single view and hence return more accurate
results. Motivated by this, we developed a mobile multi-view object image
search system, using a client-server architecture. Multi-view images of objects
acquired by the mobile clients are processed and local features are sent to the
server, which combines the query image representations with early/late fusion
methods based on bag-of-visual-words and sends back the query results. We
performed a comprehensive analysis of early and late fusion approaches using
various similarity functions, on an existing single view and a new multi-view
object image database. The experimental results show that multi-view search
provides significantly better retrieval accuracy compared to single view
search.
</summary>
    <author>
      <name>Fatih Calisir</name>
    </author>
    <author>
      <name>Muhammet Bastan</name>
    </author>
    <author>
      <name>Ozgur Ulusoy</name>
    </author>
    <author>
      <name>Ugur Gudukbay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Multimedia Tools and Applications, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.08861v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08861v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.01055v1</id>
    <updated>2015-08-05T12:46:26Z</updated>
    <published>2015-08-05T12:46:26Z</published>
    <title>Estimating snow cover from publicly available images</title>
    <summary>  In this paper we study the problem of estimating snow cover in mountainous
regions, that is, the spatial extent of the earth surface covered by snow. We
argue that publicly available visual content, in the form of user generated
photographs and image feeds from outdoor webcams, can both be leveraged as
additional measurement sources, complementing existing ground, satellite and
airborne sensor data. To this end, we describe two content acquisition and
processing pipelines that are tailored to such sources, addressing the specific
challenges posed by each of them, e.g., identifying the mountain peaks,
filtering out images taken in bad weather conditions, handling varying
illumination conditions. The final outcome is summarized in a snow cover index,
which indicates for a specific mountain and day of the year, the fraction of
visible area covered by snow, possibly at different elevations. We created a
manually labelled dataset to assess the accuracy of the image snow covered area
estimation, achieving 90.0% precision at 91.1% recall. In addition, we show
that seasonal trends related to air temperature are captured by the snow cover
index.
</summary>
    <author>
      <name>Roman Fedorov</name>
    </author>
    <author>
      <name>Alessandro Camerada</name>
    </author>
    <author>
      <name>Piero Fraternali</name>
    </author>
    <author>
      <name>Marco Tagliasacchi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2016.2535356</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2016.2535356" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to IEEE Transactions on Multimedia</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.01055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.01055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02284v1</id>
    <updated>2015-08-10T15:28:39Z</updated>
    <published>2015-08-10T15:28:39Z</published>
    <title>Approaching Maximum Embedding Efficiency on Small Covers Using
  Staircase-Generator Codes</title>
    <summary>  We introduce a new family of binary linear codes suitable for steganographic
matrix embedding. The main characteristic of the codes is the staircase random
block structure of the generator matrix. We propose an efficient list decoding
algorithm for the codes that finds a close codeword to a given random word. We
provide both theoretical analysis of the performance and stability of the
decoding algorithm, as well as practical results. Used for matrix embedding,
these codes achieve almost the upper theoretical bound of the embedding
efficiency for covers in the range of 1000 - 1500 bits, which is at least an
order of magnitude smaller than the values reported in related works.
</summary>
    <author>
      <name>Simona Samardjiska</name>
    </author>
    <author>
      <name>Danilo Gligoroski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of the paper presented at ISIT 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.02284v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02284v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94B05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.04978v2</id>
    <updated>2015-09-09T22:22:09Z</updated>
    <published>2015-08-20T13:35:43Z</published>
    <title>"The Good, The Bad And The Ugly": Evaluation of Wi-Fi Steganography</title>
    <summary>  In this paper we propose a new method for the evaluation of network
steganography algorithms based on the new concept of "the moving observer". We
considered three levels of undetectability named: "good", "bad", and "ugly". To
illustrate this method we chose Wi-Fi steganography as a solid family of
information hiding protocols. We present the state of the art in this area
covering well-known hiding techniques for 802.11 networks. "The moving
observer" approach could help not only in the evaluation of steganographic
algorithms, but also might be a starting point for a new detection system of
network steganography. The concept of a new detection system, called MoveSteg,
is explained in detail.
</summary>
    <author>
      <name>Krzysztof Szczypiorski</name>
    </author>
    <author>
      <name>Artur Janicki</name>
    </author>
    <author>
      <name>Steffen Wendzel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures, to appear in Proc. of: ICNIT 2015 - 6th
  International Conference on Networking and Information Technology, Tokyo,
  Japan, November 5-6, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.04978v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.04978v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05056v2</id>
    <updated>2015-08-24T09:43:18Z</updated>
    <published>2015-08-20T17:36:48Z</published>
    <title>Diving Deep into Sentiment: Understanding Fine-tuned CNNs for Visual
  Sentiment Prediction</title>
    <summary>  Visual media are powerful means of expressing emotions and sentiments. The
constant generation of new content in social networks highlights the need of
automated visual sentiment analysis tools. While Convolutional Neural Networks
(CNNs) have established a new state-of-the-art in several vision problems,
their application to the task of sentiment analysis is mostly unexplored and
there are few studies regarding how to design CNNs for this purpose. In this
work, we study the suitability of fine-tuning a CNN for visual sentiment
prediction as well as explore performance boosting techniques within this deep
learning setting. Finally, we provide a deep-dive analysis into a benchmark,
state-of-the-art network architecture to gain insight about how to design
patterns for CNNs on the task of visual sentiment prediction.
</summary>
    <author>
      <name>Victor Campos</name>
    </author>
    <author>
      <name>Amaia Salvador</name>
    </author>
    <author>
      <name>Brendan Jou</name>
    </author>
    <author>
      <name>Xavier Gir√≥-i-Nieto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2813524.2813530</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2813524.2813530" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint of the paper accepted at the 1st Workshop on Affect and
  Sentiment in Multimedia (ASM), in ACM MultiMedia 2015. Brisbane, Australia</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.05056v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05056v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10; H.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05373v1</id>
    <updated>2015-08-21T19:06:53Z</updated>
    <published>2015-08-21T19:06:53Z</published>
    <title>Dot-Diffused Halftoning with Improved Homogeneity</title>
    <summary>  Compared to the error diffusion, dot diffusion provides an additional
pixel-level parallelism for digital halftoning. However, even though its
periodic and blocking artifacts had been eased by previous works, it was still
far from satisfactory in terms of the blue noise spectrum perspective. In this
work, we strengthen the relationship among the pixel locations of the same
processing order by an iterative halftoning method, and the results demonstrate
a significant improvement. Moreover, a new approach of deriving the averaged
power spectrum density (APSD) is proposed to avoid the regular sampling of the
well-known Bartlett's procedure which inaccurately presents the halftone
periodicity of certain halftoning techniques with parallelism. As a result, the
proposed dot diffusion is substantially superior to the state-of-the-art
parallel halftoning methods in terms of visual quality and artifact-free
property, and competitive runtime to the theoretical fastest ordered dithering
is offered simultaneously.
</summary>
    <author>
      <name>Yun-Fu Liu</name>
    </author>
    <author>
      <name>Jing-Ming Guo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2015.2470599</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2015.2470599" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE Trans. on Image Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.05373v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05373v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.07640v1</id>
    <updated>2015-08-30T21:47:00Z</updated>
    <published>2015-08-30T21:47:00Z</published>
    <title>Compressive Video Sensing via Dictionary Learning and Forward Prediction</title>
    <summary>  In this paper, we propose a new framework for compressive video sensing (CVS)
that exploits the inherent spatial and temporal redundancies of a video
sequence, effectively. The proposed method splits the video sequence into the
key and non-key frames followed by dividing each frame into small
non-overlapping blocks of equal sizes. At the decoder side, the key frames are
reconstructed using adaptively learned sparsifying (ALS) basis via $\ell_0$
minimization, in order to exploit the spatial redundancy. Also, the
effectiveness of three well-known dictionary learning algorithms is
investigated in our method. For recovery of the non-key frames, a prediction of
the current frame is initialized, by using the previous reconstructed frame, in
order to exploit the temporal redundancy. The prediction is employed in a
proper optimization problem to recover the current non-key frame. To compare
our experimental results with the results of some other methods, we employ peak
signal to noise ratio (PSNR) and structural similarity (SSIM) index as the
quality assessor. The numerical results show the adequacy of our proposed
method in CVS.
</summary>
    <author>
      <name>Nasser Eslahi</name>
    </author>
    <author>
      <name>Ali Aghagolzadeh</name>
    </author>
    <author>
      <name>Seyed Mehdi Hosseini Andargoli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 Pages, 5 Figures, 3 Tables, This paper was presented in part at
  the 7th International Symposium on Telecommunications. arXiv admin note: text
  overlap with arXiv:1404.7566 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.07640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.07640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.02630v1</id>
    <updated>2015-09-09T04:00:59Z</updated>
    <published>2015-09-09T04:00:59Z</published>
    <title>Audio Steganography: LSB Technique Using a Pyramid Structure and Range
  of Bytes</title>
    <summary>  The demand for keeping the information secure and confidential simultaneously
has been progressively increasing. Among various techniques- Audio
Steganography, a technique of embedding information transparently in a digital
media thereby restricting the access to such information has been prominently
developed. Imperceptibility, robustness, and payload or hiding capacity are the
main character for it. In earlier, LSB techniques increased payload capacity
would hamper robustness as well as imperceptibility of the cover media and vice
versa. The proposed technique overcomes the problem. It provides relatively
good improvement in the payload capacity by dividing the bytes of cover media
into ranges to hide the bits of secret message appropriately. As well as due to
the use of ranges of bytes the robustness of cover media has maintained and
imperceptibility preserved by using a pyramid structure.
</summary>
    <author>
      <name>Satish Bhalshankar</name>
    </author>
    <author>
      <name>Avinash K. Gulve</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 page, 16 Figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Advanced Computer Research (IJACR),
  Volume-5, Issue-20, September-2015 ,pp.233-248</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1509.02630v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.02630v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.05671v1</id>
    <updated>2015-09-18T15:45:41Z</updated>
    <published>2015-09-18T15:45:41Z</published>
    <title>User-Curated Image Collections: Modeling and Recommendation</title>
    <summary>  Most state-of-the-art image retrieval and recommendation systems
predominantly focus on individual images. In contrast, socially curated image
collections, condensing distinctive yet coherent images into one set, are
largely overlooked by the research communities. In this paper, we aim to design
a novel recommendation system that can provide users with image collections
relevant to individual personal preferences and interests. To this end, two key
issues need to be addressed, i.e., image collection modeling and similarity
measurement. For image collection modeling, we consider each image collection
as a whole in a group sparse reconstruction framework and extract concise
collection descriptors given the pretrained dictionaries. We then consider
image collection recommendation as a dynamic similarity measurement problem in
response to user's clicked image set, and employ a metric learner to measure
the similarity between the image collection and the clicked image set. As there
is no previous work directly comparable to this study, we implement several
competitive baselines and related methods for comparison. The evaluations on a
large scale Pinterest data set have validated the effectiveness of our proposed
methods for modeling and recommending image collections.
</summary>
    <author>
      <name>Yuncheng Li</name>
    </author>
    <author>
      <name>Yang Cong</name>
    </author>
    <author>
      <name>Tao Mei</name>
    </author>
    <author>
      <name>Jiebo Luo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/BigData.2015.7363803</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/BigData.2015.7363803" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in IEEE BigData 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.05671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.05671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.06792v1</id>
    <updated>2015-09-22T21:44:28Z</updated>
    <published>2015-09-22T21:44:28Z</published>
    <title>A Resource Allocation Mechanism for Video Mixing as a Cloud Computing
  Service in Multimedia Conferencing Applications</title>
    <summary>  Multimedia conferencing is the conversational exchange of multimedia content
between multiple parties. It has a wide range of applications (e.g. Massively
Multiplayer Online Games (MMOGs) and distance learning). Many multimedia
conferencing applications use video extensively, thus video mixing in
conferencing settings is of critical importance. Cloud computing is a
technology that can solve the scalability issue in multimedia conferencing,
while bringing other benefits, such as, elasticity, efficient use of resources,
rapid development, and introduction of new applications. However, proposed
cloud-based multimedia conferencing approaches so far have several deficiencies
when it comes to efficient resource usage while meeting Quality of Service
(QoS) requirements. We propose a solution to optimize resource allocation for
cloud-based video mixing service in multimedia conferencing applications, which
can support scalability in terms of number of users, while guaranteeing QoS. We
formulate the resource allocation problem mathematically as an Integer Linear
Programming (ILP) problem and design a heuristic for it. Simulation results
show that our resource allocation model can support more participants compared
to the state-of-the-art, while honoring QoS, with respect to end-to-end delay.
</summary>
    <author>
      <name>Abbas Soltanian</name>
    </author>
    <author>
      <name>Mohammad A. Salahuddin</name>
    </author>
    <author>
      <name>Halima Elbiaze</name>
    </author>
    <author>
      <name>Roch Glitho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, CNSM 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.06792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.06792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00561v1</id>
    <updated>2015-10-02T11:08:45Z</updated>
    <published>2015-10-02T11:08:45Z</published>
    <title>CVC: The Contourlet Video Compression algorithm for real-time
  applications</title>
    <summary>  Nowadays, real-time video communication over the internet through video
conferencing applications has become an invaluable tool in everyone's
professional and personal life. This trend underlines the need for video coding
algorithms that provide acceptable quality on low bitrates and can support
various resolutions inside the same stream in order to cope with limitations on
computational resources and network bandwidth. In this work, a novel scalable
video coding algorithm based on the contourlet transform is presented. The
algorithm utilizes both lossy and lossless methods in order to achieve
compression. One of its most notable features is that due to the transform
utilised, it does not suffer from blocking artifacts that occur with many
widely adopted compression algorithms. The proposed algorithm takes advantage
of the vast computational capabilities of modern GPUs, in order to achieve
real-time performance and provide satisfactory encoding and decoding times at
relatively low cost, making it suitable for applications like video
conferencing. Experiments show that the proposed algorithm performs
satisfactorily in terms of compression ratio and speed, while it outperforms
standard methods in terms of perceptual quality on lower bitrates.
</summary>
    <author>
      <name>Stamos Katsigiannis</name>
    </author>
    <author>
      <name>Georgios Papaioannou</name>
    </author>
    <author>
      <name>Dimitris Maroulis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.00561v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00561v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.01134v2</id>
    <updated>2016-08-22T13:36:58Z</updated>
    <published>2015-10-05T13:04:55Z</published>
    <title>A System for Precise End-to-End Delay Measurements in Video
  Communication</title>
    <summary>  Low delay video transmission is becoming increasingly important. Delay
critical, video enabled applications range from teleoperation scenarios such as
controlling drones or telesurgery to autonomous control through computer vision
algorithms applied on real-time video. To judge the quality of the video
transmission in such a system, it is important to be able to precisely measure
the end-to-end (E2E) delay of the transmitted video. We present a
low-complexity system that automatically takes pairwise independent
measurements of E2E delay. The precision can be far below the millisecond
order, mainly limited by the sampling rate of the measurement system. In our
implementation, we achieve a precision of 0.5 milliseconds with a sampling rate
of 2kHz.
</summary>
    <author>
      <name>Christoph Bachhuber</name>
    </author>
    <author>
      <name>Eckehard Steinbach</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICIP.2016.7532735</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICIP.2016.7532735" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures, IEEE International Conference on Image Processing
  (ICIP 2016), Phoenix, AZ, USA, 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.01134v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.01134v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.05405v1</id>
    <updated>2015-10-19T09:40:17Z</updated>
    <published>2015-10-19T09:40:17Z</published>
    <title>The Virtual Splitter: Refactoring Web Applications for the Multiscreen
  Environment</title>
    <summary>  Creating web applications for the multiscreen environment is still a
challenge. One approach is to transform existing single-screen applications but
this has not been done yet automatically or generically. This paper proposes a
refactor-ing system. It consists of a generic and extensible mapping phase that
automatically analyzes the application content based on a semantic or a visual
criterion determined by the author or the user, and prepares it for the
splitting process. The system then splits the application and as a result
delivers two instrumented applications ready for distribution across devices.
During runtime, the system uses a mirroring phase to maintain the functionality
of the distributed application and to support a dynamic splitting process.
Developed as a Chrome extension, our approach is validated on several web
applications, including a YouTube page and a video application from Mozilla.
</summary>
    <author>
      <name>Mira Sarkis</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <author>
      <name>Cyril Concolato</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <author>
      <name>Jean-Claude Dufourd</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LTCI</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2644866.2644893</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2644866.2644893" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">DocEng'14: ACM Symposium on Document Engineering, ACM, 2014,
  pp.Pages139-142 \&amp;lt;10.1145/2644866.2644893\&amp;gt;</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1510.05405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.05405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.02656v1</id>
    <updated>2015-11-09T12:20:11Z</updated>
    <published>2015-11-09T12:20:11Z</published>
    <title>A Novel Adaptation Method for HTTP Streaming of VBR Videos over Mobile
  Networks</title>
    <summary>  Recently, HTTP streaming has become very popular for delivering video over
the Internet. For adaptivity, a provider should generate multiple versions of a
video as well as the related metadata. Various adaptation methods have been
proposed to support a streaming client in coping with strong bandwidth
variations. However, most of existing methods target at constant bitrate (CBR)
videos only. In this paper, we present a new method for quality adaptation in
on-demand streaming of variable bitrate (VBR) videos. To cope with strong
variations of VBR bitrate, we use a local average bitrate as the representative
bitrate of a version. A buffer-based algorithm is then proposed to
conservatively adapt video quality. Through experiments, we show that our
method can provide quality stability as well as buffer stability even under
very strong variations of bandwidth and video bitrates.
</summary>
    <author>
      <name>Hung. T Le</name>
    </author>
    <author>
      <name>Hai N. Nguyen</name>
    </author>
    <author>
      <name>Nam Pham Ngoc</name>
    </author>
    <author>
      <name>Anh T. Pham</name>
    </author>
    <author>
      <name>Truong Cong Thang</name>
    </author>
    <link href="http://arxiv.org/abs/1511.02656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.02656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.03351v1</id>
    <updated>2015-11-11T01:32:47Z</updated>
    <published>2015-11-11T01:32:47Z</published>
    <title>Attribute-Based Multi-Dimensional Scalable Access Control For Social
  Media Sharing</title>
    <summary>  Media sharing is an extremely popular paradigm of social interaction in
online social networks (OSNs) nowadays. The scalable media access control is
essential to perform information sharing among users with various access
privileges. In this paper, we present a multi-dimensional scalable media access
control (MD-SMAC) system based on the proposed scalable ciphertext policy
attribute-based encryption (SCP-ABE) algorithm. In the proposed MD-SMAC system,
fine-grained access control can be performed on the media contents encoded in a
multi-dimensional scalable manner based on data consumers' diverse attributes.
Through security analysis, we show that the proposed MC-SMAC system is able to
resist collusion attacks. Additionally, we conduct experiments to evaluate the
efficiency performance of the proposed system, especially on mobile devices.
</summary>
    <author>
      <name>Changsha Ma</name>
    </author>
    <author>
      <name>Chang Wen Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1511.03351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.03351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.04691v1</id>
    <updated>2015-11-15T12:27:43Z</updated>
    <published>2015-11-15T12:27:43Z</published>
    <title>Optimization of the Block-level Bit Allocation in Perceptual Video
  Coding based on MINMAX</title>
    <summary>  In video coding, it is expected that the encoder could adaptively select the
encoding parameters (e.g., quantization parameter) to optimize the bit
allocation to different sources under the given constraint. However, in hybrid
video coding, the dependency between sources brings high complexity for the bit
allocation optimization, especially in the block-level, and existing
optimization methods mostly focus on frame-level bit allocation. In this paper,
we propose a macroblock (MB) level bit allocation method based on the minimum
maximum (MINMAX) criterion, which has acceptable encoding complexity for
offline applications. An iterative-based algorithm, namely maximum distortion
descend (MDD), is developed to reduce quality fluctuation among MBs within a
frame, where the Structure SIMilarity (SSIM) index is used to measure the
perceptual distortion of MBs. Our extensive experimental results on benchmark
video sequences show that the proposed method can greatly enhance the encoding
performance in terms of both bits saving and perceptual quality improvement.
</summary>
    <author>
      <name>Chao Wang</name>
    </author>
    <author>
      <name>Xuanqin Mou</name>
    </author>
    <author>
      <name>Lei Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.04691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.04691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.2; E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.08507v1</id>
    <updated>2015-11-26T21:07:05Z</updated>
    <published>2015-11-26T21:07:05Z</published>
    <title>Creativity in Mind: Evaluating and Maintaining Advances in Network
  Steganographic Research</title>
    <summary>  The research discipline of network steganography deals with the hiding of
information within network transmissions, e.g. to transfer illicit information
in networks with Internet censorship. The last decades of research on network
steganography led to more than hundred techniques for hiding data in network
transmissions. However, previous research has shown that most of these hiding
techniques are either based on the same idea or introduce limited novelty,
enabling the application of existing countermeasures. In this paper, we provide
a link between the field of creativity and network steganographic research. We
propose a framework and a metric to help evaluating the creativity bound to a
given hiding technique. This way, we support two sides of the scientific peer
review process as both authors and reviewers can use our framework to analyze
the novelty and applicability of hiding techniques. At the same time, we
contribute to a uniform terminology in network steganography.
</summary>
    <author>
      <name>Steffen Wendzel</name>
    </author>
    <author>
      <name>Carolin Palmer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3217/jucs-021-12-1684</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3217/jucs-021-12-1684" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in Journal of Universal Computer Science (J.UCS)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Universal Computer Science, Vol. 21(12), 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1511.08507v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.08507v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="D.2.11; D.4.6; K.6.5; K.7.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.08854v1</id>
    <updated>2015-12-30T05:41:21Z</updated>
    <published>2015-12-30T05:41:21Z</published>
    <title>An Overview of Emerging Technologies for High Efficiency 3D Video Coding</title>
    <summary>  3D video coding is one of the most popular research area in multimedia. This
paper reviews the recent progress of the coding technologies for multiview
video (MVV) and free view-point video (FVV) which is represented by MVV and
depth maps. We first discuss the traditional multiview video coding (MVC)
framework with different prediction structures. The rate-distortion performance
and the view switching delay of the three main coding prediction structures are
analyzed. We further introduce the joint coding technologies for MVV and depth
maps and evaluate the rate-distortion performance of them. The scalable 3D
video coding technologies are reviewed by the quality and view scalability,
respectively. Finally, we summarize the bit allocation work of 3D video coding.
This paper also points out some future research problems in high efficiency 3D
video coding such as the view switching latency optimization in coding
structure and bit allocation.
</summary>
    <author>
      <name>Qifei Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.08854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.08854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.01408v1</id>
    <updated>2016-01-07T06:15:16Z</updated>
    <published>2016-01-07T06:15:16Z</published>
    <title>Comparison of cinepak, intel, microsoft video and indeo codec for video
  compression</title>
    <summary>  The file size and picture quality are factors to be considered for streaming,
storage and transmitting videos over networks. This work compares Cinepak,
Intel, Microsoft Video and Indeo Codec for video compression. The peak signal
to noise ratio is used to compare the quality of such video compressed using
AVI codecs. The most widely used objective measurement by developers of video
processing systems is Peak Signal-to-Noise Ratio (PSNR). Peak Signal to Noise
Ration is measured on a logarithmic scale and depends on the mean squared error
(MSE) between an original and an impaired image or video, relative to (2n-1)2.
  Previous research done regarding assessing of video quality has been mainly
by the use of subjective methods, and there is still no standard method for
objective assessments. Although it has been considered that compression might
not be significant in future as storage and transmission capabilities improve,
but at low bandwidths compression makes communication possible.
</summary>
    <author>
      <name>Suleiman Mustafa</name>
    </author>
    <author>
      <name>Hannan Xiao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 1 figure, 7 tables, journal paper</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The International Journal of Multimedia and Its Applications
  (IJMA), Volume 7, Number 6 December 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.01408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.01408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.02076v1</id>
    <updated>2016-01-09T04:48:08Z</updated>
    <published>2016-01-09T04:48:08Z</published>
    <title>An Enhanced Edge Adaptive Steganography Approach Using Threshold Value
  for Region Selection</title>
    <summary>  This paper attempts to improve the quality and the modification rate of a
Stego Image. The input image provided for estimating the quality of an image
and the modified rate is a bitmap image. The threshold value is used as a
parameter for selecting the high frequency pixels from the Cover Image. The
data embedding process are performed on the pixels that are found with the help
of Threshold value by using LSBMR. The quality of an image is estimated by the
value of PSNR and the modification rate of an image is estimated by the value
of MSE. The proposed approach achieves about 0.2 to 0.6 % of improvement in the
quality of an image and about 4 to 10 % of improvement in the modification rate
of an image compared to the edge detection techniques such as Sobel and Canny.
</summary>
    <author>
      <name>Sachin Mungmode</name>
    </author>
    <author>
      <name>R. R. Sedamkar</name>
    </author>
    <author>
      <name>Niranjan Kulkarni</name>
    </author>
    <link href="http://arxiv.org/abs/1601.02076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.02076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.04473v1</id>
    <updated>2016-01-18T11:22:29Z</updated>
    <published>2016-01-18T11:22:29Z</published>
    <title>Lossless Intra Coding in HEVC with 3-tap Filters</title>
    <summary>  This paper presents a pixel-by-pixel spatial prediction method for lossless
intra coding within High Efficiency Video Coding (HEVC). A well-known previous
pixel-by-pixel spatial prediction method uses only two neighboring pixels for
prediction, based on the angular projection idea borrowed from block-based
intra prediction in lossy coding. This paper explores a method which uses three
neighboring pixels for prediction according to a two-dimensional correlation
model, and the used neighbor pixels and prediction weights change depending on
intra mode. To find the best prediction weights for each intra mode, a
two-stage offline optimization algorithm is used and a number of implementation
aspects are discussed to simplify the proposed prediction method. The proposed
method is implemented in the HEVC reference software and experimental results
show that the explored 3-tap filtering method can achieve an average 11.34%
bitrate reduction over the default lossless intra coding in HEVC. The proposed
method also decreases average decoding time by 12.7% while it increases average
encoding time by 9.7%
</summary>
    <author>
      <name>Saeed R. Alvar</name>
    </author>
    <author>
      <name>Fatih Kamisli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.04473v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04473v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.04522v1</id>
    <updated>2016-01-18T14:15:06Z</updated>
    <published>2016-01-18T14:15:06Z</published>
    <title>Multiple Watermarking Algorithm Based on Spread Transform Dither
  Modulation</title>
    <summary>  Multiple watermarking technique, embedding several watermarks in one carrier,
has enabled many interesting applications. In this study, a novel multiple
watermarking algorithm is proposed based on the spirit of spread transform
dither modulation (STDM). It can embed multiple watermarks into the same region
and the same transform domain of one image; meanwhile, the embedded watermarks
can be extracted independently and blindly in the detector without any
interference. Furthermore, to improve the fidelity of the watermarked image,
the properties of the dither modulation quantizer and the proposed multiple
watermarks embedding strategy are investigated, and two practical optimization
methods are proposed. Finally, to enhance the application flexibility, an
extension of the proposed algorithm is proposed which can sequentially embeds
different watermarks into one image during each stage of its circulation.
Compared with the pioneering multiple watermarking algorithms, the proposed one
owns more flexibility in practical application and is more robust against
distortion due to basic operations such as random noise, JPEG compression and
volumetric scaling.
</summary>
    <author>
      <name>Xinchao Li</name>
    </author>
    <author>
      <name>Ju Liu</name>
    </author>
    <author>
      <name>Jiande Sun</name>
    </author>
    <author>
      <name>Xiaohui Yang</name>
    </author>
    <author>
      <name>Wei Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1601.04522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.04522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.06603v1</id>
    <updated>2016-01-25T13:57:07Z</updated>
    <published>2016-01-25T13:57:07Z</published>
    <title>Egocentric Activity Recognition with Multimodal Fisher Vector</title>
    <summary>  With the increasing availability of wearable devices, research on egocentric
activity recognition has received much attention recently. In this paper, we
build a Multimodal Egocentric Activity dataset which includes egocentric videos
and sensor data of 20 fine-grained and diverse activity categories. We present
a novel strategy to extract temporal trajectory-like features from sensor data.
We propose to apply the Fisher Kernel framework to fuse video and temporal
enhanced sensor features. Experiment results show that with careful design of
feature extraction and fusion algorithm, sensor data can enhance
information-rich video data. We make publicly available the Multimodal
Egocentric Activity dataset to facilitate future research.
</summary>
    <author>
      <name>Sibo Song</name>
    </author>
    <author>
      <name>Ngai-Man Cheung</name>
    </author>
    <author>
      <name>Vijay Chandrasekhar</name>
    </author>
    <author>
      <name>Bappaditya Mandal</name>
    </author>
    <author>
      <name>Jie Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures, ICASSP 2016 accepted</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.06603v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.06603v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.07262v1</id>
    <updated>2016-01-27T04:58:16Z</updated>
    <published>2016-01-27T04:58:16Z</published>
    <title>Revisiting copy-move forgery detection by considering realistic image
  with similar but genuine objects</title>
    <summary>  Many images, of natural or man-made scenes often contain Similar but Genuine
Objects (SGO). This poses a challenge to existing Copy-Move Forgery Detection
(CMFD) methods which match the key points / blocks, solely based on the pair
similarity in the scene. To address such issue, we propose a novel CMFD method
using Scaled Harris Feature Descriptors (SHFD) that preform consistently well
on forged images with SGO. It involves the following main steps: (i) Pyramid
scale space and orientation assignment are used to keep scaling and rotation
invariance; (ii) Combined features are applied for precise texture description;
(iii) Similar features of two points are matched and RANSAC is used to remove
the false matches. The experimental results indicate that the proposed
algorithm is effective in detecting SGO and copy-move forgery, which compares
favorably to existing methods. Our method exhibits high robustness even when an
image is operated by geometric transformation and post-processing
</summary>
    <author>
      <name>Ye Zhu</name>
    </author>
    <author>
      <name>Tian-Tsong Ng</name>
    </author>
    <author>
      <name>Xuanjing Shen</name>
    </author>
    <author>
      <name>Bihan Wen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The version of ICASSP2016 submission</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.07262v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.07262v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.07884v1</id>
    <updated>2016-01-28T20:13:01Z</updated>
    <published>2016-01-28T20:13:01Z</published>
    <title>Geo-distinctive Visual Element Matching for Location Estimation of
  Images</title>
    <summary>  We propose an image representation and matching approach that substantially
improves visual-based location estimation for images. The main novelty of the
approach, called distinctive visual element matching (DVEM), is its use of
representations that are specific to the query image whose location is being
predicted. These representations are based on visual element clouds, which
robustly capture the connection between the query and visual evidence from
candidate locations. We then maximize the influence of visual elements that are
geo-distinctive because they do not occur in images taken at many other
locations. We carry out experiments and analysis for both geo-constrained and
geo-unconstrained location estimation cases using two large-scale,
publicly-available datasets: the San Francisco Landmark dataset with $1.06$
million street-view images and the MediaEval '15 Placing Task dataset with
$5.6$ million geo-tagged images from Flickr. We present examples that
illustrate the highly-transparent mechanics of the approach, which are based on
common sense observations about the visual patterns in image collections. Our
results show that the proposed method delivers a considerable performance
improvement compared to the state of the art.
</summary>
    <author>
      <name>Xinchao Li</name>
    </author>
    <author>
      <name>Martha A. Larson</name>
    </author>
    <author>
      <name>Alan Hanjalic</name>
    </author>
    <link href="http://arxiv.org/abs/1601.07884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.07884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.00489v2</id>
    <updated>2016-02-19T15:23:02Z</updated>
    <published>2016-02-01T12:12:17Z</published>
    <title>Real Time Video Quality Representation Classification of Encrypted HTTP
  Adaptive Video Streaming - the Case of Safari</title>
    <summary>  The increasing popularity of HTTP adaptive video streaming services has
dramatically increased bandwidth requirements on operator networks, which
attempt to shape their traffic through Deep Packet Inspection (DPI). However,
Google and certain content providers have started to encrypt their video
services. As a result, operators often encounter difficulties in shaping their
encrypted video traffic via DPI. This highlights the need for new traffic
classification methods for encrypted HTTP adaptive video streaming to enable
smart traffic shaping. These new methods will have to effectively estimate the
quality representation layer and playout buffer. We present a new method and
show for the first time that video quality representation classification for
(YouTube) encrypted HTTP adaptive streaming is possible. We analyze the
performance of this classification method with Safari over HTTPS. Based on a
large number of offline and online traffic classification experiments, we
demonstrate that it can independently classify, in real time, every video
segment into one of the quality representation layers with 97.18% average
accuracy.
</summary>
    <author>
      <name>Ran Dubin</name>
    </author>
    <author>
      <name>Amit Dvir</name>
    </author>
    <author>
      <name>Ofir Pele</name>
    </author>
    <author>
      <name>Ofer Hadar</name>
    </author>
    <author>
      <name>Itay Richman</name>
    </author>
    <author>
      <name>Ofir Trabelsi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.00489v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.00489v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.02030v1</id>
    <updated>2016-02-05T14:17:52Z</updated>
    <published>2016-02-05T14:17:52Z</published>
    <title>Adaptation Logic for HTTP Dynamic Adaptive Streaming using
  Geo-Predictive Crowdsourcing</title>
    <summary>  The increasing demand for video streaming services with high Quality of
Experience (QoE) has prompted a lot of research on client-side adaptation logic
approaches. However, most algorithms use the client's previous download
experience and do not use a crowd knowledge database generated by users of a
professional service. We propose a new crowd algorithm that maximizes the QoE.
Additionally, we show how crowd information can be integrated into existing
algorithms and illustrate this with two state-of-the-art algorithms. We
evaluate our algorithm and state-of-the-art algorithms (including our modified
algorithms) on a large, real-life crowdsourcing dataset that contains 336,551
samples on network performance. The dataset was provided by WeFi LTD. Our new
algorithm outperforms all other methods in terms of QoS (eMOS).
</summary>
    <author>
      <name>Ran Dubin</name>
    </author>
    <author>
      <name>Amit Dvir</name>
    </author>
    <author>
      <name>Ofir Pele</name>
    </author>
    <author>
      <name>Ofer Hadar</name>
    </author>
    <author>
      <name>Itay Katz</name>
    </author>
    <author>
      <name>Ori Mashiach</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00530-016-0525-6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00530-016-0525-6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.02030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.02030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.05209v1</id>
    <updated>2016-02-16T21:27:58Z</updated>
    <published>2016-02-16T21:27:58Z</published>
    <title>Perceptual Vector Quantization For Video Coding</title>
    <summary>  This paper applies energy conservation principles to the Daala video codec
using gain-shape vector quantization to encode a vector of AC coefficients as a
length (gain) and direction (shape). The technique originates from the CELT
mode of the Opus audio codec, where it is used to conserve the spectral
envelope of an audio signal. Conserving energy in video has the potential to
preserve textures rather than low-passing them. Explicitly quantizing a gain
allows a simple contrast masking model with no signaling cost. Vector
quantizing the shape keeps the number of degrees of freedom the same as scalar
quantization, avoiding redundancy in the representation. We demonstrate how to
predict the vector by transforming the space it is encoded in, rather than
subtracting off the predictor, which would make energy conservation impossible.
We also derive an encoding of the vector-quantized codewords that takes
advantage of their non-uniform distribution. We show that the resulting
technique outperforms scalar quantization by an average of 0.90 dB on still
images, equivalent to a 24.8% reduction in bitrate at equal quality, while for
videos, the improvement averages 0.83 dB, equivalent to a 13.7% reduction in
bitrate.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Timothy B. Terriberry</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.2080529</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.2080529" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, Proceedings of SPIE Visual Information Processing and
  Communication, 2015</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. SPIE 9410, Visual Information Processing and Communication
  VI, 941009 (March 4, 2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1602.05209v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05209v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.02980v3</id>
    <updated>2016-08-01T22:20:23Z</updated>
    <published>2016-03-09T17:59:26Z</published>
    <title>Impact Analysis of Baseband Quantizer on Coding Efficiency for HDR Video</title>
    <summary>  Digitally acquired high dynamic range (HDR) video baseband signal can take 10
to 12 bits per color channel. It is economically important to be able to reuse
the legacy 8 or 10-bit video codecs to efficiently compress the HDR video.
Linear or nonlinear mapping on the intensity can be applied to the baseband
signal to reduce the dynamic range before the signal is sent to the codec, and
we refer to this range reduction step as a baseband quantization. We show
analytically and verify using test sequences that the use of the baseband
quantizer lowers the coding efficiency. Experiments show that as the baseband
quantizer is strengthened by 1.6 bits, the drop of PSNR at a high bitrate is up
to 1.60dB. Our result suggests that in order to achieve high coding efficiency,
information reduction of videos in terms of quantization error should be
introduced in the video codec instead of on the baseband signal.
</summary>
    <author>
      <name>Chau-Wai Wong</name>
    </author>
    <author>
      <name>Guan-Ming Su</name>
    </author>
    <author>
      <name>Min Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2016.2597175</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2016.2597175" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in IEEE Signal Processing Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.02980v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.02980v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.03482v1</id>
    <updated>2016-03-10T22:55:36Z</updated>
    <published>2016-03-10T22:55:36Z</published>
    <title>Predicting Chroma from Luma with Frequency Domain Intra Prediction</title>
    <summary>  This paper describes a technique for performing intra prediction of the
chroma planes based on the reconstructed luma plane in the frequency domain.
This prediction exploits the fact that while RGB to YUV color conversion has
the property that it decorrelates the color planes globally across an image,
there is still some correlation locally at the block level. Previous proposals
compute a linear model of the spatial relationship between the luma plane (Y)
and the two chroma planes (U and V). In codecs that use lapped transforms this
is not possible since transform support extends across the block boundaries and
thus neighboring blocks are unavailable during intra-prediction. We design a
frequency domain intra predictor for chroma that exploits the same local
correlation with lower complexity than the spatial predictor and which works
with lapped transforms. We then describe a low-complexity algorithm that
directly uses luma coefficients as a chroma predictor based on gain-shape
quantization and band partitioning. An experiment is performed that compares
these two techniques inside the experimental Daala video codec and shows the
lower complexity algorithm to be a better chroma predictor.
</summary>
    <author>
      <name>Nathan E. Egge</name>
    </author>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.2080837</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.2080837" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of SPIE 9410, Visual Information Processing and
  Communication VI, 941009 (March 4, 2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1603.03482v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.03482v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.06083v2</id>
    <updated>2016-03-28T19:30:21Z</updated>
    <published>2016-03-19T11:43:01Z</published>
    <title>Towards Coordinated Bandwidth Adaptations for Hundred-Scale 3D
  Tele-Immersive Systems</title>
    <summary>  3D tele-immersion improves the state of collaboration among geographically
distributed participants. Unlike the traditional 2D videos, a 3D tele-immersive
system employs multiple 3D cameras based in each physical site to cover a much
larger field of view, generating a very large amount of stream data. One of the
major challenges is how to efficiently transmit these bulky 3D streaming data
to bandwidth-constrained sites. In this paper, we study an adaptive Human
Visual System (HVS) -compliant bandwidth management framework for efficient
delivery of hundred-scale streams produced from distributed 3D tele-immersive
sites to a receiver site with limited bandwidth budget. Our adaptation
framework exploits the semantics link of HVS with multiple 3D streams in the 3D
tele-immersive environment. We developed TELEVIS, a visual simulation tool to
showcase a HVS-aware tele-immersive system for realistic cases. Our evaluation
results show that the proposed adaptation can improve the total quality per
unit of bandwidth used to deliver streams in 3D tele-immersive systems.
</summary>
    <author>
      <name>Mohammad Hosseini</name>
    </author>
    <author>
      <name>Gregorij Kurillo</name>
    </author>
    <author>
      <name>Seyed Rasoul Etesami</name>
    </author>
    <author>
      <name>Jiang Yu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00530-016-0511-z</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00530-016-0511-z" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Springer Multimedia Systems Journal, 14 pages, March 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.06083v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.06083v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09012v1</id>
    <updated>2016-03-30T01:16:37Z</updated>
    <published>2016-03-30T01:16:37Z</published>
    <title>A framework for event co-occurrence detection in event streams</title>
    <summary>  This paper shows that characterizing co-occurrence between events is an
important but non-trivial and neglected aspect of discovering potential causal
relationships in multimedia event streams. First an introduction to the notion
of event co-occurrence and its relation to co-occurrence pattern detection is
given. Then a finite state automaton extended with a time model and event
parameterization is introduced to convert high level co-occurrence pattern
definition to its corresponding pattern matching automaton. Finally a
processing algorithm is applied to count the occurrence frequency of a
collection of patterns with only one pass through input event streams. The
method proposed in this paper can be used for detecting co-occurrences between
both events of one event stream (Auto co-occurrence), and events from multiple
event streams (Cross co-occurrence). Some fundamental results concerning the
characterization of event co-occurrence are presented in form of a visual co-
occurrence matrix. Reusable causality rules can be extracted easily from
co-occurrence matrix and fed into various analysis tools, such as
recommendation systems and complex event processing systems for further
analysis.
</summary>
    <author>
      <name>Laleh Jalali</name>
    </author>
    <author>
      <name>Ramesh Jain</name>
    </author>
    <link href="http://arxiv.org/abs/1603.09012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09396v1</id>
    <updated>2016-03-30T22:03:19Z</updated>
    <published>2016-03-30T22:03:19Z</published>
    <title>Robust Hybrid Image Watermarking based on Discrete Wavelet and Shearlet
  Transforms</title>
    <summary>  With the growth of digital networks such as the Internet, digital media have
been explosively developed in e-commerce and online services. This causes
problems such as illegal copy and fake ownership. Watermarking is proposed as
one of the solutions to such cases. Among different watermarking techniques,
the wavelet transform has been used more because of its good ability in
modeling the human visual system. Recently, Shearlet transform as an extension
of Wavelet transform which is based on multi-resolution and multi-directional
analysis is introduced. The most important feature of this transform is the
appropriate representation of image edges. In this paper a hybrid scheme using
Discrete Wavelet Transform (DWT) and Discrete Shearlet Transform (DST) is
presented. In this way, the host image is decomposed using DWT, and then its
low frequency sub-band is decomposed by DST. After that, the bidiagonal
singular value decomposition (BSVD) is applied on the selected sub-band from
Shearlet transform and the gray-scale watermark image is embedded into its
bidiagonal singular values. The proposed method is examined on the images with
different textures and resistance is evaluated against various attacks like
image processing and geometric attacks. The results show good transparency and
high robustness in proposed method.
</summary>
    <author>
      <name>Malihe Mardanpour</name>
    </author>
    <author>
      <name>Mohammad Ali Zare Chahooki</name>
    </author>
    <link href="http://arxiv.org/abs/1603.09396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00493v1</id>
    <updated>2016-04-02T12:21:52Z</updated>
    <published>2016-04-02T12:21:52Z</published>
    <title>Steganography -- A Game of Hide and Seek in Information Communication</title>
    <summary>  With the growth of communication over computer networks, how to maintain the
confidentiality and security of transmitted information have become some of the
important issues. In order to transfer data securely to the destination without
unwanted disclosure or damage, nature inspired hide and seek tricks such as,
cryptography and Steganography are heavily in use. Just like the Chameleon and
many other bio-species those change their body color and hide themselves in the
background in order to protect them from external attacks, Cryptography and
Steganography are techniques those are used to encrypt and hide the secret data
inside other media to ensure data security. This paper discusses the concept of
a simple spatial domain LSB Steganography that encrypts the secrets using
Fibonacci- Lucas transformation, before hiding, for better security.
</summary>
    <author>
      <name>Sanjeeb Kumar Behera</name>
    </author>
    <author>
      <name>Minati Mishra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures, National Conference on Recent Innovations in
  Engineering and Management Sciences (RIEMS-2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00493v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00493v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02778v1</id>
    <updated>2016-04-11T02:58:06Z</updated>
    <published>2016-04-11T02:58:06Z</published>
    <title>Trends toward real-time network data steganography</title>
    <summary>  Network steganography has been a well-known covert data channeling method for
over three decades. The basic set of techniques and implementation tools have
not changed significantly since their introduction in the early 1980's. In this
paper, we review the predominant methods of classical network steganography,
describing the detailed operations and resultant challenges involved in
embedding data in the network transport domain. We also consider the various
cyber threat vectors of network steganography and point out the major
differences between classical network steganography and the widely known
end-point multimedia embedding techniques, which focus exclusively on static
data modification for data hiding. We then challenge the security community by
introducing an entirely new network dat hiding methodology, which we refer to
as real-time network data steganography. Finally we provide the groundwork for
this fundamental change of covert network data embedding by forming a basic
framework for real-time network data operations that will open the path for
even further advances in computer network security.
</summary>
    <author>
      <name>James Collins</name>
    </author>
    <author>
      <name>Sos Agaian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages introducing the concept of real-time network steganography</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.02778v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02778v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.03688v2</id>
    <updated>2016-04-27T07:47:54Z</updated>
    <published>2016-04-13T08:33:36Z</published>
    <title>A Practical Approach to Spatiotemporal Data Compression</title>
    <summary>  Datasets representing the world around us are becoming ever more unwieldy as
data volumes grow. This is largely due to increased measurement and modelling
resolution, but the problem is often exacerbated when data are stored at
spuriously high precisions. In an effort to facilitate analysis of these
datasets, computationally intensive calculations are increasingly being
performed on specialised remote servers before the reduced data are transferred
to the consumer. Due to bandwidth limitations, this often means data are
displayed as simple 2D data visualisations, such as scatter plots or images. We
present here a novel way to efficiently encode and transmit 4D data fields
on-demand so that they can be locally visualised and interrogated. This nascent
"4D video" format allows us to more flexibly move the boundary between data
server and consumer client. However, it has applications beyond purely
scientific visualisation, in the transmission of data to virtual and augmented
reality.
</summary>
    <author>
      <name>Niall H. Robinson</name>
    </author>
    <author>
      <name>Rachel Prudden</name>
    </author>
    <author>
      <name>Alberto Arribas</name>
    </author>
    <link href="http://arxiv.org/abs/1604.03688v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.03688v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07051v1</id>
    <updated>2016-04-24T16:58:08Z</updated>
    <published>2016-04-24T16:58:08Z</published>
    <title>Lossless Intra Coding in HEVC with Adaptive 3-tap Filters</title>
    <summary>  In pixel-by-pixel spatial prediction methods for lossless intra coding, the
prediction is obtained by a weighted sum of neighbouring pixels. The proposed
prediction approach in this paper uses a weighted sum of three neighbor pixels
according to a two-dimensional correlation model. The weights are obtained
after a three step optimization procedure. The first two stages are offline
procedures where the computed prediction weights are obtained offline from
training sequences. The third stage is an online optimization procedure where
the offline obtained prediction weights are further fine-tuned and adapted to
each encoded block during encoding using a rate-distortion optimized method and
the modification in this third stage is transmitted to the decoder as side
information. The results of the simulations show average bit rate reductions of
12.02% and 3.28% over the default lossless intra coding in HEVC and the
well-known Sample-based Angular Prediction (SAP) method, respectively.
</summary>
    <author>
      <name>Saeed Ranjbar Alvar</name>
    </author>
    <author>
      <name>Fatih Kamisli</name>
    </author>
    <link href="http://arxiv.org/abs/1604.07051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07211v1</id>
    <updated>2016-04-25T11:43:09Z</updated>
    <published>2016-04-25T11:43:09Z</published>
    <title>Towards Reduced Reference Parametric Models for Estimating Audiovisual
  Quality in Multimedia Services</title>
    <summary>  We have developed reduced reference parametric models for estimating
perceived quality in audiovisual multimedia services. We have created 144
unique configurations for audiovisual content including various application and
network parameters such as bitrates and distortions in terms of bandwidth,
packet loss rate and jitter. To generate the data needed for model training and
validation we have tasked 24 subjects, in a controlled environment, to rate the
overall audiovisual quality on the absolute category rating (ACR) 5-level
quality scale. We have developed models using Random Forest and Neural Network
based machine learning methods in order to estimate Mean Opinion Scores (MOS)
values. We have used information retrieved from the packet headers and side
information provided as network parameters for model training. Random Forest
based models have performed better in terms of Root Mean Square Error (RMSE)
and Pearson correlation coefficient. The side information proved to be very
effective in developing the model. We have found that, while the model
performance might be improved by replacing the side information with more
accurate bit stream level measurements, they are performing well in estimating
perceived quality in audiovisual multimedia services.
</summary>
    <author>
      <name>Edip Demirbilek</name>
    </author>
    <author>
      <name>Jean-Charles Gr√©goire</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ICC 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07322v2</id>
    <updated>2016-04-27T06:16:40Z</updated>
    <published>2016-04-25T16:34:17Z</published>
    <title>Predictive No-Reference Assessment of Video Quality</title>
    <summary>  Among the various means to evaluate the quality of video streams,
No-Reference (NR) methods have low computation and may be executed on thin
clients. Thus, NR algorithms would be perfect candidates in cases of real-time
quality assessment, automated quality control and, particularly, in adaptive
mobile streaming. Yet, existing NR approaches are often inaccurate, in
comparison to Full-Reference (FR) algorithms, especially under lossy network
conditions. In this work, we present an NR method that combines machine
learning with simple NR metrics to achieve a quality index comparably as
accurate as the Video Quality Metric (VQM) Full-Reference algorithm. Our method
is tested in an extensive dataset (960 videos), under lossy network conditions
and considering nine different machine learning algorithms. Overall, we achieve
an over 97% correlation with VQM, while allowing real-time assessment of video
quality of experience in realistic streaming scenarios.
</summary>
    <author>
      <name>Maria Torres Vega</name>
    </author>
    <author>
      <name>Decebal Constantin Mocanu</name>
    </author>
    <author>
      <name>Antonio Liotta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 8 figures, IEEE Selected Topics on Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07322v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07322v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07339v1</id>
    <updated>2016-04-25T17:39:25Z</updated>
    <published>2016-04-25T17:39:25Z</published>
    <title>Compressed-domain visual saliency models: A comparative study</title>
    <summary>  Computational modeling of visual saliency has become an important research
problem in recent years, with applications in video quality estimation, video
compression, object tracking, retargeting, summarization, and so on. While most
visual saliency models for dynamic scenes operate on raw video, several models
have been developed for use with compressed-domain information such as motion
vectors and transform coefficients. This paper presents a comparative study of
eleven such models as well as two high-performing pixel-domain saliency models
on two eye-tracking datasets using several comparison metrics. The results
indicate that highly accurate saliency estimation is possible based only on a
partially decoded video bitstream. The strategies that have shown success in
compressed-domain saliency modeling are highlighted, and certain challenges are
identified as potential avenues for further improvement.
</summary>
    <author>
      <name>Sayed Hossein Khatoonabadi</name>
    </author>
    <author>
      <name>Ivan V. Bajic</name>
    </author>
    <author>
      <name>Yufeng Shan</name>
    </author>
    <link href="http://arxiv.org/abs/1604.07339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10; I.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07519v1</id>
    <updated>2016-04-26T05:02:33Z</updated>
    <published>2016-04-26T05:02:33Z</published>
    <title>Subjective Assessment of H.264 Compressed Stereoscopic Video</title>
    <summary>  The tremendous growth in 3D (stereo) imaging and display technologies has led
to stereoscopic content (video and image) becoming increasingly popular.
However, both the subjective and the objective evaluation of stereoscopic video
content has not kept pace with the rapid growth of the content. Further, the
availability of standard stereoscopic video databases is also quite limited. In
this work, we attempt to alleviate these shortcomings. We present a
stereoscopic video database and its subjective evaluation. We have created a
database containing a set of 144 distorted videos. We limit our attention to
H.264 compression artifacts. The distorted videos were generated using 6
uncompressed pristine videos of left and right views originally created by
Goldmann et al. at EPFL [1]. Further, 19 subjects participated in the
subjective assessment task. Based on the subjective study, we have formulated a
relation between the 2D and stereoscopic subjective scores as a function of
compression rate and depth range. We have also evaluated the performance of
popular 2D and 3D image/video quality assessment (I/VQA) algorithms on our
database.
</summary>
    <author>
      <name>Manasa K</name>
    </author>
    <author>
      <name>Balasubramanyam Appina</name>
    </author>
    <author>
      <name>Sumohana S. Channappayya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07939v2</id>
    <updated>2016-07-12T17:58:16Z</updated>
    <published>2016-04-27T05:46:52Z</published>
    <title>Large-Scale Query-by-Image Video Retrieval Using Bloom Filters</title>
    <summary>  We consider the problem of using image queries to retrieve videos from a
database. Our focus is on large-scale applications, where it is infeasible to
index each database video frame independently. Our main contribution is a
framework based on Bloom filters, which can be used to index long video
segments, enabling efficient image-to-video comparisons. Using this framework,
we investigate several retrieval architectures, by considering different types
of aggregation and different functions to encode visual information -- these
play a crucial role in achieving high performance. Extensive experiments show
that the proposed technique improves mean average precision by 24% on a public
dataset, while being 4X faster, compared to the previous state-of-the-art.
</summary>
    <author>
      <name>Andre Araujo</name>
    </author>
    <author>
      <name>Jason Chaves</name>
    </author>
    <author>
      <name>Haricharan Lakshman</name>
    </author>
    <author>
      <name>Roland Angst</name>
    </author>
    <author>
      <name>Bernd Girod</name>
    </author>
    <link href="http://arxiv.org/abs/1604.07939v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07939v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.08088v1</id>
    <updated>2016-04-27T14:32:16Z</updated>
    <published>2016-04-27T14:32:16Z</published>
    <title>Detecting Violence in Video using Subclasses</title>
    <summary>  This paper attacks the challenging problem of violence detection in videos.
Different from existing works focusing on combining multi-modal features, we go
one step further by adding and exploiting subclasses visually related to
violence. We enrich the MediaEval 2015 violence dataset by \emph{manually}
labeling violence videos with respect to the subclasses. Such fine-grained
annotations not only help understand what have impeded previous efforts on
learning to fuse the multi-modal features, but also enhance the generalization
ability of the learned fusion to novel test data. The new subclass based
solution, with AP of 0.303 and P100 of 0.55 on the MediaEval 2015 test set,
outperforms several state-of-the-art alternatives. Notice that our solution
does not require fine-grained annotations on the test set, so it can be
directly applied on novel and fully unlabeled videos. Interestingly, our study
shows that motion related features, though being essential part in previous
systems, are dispensable.
</summary>
    <author>
      <name>Xirong Li</name>
    </author>
    <author>
      <name>Yujia Huo</name>
    </author>
    <author>
      <name>Jieping Xu</name>
    </author>
    <author>
      <name>Qin Jin</name>
    </author>
    <link href="http://arxiv.org/abs/1604.08088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.08088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00957v1</id>
    <updated>2016-05-03T15:50:54Z</updated>
    <published>2016-05-03T15:50:54Z</published>
    <title>Bloom Filters and Compact Hash Codes for Efficient and Distributed Image
  Retrieval</title>
    <summary>  This paper presents a novel method for efficient image retrieval, based on a
simple and effective hashing of CNN features and the use of an indexing
structure based on Bloom filters. These filters are used as gatekeepers for the
database of image features, allowing to avoid to perform a query if the query
features are not stored in the database and speeding up the query process,
without affecting retrieval performance. Thanks to the limited memory
requirements the system is suitable for mobile applications and distributed
databases, associating each filter to a distributed portion of the database.
Experimental validation has been performed on three standard image retrieval
datasets, outperforming state-of-the-art hashing methods in terms of precision,
while the proposed indexing method obtains a $2\times$ speedup.
</summary>
    <author>
      <name>Andrea Salvi</name>
    </author>
    <author>
      <name>Simone Ercoli</name>
    </author>
    <author>
      <name>Marco Bertini</name>
    </author>
    <author>
      <name>Alberto Del Bimbo</name>
    </author>
    <link href="http://arxiv.org/abs/1605.00957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02605v1</id>
    <updated>2016-05-09T14:39:02Z</updated>
    <published>2016-05-09T14:39:02Z</published>
    <title>Efficient Reversible Data Hiding Algorithms Based on Dual Prediction</title>
    <summary>  In this paper, a new reversible data hiding (RDH) algorithm that is based on
the concept of shifting of prediction error histograms is proposed. The
algorithm extends the efficient modification of prediction errors (MPE)
algorithm by incorporating two predictors and using one prediction error value
for data embedding. The motivation behind using two predictors is driven by the
fact that predictors have different prediction accuracy which is directly
related to the embedding capacity and quality of the stego image. The key
feature of the proposed algorithm lies in using two predictors without the need
to communicate additional overhead with the stego image. Basically, the
identification of the predictor that is used during embedding is done through a
set of rules. The proposed algorithm is further extended to use two and three
bins in the prediction errors histogram in order to increase the embedding
capacity. Performance evaluation of the proposed algorithm and its extensions
showed the advantage of using two predictors in boosting the embedding capacity
while providing competitive quality for the stego image.
</summary>
    <author>
      <name>Enas N. Jaara</name>
    </author>
    <author>
      <name>Iyad F. Jafar</name>
    </author>
    <link href="http://arxiv.org/abs/1605.02605v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02605v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02976v1</id>
    <updated>2016-05-10T12:29:18Z</updated>
    <published>2016-05-10T12:29:18Z</published>
    <title>Frame-level quality and memory traffic allocation for lossy embedded
  compression in video codec systems</title>
    <summary>  For mobile video codecs, the huge energy dissipation for external memory
traffic is a critical challenge under the battery power constraint. Lossy
embedded compression (EC), as a solution to this challenge, is considered in
this paper. While previous studies in EC mostly focused on compression
algorithms at the block level, this work, to the best of our knowledge, is the
first one that addresses the allocation of video quality and memory traffic at
the frame level. For lossy EC, a main difficulty of its application lies in the
error propagation from quality degradation of reference frames. Instinctively,
it is preferred to perform more lossy EC in non-reference frames to minimize
the quality loss. The analysis and experiments in this paper, however, will
show lossy EC should actually be distributed to more frames. Correspondingly,
for hierarchical-B GOPs, we developed an efficient allocation that outperforms
the non-reference-only allocation by up to 4.5 dB in PSNR. In comparison, the
proposed allocation also delivers more consistent quality between frames by
having lower PSNR fluctuation.
</summary>
    <author>
      <name>Li Guo</name>
    </author>
    <author>
      <name>Dajiang Zhou</name>
    </author>
    <author>
      <name>Shinji Kimura</name>
    </author>
    <author>
      <name>Satoshi Goto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICME Workshops 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03754v1</id>
    <updated>2016-05-12T10:50:18Z</updated>
    <published>2016-05-12T10:50:18Z</published>
    <title>Regression-based Intra-prediction for Image and Video Coding</title>
    <summary>  By utilizing previously known areas in an image, intra-prediction techniques
can find a good estimate of the current block. This allows the encoder to store
only the error between the original block and the generated estimate, thus
leading to an improvement in coding efficiency. Standards such as AVC and HEVC
describe expert-designed prediction modes operating in certain angular
orientations alongside separate DC and planar prediction modes. Being designed
predictors, while these techniques have been demonstrated to perform well in
image and video coding applications, they do not necessarily fully utilize
natural image structures. In this paper, we describe a novel system for
developing predictors derived from natural image blocks. The proposed algorithm
is seeded with designed predictors (e.g. HEVC-style prediction) and allowed to
iteratively refine these predictors through regularized regression. The
resulting prediction models show significant improvements in estimation quality
over their designed counterparts across all conditions while maintaining
reasonable computational complexity. We also demonstrate how the proposed
algorithm handles the worst-case scenario of intra-prediction with no error
reporting.
</summary>
    <author>
      <name>Carlo Noel Ochotorena</name>
    </author>
    <author>
      <name>Yukihiko Yamashita</name>
    </author>
    <link href="http://arxiv.org/abs/1605.03754v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03754v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03815v1</id>
    <updated>2016-05-12T14:01:14Z</updated>
    <published>2016-05-12T14:01:14Z</published>
    <title>Backward-Shifted Strategies Based on SVC for HTTP Adaptive Video
  Streaming</title>
    <summary>  Although HTTP-based video streaming can easily penetrate firewalls and profit
from Web caches, the underlying TCP may introduce large delays in case of a
sudden capacity loss. To avoid an interruption of the video stream in such
cases we propose the Backward-Shifted Coding (BSC). Based on Scalable Video
Coding (SVC), BSC adds a time-shifted layer of redundancy to the video stream
such that future frames are downloaded at any instant. This pre-fetched content
maintains a fluent video stream even under highly variant network conditions
and leads to high Quality of Experience (QoE). We characterize this QoE gain by
analyzing initial buffering time, re-buffering time and content resolution
using the Ballot theorem. The probability generating functions of the playback
interruption and of the initial buffering latency are provided in closed form.
We further compute the quasi-stationary distribution of the video quality, in
order to compute the average quality, as well as temporal variability in video
quality. Employing these analytic results to optimize QoE shows interesting
trade-offs and video streaming at outstanding fluency.
</summary>
    <author>
      <name>Zakaria Ye</name>
    </author>
    <author>
      <name>Rachid El-Azouzi</name>
    </author>
    <author>
      <name>Tania Jimenez</name>
    </author>
    <author>
      <name>Eitan Altman</name>
    </author>
    <author>
      <name>Stefan Valentin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.03815v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03815v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05319v1</id>
    <updated>2016-05-17T11:54:04Z</updated>
    <published>2016-05-17T11:54:04Z</published>
    <title>Lossless Intra Coding in HEVC with Integer-to-Integer DST</title>
    <summary>  It is desirable to support efficient lossless coding within video coding
standards, which are primarily designed for lossy coding, with as little
modification as possible. A simple approach is to skip transform and
quantization, and directly entropy code the prediction residual, but this is
inefficient for compression. A more efficient and popular approach is to
process the residual block with DPCM prior to entropy coding. This paper
explores an alternative approach based on processing the residual block with
integer-to-integer (i2i) transforms. I2i transforms map integers to integers,
however, unlike the integer transforms used in HEVC for lossy coding, they do
not increase the dynamic range at the output and can be used in lossless
coding. We use both an i2i DCT from the literature and a novel i2i
approximation of the DST. Experiments with the HEVC reference software show
competitive results.
</summary>
    <author>
      <name>Fatih Kamisli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/EUSIPCO.2016.7760687</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/EUSIPCO.2016.7760687" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1605.05118</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05758v1</id>
    <updated>2016-05-18T21:10:16Z</updated>
    <published>2016-05-18T21:10:16Z</published>
    <title>Resource Provisioning and Profit Maximization for Transcoding in
  Information Centric Networking</title>
    <summary>  Adaptive bitrate streaming (ABR) has been widely adopted to support video
streaming services over heterogeneous devices and varying network conditions.
With ABR, each video content is transcoded into multiple representations in
different bitrates and resolutions. However, video transcoding is computing
intensive, which requires the transcoding service providers to deploy a large
number of servers for transcoding the video contents published by the content
producers. As such, a natural question for the transcoding service provider is
how to provision the computing resource for transcoding the video contents
while maximizing service profit. To address this problem, we design a cloud
video transcoding system by taking the advantage of cloud computing technology
to elastically allocate computing resource. We propose a method for jointly
considering the task scheduling and resource provisioning problem in two
timescales, and formulate the service profit maximization as a two-timescale
stochastic optimization problem. We derive some approximate policies for the
task scheduling and resource provisioning. Based on our proposed methods, we
implement our open source cloud video transcoding system Morph and evaluate its
performance in a real environment. The experiment results demonstrate that our
proposed method can reduce the resource consumption and achieve a higher profit
compared with the baseline schemes.
</summary>
    <author>
      <name>Guanyu Gao</name>
    </author>
    <author>
      <name>Yonggang Wen</name>
    </author>
    <author>
      <name>Cedric Westphal</name>
    </author>
    <link href="http://arxiv.org/abs/1605.05758v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05758v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.08969v1</id>
    <updated>2016-05-29T06:03:26Z</updated>
    <published>2016-05-29T06:03:26Z</published>
    <title>Improving Crowdsourced Live Streaming with Aggregated Edge Networks</title>
    <summary>  Recent years have witnessed a dramatic increase of user-generated video
services. In such user-generated video services, crowdsourced live streaming
(e.g., Periscope, Twitch) has significantly challenged today's edge network
infrastructure: today's edge networks (e.g., 4G, Wi-Fi) have limited uplink
capacity support, making high-bitrate live streaming over such links
fundamentally impossible. In this paper, we propose to let broadcasters (i.e.,
users who generate the video) upload crowdsourced video streams using
aggregated network resources from multiple edge networks. There are several
challenges in the proposal: First, how to design a framework that aggregates
bandwidth from multiple edge networks? Second, how to make this framework
transparent to today's crowdsourced live streaming services? Third, how to
maximize the streaming quality for the whole system? We design a
multi-objective and deployable bandwidth aggregation system BASS to address
these challenges: (1) We propose an aggregation framework transparent to
today's crowdsourced live streaming services, using an edge proxy box and
aggregation cloud paradigm; (2) We dynamically allocate geo-distributed cloud
aggregation servers to enable MPTCP (i.e., multi-path TCP), according to
location and network characteristics of both broadcasters and the original
streaming servers; (3) We maximize the overall performance gain for the whole
system, by matching streams with the best aggregation paths.
</summary>
    <author>
      <name>Chenglei Wu</name>
    </author>
    <author>
      <name>Zhi Wang</name>
    </author>
    <author>
      <name>Jiangchuan Liu</name>
    </author>
    <author>
      <name>Shiqiang Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1605.08969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.08969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.09425v1</id>
    <updated>2016-05-30T21:46:31Z</updated>
    <published>2016-05-30T21:46:31Z</published>
    <title>Models and Algorithms for Graph Watermarking</title>
    <summary>  We introduce models and algorithmic foundations for graph watermarking. Our
frameworks include security definitions and proofs, as well as
characterizations when graph watermarking is algorithmically feasible, in spite
of the fact that the general problem is NP-complete by simple reductions from
the subgraph isomorphism or graph edit distance problems. In the digital
watermarking of many types of files, an implicit step in the recovery of a
watermark is the mapping of individual pieces of data, such as image pixels or
movie frames, from one object to another. In graphs, this step corresponds to
approximately matching vertices of one graph to another based on graph
invariants such as vertex degree. Our approach is based on characterizing the
feasibility of graph watermarking in terms of keygen, marking, and
identification functions defined over graph families with known distributions.
We demonstrate the strength of this approach with exemplary watermarking
schemes for two random graph models, the classic Erd\H{o}s-R\'{e}nyi model and
a random power-law graph model, both of which are used to model real-world
networks.
</summary>
    <author>
      <name>David Eppstein</name>
    </author>
    <author>
      <name>Michael T. Goodrich</name>
    </author>
    <author>
      <name>Jenny Lam</name>
    </author>
    <author>
      <name>Nil Mamano</name>
    </author>
    <author>
      <name>Michael Mitzenmacher</name>
    </author>
    <author>
      <name>Manuel Torres</name>
    </author>
    <link href="http://arxiv.org/abs/1605.09425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.09425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00264v1</id>
    <updated>2016-06-01T12:49:09Z</updated>
    <published>2016-06-01T12:49:09Z</published>
    <title>Advanced Transport Options for the Dynamic Adaptive Streaming over HTTP</title>
    <summary>  Multimedia streaming over HTTP is no longer a niche research topic as it has
entered our daily live. The common assumption is that it is deployed on top of
the existing infrastructure utilizing application (HTTP) and transport (TCP)
layer protocols as is. Interestingly, standards like MPEG's Dynamic Adaptive
Streaming over HTTP (DASH) do not mandate the usage of any specific transport
protocol allowing for sufficient deployment flexibility which is further
supported by emerging developments within both protocol layers. This paper
investigates and evaluates the usage of advanced transport options for the
dynamic adaptive streaming over HTTP. We utilize a common test setup to
evaluate HTTP/2.0 and Google's Quick UDP Internet Connections (QUIC) protocol
in the context of DASH-based services.
</summary>
    <author>
      <name>Christian Timmerer</name>
    </author>
    <author>
      <name>Alan Bertoni</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.00264v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00264v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00341v1</id>
    <updated>2016-06-01T16:05:14Z</updated>
    <published>2016-06-01T16:05:14Z</published>
    <title>Which Adaptation Logic? An Objective and Subjective Performance
  Evaluation of HTTP-based Adaptive Media Streaming Systems</title>
    <summary>  Multimedia content delivery over the Internet is predominantly using the
Hypertext Transfer Protocol (HTTP) as its primary protocol and multiple
proprietary solutions exits. The MPEG standard Dynamic Adaptive Streaming over
HTTP (DASH) provides an interoperable solution and in recent years various
adaptation logics/algorithms have been proposed. However, to the best of our
knowledge, there is no comprehensive evaluation of the various
logics/algorithms. Therefore, this paper provides a comprehensive evaluation of
ten different adaptation logics/algorithms, which have been proposed in the
past years. The evaluation is done both objectively and subjectively. The
former is using a predefined bandwidth trajectory within a controlled
environment and the latter is done in a real-world environment adopting
crowdsourcing. The results shall provide insights about which strategy can be
adopted in actual deployment scenarios. Additionally, the evaluation
methodology described in this paper can be used to evaluate any other/new
adaptation logic and to compare it directly with the results reported here.
</summary>
    <author>
      <name>Christian Timmerer</name>
    </author>
    <author>
      <name>Matteo Maiero</name>
    </author>
    <author>
      <name>Benjamin Rainer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.00341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02312v1</id>
    <updated>2016-06-07T20:00:41Z</updated>
    <published>2016-06-07T20:00:41Z</published>
    <title>High Capacity Image Steganography using Adjunctive Numerical
  Representations with Multiple Bit-Plane Decomposition Methods</title>
    <summary>  LSB steganography is a one of the most widely used methods for implementing
covert data channels in image file exchanges [1][2]. The low computational
complexity and implementation simplicity of the algorithm are significant
factors for its popularity with the primary reason being low image distortion.
Many attempts have been made to increase the embedding capacity of LSB
algorithms by expanding into the second or third binary layers of the image
while maintaining a low probability of detection with minimal distortive
effects [2][3][4]. In this paper, we introduce an advanced technique for
covertly embedding data within images using redundant number system
decomposition over non-standard digital bit planes. Both grayscale and
bit-mapped images are equally effective as cover files. It will be shown that
this unique steganography method has minimal visual distortive affects while
also preserving the cover file statistics, making it less susceptible to most
general steganography detection algorithms.
</summary>
    <author>
      <name>James Collins</name>
    </author>
    <author>
      <name>Sos Agaian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.02312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03333v1</id>
    <updated>2016-06-10T14:09:32Z</updated>
    <published>2016-06-10T14:09:32Z</published>
    <title>Automatic Genre and Show Identification of Broadcast Media</title>
    <summary>  Huge amounts of digital videos are being produced and broadcast every day,
leading to giant media archives. Effective techniques are needed to make such
data accessible further. Automatic meta-data labelling of broadcast media is an
essential task for multimedia indexing, where it is standard to use multi-modal
input for such purposes. This paper describes a novel method for automatic
detection of media genre and show identities using acoustic features, textual
features or a combination thereof. Furthermore the inclusion of available
meta-data, such as time of broadcast, is shown to lead to very high
performance. Latent Dirichlet Allocation is used to model both acoustics and
text, yielding fixed dimensional representations of media recordings that can
then be used in Support Vector Machines based classification. Experiments are
conducted on more than 1200 hours of TV broadcasts from the British
Broadcasting Corporation (BBC), where the task is to categorise the broadcasts
into 8 genres or 133 show identities. On a 200-hour test set, accuracies of
98.6% and 85.7% were achieved for genre and show identification respectively,
using a combination of acoustic and textual features with meta-data.
</summary>
    <author>
      <name>Mortaza Doulaty</name>
    </author>
    <author>
      <name>Oscar Saz</name>
    </author>
    <author>
      <name>Raymond W. M. Ng</name>
    </author>
    <author>
      <name>Thomas Hain</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of 17th Interspeech (2016), San Francisco, California, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03333v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03333v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04631v1</id>
    <updated>2016-06-15T03:26:53Z</updated>
    <published>2016-06-15T03:26:53Z</published>
    <title>Bidirectional Long-Short Term Memory for Video Description</title>
    <summary>  Video captioning has been attracting broad research attention in multimedia
community. However, most existing approaches either ignore temporal information
among video frames or just employ local contextual temporal knowledge. In this
work, we propose a novel video captioning framework, termed as
\emph{Bidirectional Long-Short Term Memory} (BiLSTM), which deeply captures
bidirectional global temporal structure in video. Specifically, we first devise
a joint visual modelling approach to encode video data by combining a forward
LSTM pass, a backward LSTM pass, together with visual features from
Convolutional Neural Networks (CNNs). Then, we inject the derived video
representation into the subsequent language model for initialization. The
benefits are in two folds: 1) comprehensively preserving sequential and visual
information; and 2) adaptively learning dense visual features and sparse
semantic representations for videos and sentences, respectively. We verify the
effectiveness of our proposed video captioning framework on a commonly-used
benchmark, i.e., Microsoft Video Description (MSVD) corpus, and the
experimental results demonstrate that the superiority of the proposed approach
as compared to several state-of-the-art methods.
</summary>
    <author>
      <name>Yi Bin</name>
    </author>
    <author>
      <name>Yang Yang</name>
    </author>
    <author>
      <name>Zi Huang</name>
    </author>
    <author>
      <name>Fumin Shen</name>
    </author>
    <author>
      <name>Xing Xu</name>
    </author>
    <author>
      <name>Heng Tao Shen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04631v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04631v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05294v1</id>
    <updated>2016-06-16T17:52:59Z</updated>
    <published>2016-06-16T17:52:59Z</published>
    <title>Can Machine Learn Steganography? - Implementing LSB Substitution and
  Matrix Coding Steganography with Feed-Forward Neural Networks</title>
    <summary>  In recent years, due to the powerful abilities to deal with highly complex
tasks, the artificial neural networks (ANNs) have been studied in the hope of
achieving human-like performance in many applications. Since the ANNs have the
ability to approximate complex functions from observations, it is
straightforward to consider the ANNs for steganography. In this paper, we aim
to implement the well-known LSB substitution and matrix coding steganography
with the feed-forward neural networks (FNNs). Our experimental results have
shown that, the used FNNs can achieve the data embedding operation of the LSB
substitution and matrix coding steganography. For steganography with the ANNs,
though there may be some challenges to us, it would be very promising and
valuable to pay attention to the ANNs for steganography, which may be a new
direction for steganography.
</summary>
    <author>
      <name>Han-Zhou Wu</name>
    </author>
    <author>
      <name>Hong-Xia Wang</name>
    </author>
    <author>
      <name>Yun-Qing Shi</name>
    </author>
    <link href="http://arxiv.org/abs/1606.05294v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05294v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.06873v2</id>
    <updated>2016-07-18T08:45:14Z</updated>
    <published>2016-06-22T10:02:39Z</published>
    <title>Personality, Culture, and System Factors - Impact on Affective Response
  to Multimedia</title>
    <summary>  Whilst affective responses to various forms and genres of multimedia content
have been well researched, precious few studies have investigated the combined
impact that multimedia system parameters and human factors have on affect.
Consequently, in this paper we explore the role that two primordial dimensions
of human factors - personality and culture - in conjunction with system factors
- frame rate, resolution, and bit rate - have on user affect and enjoyment of
multimedia presentations. To this end, a two-site, cross-cultural study was
undertaken, the results of which produced three predictve models. Personality
and Culture traits were shown statistically to represent 5.6% of the variance
in positive affect, 13.6% in negative affect and 9.3% in enjoyment. The
correlation between affect and enjoyment, was significant. Predictive modeling
incorporating human factors showed about 8%, 7% and 9% improvement in
predicting positive affect, negative affect and enjoyment respectively when
compared to models trained only on system factors. Results and analysis
indicate the significant role played by human factors in influencing affect
that users experience while watching multimedia.
</summary>
    <author>
      <name>Sharath Chandra Guntuku</name>
    </author>
    <author>
      <name>Michael James Scott</name>
    </author>
    <author>
      <name>Gheorghita Ghinea</name>
    </author>
    <author>
      <name>Weisi Lin</name>
    </author>
    <link href="http://arxiv.org/abs/1606.06873v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.06873v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07583v1</id>
    <updated>2016-06-24T07:18:42Z</updated>
    <published>2016-06-24T07:18:42Z</published>
    <title>N-queens-based algorithm for moving object detection in distributed
  wireless sensor networks</title>
    <summary>  The main constraint of wireless sensor networks (WSN) in enabling wireless
image communication is the high energy requirement, which may exceed even the
future capabilities of battery technologies. In this paper we have shown that
this bottleneck can be overcome by developing local in-network image processing
algorithm that offers optimal energy consumption. Our algorithm is very
suitable for intruder detection applications. Each node is responsible for
processing the image captured by the video sensor, which consists of NxN
blocks. If an intruder is detected in the monitoring region, the node will
transmit the image for further processing. Otherwise, the node takes no action.
Results provided from our experiments show that our algorithm is better than
the traditional moving object detection techniques by a factor of (N/2) in
terms of energy savings.
</summary>
    <author>
      <name>Biljana Stojkoska</name>
    </author>
    <author>
      <name>Danco Davcev</name>
    </author>
    <author>
      <name>Vladimir Trajkovik</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ITI.2008.4588530</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ITI.2008.4588530" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the ITI 2008 30th Int. Conf. on Information
  Technology Interfaces, June 23-26, 2008, Cavtat, Croatia, pp.899-904</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1606.07583v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07583v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.07908v2</id>
    <updated>2016-07-26T11:42:20Z</updated>
    <published>2016-06-25T12:57:44Z</published>
    <title>Label Tree Embeddings for Acoustic Scene Classification</title>
    <summary>  We present in this paper an efficient approach for acoustic scene
classification by exploring the structure of class labels. Given a set of class
labels, a category taxonomy is automatically learned by collectively optimizing
a clustering of the labels into multiple meta-classes in a tree structure. An
acoustic scene instance is then embedded into a low-dimensional feature
representation which consists of the likelihoods that it belongs to the
meta-classes. We demonstrate state-of-the-art results on two different datasets
for the acoustic scene classification task, including the DCASE 2013 and LITIS
Rouen datasets.
</summary>
    <author>
      <name>Huy Phan</name>
    </author>
    <author>
      <name>Lars Hertel</name>
    </author>
    <author>
      <name>Marco Maass</name>
    </author>
    <author>
      <name>Philipp Koch</name>
    </author>
    <author>
      <name>Alfred Mertins</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2964284.2967268</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2964284.2967268" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">to appear in the Proceedings of ACM Multimedia 2016 (ACMMM 2016)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.07908v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.07908v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; I.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08955v1</id>
    <updated>2016-06-29T05:04:27Z</updated>
    <published>2016-06-29T05:04:27Z</published>
    <title>Leveraging Contextual Cues for Generating Basketball Highlights</title>
    <summary>  The massive growth of sports videos has resulted in a need for automatic
generation of sports highlights that are comparable in quality to the
hand-edited highlights produced by broadcasters such as ESPN. Unlike previous
works that mostly use audio-visual cues derived from the video, we propose an
approach that additionally leverages contextual cues derived from the
environment that the game is being played in. The contextual cues provide
information about the excitement levels in the game, which can be ranked and
selected to automatically produce high-quality basketball highlights. We
introduce a new dataset of 25 NCAA games along with their play-by-play stats
and the ground-truth excitement data for each basket. We explore the
informativeness of five different cues derived from the video and from the
environment through user studies. Our experiments show that for our study
participants, the highlights produced by our system are comparable to the ones
produced by ESPN for the same games.
</summary>
    <author>
      <name>Vinay Bettadapura</name>
    </author>
    <author>
      <name>Caroline Pantofaru</name>
    </author>
    <author>
      <name>Irfan Essa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of ACM Multimedia 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.08955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.1; I.2.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.08999v1</id>
    <updated>2016-06-29T08:33:28Z</updated>
    <published>2016-06-29T08:33:28Z</published>
    <title>De-Hashing: Server-Side Context-Aware Feature Reconstruction for Mobile
  Visual Search</title>
    <summary>  Due to the prevalence of mobile devices, mobile search becomes a more
convenient way than desktop search. Different from the traditional desktop
search, mobile visual search needs more consideration for the limited resources
on mobile devices (e.g., bandwidth, computing power, and memory consumption).
The state-of-the-art approaches show that bag-of-words (BoW) model is robust
for image and video retrieval; however, the large vocabulary tree might not be
able to be loaded on the mobile device. We observe that recent works mainly
focus on designing compact feature representations on mobile devices for
bandwidth-limited network (e.g., 3G) and directly adopt feature matching on
remote servers (cloud). However, the compact (binary) representation might fail
to retrieve target objects (images, videos). Based on the hashed binary codes,
we propose a de-hashing process that reconstructs BoW by leveraging the
computing power of remote servers. To mitigate the information loss from binary
codes, we further utilize contextual information (e.g., GPS) to reconstruct a
context-aware BoW for better retrieval results. Experiment results show that
the proposed method can achieve competitive retrieval accuracy as BoW while
only transmitting few bits from mobile devices.
</summary>
    <author>
      <name>Yin-Hsi Kuo</name>
    </author>
    <author>
      <name>Winston H. Hsu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in IEEE Transactions on Circuits and Systems
  for Video Technology (TCSVT)</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.08999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.08999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.09047v1</id>
    <updated>2016-06-29T11:18:41Z</updated>
    <published>2016-06-29T11:18:41Z</published>
    <title>Minimum-latency Time-frequency Analysis Using Asymmetric Window
  Functions</title>
    <summary>  We study the real-time dynamics retrieval from a time series via the
time-frequency (TF) analysis with the minimal latency guarantee. While
different from the well-known intrinsic latency definition in the filter
design, a rigorous definition of intrinsic latency for different time-frequency
representations (TFR) is provided, including the short time Fourier transform
(STFT), synchrosqeezing transform (SST) and reassignment method (RM). To
achieve the minimal latency, a systematic method is proposed to construct an
asymmetric window from a well-designed symmetric one based on the concept of
minimum-phase, if the window satisfies some weak conditions. We theoretically
show that the TFR determined by SST with the constructed asymmetric window does
have a smaller intrinsic latency. Finally, the music onset detection problem is
studied to show the strength of the proposed algorithm.
</summary>
    <author>
      <name>Li Su</name>
    </author>
    <author>
      <name>Hau-tieng Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.09047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.09047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.00719v1</id>
    <updated>2016-07-04T01:56:20Z</updated>
    <published>2016-07-04T01:56:20Z</published>
    <title>Coarse2Fine: Two-Layer Fusion For Image Retrieval</title>
    <summary>  This paper addresses the problem of large-scale image retrieval. We propose a
two-layer fusion method which takes advantage of global and local cues and
ranks database images from coarse to fine (C2F). Departing from the previous
methods fusing multiple image descriptors simultaneously, C2F is featured by a
layered procedure composed by filtering and refining. In particular, C2F
consists of three components. 1) Distractor filtering. With holistic
representations, noise images are filtered out from the database, so the number
of candidate images to be used for comparison with the query can be greatly
reduced. 2) Adaptive weighting. For a certain query, the similarity of
candidate images can be estimated by holistic similarity scores in
complementary to the local ones. 3) Candidate refining. Accurate retrieval is
conducted via local features, combining the pre-computed adaptive weights.
Experiments are presented on two benchmarks, \emph{i.e.,} Holidays and Ukbench
datasets. We show that our method outperforms recent fusion methods in terms of
storage consumption and computation complexity, and that the accuracy is
competitive to the state-of-the-arts.
</summary>
    <author>
      <name>Gaipeng Kong</name>
    </author>
    <author>
      <name>Le Dong</name>
    </author>
    <author>
      <name>Wenpu Dong</name>
    </author>
    <author>
      <name>Liang Zheng</name>
    </author>
    <author>
      <name>Qi Tian</name>
    </author>
    <link href="http://arxiv.org/abs/1607.00719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.00719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.01172v1</id>
    <updated>2016-07-05T09:51:25Z</updated>
    <published>2016-07-05T09:51:25Z</published>
    <title>A Measurement Study of TCP Performance for Chunk Delivery in DASH</title>
    <summary>  Dynamic Adaptive Streaming over HTTP (DASH) has emerged as an increasingly
popular paradigm for video streaming [13], in which a video is segmented into
many chunks delivered to users by HTTP request/response over Transmission
Control Protocol (TCP) con- nections. Therefore, it is intriguing to study the
performance of strategies implemented in conventional TCPs, which are not
dedicated for video streaming, e.g., whether chunks are efficiently delivered
when users per- form interactions with the video players. In this paper, we
conduct mea- surement studies on users chunk requesting traces in DASH from a
rep- resentative video streaming provider, to investigate users behaviors in
DASH, and TCP-connection-level traces from CDN servers, to investi- gate the
performance of TCP for DASH. By studying how video chunks are delivered in both
the slow start and congestion avoidance phases, our observations have revealed
the performance characteristics of TCP for DASH as follows: (1) Request
patterns in DASH have a great impact on the performance of TCP variations
including cubic; (2) Strategies in conventional TCPs may cause user perceived
quality degradation in DASH streaming; (3) Potential improvement to TCP
strategies for better delivery in DASH can be further explored.
</summary>
    <author>
      <name>Wen Hu</name>
    </author>
    <author>
      <name>Zhi Wang</name>
    </author>
    <author>
      <name>Lifeng Sun</name>
    </author>
    <link href="http://arxiv.org/abs/1607.01172v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.01172v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.03257v1</id>
    <updated>2016-07-12T08:30:45Z</updated>
    <published>2016-07-12T08:30:45Z</published>
    <title>City-Identification of Flickr Videos Using Semantic Acoustic Features</title>
    <summary>  City-identification of videos aims to determine the likelihood of a video
belonging to a set of cities. In this paper, we present an approach using only
audio, thus we do not use any additional modality such as images, user-tags or
geo-tags. In this manner, we show to what extent the city-location of videos
correlates to their acoustic information. Success in this task suggests
improvements can be made to complement the other modalities. In particular, we
present a method to compute and use semantic acoustic features to perform
city-identification and the features show semantic evidence of the
identification. The semantic evidence is given by a taxonomy of urban sounds
and expresses the potential presence of these sounds in the city- soundtracks.
We used the MediaEval Placing Task set, which contains Flickr videos labeled by
city. In addition, we used the UrbanSound8K set containing audio clips labeled
by sound- type. Our method improved the state-of-the-art performance and
provides a novel semantic approach to this task
</summary>
    <author>
      <name>Benjamin Elizalde</name>
    </author>
    <author>
      <name>Guan-Lin Chao</name>
    </author>
    <author>
      <name>Ming Zeng</name>
    </author>
    <author>
      <name>Ian Lane</name>
    </author>
    <link href="http://arxiv.org/abs/1607.03257v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.03257v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05808v1</id>
    <updated>2016-07-20T03:23:49Z</updated>
    <published>2016-07-20T03:23:49Z</published>
    <title>Hybrid Video Signal Coding Technologies: Past, Current and Future</title>
    <summary>  The growing needs for high-quality video applications have resulted in a lot
of studies and developments in video signal coding. This chapter presents some
advanced techniques in enhancing the rate-distortion performance of the
block-based hybrid video coding systems. Additionally, as can be seen from the
developments of H.264/AVC and HEVC, most of the current coding tools, such as
prediction, transformation and entropy coding, have less room to improve in the
compression performance. On the other hand, loop filer in the modern video
standards shows the promising results. Thus, we believe that loop filter can be
the candidate in contributing to higher video compression for the
next-generation video coding. Specifically, improvements on ALF and SAO are
also introduced, and the simulation results show that the proposed methods
outperform the existing method, which offer new degrees of freedom to improve
the overall rate-distortion performance. As a result, they can be the candidate
coding tools for the next-generation video codec.
</summary>
    <author>
      <name>Miaohui Wang</name>
    </author>
    <author>
      <name>Ngan King Ngi</name>
    </author>
    <link href="http://arxiv.org/abs/1607.05808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.06803v3</id>
    <updated>2017-02-16T16:55:20Z</updated>
    <published>2016-07-22T19:44:01Z</published>
    <title>Restoring highly corrupted images by impulse noise using radial basis
  functions interpolation</title>
    <summary>  Preserving details in restoring images highly corrupted by impulse noise
remains a challenging problem. We proposed an algorithm based on radial basis
functions (RBF) interpolation which estimates the intensities of corrupted
pixels by their neighbors. In this algorithm, first intensity values of noisy
pixels in the corrupted image are estimated using RBFs. Next, the image is
smoothed. The proposed algorithm can effectively remove the highly dense
impulse noise. Experimental results show the superiority of the proposed
algorithm in comparison to the recent similar methods both in noise suppression
and detail preservation. Extensive simulations show better results in measure
of peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM),
especially when the image is corrupted by very highly dense impulse noise.
</summary>
    <author>
      <name>Fariborz Taherkhani</name>
    </author>
    <author>
      <name>Mansour Jamzad</name>
    </author>
    <link href="http://arxiv.org/abs/1607.06803v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.06803v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07697v1</id>
    <updated>2016-07-15T06:13:21Z</updated>
    <published>2016-07-15T06:13:21Z</published>
    <title>Low-complexity feedback-channel-free distributed video coding using
  Local Rank Transform</title>
    <summary>  In this paper, we propose a new feedback-channel-free Distributed Video
Coding (DVC) algorithm using Local Rank Transform (LRT). The encoder computes
LRT by considering selected neighborhood pixels of Wyner-Ziv frame. The ranks
from the modified LRT are merged, and their positions are entropy coded and
sent to the decoder. In addition, means of each block of Wyner-Ziv frame are
also transmitted to assist motion estimation. Using these measurements, the
decoder generates side information (SI) by implementing motion estimation and
compensation in LRT domain. An iterative algorithm is executed on SI using LRT
to reconstruct the Wyner-Ziv frame. Experimental results show that the coding
efficiency of our codec is close to the efficiency of pixel domain distributed
video coders based on Low-Density Parity Check and Accumulate (LDPCA) or turbo
codes, with less encoder complexity.
</summary>
    <author>
      <name>P Raj Bhagath</name>
    </author>
    <author>
      <name>Kallol Mallick</name>
    </author>
    <author>
      <name>Jayanta Mukherjee</name>
    </author>
    <author>
      <name>Sudipta Mukopadhayay</name>
    </author>
    <link href="http://arxiv.org/abs/1607.07697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07824v1</id>
    <updated>2016-07-26T18:02:44Z</updated>
    <published>2016-07-26T18:02:44Z</published>
    <title>Natural Steganography: cover-source switching for better steganography</title>
    <summary>  This paper proposes a new steganographic scheme relying on the principle of
cover-source switching, the key idea being that the embedding should switch
from one cover-source to another. The proposed implementation, called Natural
Steganography, considers the sensor noise naturally present in the raw images
and uses the principle that, by the addition of a specific noise the
steganographic embedding tries to mimic a change of ISO sensitivity. The
embedding methodology consists in 1) perturbing the image in the raw domain, 2)
modeling the perturbation in the processed domain, 3) embedding the payload in
the processed domain. We show that this methodology is easily tractable
whenever the processes are known and enables to embed large and undetectable
payloads. We also show that already used heuristics such as synchronization of
embedding changes or detectability after rescaling can be respectively
explained by operations such as color demosaicing and down-scaling kernels.
</summary>
    <author>
      <name>Patrick Bas</name>
    </author>
    <link href="http://arxiv.org/abs/1607.07824v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07824v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.00905v2</id>
    <updated>2016-09-15T11:16:36Z</updated>
    <published>2016-08-02T17:09:19Z</published>
    <title>PicHunt: Social Media Image Retrieval for Improved Law Enforcement</title>
    <summary>  First responders are increasingly using social media to identify and reduce
crime for well-being and safety of the society. Images shared on social media
hurting religious, political, communal and other sentiments of people, often
instigate violence and create law &amp; order situations in society. This results
in the need for first responders to inspect the spread of such images and users
propagating them on social media. In this paper, we present a comparison
between different hand-crafted features and a Convolutional Neural Network
(CNN) model to retrieve similar images, which outperforms state-of-art
hand-crafted features. We propose an Open-Source-Intelligent (OSINT) real-time
image search system, robust to retrieve modified images that allows first
responders to analyze the current spread of images, sentiments floating and
details of users propagating such content. The system also aids officials to
save time of manually analyzing the content by reducing the search space on an
average by 67%.
</summary>
    <author>
      <name>Sonal Goel</name>
    </author>
    <author>
      <name>Niharika Sachdeva</name>
    </author>
    <author>
      <name>Ponnurangam Kumaraguru</name>
    </author>
    <author>
      <name>A V Subramanyam</name>
    </author>
    <author>
      <name>Divam Gupta</name>
    </author>
    <link href="http://arxiv.org/abs/1608.00905v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.00905v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01947v1</id>
    <updated>2016-08-05T17:36:51Z</updated>
    <published>2016-08-05T17:36:51Z</published>
    <title>Daala: Building A Next-Generation Video Codec From Unconventional
  Technology</title>
    <summary>  Daala is a new royalty-free video codec that attempts to compete with
state-of-the-art royalty-bearing codecs. To do so, it must achieve good
compression while avoiding all of their patented techniques. We use technology
that is as different as possible from traditional approaches to achieve this.
This paper describes the technology behind Daala and discusses where it fits in
the newly created AV1 codec from the Alliance for Open Media. We show that
Daala is approaching the performance level of more mature, state-of-the art
video codecs and can contribute to improving AV1.
</summary>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <author>
      <name>Timothy B. Terriberry</name>
    </author>
    <author>
      <name>Nathan E. Egge</name>
    </author>
    <author>
      <name>Thomas Daede</name>
    </author>
    <author>
      <name>Yushin Cho</name>
    </author>
    <author>
      <name>Christopher Montgomery</name>
    </author>
    <author>
      <name>Michael Bebenita</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, accepted for multimedia signal processing (MMSP) workshop,
  2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.02291v1</id>
    <updated>2016-08-08T01:10:11Z</updated>
    <published>2016-08-08T01:10:11Z</published>
    <title>Semi-Fragile Image Authentication based on CFD and 3-Bit Quantization</title>
    <summary>  There is a great adventure of watermarking usage in the context of
conventional authentication since it does not require additional storage space
for supplementary metadata. However JPEG compression, being a conventional
method to compress images, leads to exact authentication breaking. We discuss a
semi-fragile watermarking system for digital images tolerant to JPEG/JPEG2000
compression. Recently we have published a selective authentication method based
on Zernike moments. But unfortunately it has large computational complexity and
not sufficiently good detection of small image modifications. In the current
paper it is proposed (in contrast to Zernike moments approach) the usage of
image finite differences and 3-bit quantization as the main technique. In order
to embed a watermark (WM) into the image, some areas of the Haar wavelet
transform coefficients are used. Simulation results show a good resistance of
this method to JPEG compression with $\mbox{\rm CR}\leq 30\%$ (Compression
Ratio), high probability of small image modification recognition, image quality
assessments $\mbox{\rm PSNR}\geq 40$ (Peak signal-to-noise ratio) dB and
$\mbox{\rm SSIM}\geq 0.98$ (Structural Similarity Index Measure) after
embedding and lower computation complexity of WM embedding and extraction. All
these properties qualify this approach as effective.
</summary>
    <author>
      <name>Aleksey Zhuvikin</name>
    </author>
    <author>
      <name>Valery Korzhik</name>
    </author>
    <author>
      <name>Guillermo Morales-Luna</name>
    </author>
    <link href="http://arxiv.org/abs/1608.02291v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.02291v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.03016v2</id>
    <updated>2017-04-15T05:26:23Z</updated>
    <published>2016-08-10T01:11:32Z</published>
    <title>Mining Fashion Outfit Composition Using An End-to-End Deep Learning
  Approach on Set Data</title>
    <summary>  Composing fashion outfits involves deep understanding of fashion standards
while incorporating creativity for choosing multiple fashion items (e.g.,
Jewelry, Bag, Pants, Dress). In fashion websites, popular or high-quality
fashion outfits are usually designed by fashion experts and followed by large
audiences. In this paper, we propose a machine learning system to compose
fashion outfits automatically. The core of the proposed automatic composition
system is to score fashion outfit candidates based on the appearances and
meta-data. We propose to leverage outfit popularity on fashion oriented
websites to supervise the scoring component. The scoring component is a
multi-modal multi-instance deep learning system that evaluates instance
aesthetics and set compatibility simultaneously. In order to train and evaluate
the proposed composition system, we have collected a large scale fashion outfit
dataset with 195K outfits and 368K fashion items from Polyvore. Although the
fashion outfit scoring and composition is rather challenging, we have achieved
an AUC of 85% for the scoring component, and an accuracy of 77% for a
constrained composition task.
</summary>
    <author>
      <name>Yuncheng Li</name>
    </author>
    <author>
      <name>LiangLiang Cao</name>
    </author>
    <author>
      <name>Jiang Zhu</name>
    </author>
    <author>
      <name>Jiebo Luo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2017.2690144</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2017.2690144" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE TMM</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.03016v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.03016v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05850v1</id>
    <updated>2016-08-20T17:26:15Z</updated>
    <published>2016-08-20T17:26:15Z</published>
    <title>Steganalyzer performances in operational contexts</title>
    <summary>  Steganography and steganalysis are two important branches of the information
hiding field of research. Steganography methods consist in hiding information
in such a way that the secret message is undetectable for the uninitiated.
Steganalyzis encompasses all the techniques that attempt to detect the presence
of such hidden information. This latter is usually designed by making
classifiers able to separate innocent images from steganographied ones
according to their differences on well-selected features. We wonder, in this
article whether it is possible to construct a kind of universal steganalyzer
without any knowledge regarding the steganographier side. The effects on the
classification score of a modification of either parameters or methods between
the learning and testing stages are then evaluated, while the possibility to
improve the separation score by merging many methods during learning stage is
deeper investigated.
</summary>
    <author>
      <name>Yousra A. Fadil</name>
    </author>
    <author>
      <name>Jean-Fran√ßois Couchot</name>
    </author>
    <author>
      <name>Rapha√´l Couturier</name>
    </author>
    <author>
      <name>Christophe Guyeux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of IIH-MSP 2015, The Eleventh International Conference on
  Intelligent Information Hiding and Multimedia Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.05850v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05850v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06690v2</id>
    <updated>2016-10-29T11:23:20Z</updated>
    <published>2016-08-24T02:15:06Z</published>
    <title>A Convolutional Neural Network Approach for Post-Processing in HEVC
  Intra Coding</title>
    <summary>  Lossy image and video compression algorithms yield visually annoying
artifacts including blocking, blurring, and ringing, especially at low
bit-rates. To reduce these artifacts, post-processing techniques have been
extensively studied. Recently, inspired by the great success of convolutional
neural network (CNN) in computer vision, some researches were performed on
adopting CNN in post-processing, mostly for JPEG compressed images. In this
paper, we present a CNN-based post-processing algorithm for High Efficiency
Video Coding (HEVC), the state-of-the-art video coding standard. We redesign a
Variable-filter-size Residue-learning CNN (VRCNN) to improve the performance
and to accelerate network training. Experimental results show that using our
VRCNN as post-processing leads to on average 4.6% bit-rate reduction compared
to HEVC baseline. The VRCNN outperforms previously studied networks in
achieving higher bit-rate reduction, lower memory cost, and multiplied
computational speedup.
</summary>
    <author>
      <name>Yuanying Dai</name>
    </author>
    <author>
      <name>Dong Liu</name>
    </author>
    <author>
      <name>Feng Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-51811-4_3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-51811-4_3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MMM 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.06690v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06690v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.06770v2</id>
    <updated>2017-01-16T11:19:49Z</updated>
    <published>2016-08-24T10:17:16Z</published>
    <title>Automatic Synchronization of Multi-User Photo Galleries</title>
    <summary>  In this paper we address the issue of photo galleries synchronization, where
pictures related to the same event are collected by different users. Existing
solutions to address the problem are usually based on unrealistic assumptions,
like time consistency across photo galleries, and often heavily rely on
heuristics, limiting therefore the applicability to real-world scenarios. We
propose a solution that achieves better generalization performance for the
synchronization task compared to the available literature. The method is
characterized by three stages: at first, deep convolutional neural network
features are used to assess the visual similarity among the photos; then, pairs
of similar photos are detected across different galleries and used to construct
a graph; eventually, a probabilistic graphical model is used to estimate the
temporal offset of each pair of galleries, by traversing the minimum spanning
tree extracted from this graph. The experimental evaluation is conducted on
four publicly available datasets covering different types of events,
demonstrating the strength of our proposed method. A thorough discussion of the
obtained results is provided for a critical assessment of the quality in
synchronization.
</summary>
    <author>
      <name>E. Sansone</name>
    </author>
    <author>
      <name>K. Apostolidis</name>
    </author>
    <author>
      <name>N. Conci</name>
    </author>
    <author>
      <name>G. Boato</name>
    </author>
    <author>
      <name>V. Mezaris</name>
    </author>
    <author>
      <name>F. G. B. De Natale</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACCEPTED to IEEE Transactions on Multimedia</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.06770v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.06770v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08469v1</id>
    <updated>2016-08-29T05:02:32Z</updated>
    <published>2016-08-29T05:02:32Z</published>
    <title>On the Efficiency and Fairness of Multiplayer HTTP-based Adaptive Video
  Streaming</title>
    <summary>  User-perceived quality-of-experience (QoE) is critical in internet video
delivery systems. Extensive prior work has studied the design of client-side
bitrate adaptation algorithms to maximize single-player QoE. However,
multiplayer QoE fairness becomes critical as the growth of video traffic makes
it more likely that multiple players share a bottleneck in the network. Despite
several recent proposals, there is still a series of open questions. In this
paper, we bring the problem space to light from a control theory perspective by
formalizing the multiplayer QoE fairness problem and addressing two key
questions in the broader problem space. First, we derive the sufficient
conditions of convergence to steady state QoE fairness under TCP-based
bandwidth sharing scheme. Based on the insight from this analysis that
in-network active bandwidth allocation is needed, we propose a non-linear
MPC-based, router-assisted bandwidth allocation algorithm that regards each
player as closed-loop systems. We use trace-driven simulation to show the
improvement over existing approaches. We identify several research directions
enabled by the control theoretic modeling and envision that control theory can
play an important role on guiding real system design in adaptive video
streaming.
</summary>
    <author>
      <name>Xiaoqi Yin</name>
    </author>
    <author>
      <name>Mihovil Bartuloviƒá</name>
    </author>
    <author>
      <name>Vyas Sekar</name>
    </author>
    <author>
      <name>Bruno Sinopoli</name>
    </author>
    <link href="http://arxiv.org/abs/1608.08469v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08469v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.00045v1</id>
    <updated>2016-08-31T21:14:57Z</updated>
    <published>2016-08-31T21:14:57Z</published>
    <title>Towards Hybrid Cloud-assisted Crowdsourced Live Streaming: Measurement
  and Analysis</title>
    <summary>  Crowdsourced Live Streaming (CLS), most notably Twitch.tv, has seen explosive
growth in its popularity in the past few years. In such systems, any user can
lively broadcast video content of interest to others, e.g., from a game player
to many online viewers. To fulfill the demands from both massive and
heterogeneous broadcasters and viewers, expensive server clusters have been
deployed to provide video ingesting and transcoding services. Despite the
existence of highly popular channels, a significant portion of the channels is
indeed unpopular. Yet as our measurement shows, these broadcasters are
consuming considerable system resources; in particular, 25% (resp. 30%) of
bandwidth (resp. computation) resources are used by the broadcasters who do not
have any viewers at all. In this paper, we closely examine the challenge of
handling unpopular live-broadcasting channels in CLS systems and present a
comprehensive solution for service partitioning on hybrid cloud. The
trace-driven evaluation shows that our hybrid cloud-assisted design can smartly
assign ingesting and transcoding tasks to the elastic cloud virtual machines,
providing flexible system deployment cost-effectively.
</summary>
    <author>
      <name>Cong Zhang</name>
    </author>
    <author>
      <name>Jiangchuan Liu</name>
    </author>
    <author>
      <name>Haiyang Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2910642.2910644</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2910642.2910644" rel="related"/>
    <link href="http://arxiv.org/abs/1609.00045v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.00045v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.05345v1</id>
    <updated>2016-09-17T14:50:06Z</updated>
    <published>2016-09-17T14:50:06Z</published>
    <title>Generalized residual vector quantization for large scale data</title>
    <summary>  Vector quantization is an essential tool for tasks involving large scale
data, for example, large scale similarity search, which is crucial for
content-based information retrieval and analysis. In this paper, we propose a
novel vector quantization framework that iteratively minimizes quantization
error. First, we provide a detailed review on a relevant vector quantization
method named \textit{residual vector quantization} (RVQ). Next, we propose
\textit{generalized residual vector quantization} (GRVQ) to further improve
over RVQ. Many vector quantization methods can be viewed as the special cases
of our proposed framework. We evaluate GRVQ on several large scale benchmark
datasets for large scale search, classification and object retrieval. We
compared GRVQ with existing methods in detail. Extensive experiments
demonstrate our GRVQ framework substantially outperforms existing methods in
term of quantization accuracy and computation efficiency.
</summary>
    <author>
      <name>Shicong Liu</name>
    </author>
    <author>
      <name>Junru Shao</name>
    </author>
    <author>
      <name>Hongtao Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">published on International Conference on Multimedia and Expo 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.05345v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.05345v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.06109v3</id>
    <updated>2017-05-03T19:52:34Z</updated>
    <published>2016-09-20T11:27:16Z</published>
    <title>FPGA implementation of the procedures for video quality assessment</title>
    <summary>  Video resolutions used in variety of media are constantly rising. While
manufacturers struggle to perfect their screens it is also important to ensure
high quality of displayed image. Overall quality can be measured using Mean
Opinion Score (MOS). Video quality can be affected by miscellaneous artifacts,
appearing at every stage of video creation and transmission. In this paper, we
present a solution to calculate four distinct video quality metrics that can be
applied to a real time video quality assessment system. Our assessment module
is capable of processing 8K resolution in real time set at the level of 30
frames per second. Throughput of 2.19 GB/s surpasses performance of pure
software solutions. To concentrate on architectural optimization, the module
was created using high level language.
</summary>
    <author>
      <name>Maciej Wielgosz</name>
    </author>
    <author>
      <name>Micha≈Ç Karwatowski</name>
    </author>
    <author>
      <name>Marcin Pietro≈Ñ</name>
    </author>
    <author>
      <name>Kazimierz Wiatr</name>
    </author>
    <link href="http://arxiv.org/abs/1609.06109v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.06109v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.06442v1</id>
    <updated>2016-09-21T07:30:18Z</updated>
    <published>2016-09-21T07:30:18Z</published>
    <title>Minimizing Compression Artifacts for High Resolutions with Adaptive
  Quantization Matrices for HEVC</title>
    <summary>  Visual Display Units (VDUs), capable of displaying video data at High
Definition (HD) and Ultra HD (UHD) resolutions, are frequently employed in a
variety of technological domains. Quantization-induced video compression
artifacts, which are usually unnoticeable in low resolution environments, are
typically conspicuous on high resolution VDUs and video data. The default
quantization matrices (QMs) in HEVC do not take into account specific display
resolutions of VDUs or video data to determine the appropriate levels of
quantization required to reduce unwanted compression artifacts. Therefore, we
propose a novel, adaptive quantization matrix technique for the HEVC standard
including Scalable HEVC (SHVC). Our technique, which is based on a refinement
of the current QM technique in HEVC, takes into consideration specific display
resolutions of the target VDUs in order to minimize compression artifacts. We
undertake a thorough evaluation of the proposed technique by utilizing SHVC SHM
9.0 (two-layered bit-stream) and the BD-Rate and SSIM metrics. For the BD-Rate
evaluation, the proposed method achieves maximum BD-Rate reductions of 56.5% in
the enhancement layer. For the SSIM evaluation, our technique achieves a
maximum structural improvement of 0.8660 vs. 0.8538.
</summary>
    <author>
      <name>Lee Prangnell</name>
    </author>
    <author>
      <name>Victor Sanchez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD Working Paper, University of Warwick, UK. arXiv admin note:
  substantial text overlap with arXiv:1606.02042</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.06442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.06442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07170v1</id>
    <updated>2016-09-22T21:26:21Z</updated>
    <published>2016-09-22T21:26:21Z</published>
    <title>Deep Quality: A Deep No-reference Quality Assessment System</title>
    <summary>  Image quality assessment (IQA) continues to garner great interest in the
research community, particularly given the tremendous rise in consumer video
capture and streaming. Despite significant research effort in IQA in the past
few decades, the area of no-reference image quality assessment remains a great
challenge and is largely unsolved. In this paper, we propose a novel
no-reference image quality assessment system called Deep Quality, which
leverages the power of deep learning to model the complex relationship between
visual content and the perceived quality. Deep Quality consists of a novel
multi-scale deep convolutional neural network, trained to learn to assess image
quality based on training samples consisting of different distortions and
degradations such as blur, Gaussian noise, and compression artifacts.
Preliminary results using the CSIQ benchmark image quality dataset showed that
Deep Quality was able to achieve strong quality prediction performance (89%
patch-level and 98% image-level prediction accuracy), being able to achieve
similar performance as full-reference IQA methods.
</summary>
    <author>
      <name>Prajna Paramita Dash</name>
    </author>
    <author>
      <name>Akshaya Mishra</name>
    </author>
    <author>
      <name>Alexander Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.07170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02263v1</id>
    <updated>2016-10-07T13:01:30Z</updated>
    <published>2016-10-07T13:01:30Z</published>
    <title>Backward-Shifted Coding (BSC) based on Scalable Video Coding for HAS</title>
    <summary>  The main task of HTTP Adaptive Streaming is to adapt video quality
dynamically under variable network conditions. This is a key feature for
multimedia delivery especially when quality of service cannot be granted
network-wide and, e.g., throughput may suffer short term fluctuations.
  Hence, robust bitrate adaptation schemes become crucial in order to improve
video quality. The objective, in this context, is to control the filling level
of the playback buffer and maximize the quality of the video, while avoiding
unnecessary video quality variations.
  In this paper we study bitrate adaptation algorithms based on
Backward-Shifted Coding (BSC), a scalable video coding scheme able to greatly
improve video quality. We design bitrate adaptation algorithms that balance
video rate smoothness and high network capacity utilization, leveraging both on
throughput-based and buffer-based adaptation mechanisms.
  Extensive simulations using synthetic and real-world video traffic traces
show that the proposed scheme performs remarkably well even under challenging
network conditions.
</summary>
    <author>
      <name>Zakaria Ye</name>
    </author>
    <author>
      <name>Rachid El-Azouzi</name>
    </author>
    <author>
      <name>Tania Jimenez</name>
    </author>
    <author>
      <name>Francesco De Pellegrini</name>
    </author>
    <link href="http://arxiv.org/abs/1610.02263v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02263v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02488v1</id>
    <updated>2016-10-08T05:34:56Z</updated>
    <published>2016-10-08T05:34:56Z</published>
    <title>Perceptually-Driven Video Coding with the Daala Video Codec</title>
    <summary>  The Daala project is a royalty-free video codec that attempts to compete with
the best patent-encumbered codecs. Part of our strategy is to replace core
tools of traditional video codecs with alternative approaches, many of them
designed to take perceptual aspects into account, rather than optimizing for
simple metrics like PSNR. This paper documents some of our experiences with
these tools, which ones worked and which did not. We evaluate which tools are
easy to integrate into a more traditional codec design, and show results in the
context of the codec being developed by the Alliance for Open Media.
</summary>
    <author>
      <name>Yushin Cho</name>
    </author>
    <author>
      <name>Thomas J. Daede</name>
    </author>
    <author>
      <name>Nathan E. Egge</name>
    </author>
    <author>
      <name>Guillaume Martres</name>
    </author>
    <author>
      <name>Tristan Matthews</name>
    </author>
    <author>
      <name>Christopher Montgomery</name>
    </author>
    <author>
      <name>Timothy B. Terriberry</name>
    </author>
    <author>
      <name>Jean-Marc Valin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.2238417</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.2238417" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, Proceedings of SPIE Workshop on Applications of Digital
  Image Processing (ADIP), 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.02488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.04346v1</id>
    <updated>2016-10-14T07:27:46Z</updated>
    <published>2016-10-14T07:27:46Z</published>
    <title>Steganography between Silence Intervals of Audio in Video Content Using
  Chaotic Maps</title>
    <summary>  Steganography is the art of hiding data, in such a way that it is
undetectable under traffic-pattern analysis and the data hidden is only known
to the receiver and the sender. In this paper new method of text steganography
over the silence interval of audio in a video file, is presented. In the
proposed method first the audio signal is extracted from the video. After doing
audio enhancement, the data on the audio signal is steganographed using new
technique and then audio signal is rewritten in video file again.
http://www.learnrnd.com/All_latest_research_findings.php
  To enhance the security level we apply chaotic maps on arbitrary text.
Furthermore, the algorithm in this paper, gives a technique which states that
undetectable stegotext and cover-text has same probability distribution and no
statistical test can detect the presence of the hidden message.
http://www.learnrnd.com/detail.php?id=Biohack_Eyes_through_Chlorin_e6_eye_drop_:Stanford_University_Research
  Moreover, hidden message does not affect the transmission rate of video file
at all.
</summary>
    <author>
      <name>Muhammad Fahad Khan</name>
    </author>
    <author>
      <name>Faisal Baig</name>
    </author>
    <author>
      <name>Saira Beg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00034-014-9830-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00034-014-9830-5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Khan, Muhammad Fahad, Faisal Baig, and Saira Beg. "Steganography
  between silence intervals of audio in video content using chaotic maps."
  Circuits, Systems, and Signal Processing 33.12 (2014): 3901-3919</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.04346v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.04346v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94B35" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.06347v1</id>
    <updated>2016-10-20T10:27:10Z</updated>
    <published>2016-10-20T10:27:10Z</published>
    <title>A Classification Engine for Image Ballistics of Social Data</title>
    <summary>  Image Forensics has already achieved great results for the source camera
identification task on images. Standard approaches for data coming from Social
Network Platforms cannot be applied due to different processes involved (e.g.,
scaling, compression, etc.). Over 1 billion images are shared each day on the
Internet and obtaining information about their history from the moment they
were acquired could be exploited for investigation purposes. In this paper, a
classification engine for the reconstruction of the history of an image, is
presented. Specifically, exploiting K-NN and decision trees classifiers and
a-priori knowledge acquired through image analysis, we propose an automatic
approach that can understand which Social Network Platform has processed an
image and the software application used to perform the image upload. The engine
makes use of proper alterations introduced by each platform as features.
Results, in terms of global accuracy on a dataset of 2720 images, confirm the
effectiveness of the proposed strategy.
</summary>
    <author>
      <name>Oliver Giudice</name>
    </author>
    <author>
      <name>Antonino Paratore</name>
    </author>
    <author>
      <name>Marco Moltisanti</name>
    </author>
    <author>
      <name>Sebastiano Battiato</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-68548-9_57</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-68548-9_57" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Image Analysis and Processing - ICIAP 2017: 19th International
  Conference, Catania, Italy, September 11-15, 2017, Proceedings, Part II,
  Springer International Publishing</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.06347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.06347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.07386v1</id>
    <updated>2016-10-24T12:30:52Z</updated>
    <published>2016-10-24T12:30:52Z</published>
    <title>An Efficient Adaptive Boundary Matching Algorithm for Video Error
  Concealment</title>
    <summary>  Sending compressed video data in error-prone environments (like the Internet
and wireless networks) might cause data degradation. Error concealment
techniques try to conceal the received data in the decoder side. In this paper,
an adaptive boundary matching algorithm is presented for recovering the damaged
motion vectors (MVs). This algorithm uses an outer boundary matching or
directional temporal boundary matching method to compare every boundary of
candidate macroblocks (MBs), adaptively. It gives a specific weight according
to the accuracy of each boundary of the damaged MB. Moreover, if each of the
adjacent MBs is already concealed, different weights are given to the
boundaries. Finally, the MV with minimum adaptive boundary distortion is
selected as the MV of the damaged MB. Experimental results show that the
proposed algorithm can improve both objective and subjective quality of
reconstructed frames without any considerable computational complexity. The
average PSNR in some frames of test sequences increases about 5.20, 5.78, 5.88,
4.37, 4.41, and 3.50 dB compared to average MV, classic boundary matching,
directional boundary matching, directional temporal boundary matching, outer
boundary matching, and dynamical temporal error concealment algorithm,
respectively.
</summary>
    <author>
      <name>Seyed Mojtaba Marvasti-Zadeh</name>
    </author>
    <author>
      <name>Hossein Ghanei-Yakhdan</name>
    </author>
    <author>
      <name>Shohreh Kasaei</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Iranian Journal of Electrical &amp; Electronic Engineering, Vol. 10, No.
  3, Sep. 2014</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJEEE. 2014; 10 (3) :188-202</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.07386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.07386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.07753v1</id>
    <updated>2016-10-25T07:11:41Z</updated>
    <published>2016-10-25T07:11:41Z</published>
    <title>A Novel Boundary Matching Algorithm for Video Temporal Error Concealment</title>
    <summary>  With the fast growth of communication networks, the video data transmission
from these networks is extremely vulnerable. Error concealment is a technique
to estimate the damaged data by employing the correctly received data at the
decoder. In this paper, an efficient boundary matching algorithm for estimating
damaged motion vectors (MVs) is proposed. The proposed algorithm performs error
concealment for each damaged macro block (MB) according to the list of
identified priority of each frame. It then uses a classic boundary matching
criterion or the proposed boundary matching criterion adaptively to identify
matching distortion in each boundary of candidate MB. Finally, the candidate MV
with minimum distortion is selected as an MV of damaged MB and the list of
priorities is updated. Experimental results show that the proposed algorithm
improves both objective and subjective qualities of reconstructed frames
without any significant increase in computational cost. The PSNR for test
sequences in some frames is increased about 4.7, 4.5, and 4.4 dB compared to
the classic boundary matching, directional boundary matching, and directional
temporal boundary matching algorithm, respectively.
</summary>
    <author>
      <name>Seyed Mojtaba Marvasti-Zadeh</name>
    </author>
    <author>
      <name>Hossein Ghanei-Yakhdan</name>
    </author>
    <author>
      <name>Shohreh Kasaei</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5815/ijigsp.2014.06.01</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5815/ijigsp.2014.06.01" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1610.07386</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Image, Graphics, and Signal Processing
  (IJIGSP), vol. 6, no. 6, pp. 1-10, May. 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1610.07753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.07753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00869v1</id>
    <updated>2016-11-03T03:19:15Z</updated>
    <published>2016-11-03T03:19:15Z</published>
    <title>QoE-based MAC Layer Optimization for Video Teleconferencing over WiFi</title>
    <summary>  In IEEE 802.11, the retry limit is set the same value for all packets. In
this paper, we dynamically classify video teleconferencing packets based on the
type of the video frame that a packet carries and the packet loss events that
have happened in the network, and assign them different retry limits. We
consider the IPPP video encoding structure with instantaneous decoder refresh
(IDR) frame insertion based on packet loss feedback. The loss of a single frame
causes error propagation for a period of time equal to the packet loss feedback
delay. To optimize the video quality, we propose a method to concentrate the
packet losses to small segments of the entire video sequence, and study the
performance by an analytic model. Our proposed method is implemented only on
the stations interested in enhanced video quality, and is compatible with
unmodified IEEE 802.11 stations and access points in terms of performance.
Simulation results show that the performance gain can be significant compared
to the IEEE 802.11 standard without negatively affecting cross traffic.
</summary>
    <author>
      <name>Tianyi Xu</name>
    </author>
    <author>
      <name>Liangping Ma</name>
    </author>
    <author>
      <name>Gregory Sternberg</name>
    </author>
    <link href="http://arxiv.org/abs/1611.00869v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00869v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.01715v3</id>
    <updated>2017-07-10T23:34:35Z</updated>
    <published>2016-11-06T01:34:47Z</published>
    <title>Recover Subjective Quality Scores from Noisy Measurements</title>
    <summary>  Simple quality metrics such as PSNR are known to not correlate well with
subjective quality when tested across a wide spectrum of video content or
quality regime. Recently, efforts have been made in designing objective quality
metrics trained on subjective data (e.g. VMAF), demonstrating better
correlation with video quality perceived by human. Clearly, the accuracy of
such a metric heavily depends on the quality of the subjective data that it is
trained on. In this paper, we propose a new approach to recover subjective
quality scores from noisy raw measurements, using maximum likelihood
estimation, by jointly estimating the subjective quality of impaired videos,
the bias and consistency of test subjects, and the ambiguity of video contents
all together. We also derive closed-from expression for the confidence interval
of each estimate. Compared to previous methods which partially exploit the
subjective information, our approach is able to exploit the information in
full, yielding tighter confidence interval and better handling of outliers
without the need for z-scoring or subject rejection. It also handles missing
data more gracefully. Finally, as side information, it provides interesting
insights on the test subjects and video contents.
</summary>
    <author>
      <name>Zhi Li</name>
    </author>
    <author>
      <name>Christos G. Bampis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages; abridged version appeared in Data Compression Conference
  (DCC) 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.01715v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.01715v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.08397v1</id>
    <updated>2016-11-25T09:38:10Z</updated>
    <published>2016-11-25T09:38:10Z</published>
    <title>A Second Order Derivatives based Approach for Steganography</title>
    <summary>  Steganography schemes are designed with the objective of minimizing a defined
distortion function. In most existing state of the art approaches, this
distortion function is based on image feature preservation. Since smooth
regions or clean edges define image core, even a small modification in these
areas largely modifies image features and is thus easily detectable. On the
contrary, textures, noisy or chaotic regions are so difficult to model that the
features having been modified inside these areas are similar to the initial
ones. These regions are characterized by disturbed level curves. This work
presents a new distortion function for steganography that is based on second
order derivatives, which are mathematical tools that usually evaluate level
curves. Two methods are explained to compute these partial derivatives and have
been completely implemented. The first experiments show that these approaches
are promising.
</summary>
    <author>
      <name>Jean-Fran√ßois Couchot</name>
    </author>
    <author>
      <name>Rapha√´l Couturier</name>
    </author>
    <author>
      <name>Yousra Ahmed Fadil</name>
    </author>
    <author>
      <name>Christophe Guyeux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to SECRYPT 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.08397v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.08397v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.04688v1</id>
    <updated>2016-07-03T12:39:53Z</updated>
    <published>2016-07-03T12:39:53Z</published>
    <title>Algorithmic Analysis of Invisible Video Watermarking using LSB Encoding
  Over a Client-Server Framework</title>
    <summary>  Video watermarking is extensively used in many media-oriented applications
for embedding watermarks, i.e. hidden digital data, in a video sequence to
protect the video from illegal copying and to identify manipulations made in
the video. In case of an invisible watermark, the human eye can not perceive
any difference in the video, but a watermark extraction application can read
the watermark and obtain the embedded information. Although numerous
methodologies exist for embedding watermarks, many of them have shortcomings
with respect to performance efficiency, especially over a distributed network.
This paper proposes and analyses a 2-bit Least Significant Bit (LSB) parallel
algorithmic approach for achieving performance efficiency to watermark and
distribute videos over a client-server framework.
</summary>
    <author>
      <name>Poorna Banerjee Dasgupta</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.14445/22312803/IJCTT-V36P125</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.14445/22312803/IJCTT-V36P125" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Published with International Journal of Computer Trends and
  Technology (IJCTT), Volume-36 Number-3, June-2016</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Trends and Technology (IJCTT)
  V36(3):143-146, June 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1612.04688v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.04688v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.07893v4</id>
    <updated>2018-02-12T18:49:24Z</updated>
    <published>2016-12-23T07:57:34Z</published>
    <title>Cross-Color Channel Perceptually Adaptive Quantization for HEVC</title>
    <summary>  HEVC includes a Coding Unit (CU) level luminance-based perceptual
quantization technique known as AdaptiveQP. AdaptiveQP perceptually adjusts the
Quantization Parameter (QP) at the CU level based on the spatial activity of
raw input video data in a luma Coding Block (CB). In this paper, we propose a
novel cross-color channel adaptive quantization scheme which perceptually
adjusts the CU level QP according to the spatial activity of raw input video
data in the constituent luma and chroma CBs; i.e., the combined spatial
activity across all three color channels (the Y, Cb and Cr channels). Our
technique is evaluated in HM 16 with 4:4:4, 4:2:2 and 4:2:0 YCbCr JCT-VC test
sequences. Both subjective and objective visual quality evaluations are
undertaken during which we compare our method with AdaptiveQP. Our technique
achieves considerable coding efficiency improvements, with maximum BD-Rate
reductions of 15.9% (Y), 13.1% (Cr) and 16.1% (Cb) in addition to a maximum
decoding time reduction of 11.0%.
</summary>
    <author>
      <name>Lee Prangnell</name>
    </author>
    <author>
      <name>Miguel Hern√°ndez-Cabronero</name>
    </author>
    <author>
      <name>Victor Sanchez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Data Compression Conference 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.07893v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.07893v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05611v3</id>
    <updated>2018-04-02T18:58:29Z</updated>
    <published>2017-01-19T21:43:07Z</published>
    <title>Comprehensive Review of Audio Steganalysis Methods</title>
    <summary>  Recently, merging signal processing techniques with information security
services has found a lot of attention. Steganography and steganalysis are among
those trends. Like their counterparts in cryptology, steganography and
steganalysis are in a constant battle. Steganography methods try to hide the
presence of covert messages in innocuous-looking data, whereas steganalysis
methods try to reveal existence of such messages and to break steganography
methods. The stream nature of audio signals, their popularity, and their wide
spread usage make them very suitable media for steganography. This has led to a
very rich literature on both steganography and steganalysis of audio signals.
This paper intends to conduct a comprehensive review of audio steganalysis
methods aggregated over near fifteen years. Furthermore, we implement some of
the most recent audio steganalysis methods and conduct a comparative analysis
on their performances. Finally, the paper provides some possible directions for
future researches on audio steganalysis.
</summary>
    <author>
      <name>Hamzeh Ghasemzadeh</name>
    </author>
    <author>
      <name>Mohammad H. Kayvanrad</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1049/iet-spr.2016.0651</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1049/iet-spr.2016.0651" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted journal paper</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IET Signal Processing, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.05611v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05611v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.06509v1</id>
    <updated>2017-01-23T17:11:32Z</updated>
    <published>2017-01-23T17:11:32Z</published>
    <title>Adaptive 360 VR Video Streaming based on MPEG-DASH SRD</title>
    <summary>  We demonstrate an adaptive bandwidth-efficient 360 VR video streaming system
based on MPEG-DASH SRD. We extend MPEG-DASH SRD to the 3D space of 360 VR
videos, and showcase a dynamic view-aware adaptation technique to tackle the
high bandwidth demands of streaming 360 VR videos to wireless VR headsets. We
spatially partition the underlying 3D mesh into multiple 3D sub-meshes, and
construct an efficient 3D geometry mesh called hexaface sphere to optimally
represent tiled 360 VR videos in the 3D space. We then spatially divide the 360
videos into multiple tiles while encoding and packaging, use MPEG-DASH SRD to
describe the spatial relationship of tiles in the 3D space, and prioritize the
tiles in the Field of View (FoV) for view-aware adaptation. Our initial
evaluation results show that we can save up to 72% of the required bandwidth on
360 VR video streaming with minor negative quality impacts compared to the
baseline scenario when no adaptations is applied.
</summary>
    <author>
      <name>Mohammad Hosseini</name>
    </author>
    <author>
      <name>Viswanathan Swaminathan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Symposium on Multimedia 2016 (ISM '16), December
  4-7, San Jose, California, USA. arXiv admin note: substantial text overlap
  with arXiv:1609.08729</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.06509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.06509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.09182v2</id>
    <updated>2017-02-14T04:23:50Z</updated>
    <published>2017-01-01T18:09:08Z</published>
    <title>Analysis of challenges faced by WebRTC videoconferencing and a remedial
  architecture</title>
    <summary>  Lately, World Wide Web came up with an evolution in the niche of
videoconference applications. Latest technologies give browsers a capacity to
initiate real-time communications. WebRTC is one of the free and open source
projects that aim at providing the users freedom to enjoy real-time
communications, and it does so by following and redefining the standards.
However, WebRTC is still a new project and it lacks some high-end
videoconferencing features such as media mixing, recording of a session and
different network conditions adaptation. This paper is an attempt at analyzing
the shortcomings and challenges faced by WebRTC and proposing a Multipoint
Control Unit or traditional communications entity based architecture as a
solution.
</summary>
    <author>
      <name>Maruf Pasha</name>
    </author>
    <author>
      <name>Furrakh Shahzad</name>
    </author>
    <author>
      <name>Arslan Ahmad</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Vol. 14 No. 10 OCTOBER 2016 International Journal of Computer
  Science and Information Security</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1701.09182v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.09182v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00182v1</id>
    <updated>2017-02-01T10:01:44Z</updated>
    <published>2017-02-01T10:01:44Z</published>
    <title>Inkjet printing-based volumetric display projecting multiple full-colour
  2D patterns</title>
    <summary>  In this study, a method to construct a full-colour volumetric display is
presented using a commercially available inkjet printer. Photoreactive
luminescence materials are minutely and automatically printed as the volume
elements, and volumetric displays are constructed with high resolution using
easy-to-fabricate means that exploit inkjet printing technologies. The results
experimentally demonstrate the first prototype of an inkjet printing-based
volumetric display composed of multiple layers of transparent films that yield
a full-colour three-dimensional (3D) image. Moreover, we propose a design
algorithm with 3D structures that provide multiple different 2D full-colour
patterns when viewed from different directions and experimentally demonstrates
prototypes. It is considered that these types of 3D volumetric structures and
their fabrication methods based on widely deployed existing printing
technologies can be utilised as novel information display devices and systems,
including digital signage, media art, entertainment and security.
</summary>
    <author>
      <name>Ryuji Hirayama</name>
    </author>
    <author>
      <name>Tomotaka Suzuki</name>
    </author>
    <author>
      <name>Tomoyoshi Shimobaba</name>
    </author>
    <author>
      <name>Atsushi Shiraki</name>
    </author>
    <author>
      <name>Makoto Naruse</name>
    </author>
    <author>
      <name>Hirotaka Nakayama</name>
    </author>
    <author>
      <name>Takashi Kakue</name>
    </author>
    <author>
      <name>Tomoyoshi Ito</name>
    </author>
    <link href="http://arxiv.org/abs/1702.00182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.01325v2</id>
    <updated>2018-01-18T17:37:43Z</updated>
    <published>2017-02-04T18:55:16Z</published>
    <title>Combining and Steganography of 3D Face Textures</title>
    <summary>  One of the serious issues in communication between people is hiding
information from others, and the best way for this, is deceiving them. Since
nowadays face images are mostly used in three dimensional format, in this paper
we are going to steganography 3D face images, detecting which by curious people
will be impossible. As in detecting face only its texture is important, we
separate texture from shape matrices, for eliminating half of the extra
information, steganography is done only for face texture, and for
reconstructing 3D face, we can use any other shape. Moreover, we will indicate
that, by using two textures, how two 3D faces can be combined. For a complete
description of the process, first, 2D faces are used as an input for building
3D faces, and then 3D textures are hidden within other images.
</summary>
    <author>
      <name>Mohsen Moradi</name>
    </author>
    <author>
      <name>Mohammad-Reza Sadeghi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.22061/JECEI.2017.690</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.22061/JECEI.2017.690" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 10 figures, 16 equations, 5 sections</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Volume 5, Issue 2 - Issue Serial Number 10, Autumn 2017, Page 1-1</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1702.01325v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.01325v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="14G50" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.05878v1</id>
    <updated>2017-02-20T06:53:21Z</updated>
    <published>2017-02-20T06:53:21Z</published>
    <title>From Photo Streams to Evolving Situations</title>
    <summary>  Photos are becoming spontaneous, objective, and universal sources of
information. This paper develops evolving situation recognition using photo
streams coming from disparate sources combined with the advances of deep
learning. Using visual concepts in photos together with space and time
information, we formulate the situation detection into a semi-supervised
learning framework and propose new graph-based models to solve the problem. To
extend the method for unknown situations, we introduce a soft label method
which enables the traditional semi-supervised learning framework to accurately
predict predefined labels as well as effectively form new clusters. To overcome
the noisy data which degrades graph quality, leading to poor recognition
results, we take advantage of two kinds of noise-robust norms which can
eliminate the adverse effects of outliers in visual concepts and improve the
accuracy of situation recognition. Finally, we demonstrate the idea and the
effectiveness of the proposed model on Yahoo Flickr Creative Commons 100
Million.
</summary>
    <author>
      <name>Mengfan Tang</name>
    </author>
    <author>
      <name>Feiping Nie</name>
    </author>
    <author>
      <name>Siripen Pongpaichet</name>
    </author>
    <author>
      <name>Ramesh Jain</name>
    </author>
    <link href="http://arxiv.org/abs/1702.05878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.05878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.06277v1</id>
    <updated>2017-02-21T06:35:01Z</updated>
    <published>2017-02-21T06:35:01Z</published>
    <title>Projection based advanced motion model for cubic mapping for 360-degree
  video</title>
    <summary>  This paper proposes a novel advanced motion model to handle the irregular
motion for the cubic map projection of 360-degree video. Since the irregular
motion is mainly caused by the projection from the sphere to the cube map, we
first try to project the pixels in both the current picture and reference
picture from unfolding cube back to the sphere. Then through utilizing the
characteristic that most of the motions in the sphere are uniform, we can
derive the relationship between the motion vectors of various pixels in the
unfold cube. The proposed advanced motion model is implemented in the High
Efficiency Video Coding reference software. Experimental results demonstrate
that quite obvious performance improvement can be achieved for the sequences
with obvious motions.
</summary>
    <author>
      <name>Li Li</name>
    </author>
    <author>
      <name>Zhu Li</name>
    </author>
    <author>
      <name>Madhukar Budagavi</name>
    </author>
    <author>
      <name>Houqiang Li</name>
    </author>
    <link href="http://arxiv.org/abs/1702.06277v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.06277v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.07548v1</id>
    <updated>2017-02-24T11:53:22Z</updated>
    <published>2017-02-24T11:53:22Z</published>
    <title>Analysis of video quality losses in the homogenous HEVC video
  transcoding</title>
    <summary>  The paper presents quantitative analysis of the video quality losses in the
homogenous HEVC video transcoder. With the use of HM15.0 reference software and
a set of test video sequences, cascaded pixel domain video transcoder (CPDT)
concept has been used to gather all the necessary data needed for the analysis.
This experiment was done for wide range of source and target bitrates. The
essential result of the work is extensive evaluation of CPDT, commonly used as
a reference in works on effective video transcoding. Until now no such
extensively performed study have been made available in the literature. Quality
degradation between transcoded video and the video that would be result of
direct compression of the original video at the same bitrate as the transcoded
one have been reported. The dependency between quality degradation caused by
transcoding and the bitrate changes of the transcoded data stream are clearly
presented on graphs.
</summary>
    <author>
      <name>Tomasz Grajek</name>
    </author>
    <author>
      <name>Jakub Stankowski</name>
    </author>
    <author>
      <name>Damian Karwowski</name>
    </author>
    <author>
      <name>Krzysztof Klimaszewski</name>
    </author>
    <author>
      <name>Olgierd Stankiewicz</name>
    </author>
    <author>
      <name>Krzysztof Wegner</name>
    </author>
    <link href="http://arxiv.org/abs/1702.07548v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.07548v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00304v1</id>
    <updated>2017-03-01T14:06:44Z</updated>
    <published>2017-03-01T14:06:44Z</published>
    <title>Second Screen User Profiling and Multi-level Smart Recommendations in
  the context of Social TVs</title>
    <summary>  In the context of Social TV, the increasing popularity of first and second
screen users, interacting and posting content online, illustrates new business
opportunities and related technical challenges, in order to enrich user
experience on such environments. SAM (Socializing Around Media) project uses
Social Media-connected infrastructure to deal with the aforementioned
challenges, providing intelligent user context management models and mechanisms
capturing social patterns, to apply collaborative filtering techniques and
personalized recommendations towards this direction. This paper presents the
Context Management mechanism of SAM, running in a Social TV environment to
provide smart recommendations for first and second screen content. Work
presented is evaluated using real movie rating dataset found online, to
validate the SAM's approach in terms of effectiveness as well as efficiency.
</summary>
    <author>
      <name>Angelos Valsamis</name>
    </author>
    <author>
      <name>Alexandros Psychas</name>
    </author>
    <author>
      <name>Fotis Aisopos</name>
    </author>
    <author>
      <name>Andreas Menychtas</name>
    </author>
    <author>
      <name>Theodora Varvarigou</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-52836-6_55</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-52836-6_55" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In: Wu TT., Gennari R., Huang YM., Xie H., Cao Y. (eds) Emerging
  Technologies for Education. SETE 2016</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Computer Science, vol 10108. Springer, Cham,
  2017, pp 514-525</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1703.00304v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00304v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00383v1</id>
    <updated>2017-03-01T16:52:16Z</updated>
    <published>2017-03-01T16:52:16Z</published>
    <title>Identification of image source using serialnumber-based watermarking
  under Compressive Sensing conditions</title>
    <summary>  Although the protection of ownership and the prevention of unauthorized
manipulation of digital images becomes an important concern, there is also a
big issue of image source origin authentication. This paper proposes a
procedure for the identification of the image source and content by using the
Public Key Cryptography Signature (PKCS). The procedure is based on the PKCS
watermarking of the images captured with numerous automatic observing cameras
in the Trap View cloud system. Watermark is created based on 32-bit PKCS serial
number and embedded into the captured image. Watermark detection on the
receiver side extracts the serial number and indicates the camera which
captured the image by comparing the original and the extracted serial numbers.
The watermarking procedure is designed to provide robustness to image
optimization based on the Compressive Sensing approach. Also, the procedure is
tested under various attacks and shows successful identification of ownership.
</summary>
    <author>
      <name>Andjela Draganic</name>
    </author>
    <author>
      <name>Milan Maric</name>
    </author>
    <author>
      <name>Irena Orovic</name>
    </author>
    <author>
      <name>Srdjan Stankovic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to MIPRO 2017 conference, Opatija, Croatia</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.00383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00796v1</id>
    <updated>2017-03-02T14:25:13Z</updated>
    <published>2017-03-02T14:25:13Z</published>
    <title>Unsupervised Steganalysis Based on Artificial Training Sets</title>
    <summary>  In this paper, an unsupervised steganalysis method that combines artificial
training setsand supervised classification is proposed. We provide a formal
framework for unsupervisedclassification of stego and cover images in the
typical situation of targeted steganalysis (i.e.,for a known algorithm and
approximate embedding bit rate). We also present a completeset of experiments
using 1) eight different image databases, 2) image features based on
RichModels, and 3) three different embedding algorithms: Least Significant Bit
(LSB) matching,Highly undetectable steganography (HUGO) and Wavelet Obtained
Weights (WOW). Weshow that the experimental results outperform previous methods
based on Rich Models inthe majority of the tested cases. At the same time, the
proposed approach bypasses theproblem of Cover Source Mismatch -when the
embedding algorithm and bit rate are known-, since it removes the need of a
training database when we have a large enough testing set.Furthermore, we
provide a generic proof of the proposed framework in the machine
learningcontext. Hence, the results of this paper could be extended to other
classification problemssimilar to steganalysis.
</summary>
    <author>
      <name>Daniel Lerch-Hostalot</name>
    </author>
    <author>
      <name>David Meg√≠as</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.engappai.2015.12.013</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.engappai.2015.12.013" rel="related"/>
    <link href="http://arxiv.org/abs/1703.00796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00817v1</id>
    <updated>2017-03-02T15:01:03Z</updated>
    <published>2017-03-02T15:01:03Z</published>
    <title>LSB Matching Steganalysis Based on Patterns of Pixel Differences and
  Random Embedding</title>
    <summary>  This paper presents a novel method for detection of LSB matching steganogra-
phy in grayscale images. This method is based on the analysis of the
differences between neighboring pixels before and after random data embedding.
In natu- ral images, there is a strong correlation between adjacent pixels.
This correla- tion is disturbed by LSB matching generating new types of
correlations. The pre- sented method generates patterns from these correlations
and analyzes their varia- tion when random data are hidden. The experiments
performed for two different image databases show that the method yields better
classification accuracy com- pared to prior art for both LSB matching and HUGO
steganography. In addition, although the method is designed for the spatial
domain, some experiments show its applicability also for detecting JPEG
steganography.
</summary>
    <author>
      <name>Daniel Lerch-Hostalot</name>
    </author>
    <author>
      <name>David Meg√≠as</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cose.2012.11.005.</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cose.2012.11.005." rel="related"/>
    <link href="http://arxiv.org/abs/1703.00817v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00817v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.00919v2</id>
    <updated>2017-11-10T12:15:20Z</updated>
    <published>2017-03-02T19:05:05Z</published>
    <title>Depth Estimation using Modified Cost Function for Occlusion Handling</title>
    <summary>  The paper presents a novel approach to occlusion handling problem in depth
estimation using three views. A solution based on modification of similarity
cost function is proposed. During the depth estimation via optimization
algorithms like Graph Cut similarity metric is constantly updated so that only
non-occluded fragments in side views are considered. At each iteration of the
algorithm non-occluded fragments are detected based on side view virtual depth
maps synthesized from the best currently estimated depth map of the center
view. Then similarity metric is updated for correspondence search only in
non-occluded regions of the side views. The experimental results, conducted on
well-known 3D video test sequences, have proved that the depth maps estimated
with the proposed approach provide about 1.25 dB virtual view quality
improvement in comparison to the virtual view synthesized based on depth maps
generated by the state-of-the-art MPEG Depth Estimation Reference Software.
</summary>
    <author>
      <name>Krzysztof Wegner</name>
    </author>
    <author>
      <name>Olgierd Stankiewicz</name>
    </author>
    <author>
      <name>Marek Domanski</name>
    </author>
    <link href="http://arxiv.org/abs/1703.00919v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.00919v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.01986v3</id>
    <updated>2017-08-21T16:47:34Z</updated>
    <published>2017-03-06T17:21:31Z</published>
    <title>Learning from Experience: A Dynamic Closed-Loop QoE Optimization for
  Video Adaptation and Delivery</title>
    <summary>  The quality of experience (QoE) is known to be subjective and
context-dependent. Identifying and calculating the factors that affect QoE is
indeed a difficult task. Recently, a lot of effort has been devoted to estimate
the users QoE in order to improve video delivery. In the literature, most of
the QoE-driven optimization schemes that realize trade-offs among different
quality metrics have been addressed under the assumption of homogenous
populations. Nevertheless, people perceptions on a given video quality may not
be the same, which makes the QoE optimization harder. This paper aims at taking
a step further in order to address this limitation and meet users profiles. To
do so, we propose a closed-loop control framework based on the
users(subjective) feedbacks to learn the QoE function and optimize it at the
same time. Our simulation results show that our system converges to a steady
state, where the resulting QoE function noticeably improves the users
feedbacks.
</summary>
    <author>
      <name>Imen Triki</name>
    </author>
    <author>
      <name>Quanyan Zhu</name>
    </author>
    <author>
      <name>Rachid Elazouzi</name>
    </author>
    <author>
      <name>Majed Haddad</name>
    </author>
    <author>
      <name>Zhiheng Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.01986v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.01986v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.03502v1</id>
    <updated>2017-03-10T01:19:47Z</updated>
    <published>2017-03-10T01:19:47Z</published>
    <title>A Convolutional Neural Network Approach for Half-Pel Interpolation in
  Video Coding</title>
    <summary>  Motion compensation is a fundamental technology in video coding to remove the
temporal redundancy between video frames. To further improve the coding
efficiency, sub-pel motion compensation has been utilized, which requires
interpolation of fractional samples. The video coding standards usually adopt
fixed interpolation filters that are derived from the signal processing theory.
However, as video signal is not stationary, the fixed interpolation filters may
turn out less efficient. Inspired by the great success of convolutional neural
network (CNN) in computer vision, we propose to design a CNN-based
interpolation filter (CNNIF) for video coding. Different from previous studies,
one difficulty for training CNNIF is the lack of ground-truth since the
fractional samples are actually not available. Our solution for this problem is
to derive the "ground-truth" of fractional samples by smoothing high-resolution
images, which is verified to be effective by the conducted experiments.
Compared to the fixed half-pel interpolation filter for luma in High Efficiency
Video Coding (HEVC), our proposed CNNIF achieves up to 3.2% and on average 0.9%
BD-rate reduction under low-delay P configuration.
</summary>
    <author>
      <name>Ning Yan</name>
    </author>
    <author>
      <name>Dong Liu</name>
    </author>
    <author>
      <name>Houqiang Li</name>
    </author>
    <author>
      <name>Feng Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISCAS.2017.8050458</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISCAS.2017.8050458" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Symposium on Circuits and Systems (ISCAS) 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.03502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.03502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05451v1</id>
    <updated>2017-03-16T01:36:49Z</updated>
    <published>2017-03-16T01:36:49Z</published>
    <title>Refining Image Categorization by Exploiting Web Images and General
  Corpus</title>
    <summary>  Studies show that refining real-world categories into semantic subcategories
contributes to better image modeling and classification. Previous image
sub-categorization work relying on labeled images and WordNet's hierarchy is
not only labor-intensive, but also restricted to classify images into NOUN
subcategories. To tackle these problems, in this work, we exploit general
corpus information to automatically select and subsequently classify web images
into semantic rich (sub-)categories. The following two major challenges are
well studied: 1) noise in the labels of subcategories derived from the general
corpus; 2) noise in the labels of images retrieved from the web. Specifically,
we first obtain the semantic refinement subcategories from the text perspective
and remove the noise by the relevance-based approach. To suppress the search
error induced noisy images, we then formulate image selection and classifier
learning as a multi-class multi-instance learning problem and propose to solve
the employed problem by the cutting-plane algorithm. The experiments show
significant performance gains by using the generated data of our way on both
image categorization and sub-categorization tasks. The proposed approach also
consistently outperforms existing weakly supervised and web-supervised
approaches.
</summary>
    <author>
      <name>Yazhou Yao</name>
    </author>
    <author>
      <name>Jian Zhang</name>
    </author>
    <author>
      <name>Fumin Shen</name>
    </author>
    <author>
      <name>Xiansheng Hua</name>
    </author>
    <author>
      <name>Wankou Yang</name>
    </author>
    <author>
      <name>Zhenmin Tang</name>
    </author>
    <link href="http://arxiv.org/abs/1703.05451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05502v2</id>
    <updated>2019-10-07T19:56:14Z</updated>
    <published>2017-03-16T08:28:11Z</published>
    <title>Steganographic Generative Adversarial Networks</title>
    <summary>  Steganography is collection of methods to hide secret information ("payload")
within non-secret information "container"). Its counterpart, Steganalysis, is
the practice of determining if a message contains a hidden payload, and
recovering it if possible. Presence of hidden payloads is typically detected by
a binary classifier. In the present study, we propose a new model for
generating image-like containers based on Deep Convolutional Generative
Adversarial Networks (DCGAN). This approach allows to generate more
setganalysis-secure message embedding using standard steganography algorithms.
Experiment results demonstrate that the new model successfully deceives the
steganography analyzer, and for this reason, can be used in steganographic
applications.
</summary>
    <author>
      <name>Denis Volkhonskiy</name>
    </author>
    <author>
      <name>Ivan Nazarov</name>
    </author>
    <author>
      <name>Evgeny Burnaev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 10 figures, 5 tables, Workshop on Adversarial Training
  (NIPS 2016, Barcelona, Spain)</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.05502v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05502v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.06499v1</id>
    <updated>2017-03-19T19:47:49Z</updated>
    <published>2017-03-19T19:47:49Z</published>
    <title>Image denoising by median filter in wavelet domain</title>
    <summary>  The details of an image with noise may be restored by removing noise through
a suitable image de-noising method. In this research, a new method of image
de-noising based on using median filter (MF) in the wavelet domain is proposed
and tested. Various types of wavelet transform filters are used in conjunction
with median filter in experimenting with the proposed approach in order to
obtain better results for image de-noising process, and, consequently to select
the best suited filter. Wavelet transform working on the frequencies of
sub-bands split from an image is a powerful method for analysis of images.
According to this experimental work, the proposed method presents better
results than using only wavelet transform or median filter alone. The MSE and
PSNR values are used for measuring the improvement in de-noised images.
</summary>
    <author>
      <name>Afrah Ramadhan</name>
    </author>
    <author>
      <name>Firas Mahmood</name>
    </author>
    <author>
      <name>Atilla Elci</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijma.2017.9104</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijma.2017.9104" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 9 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The International Journal of Multimedia &amp; Its Applications (IJMA)
  Vol.9, No.1, pp: 31-40, February 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1703.06499v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.06499v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.09103v1</id>
    <updated>2017-03-27T14:29:15Z</updated>
    <published>2017-03-27T14:29:15Z</published>
    <title>Theoretical Evaluation of Li et al.'s Approach for Improving a Binary
  Watermark-Based Scheme in Remote Sensing Data Communications</title>
    <summary>  This letter is about a principal weakness of the published article by Li et
al. in 2014. It seems that the mentioned work has a terrible conceptual mistake
while presenting its theoretical approach. In fact, the work has tried to
design a new attack and its effective solution for a basic watermarking
algorithm by Zhu et al. published in 2013, however in practice, we show the Li
et al.'s approach is not correct to obtain the aim. For disproof of the
incorrect approach, we only apply a numerical example as the counterexample of
the Li et al.'s approach.
</summary>
    <author>
      <name>Mohammad Reza Khosravi</name>
    </author>
    <author>
      <name>Mohammad Kazem Moghimi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Majlesi Journal of Telecommunication Devices, 5, 4, 151-153 (2016)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1703.09103v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09103v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.09968v2</id>
    <updated>2017-03-30T08:06:10Z</updated>
    <published>2017-03-29T10:59:33Z</published>
    <title>An Evaluation of Digital Image Forgery Detection Approaches</title>
    <summary>  With the headway of the advanced image handling software and altering tools,
a computerized picture can be effectively controlled. The identification of
image manipulation is vital in light of the fact that an image can be utilized
as legitimate confirmation, in crime scene investigation, and in numerous
different fields. The image forgery detection techniques intend to confirm the
credibility of computerized pictures with no prior information about the
original image. There are numerous routes for altering a picture, for example,
resampling, splicing, and copy-move. In this paper, we have examined different
type of image forgery and their detection techniques; mainly we focused on
pixel based image forgery detection techniques.
</summary>
    <author>
      <name>Abhishek Kashyap</name>
    </author>
    <author>
      <name>Rajesh Singh Parmar</name>
    </author>
    <author>
      <name>Megha Agrawal</name>
    </author>
    <author>
      <name>Hariom Gupta</name>
    </author>
    <link href="http://arxiv.org/abs/1703.09968v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09968v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00631v1</id>
    <updated>2017-04-03T15:06:37Z</updated>
    <published>2017-04-03T15:06:37Z</published>
    <title>Detection of Copy-move Image forgery using SVD and Cuckoo Search
  Algorithm</title>
    <summary>  Copy-move forgery is one of the simple and effective operations to create
forged images. Recently, techniques based on singular value decomposition (SVD)
are widely used to detect copy-move forgery (CMF). Some approaches based on SVD
are most acceptable to detect copy-move forgery but some copy-move forgery
detection approaches can not produce satisfactory detection results. Sometimes
these approaches may even produce error results. According to our observation,
detection result produced using SVD depend highly on those parameters whose
values are often determined with experiences. These values are only applicable
to a few images, which limit their application. To solve this problem, a novel
approach named as copy-move forgery detection using Cuckoo search algorithm
(CMFD-CS) is proposed in this paper. CMFD-CS integrates the CS algorithm into
SVD. It utilizes the CS algorithm to generate customized parameter values for
images, which are used CMFD under block-based framework.
</summary>
    <author>
      <name>Abhishek Kashyap</name>
    </author>
    <author>
      <name>Megha Agarwal</name>
    </author>
    <author>
      <name>Hariom Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.00631v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00631v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02698v1</id>
    <updated>2017-04-10T03:47:42Z</updated>
    <published>2017-04-10T03:47:42Z</published>
    <title>A New Steganographic Technique Matching the Secret Message and Cover
  image Binary Value</title>
    <summary>  Steganography involves hiding a secret message or image inside another cover
image. Changes are made in the cover image without affecting visual quality of
the image. In contrast to cryptography, Steganography provides complete secrecy
of the communication. Security of very sensitive data can be enhanced by
combining cryptography and steganography. A new technique that uses the concept
of Steganography to obtain the position values from an image is suggested. This
paper proposes a new method where no change is made to the cover image, only
the pixel position LSB (Least Significant Bit) values that match with the
secret message bit values are noted in a separate position file. At the sending
end the position file along with the cover image is sent. At the receiving end
the position file is opened only with a secret key. The bit positions are taken
from the position file and the LSB values from the positions are combined to
get ASCII values and then form characters of the secret message
</summary>
    <author>
      <name>G. Umamaheswari</name>
    </author>
    <author>
      <name>Dr. C. P. Sumathi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security, January 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1704.02698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.02790v1</id>
    <updated>2017-04-10T10:32:18Z</updated>
    <published>2017-04-10T10:32:18Z</published>
    <title>Performance Analysis of Reliable Video Streaming with Strict Playout
  Deadline in Multi-Hop Wireless Networks</title>
    <summary>  Motivated by emerging vision-based intelligent services, we consider the
problem of rate adaptation for high quality and low delay visual information
delivery over wireless networks using scalable video coding. Rate adaptation in
this setting is inherently challenging due to the interplay between the
variability of the wireless channels, the queuing at the network nodes and the
frame-based decoding and playback of the video content at the receiver at very
short time scales. To address the problem, we propose a low-complexity,
model-based rate adaptation algorithm for scalable video streaming systems,
building on a novel performance model based on stochastic network calculus. We
validate the model using extensive simulations. We show that it allows fast,
near optimal rate adaptation for fixed transmission paths, as well as
cross-layer optimized routing and video rate adaptation in mesh networks, with
less than $10$\% quality degradation compared to the best achievable
performance.
</summary>
    <author>
      <name>Hussein Al-Zubaidy</name>
    </author>
    <author>
      <name>Viktoria Fodor</name>
    </author>
    <author>
      <name>Gy√∂rgy D√°n</name>
    </author>
    <author>
      <name>Markus Flierl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 single column pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.02790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.02790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03248v1</id>
    <updated>2017-04-11T11:39:02Z</updated>
    <published>2017-04-11T11:39:02Z</published>
    <title>A Robust Blind Watermarking Using Convolutional Neural Network</title>
    <summary>  This paper introduces a blind watermarking based on a convolutional neural
network (CNN). We propose an iterative learning framework to secure robustness
of watermarking. One loop of learning process consists of the following three
stages: Watermark embedding, attack simulation, and weight update. We have
learned a network that can detect a 1-bit message from a image sub-block.
Experimental results show that this learned network is an extension of the
frequency domain that is widely used in existing watermarking scheme. The
proposed scheme achieved robustness against geometric and signal processing
attacks with a learning time of one day.
</summary>
    <author>
      <name>Seung-Min Mun</name>
    </author>
    <author>
      <name>Seung-Hun Nam</name>
    </author>
    <author>
      <name>Han-Ul Jang</name>
    </author>
    <author>
      <name>Dongkyu Kim</name>
    </author>
    <author>
      <name>Heung-Kyu Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neucom.2019.01.067</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neucom.2019.01.067" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages. We are modifying this paper to submit to SPL</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.03248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05665v1</id>
    <updated>2017-04-19T09:28:39Z</updated>
    <published>2017-04-19T09:28:39Z</published>
    <title>CNN based music emotion classification</title>
    <summary>  Music emotion recognition (MER) is usually regarded as a multi-label tagging
task, and each segment of music can inspire specific emotion tags. Most
researchers extract acoustic features from music and explore the relations
between these features and their corresponding emotion tags. Considering the
inconsistency of emotions inspired by the same music segment for human beings,
seeking for the key acoustic features that really affect on emotions is really
a challenging task. In this paper, we propose a novel MER method by using deep
convolutional neural network (CNN) on the music spectrograms that contains both
the original time and frequency domain information. By the proposed method, no
additional effort on extracting specific features required, which is left to
the training procedure of the CNN model. Experiments are conducted on the
standard CAL500 and CAL500exp dataset. Results show that, for both datasets,
the proposed method outperforms state-of-the-art methods.
</summary>
    <author>
      <name>Xin Liu</name>
    </author>
    <author>
      <name>Qingcai Chen</name>
    </author>
    <author>
      <name>Xiangping Wu</name>
    </author>
    <author>
      <name>Yan Liu</name>
    </author>
    <author>
      <name>Yang Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.05665v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05665v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.06192v1</id>
    <updated>2017-03-26T00:38:30Z</updated>
    <published>2017-03-26T00:38:30Z</published>
    <title>The Design, Implementation, and Deployment of a System to Transparently
  Compress Hundreds of Petabytes of Image Files for a File-Storage Service</title>
    <summary>  We report the design, implementation, and deployment of Lepton, a
fault-tolerant system that losslessly compresses JPEG images to 77% of their
original size on average. Lepton replaces the lowest layer of baseline JPEG
compression-a Huffman code-with a parallelized arithmetic code, so that the
exact bytes of the original JPEG file can be recovered quickly. Lepton matches
the compression efficiency of the best prior work, while decoding more than
nine times faster and in a streaming manner. Lepton has been released as
open-source software and has been deployed for a year on the Dropbox
file-storage backend. As of February 2017, it had compressed more than 203 PiB
of user JPEG files, saving more than 46 PiB.
</summary>
    <author>
      <name>Daniel Reiter Horn</name>
    </author>
    <author>
      <name>Ken Elkabany</name>
    </author>
    <author>
      <name>Chris Lesniewski-Laas</name>
    </author>
    <author>
      <name>Keith Winstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. NSDI 2017, Boston. p1-15</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1704.06192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.06192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.06444v1</id>
    <updated>2017-04-21T08:41:21Z</updated>
    <published>2017-04-21T08:41:21Z</published>
    <title>FISF: Better User Experience using Smaller Bandwidth for Panoramic
  Virtual Reality Video</title>
    <summary>  The panoramic video is widely used to build virtual reality (VR) and is
expected to be one of the next generation Killer-Apps. Transmitting panoramic
VR videos is a challenging task because of two problems: 1) panoramic VR videos
are typically much larger than normal videos but they need to be transmitted
with limited bandwidth in mobile networks. 2) high-resolution and fluent views
should be provided to guarantee a superior user experience and avoid
side-effects such as dizziness and nausea. To address these two problems, we
propose a novel interactive streaming technology, namely Focus-based
Interactive Streaming Framework (FISF). FISF consists of three parts: 1) we use
the classic clustering algorithm DBSCAN to analyze real user data for Video
Focus Detection (VFD); 2) we propose a Focus-based Interactive Streaming
Technology (FIST), including a static version and a dynamic version; 3) we
propose two optimization methods: focus merging and prefetch strategy.
Experimental results show that FISF significantly outperforms the
state-of-the-art. The paper is submitted to Sigcomm 2017, VR/AR Network on 31
Mar 2017 at 10:44:04am EDT.
</summary>
    <author>
      <name>Lun Wang</name>
    </author>
    <author>
      <name>Damai Dai</name>
    </author>
    <author>
      <name>Jie Jiang</name>
    </author>
    <author>
      <name>Tong Yang</name>
    </author>
    <author>
      <name>Xiaoke Jiang</name>
    </author>
    <author>
      <name>Zekun Cai</name>
    </author>
    <author>
      <name>Yang Li</name>
    </author>
    <author>
      <name>Xiaoming Li</name>
    </author>
    <link href="http://arxiv.org/abs/1704.06444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.06444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08378v1</id>
    <updated>2017-04-26T23:15:52Z</updated>
    <published>2017-04-26T23:15:52Z</published>
    <title>Deep Convolutional Neural Network to Detect J-UNIWARD</title>
    <summary>  This paper presents an empirical study on applying convolutional neural
networks (CNNs) to detecting J-UNIWARD, one of the most secure JPEG
steganographic method. Experiments guiding the architectural design of the CNNs
have been conducted on the JPEG compressed BOSSBase containing 10,000 covers of
size 512x512. Results have verified that both the pooling method and the depth
of the CNNs are critical for performance. Results have also proved that a
20-layer CNN, in general, outperforms the most sophisticated feature-based
methods, but its advantage gradually diminishes on hard-to-detect cases. To
show that the performance generalizes to large-scale databases and to different
cover sizes, one experiment has been conducted on the CLS-LOC dataset of
ImageNet containing more than one million covers cropped to unified size of
256x256. The proposed 20-layer CNN has cut the error achieved by a CNN recently
proposed for large-scale JPEG steganalysis by 35%. Source code is available via
GitHub: https://github.com/GuanshuoXu/deep_cnn_jpeg_steganalysis
</summary>
    <author>
      <name>Guanshuo Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IH&amp;MMSec 2017. This is a personal copy</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.00341v1</id>
    <updated>2017-04-30T16:47:57Z</updated>
    <published>2017-04-30T16:47:57Z</published>
    <title>Deriving Quests from Open World Mechanics</title>
    <summary>  Open world games present players with more freedom than games with linear
progression structures. However, without clearly-defined objectives, they often
leave players without a sense of purpose. Most of the time, quests and
objectives are hand-authored and overlaid atop an open world's mechanics. But
what if they could be generated organically from the gameplay itself? The goal
of our project was to develop a model of the mechanics in Minecraft that could
be used to determine the ideal placement of objectives in an open world
setting. We formalized the game logic of Minecraft in terms of logical rules
that can be manipulated in two ways: they may be executed to generate graphs
representative of the player experience when playing an open world game with
little developer direction; and they may be statically analyzed to determine
dependency orderings, feedback loops, and bottlenecks. These analyses may then
be used to place achievements on gameplay actions algorithmically.
</summary>
    <author>
      <name>Ryan Alexander</name>
    </author>
    <author>
      <name>Chris Martens</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at Foundations of Digital Games (FDG) 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.00341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.00726v1</id>
    <updated>2017-05-01T22:08:10Z</updated>
    <published>2017-05-01T22:08:10Z</published>
    <title>Optimum Decoder for Multiplicative Spread Spectrum Image Watermarking
  with Laplacian Modeling</title>
    <summary>  This paper investigates the multiplicative spread spectrum watermarking
method for the image. The information bit is spreaded into middle-frequency
Discrete Cosine Transform (DCT) coefficients of each block of an image using a
generated pseudo-random sequence. Unlike the conventional signal modeling, we
suppose that both signal and noise are distributed with Laplacian distribution
because the sample loss of digital media can be better modeled with this
distribution than the Gaussian one. We derive the optimum decoder for the
proposed embedding method thanks to the maximum likelihood decoding scheme. We
also analyze our watermarking system in the presence of noise and provide
analytical evaluations and several simulations. The results show that it has
the suitable performance and transparency required for watermarking
applications.
</summary>
    <author>
      <name>Nematollah Zarmehi</name>
    </author>
    <author>
      <name>Mohammad Reza Aref</name>
    </author>
    <link href="http://arxiv.org/abs/1705.00726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.00726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01762v1</id>
    <updated>2017-05-04T09:39:12Z</updated>
    <published>2017-05-04T09:39:12Z</published>
    <title>A Comparative Case Study of HTTP Adaptive Streaming Algorithms in Mobile
  Networks</title>
    <summary>  HTTP Adaptive Streaming (HAS) techniques are now the dominant solution for
video delivery in mobile networks. Over the past few years, several HAS
algorithms have been introduced in order to improve user quality-of-experience
(QoE) by bit-rate adaptation. Their difference is mainly the required input
information, ranging from network characteristics to application-layer
parameters such as the playback buffer. Interestingly, despite the recent
outburst in scientific papers on the topic, a comprehensive comparative study
of the main algorithm classes is still missing. In this paper we provide such
comparison by evaluating the performance of the state-of-the-art HAS algorithms
per class, based on data from field measurements. We provide a systematic study
of the main QoE factors and the impact of the target buffer level. We conclude
that this target buffer level is a critical classifier for the studied HAS
algorithms. While buffer-based algorithms show superior QoE in most of the
cases, their performance may differ at the low target buffer levels of live
streaming services. Overall, we believe that our findings provide valuable
insight for the design and choice of HAS algorithms according to networks
conditions and service requirements.
</summary>
    <author>
      <name>Theodoros Karagkioules</name>
    </author>
    <author>
      <name>Dimitrios Tsilimantos</name>
    </author>
    <author>
      <name>Cyril Concolato</name>
    </author>
    <author>
      <name>Stefan Valentin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3083165.3083170</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3083165.3083170" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.01762v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01762v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.02940v3</id>
    <updated>2017-09-21T08:10:34Z</updated>
    <published>2017-05-08T16:04:51Z</published>
    <title>Optimized Data Representation for Interactive Multiview Navigation</title>
    <summary>  In contrary to traditional media streaming services where a unique media
content is delivered to different users, interactive multiview navigation
applications enable users to choose their own viewpoints and freely navigate in
a 3-D scene. The interactivity brings new challenges in addition to the
classical rate-distortion trade-off, which considers only the compression
performance and viewing quality. On the one hand, interactivity necessitates
sufficient viewpoints for richer navigation; on the other hand, it requires to
provide low bandwidth and delay costs for smooth navigation during view
transitions. In this paper, we formally describe the novel trade-offs posed by
the navigation interactivity and classical rate-distortion criterion. Based on
an original formulation, we look for the optimal design of the data
representation by introducing novel rate and distortion models and practical
solving algorithms. Experiments show that the proposed data representation
method outperforms the baseline solution by providing lower resource
consumptions and higher visual quality in all navigation configurations, which
certainly confirms the potential of the proposed data representation in
practical interactive navigation systems.
</summary>
    <author>
      <name>Rui Ma</name>
    </author>
    <author>
      <name>Thomas Maugey</name>
    </author>
    <author>
      <name>Pascal Frossard</name>
    </author>
    <link href="http://arxiv.org/abs/1705.02940v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.02940v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.03531v1</id>
    <updated>2017-05-09T20:34:44Z</updated>
    <published>2017-05-09T20:34:44Z</published>
    <title>New Transforms for JPEG Format</title>
    <summary>  The two-dimensional discrete cosine transform (DCT) can be found in the heart
of many image compression algorithms. Specifically, the JPEG format uses a
lossy form of compression based on that transform. Since the standardization of
the JPEG, many other transforms become practical in lossy data compression.
This article aims to analyze the use of these transforms as the DCT replacement
in the JPEG compression chain. Each transform is examined for different image
datasets and subsequently compared to other transforms using the peak
signal-to-noise ratio (PSNR). Our experiments show that an overlapping
variation of the DCT, the local cosine transform (LCT), overcame the original
block-wise transform at low bitrates. At high bitrates, the discrete wavelet
transform employing the Cohen-Daubechies-Feauveau 9/7 wavelet offers about the
same compression performance as the DCT.
</summary>
    <author>
      <name>Stanislav Svoboda</name>
    </author>
    <author>
      <name>David Barina</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">preprint submitted to SCCG 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.03531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.03531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.05103v1</id>
    <updated>2017-05-15T07:52:07Z</updated>
    <published>2017-05-15T07:52:07Z</published>
    <title>Generative Adversarial Networks for Multimodal Representation Learning
  in Video Hyperlinking</title>
    <summary>  Continuous multimodal representations suitable for multimodal information
retrieval are usually obtained with methods that heavily rely on multimodal
autoencoders. In video hyperlinking, a task that aims at retrieving video
segments, the state of the art is a variation of two interlocked networks
working in opposing directions. These systems provide good multimodal
embeddings and are also capable of translating from one representation space to
the other. Operating on representation spaces, these networks lack the ability
to operate in the original spaces (text or image), which makes it difficult to
visualize the crossmodal function, and do not generalize well to unseen data.
Recently, generative adversarial networks have gained popularity and have been
used for generating realistic synthetic data and for obtaining high-level,
single-modal latent representation spaces. In this work, we evaluate the
feasibility of using GANs to obtain multimodal representations. We show that
GANs can be used for multimodal representation learning and that they provide
multimodal representations that are superior to representations obtained with
multimodal autoencoders. Additionally, we illustrate the ability of visualizing
crossmodal translations that can provide human-interpretable insights on
learned GAN-based video hyperlinking models.
</summary>
    <author>
      <name>Vedran Vukotic</name>
    </author>
    <author>
      <name>Christian Raymond</name>
    </author>
    <author>
      <name>Guillaume Gravier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure, 2 tables, published at ACM International
  Conference in Multimedia Retrieval (ICMR) 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.05103v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.05103v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08616v2</id>
    <updated>2019-07-07T18:23:55Z</updated>
    <published>2017-05-24T05:47:23Z</published>
    <title>A New Parallel Message-distribution Technique for Cost-based
  Steganography</title>
    <summary>  This paper presents two novel approaches to increase performance bounds of
image steganography under the criteria of minimizing distortion. First, in
order to efficiently use the images' capacities, we propose using parallel
images in the embedding stage. The result is then used to prove sub-optimality
of the message distribution technique used by all cost based algorithms
including HUGO, S-UNIWARD, and HILL. Second, a new distribution approach is
presented to further improve the security of these algorithms. Experiments show
that this distribution method avoids embedding in smooth regions and thus
achieves a better performance, measured by state-of-the-art steganalysis, when
compared with the current used distribution.
</summary>
    <author>
      <name>Mehdi Sharifzadeh</name>
    </author>
    <author>
      <name>Chirag Agarwal</name>
    </author>
    <author>
      <name>Mahdi Salarian</name>
    </author>
    <author>
      <name>Dan Schonfeld</name>
    </author>
    <link href="http://arxiv.org/abs/1705.08616v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08616v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08733v1</id>
    <updated>2017-05-24T12:51:39Z</updated>
    <published>2017-05-24T12:51:39Z</published>
    <title>Traffic Profiling for Mobile Video Streaming</title>
    <summary>  This paper describes a novel system that provides key parameters of HTTP
Adaptive Streaming (HAS) sessions to the lower layers of the protocol stack. A
non-intrusive traffic profiling solution is proposed that observes packet flows
at the transmit queue of base stations, edge-routers, or gateways. By analyzing
IP flows in real time, the presented scheme identifies different phases of an
HAS session and estimates important application-layer parameters, such as
play-back buffer state and video encoding rate. The introduced estimators only
use IP-layer information, do not require standardization and work even with
traffic that is encrypted via Transport Layer Security (TLS). Experimental
results for a popular video streaming service clearly verify the high accuracy
of the proposed solution. Traffic profiling, thus, provides a valuable
alternative to cross-layer signaling and Deep Packet Inspection (DPI) in order
to perform efficient network optimization for video streaming.
</summary>
    <author>
      <name>Dimitrios Tsilimantos</name>
    </author>
    <author>
      <name>Theodoros Karagkioules</name>
    </author>
    <author>
      <name>Amaya Nogales-G√≥mez</name>
    </author>
    <author>
      <name>Stefan Valentin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 11 figures. Accepted for publication in the proceedings of
  IEEE ICC'17</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.08733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.08809v1</id>
    <updated>2017-05-24T15:03:09Z</updated>
    <published>2017-05-24T15:03:09Z</published>
    <title>HTTP adaptive streaming with indoors-outdoors detection in mobile
  networks</title>
    <summary>  In mobile networks, users may lose coverage when entering a building due to
the high signal attenuation at windows and walls. Under such conditions,
services with minimum bit-rate requirements, such as video streaming, often
show poor Quality-of-Experience (QoE). We will present a Bayesian detector that
combines measurements from two Smartphone sensors to decide if a user is inside
a building or not. Based on this coverage classification, we will propose an
HTTP adaptive streaming (HAS) algorithm to increase playback stability at a
high average bitrate. Measurements in a typical office building show high
accuracy for the presented detector and superior QoE for the proposed HAS
algorithm.
</summary>
    <author>
      <name>Sami Mekki</name>
    </author>
    <author>
      <name>Theodoros Karagkioules</name>
    </author>
    <author>
      <name>Stefan Valentin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures. Accepted at CNTCV: Communication and Networking
  Techniqes for Contemporary Video Workshop of INFOCOM'17</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.08809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.08809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00079v1</id>
    <updated>2017-05-31T20:35:26Z</updated>
    <published>2017-05-31T20:35:26Z</published>
    <title>Putting a Face to the Voice: Fusing Audio and Visual Signals Across a
  Video to Determine Speakers</title>
    <summary>  In this paper, we present a system that associates faces with voices in a
video by fusing information from the audio and visual signals. The thesis
underlying our work is that an extremely simple approach to generating (weak)
speech clusters can be combined with visual signals to effectively associate
faces and voices by aggregating statistics across a video. This approach does
not need any training data specific to this task and leverages the natural
coherence of information in the audio and visual streams. It is particularly
applicable to tracking speakers in videos on the web where a priori information
about the environment (e.g., number of speakers, spatial signals for
beamforming) is not available. We performed experiments on a real-world dataset
using this analysis framework to determine the speaker in a video. Given a
ground truth labeling determined by human rater consensus, our approach had
~71% accuracy.
</summary>
    <author>
      <name>Ken Hoover</name>
    </author>
    <author>
      <name>Sourish Chaudhuri</name>
    </author>
    <author>
      <name>Caroline Pantofaru</name>
    </author>
    <author>
      <name>Malcolm Slaney</name>
    </author>
    <author>
      <name>Ian Sturdy</name>
    </author>
    <link href="http://arxiv.org/abs/1706.00079v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00079v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00291v1</id>
    <updated>2017-06-01T13:35:13Z</updated>
    <published>2017-06-01T13:35:13Z</published>
    <title>Data Analysis in Multimedia Quality Assessment: Revisiting the
  Statistical Tests</title>
    <summary>  Assessment of multimedia quality relies heavily on subjective assessment, and
is typically done by human subjects in the form of preferences or continuous
ratings. Such data is crucial for analysis of different multimedia processing
algorithms as well as validation of objective (computational) methods for the
said purpose. To that end, statistical testing provides a theoretical framework
towards drawing meaningful inferences, and making well grounded conclusions and
recommendations. While parametric tests (such as t test, ANOVA, and error
estimates like confidence intervals) are popular and widely used in the
community, there appears to be a certain degree of confusion in the application
of such tests. Specifically, the assumption of normality and homogeneity of
variance is often not well understood. Therefore, the main goal of this paper
is to revisit them from a theoretical perspective and in the process provide
useful insights into their practical implications. Experimental results on both
simulated and real data are presented to support the arguments made. A software
implementing the said recommendations is also made publicly available, in order
to achieve the goal of reproducible research.
</summary>
    <author>
      <name>Manish Narwaria</name>
    </author>
    <author>
      <name>Lukas Krasula</name>
    </author>
    <author>
      <name>Patrick Le Callet</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2018.2794266</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2018.2794266" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Multimedia 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.00291v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00291v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.01788v1</id>
    <updated>2017-06-06T14:34:50Z</updated>
    <published>2017-06-06T14:34:50Z</published>
    <title>Localization of JPEG double compression through multi-domain
  convolutional neural networks</title>
    <summary>  When an attacker wants to falsify an image, in most of cases she/he will
perform a JPEG recompression. Different techniques have been developed based on
diverse theoretical assumptions but very effective solutions have not been
developed yet. Recently, machine learning based approaches have been started to
appear in the field of image forensics to solve diverse tasks such as
acquisition source identification and forgery detection. In this last case, the
aim ahead would be to get a trained neural network able, given a to-be-checked
image, to reliably localize the forged areas. With this in mind, our paper
proposes a step forward in this direction by analyzing how a single or double
JPEG compression can be revealed and localized using convolutional neural
networks (CNNs). Different kinds of input to the CNN have been taken into
consideration, and various experiments have been carried out trying also to
evidence potential issues to be further investigated.
</summary>
    <author>
      <name>Irene Amerini</name>
    </author>
    <author>
      <name>Tiberio Uricchio</name>
    </author>
    <author>
      <name>Lamberto Ballan</name>
    </author>
    <author>
      <name>Roberto Caldelli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to CVPRW 2017, Workshop on Media Forensics</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.01788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.01788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.04804v1</id>
    <updated>2017-06-15T10:14:25Z</updated>
    <published>2017-06-15T10:14:25Z</published>
    <title>Foveated Video Streaming for Cloud Gaming</title>
    <summary>  Good user experience with interactive cloud-based multimedia applications,
such as cloud gaming and cloud-based VR, requires low end-to-end latency and
large amounts of downstream network bandwidth at the same time. In this paper,
we present a foveated video streaming system for cloud gaming. The system
adapts video stream quality by adjusting the encoding parameters on the fly to
match the player's gaze position. We conduct measurements with a prototype that
we developed for a cloud gaming system in conjunction with eye tracker
hardware. Evaluation results suggest that such foveated streaming can reduce
bandwidth requirements by even more than 50% depending on parametrization of
the foveated video coding and that it is feasible from the latency perspective.
</summary>
    <author>
      <name>Gazi Illahi</name>
    </author>
    <author>
      <name>Matti Siekkinen</name>
    </author>
    <author>
      <name>Enrico Masala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to: IEEE 19th International Workshop on Multimedia Signal
  Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.04804v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04804v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.06064v2</id>
    <updated>2017-09-02T08:20:19Z</updated>
    <published>2017-06-19T17:14:48Z</published>
    <title>Recent Advance in Content-based Image Retrieval: A Literature Survey</title>
    <summary>  The explosive increase and ubiquitous accessibility of visual data on the Web
have led to the prosperity of research activity in image search or retrieval.
With the ignorance of visual content as a ranking clue, methods with text
search techniques for visual retrieval may suffer inconsistency between the
text words and visual content. Content-based image retrieval (CBIR), which
makes use of the representation of visual content to identify relevant images,
has attracted sustained attention in recent two decades. Such a problem is
challenging due to the intention gap and the semantic gap problems. Numerous
techniques have been developed for content-based image retrieval in the last
decade. The purpose of this paper is to categorize and evaluate those
algorithms proposed during the period of 2003 to 2016. We conclude with several
promising directions for future research.
</summary>
    <author>
      <name>Wengang Zhou</name>
    </author>
    <author>
      <name>Houqiang Li</name>
    </author>
    <author>
      <name>Qi Tian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.06064v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.06064v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07422v1</id>
    <updated>2017-06-22T17:53:57Z</updated>
    <published>2017-06-22T17:53:57Z</published>
    <title>Single Classifier-based Passive System for Source Printer Classification
  using Local Texture Features</title>
    <summary>  An important aspect of examining printed documents for potential forgeries
and copyright infringement is the identification of source printer as it can be
helpful for ascertaining the leak and detecting forged documents. This paper
proposes a system for classification of source printer from scanned images of
printed documents using all the printed letters simultaneously. This system
uses local texture patterns based features and a single classifier for
classifying all the printed letters. Letters are extracted from scanned images
using connected component analysis followed by morphological filtering without
the need of using an OCR. Each letter is sub-divided into a flat region and an
edge region, and local tetra patterns are estimated separately for these two
regions. A strategically constructed pooling technique is used to extract the
final feature vectors. The proposed method has been tested on both a publicly
available dataset of 10 printers and a new dataset of 18 printers scanned at a
resolution of 600 dpi as well as 300 dpi printed in four different fonts. The
results indicate shape independence property in the proposed method as using a
single classifier it outperforms existing handcrafted feature-based methods and
needs much smaller number of training pages by using all the printed letters.
</summary>
    <author>
      <name>Sharad Joshi</name>
    </author>
    <author>
      <name>Nitin Khanna</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIFS.2017.2779441</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIFS.2017.2779441" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.07422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.07576v1</id>
    <updated>2017-06-23T06:44:31Z</updated>
    <published>2017-06-23T06:44:31Z</published>
    <title>Further Study on GFR Features for JPEG Steganalysis</title>
    <summary>  The GFR (Gabor Filter Residual) features, built as histograms of quantized
residuals obtained with 2D Gabor filters, can achieve competitive detection
performance against adaptive JPEG steganography. In this paper, an improved
version of the GFR is proposed. First, a novel histogram merging method is
proposed according to the symmetries between different Gabor filters, thus
making the features more compact and robust. Second, a new weighted histogram
method is proposed by considering the position of the residual value in a
quantization interval, making the features more sensitive to the slight changes
in residual values. The experiments are given to demonstrate the effectiveness
of our proposed methods. Finally, we design a CNN to duplicate the detector
with the improved GFR features and the ensemble classifier, thus optimizing the
design of the filters used to form residuals in JPEG-phase-aware features.
</summary>
    <author>
      <name>Xia Chao</name>
    </author>
    <author>
      <name>Guan Qingxiao</name>
    </author>
    <author>
      <name>Zhao Xianfeng</name>
    </author>
    <link href="http://arxiv.org/abs/1706.07576v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.07576v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08136v1</id>
    <updated>2017-06-25T16:35:16Z</updated>
    <published>2017-06-25T16:35:16Z</published>
    <title>On the usefulness of information hiding techniques for wireless sensor
  networks security</title>
    <summary>  A wireless sensor network (WSN) typically consists of base stations and a
large number of wireless sensors. The sensory data gathered from the whole
network at a certain time snapshot can be visualized as an image. As a result,
information hiding techniques can be applied to this "sensory data image".
Steganography refers to the technology of hiding data into digital media
without drawing any suspicion, while steganalysis is the art of detecting the
presence of steganography. This article provides a brief review of
steganography and steganalysis applications for wireless sensor networks
(WSNs). Then we show that the steganographic techniques are both related to
sensed data authentication in wireless sensor networks, and when considering
the attacker point of view, which has not yet been investigated in the
literature. Our simulation results show that the sink level is unable to detect
an attack carried out by the nsF5 algorithm on sensed data.
</summary>
    <author>
      <name>Rola Al-Sharif</name>
    </author>
    <author>
      <name>Christophe Guyeux</name>
    </author>
    <author>
      <name>Yousra Ahmed Fadil</name>
    </author>
    <author>
      <name>Abdallah Makhoul</name>
    </author>
    <author>
      <name>Ali Jaber</name>
    </author>
    <link href="http://arxiv.org/abs/1706.08136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08764v1</id>
    <updated>2017-06-27T10:23:21Z</updated>
    <published>2017-06-27T10:23:21Z</published>
    <title>A Robust Data Hiding Process Contributing to the Development of a
  Semantic Web</title>
    <summary>  In this paper, a novel steganographic scheme based on chaotic iterations is
proposed. This research work takes place into the information hiding framework,
and focus more specifically on robust steganography. Steganographic algorithms
can participate in the development of a semantic web: medias being on the
Internet can be enriched by information related to their contents, authors,
etc., leading to better results for the search engines that can deal with such
tags. As media can be modified by users for various reasons, it is preferable
that these embedding tags can resist to changes resulting from some classical
transformations as for example cropping, rotation, image conversion, and so on.
This is why a new robust watermarking scheme for semantic search engines is
proposed in this document. For the sake of completeness, the robustness of this
scheme is finally compared to existing established algorithms.
</summary>
    <author>
      <name>Jacques M. Bahi</name>
    </author>
    <author>
      <name>Jean-Fran√ßois Couchot</name>
    </author>
    <author>
      <name>Nicolas Friot</name>
    </author>
    <author>
      <name>Christophe Guyeux</name>
    </author>
    <link href="http://arxiv.org/abs/1706.08764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.10143v1</id>
    <updated>2017-06-30T11:43:34Z</updated>
    <published>2017-06-30T11:43:34Z</published>
    <title>Evaluation of No Reference Bitstream-based Video Quality Assessment
  Methods</title>
    <summary>  Many different parametric models for video quality assessment have been
proposed in the past few years. This paper presents a review of nine recent
models which cover a wide range of methodologies and have been validated for
estimating video quality due to different degradation factors. Each model is
briefly described with key algorithms and relevant parametric formulas. The
generalization capability of each model to estimate video quality in
real-application scenarios is evaluated and compared with other models, using a
dataset created with video sequences from practical applications. These video
sequences cover a wide range of possible realistic encoding parameters, labeled
with mean opinion scores (MOS) via subjective test. The weakness and strength
of each model are remarked. Finally, future work towards a more general
parametric model that could apply for a wider range of applications is
discussed.
</summary>
    <author>
      <name>Tiantian He</name>
    </author>
    <author>
      <name>Yankai Liu</name>
    </author>
    <author>
      <name>Rong Xie</name>
    </author>
    <author>
      <name>Xin Tang</name>
    </author>
    <author>
      <name>Li Song</name>
    </author>
    <link href="http://arxiv.org/abs/1706.10143v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.10143v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.01742v1</id>
    <updated>2017-07-05T09:21:37Z</updated>
    <published>2017-07-05T09:21:37Z</published>
    <title>High Resilience Diverse Domain Multilevel Audio Watermarking with
  Adaptive Threshold</title>
    <summary>  A novel diverse domain (DCT-SVD &amp; DWT-SVD) watermarking scheme is proposed in
this paper. Here, the watermark is embedded simultaneously onto the two
domains. It is shown that an audio signal watermarked using this scheme has
better subjective and objective quality when compared with other watermarking
schemes. Also proposed are two novel watermark detection algorithms viz., AOT
(Adaptively Optimised Threshold) and AOTx (AOT eXtended). The fundamental idea
behind both is finding an optimum threshold for detecting a known character
embedded along with the actual watermarks in a known location, with the
constraint that the Bit Error Rate (BER) is minimum. This optimum threshold is
used for detecting the other characters in the watermarks. This approach is
shown to make the watermarking scheme less susceptible to various signal
processing attacks, thus making the watermarks more robust.
</summary>
    <author>
      <name>Jerrin Thomas Panachakel</name>
    </author>
    <author>
      <name>Anurenjan P. R</name>
    </author>
    <link href="http://arxiv.org/abs/1707.01742v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.01742v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.05726v1</id>
    <updated>2017-07-18T16:14:53Z</updated>
    <published>2017-07-18T16:14:53Z</published>
    <title>Halftone Image Watermarking by Content Aware Double-sided Embedding
  Error Diffusion</title>
    <summary>  In this paper, we carry out a performance analysis from a probabilistic
perspective to introduce the EDHVW methods' expected performances and
limitations. Then, we propose a new general error diffusion based halftone
visual watermarking (EDHVW) method, Content aware Double-sided Embedding Error
Diffusion (CaDEED), via considering the expected watermark decoding performance
with specific content of the cover images and watermark, different noise
tolerance abilities of various cover image content and the different importance
levels of every pixel (when being perceived) in the secret pattern (watermark).
To demonstrate the effectiveness of CaDEED, we propose CaDEED with expectation
constraint (CaDEED-EC) and CaDEED-NVF&amp;IF (CaDEED-N&amp;I). Specifically, we build
CaDEED-EC by only considering the expected performances of specific cover
images and watermark. By adopting the noise visibility function (NVF) and
proposing the importance factor (IF) to assign weights to every embedding
location and watermark pixel, respectively, we build the specific method
CaDEED-N&amp;I. In the experiments, we select the optimal parameters for NVF and IF
via extensive experiments. In both the numerical and visual comparisons, the
experimental results demonstrate the superiority of our proposed work.
</summary>
    <author>
      <name>Yuanfang Guo</name>
    </author>
    <author>
      <name>Oscar C. Au</name>
    </author>
    <author>
      <name>Rui Wang</name>
    </author>
    <author>
      <name>Lu Fang</name>
    </author>
    <author>
      <name>Xiaochun Cao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2018.2815181</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2018.2815181" rel="related"/>
    <link href="http://arxiv.org/abs/1707.05726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.05726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.07795v1</id>
    <updated>2017-07-25T02:39:50Z</updated>
    <published>2017-07-25T02:39:50Z</published>
    <title>Anti-Forensics of Camera Identification and the Triangle Test by
  Improved Fingerprint-Copy Attack</title>
    <summary>  The fingerprint-copy attack aims to confuse camera identification based on
sensor pattern noise. However, the triangle test shows that the forged images
undergone fingerprint-copy attack would share a non-PRNU (Photo-response
nonuniformity) component with every stolen image, and thus can detect
fingerprint-copy attack. In this paper, we propose an improved fingerprint-copy
attack scheme. Our main idea is to superimpose the estimated fingerprint into
the target image dispersedly, via employing a block-wise method and using the
stolen images randomly and partly. We also develop a practical method to
determine the strength of the superimposed fingerprint based on objective image
quality. In such a way, the impact of non-PRNU component on the triangle test
is reduced, and our improved fingerprint-copy attack is difficultly detected.
The experiments evaluated on 2,900 images from 4 cameras show that our scheme
can effectively fool camera identification, and significantly degrade the
performance of the triangle test simultaneously.
</summary>
    <author>
      <name>Haodong Li</name>
    </author>
    <author>
      <name>Weiqi Luo</name>
    </author>
    <author>
      <name>Quanquan Rao</name>
    </author>
    <author>
      <name>Jiwu Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1707.07795v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.07795v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.09791v1</id>
    <updated>2017-07-31T10:11:51Z</updated>
    <published>2017-07-31T10:11:51Z</published>
    <title>Intra Prediction Using In-Loop Residual Coding for the post-HEVC
  Standard</title>
    <summary>  A few years after standardization of the High Efficiency Video Coding (HEVC),
now the Joint Video Exploration Team (JVET) group is exploring post-HEVC video
compression technologies. In the intra prediction domain, this effort has
resulted in an algorithm with 67 internal modes, new filters and tools which
significantly improve HEVC. However, the improved algorithm still suffers from
the long distance prediction inaccuracy problem. In this paper, we propose an
In-Loop Residual coding Intra Prediction (ILR-IP) algorithm which utilizes
inner-block reconstructed pixels as references to reduce the distance from
predicted pixels. This is done by using the ILR signal for partially
reconstructing each pixel, right after its prediction and before its
block-level out-loop residual calculation. The ILR signal is decided in the
rate-distortion sense, by a brute-force search on a QP-dependent finite
codebook that is known to the decoder. Experiments show that the proposed
ILR-IP algorithm improves the existing method in the Joint Exploration Model
(JEM) up to 0.45% in terms of bit rate saving, without complexity overhead at
the decoder side.
</summary>
    <author>
      <name>Mohsen Abdoli</name>
    </author>
    <author>
      <name>F√©lix Henry</name>
    </author>
    <author>
      <name>Patric Brault</name>
    </author>
    <author>
      <name>Pierre Duhamel</name>
    </author>
    <author>
      <name>Fr√©d√©ric Dufaux</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figure, Conference: IEEE 19th International Workshop on
  Multimedia Signal Processing, Luton, UK</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.09791v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.09791v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00255v1</id>
    <updated>2017-08-01T11:36:57Z</updated>
    <published>2017-08-01T11:36:57Z</published>
    <title>MM2RTB: Bringing Multimedia Metrics to Real-Time Bidding</title>
    <summary>  In display advertising, users' online ad experiences are important for the
advertising effectiveness. However, users have not been well accommodated in
real-time bidding (RTB). This further influences their site visits and
perception of the displayed banner ads. In this paper, we propose a novel
computational framework which brings multimedia metrics, like the contextual
relevance, the visual saliency and the ad memorability into RTB to improve the
users' ad experiences as well as maintain the benefits of the publisher and the
advertiser. We aim at developing a vigorous ecosystem by optimizing the
trade-offs among all stakeholders. The framework considers the scenario of a
webpage with multiple ad slots. Our experimental results show that the benefits
of the advertiser and the user can be significantly improved if the publisher
would slightly sacrifice his short-term revenue. The improved benefits will
increase the advertising requests (demand) and the site visits (supply), which
can further boost the publisher's revenue in the long run.
</summary>
    <author>
      <name>Xiang Chen</name>
    </author>
    <author>
      <name>Bowei Chen</name>
    </author>
    <author>
      <name>Mohan Kankanhalli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of AdKDD and TargetAd, Halifax, NS, Canada, August,
  14, 2017, 6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.00255v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00255v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00684v1</id>
    <updated>2017-08-02T10:20:22Z</updated>
    <published>2017-08-02T10:20:22Z</published>
    <title>OmniArt: Multi-task Deep Learning for Artistic Data Analysis</title>
    <summary>  Vast amounts of artistic data is scattered on-line from both museums and art
applications. Collecting, processing and studying it with respect to all
accompanying attributes is an expensive process. With a motivation to speed up
and improve the quality of categorical analysis in the artistic domain, in this
paper we propose an efficient and accurate method for multi-task learning with
a shared representation applied in the artistic domain. We continue to show how
different multi-task configurations of our method behave on artistic data and
outperform handcrafted feature approaches as well as convolutional neural
networks. In addition to the method and analysis, we propose a challenge like
nature to the new aggregated data set with almost half a million samples and
structured meta-data to encourage further research and societal engagement.
</summary>
    <author>
      <name>Gjorgji Strezoski</name>
    </author>
    <author>
      <name>Marcel Worring</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.00684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.02859v1</id>
    <updated>2017-08-09T14:46:57Z</updated>
    <published>2017-08-09T14:46:57Z</published>
    <title>Joint Optimization of QoE and Fairness Through Network Assisted Adaptive
  Mobile Video Streaming</title>
    <summary>  MPEG has recently proposed Server and Network Assisted Dynamic Adaptive
Streaming over HTTP (SAND-DASH) for video streaming over the Internet. In
contrast to the purely client-based video streaming in which each client makes
its own decision to adjust its bitrate, SAND-DASH enables a group of
simultaneous clients to select their bitrates in a coordinated fashion in order
to improve resource utilization and quality of experience. In this paper, we
study the performance of such an adaptation strategy compared to the
traditional approach with large number of clients having mobile Internet
access. We propose a multi-servers multi-coordinators (MSs-MCs) framework to
model groups of remote clients accessing video content replicated to spatially
distributed edge servers. We then formulate an optimization problem to maximize
jointly the QoE of individual clients, proportional fairness in allocating the
limited resources of base stations as well as balancing the utilized resources
among multiple serves. We then present an efficient heuristic-based solution to
the problem and perform simulations in order to explore parameter space of the
scheme as well as to compare the performance to purely client-based DASH.
</summary>
    <author>
      <name>Abbas Mehrabi</name>
    </author>
    <author>
      <name>Matti Siekkinen</name>
    </author>
    <author>
      <name>Antti Yl√§-J√§√§ski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted, IEEE International Conference on Wireless and Mobile
  Computing, Networking and Communications (WiMob) 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.02859v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.02859v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.05922v1</id>
    <updated>2017-08-20T02:10:00Z</updated>
    <published>2017-08-20T02:10:00Z</published>
    <title>360-degree Video Stitching for Dual-fisheye Lens Cameras Based On Rigid
  Moving Least Squares</title>
    <summary>  Dual-fisheye lens cameras are becoming popular for 360-degree video capture,
especially for User-generated content (UGC), since they are affordable and
portable. Images generated by the dual-fisheye cameras have limited overlap and
hence require non-conventional stitching techniques to produce high-quality
360x180-degree panoramas. This paper introduces a novel method to align these
images using interpolation grids based on rigid moving least squares.
Furthermore, jitter is the critical issue arising when one applies the
image-based stitching algorithms to video. It stems from the unconstrained
movement of stitching boundary from one frame to another. Therefore, we also
propose a new algorithm to maintain the temporal coherence of stitching
boundary to provide jitter-free 360-degree videos. Results show that the method
proposed in this paper can produce higher quality stitched images and videos
than prior work.
</summary>
    <author>
      <name>Tuan Ho</name>
    </author>
    <author>
      <name>Ioannis Schizas</name>
    </author>
    <author>
      <name>K. R. Rao</name>
    </author>
    <author>
      <name>Madhukar Budagavi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint version</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.05922v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.05922v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.07154v1</id>
    <updated>2017-08-23T19:06:19Z</updated>
    <published>2017-08-23T19:06:19Z</published>
    <title>Lossless Image and Intra-frame Compression with Integer-to-Integer DST</title>
    <summary>  Video coding standards are primarily designed for efficient lossy
compression, but it is also desirable to support efficient lossless compression
within video coding standards using small modifications to the lossy coding
architecture. A simple approach is to skip transform and quantization, and
simply entropy code the prediction residual. However, this approach is
inefficient at compression. A more efficient and popular approach is to skip
transform and quantization but also process the residual block with DPCM, along
the horizontal or vertical direction, prior to entropy coding. This paper
explores an alternative approach based on processing the residual block with
integer-to-integer (i2i) transforms. I2i transforms can map integer pixels to
integer transform coefficients without increasing the dynamic range and can be
used for lossless compression. We focus on lossless intra coding and develop
novel i2i approximations of the odd type-3 DST (ODST-3). Experimental results
with the HEVC reference software show that the developed i2i approximations of
the ODST-3 improve lossless intra-frame compression efficiency with respect to
HEVC version 2, which uses the popular DPCM method, by an average 2.7% without
a significant effect on computational complexity.
</summary>
    <author>
      <name>Fatih Kamisli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCSVT.2017.2787638</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCSVT.2017.2787638" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Draft consisting of 16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.07154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.07154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.00649v1</id>
    <updated>2017-09-03T01:10:18Z</updated>
    <published>2017-09-03T01:10:18Z</published>
    <title>Simulated Annealing for JPEG Quantization</title>
    <summary>  JPEG is one of the most widely used image formats, but in some ways remains
surprisingly unoptimized, perhaps because some natural optimizations would go
outside the standard that defines JPEG. We show how to improve JPEG compression
in a standard-compliant, backward-compatible manner, by finding improved
default quantization tables. We describe a simulated annealing technique that
has allowed us to find several quantization tables that perform better than the
industry standard, in terms of both compressed size and image fidelity.
Specifically, we derive tables that reduce the FSIM error by over 10% while
improving compression by over 20% at quality level 95 in our tests; we also
provide similar results for other quality levels. While we acknowledge our
approach can in some images lead to visible artifacts under large
magnification, we believe use of these quantization tables, or additional
tables that could be found using our methodology, would significantly reduce
JPEG file sizes with improved overall image quality.
</summary>
    <author>
      <name>Max Hopkins</name>
    </author>
    <author>
      <name>Michael Mitzenmacher</name>
    </author>
    <author>
      <name>Sebastian Wagner-Carena</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appendix not included in arXiv version due to size restrictions. For
  full paper go to:
  http://www.eecs.harvard.edu/~michaelm/SimAnneal/PAPER/simulated-annealing-jpeg.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.00649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.00649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.01784v1</id>
    <updated>2017-09-06T11:49:46Z</updated>
    <published>2017-09-06T11:49:46Z</published>
    <title>Cross-Domain Image Retrieval with Attention Modeling</title>
    <summary>  With the proliferation of e-commerce websites and the ubiquitousness of smart
phones, cross-domain image retrieval using images taken by smart phones as
queries to search products on e-commerce websites is emerging as a popular
application. One challenge of this task is to locate the attention of both the
query and database images. In particular, database images, e.g. of fashion
products, on e-commerce websites are typically displayed with other
accessories, and the images taken by users contain noisy background and large
variations in orientation and lighting. Consequently, their attention is
difficult to locate. In this paper, we exploit the rich tag information
available on the e-commerce websites to locate the attention of database
images. For query images, we use each candidate image in the database as the
context to locate the query attention. Novel deep convolutional neural network
architectures, namely TagYNet and CtxYNet, are proposed to learn the attention
weights and then extract effective representations of the images. Experimental
results on public datasets confirm that our approaches have significant
improvement over the existing methods in terms of the retrieval accuracy and
efficiency.
</summary>
    <author>
      <name>Xin Ji</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Meihui Zhang</name>
    </author>
    <author>
      <name>Yang Yang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3123266.3123429</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3123266.3123429" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages with an extra reference page</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2017 ACM Multimedia Conference</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.01784v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.01784v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.7; I.4.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02908v1</id>
    <updated>2017-09-09T04:34:48Z</updated>
    <published>2017-09-09T04:34:48Z</published>
    <title>Image Processing Operations Identification via Convolutional Neural
  Network</title>
    <summary>  In recent years, image forensics has attracted more and more attention, and
many forensic methods have been proposed for identifying image processing
operations. Up to now, most existing methods are based on hand crafted
features, and just one specific operation is considered in their methods. In
many forensic scenarios, however, multiple classification for various image
processing operations is more practical. Besides, it is difficult to obtain
effective features by hand for some image processing operations. In this paper,
therefore, we propose a new convolutional neural network (CNN) based method to
adaptively learn discriminative features for identifying typical image
processing operations. We carefully design the high pass filter bank to get the
image residuals of the input image, the channel expansion layer to mix up the
resulting residuals, the pooling layers, and the activation functions employed
in our method. The extensive results show that the proposed method can
outperform the currently best method based on hand crafted features and three
related methods based on CNN for image steganalysis and/or forensics, achieving
the state-of-the-art results. Furthermore, we provide more supplementary
results to show the rationality and robustness of the proposed model.
</summary>
    <author>
      <name>Bolin Chen</name>
    </author>
    <author>
      <name>Haodong Li</name>
    </author>
    <author>
      <name>Weiqi Luo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11432-018-9492-6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11432-018-9492-6" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Sci. China Inf. Sci. 63, 139109 (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1709.02908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.04427v2</id>
    <updated>2022-08-30T03:13:50Z</updated>
    <published>2017-09-13T17:17:01Z</published>
    <title>Contrast Enhancement of Brightness-Distorted Images by Improved Adaptive
  Gamma Correction</title>
    <summary>  As an efficient image contrast enhancement (CE) tool, adaptive gamma
correction (AGC) was previously proposed by relating gamma parameter with
cumulative distribution function (CDF) of the pixel gray levels within an
image. ACG deals well with most dimmed images, but fails for globally bright
images and the dimmed images with local bright regions. Such two categories of
brightness-distorted images are universal in real scenarios, such as improper
exposure and white object regions. In order to attenuate such deficiencies,
here we propose an improved AGC algorithm. The novel strategy of negative
images is used to realize CE of the bright images, and the gamma correction
modulated by truncated CDF is employed to enhance the dimmed ones. As such,
local over-enhancement and structure distortion can be alleviated. Both
qualitative and quantitative experimental results show that our proposed method
yields consistently good CE results.
</summary>
    <author>
      <name>Gang Cao</name>
    </author>
    <author>
      <name>Lihui Huang</name>
    </author>
    <author>
      <name>Huawei Tian</name>
    </author>
    <author>
      <name>Xianglin Huang</name>
    </author>
    <author>
      <name>Yongbin Wang</name>
    </author>
    <author>
      <name>Ruicong Zhi</name>
    </author>
    <link href="http://arxiv.org/abs/1709.04427v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.04427v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.04583v3</id>
    <updated>2022-08-30T03:11:10Z</updated>
    <published>2017-09-14T01:50:27Z</published>
    <title>Acceleration of Histogram-Based Contrast Enhancement via Selective
  Downsampling</title>
    <summary>  In this paper, we propose a general framework to accelerate the universal
histogram-based image contrast enhancement (CE) algorithms. Both spatial and
gray-level selective down-sampling of digital images are adopted to decrease
computational cost, while the visual quality of enhanced images is still
preserved and without apparent degradation. Mapping function calibration is
novelly proposed to reconstruct the pixel mapping on the gray levels missed by
downsampling. As two case studies, accelerations of histogram equalization (HE)
and the state-of-the-art global CE algorithm, i.e., spatial mutual information
and PageRank (SMIRANK), are presented detailedly. Both quantitative and
qualitative assessment results have verified the effectiveness of our proposed
CE acceleration framework. In typical tests, computational efficiencies of HE
and SMIRANK have been speeded up by about 3.9 and 13.5 times, respectively.
</summary>
    <author>
      <name>Gang Cao</name>
    </author>
    <author>
      <name>Huawei Tian</name>
    </author>
    <author>
      <name>Lifang Yu</name>
    </author>
    <author>
      <name>Xianglin Huang</name>
    </author>
    <author>
      <name>Yongbin Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1709.04583v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.04583v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.05737v1</id>
    <updated>2017-09-18T01:32:45Z</updated>
    <published>2017-09-18T01:32:45Z</published>
    <title>Neural network-based arithmetic coding of intra prediction modes in HEVC</title>
    <summary>  In both H.264 and HEVC, context-adaptive binary arithmetic coding (CABAC) is
adopted as the entropy coding method. CABAC relies on manually designed
binarization processes as well as handcrafted context models, which may
restrict the compression efficiency. In this paper, we propose an arithmetic
coding strategy by training neural networks, and make preliminary studies on
coding of the intra prediction modes in HEVC. Instead of binarization, we
propose to directly estimate the probability distribution of the 35 intra
prediction modes with the adoption of a multi-level arithmetic codec. Instead
of handcrafted context models, we utilize convolutional neural network (CNN) to
perform the probability estimation. Simulation results show that our proposed
arithmetic coding leads to as high as 9.9% bits saving compared with CABAC.
</summary>
    <author>
      <name>Rui Song</name>
    </author>
    <author>
      <name>Dong Liu</name>
    </author>
    <author>
      <name>Houqiang Li</name>
    </author>
    <author>
      <name>Feng Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/VCIP.2017.8305104</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/VCIP.2017.8305104" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">VCIP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.05737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.05737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06536v3</id>
    <updated>2018-10-17T18:18:46Z</updated>
    <published>2017-09-05T19:17:46Z</published>
    <title>Adaptive Blind Image Watermarking Using Fuzzy Inference System Based on
  Human Visual Perception</title>
    <summary>  Development of digital content has increased the necessity of copyright
protection by means of watermarking. Imperceptibility and robustness are two
important features of watermarking algorithms. The goal of watermarking methods
is to satisfy the tradeoff between these two contradicting characteristics.
Recently watermarking methods in transform domains have displayed favorable
results. In this paper, we present an adaptive blind watermarking method which
has high transparency in areas that are important to human visual system. We
propose a fuzzy system for adaptive control of the embedding strength factor.
Features such as saliency, intensity, and edge-concentration, are used as fuzzy
attributes. Redundant embedding in discrete cosine transform (DCT) of wavelet
domain has increased the robustness of our method. Experimental results show
the efficiency of the proposed method and better results are obtained as
compared to comparable methods with same size of watermark logo.
</summary>
    <author>
      <name>Maedeh Jamali</name>
    </author>
    <author>
      <name>Shima Rafiei</name>
    </author>
    <author>
      <name>S. M. Reza Soroushmehr</name>
    </author>
    <author>
      <name>Nader Karimi</name>
    </author>
    <author>
      <name>Shahram Shirani</name>
    </author>
    <author>
      <name>Kayvan Najarian</name>
    </author>
    <author>
      <name>Shadrokh Samavi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3233/JIFS-171805</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3233/JIFS-171805" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.06536v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06536v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.06729v1</id>
    <updated>2017-09-20T05:14:54Z</updated>
    <published>2017-09-20T05:14:54Z</published>
    <title>A new adaptive method for hiding data in images</title>
    <summary>  LSB method is one of the well-known steganography methods which hides the
message bits into the least significant bit of pixel values. This method
changes the statistical information of images, which causes to have an
unsecured channel. To increase the security of this method against the
steganalysis methods, in this paper an adaptive method for hiding data into
images will be proposed. So, the amount of data and the method which is used
for hiding data in each area of image will be different. Experimental results
show that the security of the proposed method is higher than general LSB method
and in some cases the capacity of the carrier signal is increased.
</summary>
    <author>
      <name>Kazem Qazanfari</name>
    </author>
    <author>
      <name>Reza Safabaksh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, in Persian, Proceedings of the 6th Iranian Conference on
  Machine Vision and Image Processing, Tehran, Iran 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.06729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.06729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.08462v1</id>
    <updated>2017-09-25T12:58:15Z</updated>
    <published>2017-09-25T12:58:15Z</published>
    <title>Spatial-Temporal Residue Network Based In-Loop Filter for Video Coding</title>
    <summary>  Deep learning has demonstrated tremendous break through in the area of
image/video processing. In this paper, a spatial-temporal residue network
(STResNet) based in-loop filter is proposed to suppress visual artifacts such
as blocking, ringing in video coding. Specifically, the spatial and temporal
information is jointly exploited by taking both current block and co-located
block in reference frame into consideration during the processing of in-loop
filter. The architecture of STResNet only consists of four convolution layers
which shows hospitality to memory and coding complexity. Moreover, to fully
adapt the input content and improve the performance of the proposed in-loop
filter, coding tree unit (CTU) level control flag is applied in the sense of
rate-distortion optimization. Extensive experimental results show that our
scheme provides up to 5.1% bit-rate reduction compared to the state-of-the-art
video coding standard.
</summary>
    <author>
      <name>Chuanmin Jia</name>
    </author>
    <author>
      <name>Shiqi Wang</name>
    </author>
    <author>
      <name>Xinfeng Zhang</name>
    </author>
    <author>
      <name>Shanshe Wang</name>
    </author>
    <author>
      <name>Siwei Ma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/VCIP.2017.8305149</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/VCIP.2017.8305149" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures, accepted by VCIP2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.08462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.08462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.08763v2</id>
    <updated>2017-10-13T21:50:31Z</updated>
    <published>2017-09-26T00:37:58Z</published>
    <title>Encoding Bitrate Optimization Using Playback Statistics for HTTP-based
  Adaptive Video Streaming</title>
    <summary>  HTTP video streaming is in wide use to deliver video over the Internet. With
HTTP adaptive steaming, a video playback dynamically selects a video stream
from a pre-encoded representation based on available bandwidth and viewport
(screen) size. The viewer's video quality is therefore influenced by the
encoded bitrates. We minimize the average delivered bitrate subject to a
quality lower bound on a per-chunk basis by modeling the probability that a
player selects a particular encoding. Through simulation and real-world
experiments, the proposed method saves 9.6% of bandwidth while average
delivered video quality comparing with state of the art while keeping average
delivered video quality.
</summary>
    <author>
      <name>Chao Chen</name>
    </author>
    <author>
      <name>Yao-Chung Lin</name>
    </author>
    <author>
      <name>Anil Kokaram</name>
    </author>
    <author>
      <name>Steve Benting</name>
    </author>
    <link href="http://arxiv.org/abs/1709.08763v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.08763v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09106v1</id>
    <updated>2017-09-26T16:09:48Z</updated>
    <published>2017-09-26T16:09:48Z</published>
    <title>Region-Based Image Retrieval Revisited</title>
    <summary>  Region-based image retrieval (RBIR) technique is revisited. In early attempts
at RBIR in the late 90s, researchers found many ways to specify region-based
queries and spatial relationships; however, the way to characterize the
regions, such as by using color histograms, were very poor at that time. Here,
we revisit RBIR by incorporating semantic specification of objects and
intuitive specification of spatial relationships. Our contributions are the
following. First, to support multiple aspects of semantic object specification
(category, instance, and attribute), we propose a multitask CNN feature that
allows us to use deep learning technique and to jointly handle multi-aspect
object specification. Second, to help users specify spatial relationships among
objects in an intuitive way, we propose recommendation techniques of spatial
relationships. In particular, by mining the search results, a system can
recommend feasible spatial relationships among the objects. The system also can
recommend likely spatial relationships by assigned object category names based
on language prior. Moreover, object-level inverted indexing supports very fast
shortlist generation, and re-ranking based on spatial constraints provides
users with instant RBIR experiences.
</summary>
    <author>
      <name>Ryota Hinami</name>
    </author>
    <author>
      <name>Yusuke Matsui</name>
    </author>
    <author>
      <name>Shin'ichi Satoh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ACM Multimedia 2017 (Oral)</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.09106v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09106v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.00421v1</id>
    <updated>2017-10-01T21:51:52Z</updated>
    <published>2017-10-01T21:51:52Z</published>
    <title>Video Generation From Text</title>
    <summary>  Generating videos from text has proven to be a significant challenge for
existing generative models. We tackle this problem by training a conditional
generative model to extract both static and dynamic information from text. This
is manifested in a hybrid framework, employing a Variational Autoencoder (VAE)
and a Generative Adversarial Network (GAN). The static features, called "gist,"
are used to sketch text-conditioned background color and object layout
structure. Dynamic features are considered by transforming input text into an
image filter. To obtain a large amount of data for training the deep-learning
model, we develop a method to automatically create a matched text-video corpus
from publicly available online videos. Experimental results show that the
proposed framework generates plausible and diverse videos, while accurately
reflecting the input text information. It significantly outperforms baseline
models that directly adapt text-to-image generation procedures to produce
videos. Performance is evaluated both visually and by adapting the inception
score used to evaluate image generation in GANs.
</summary>
    <author>
      <name>Yitong Li</name>
    </author>
    <author>
      <name>Martin Renqiang Min</name>
    </author>
    <author>
      <name>Dinghan Shen</name>
    </author>
    <author>
      <name>David Carlson</name>
    </author>
    <author>
      <name>Lawrence Carin</name>
    </author>
    <link href="http://arxiv.org/abs/1710.00421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.00421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.02459v1</id>
    <updated>2017-10-06T15:47:10Z</updated>
    <published>2017-10-06T15:47:10Z</published>
    <title>Evaluation of the Performance of Adaptive HTTP Streaming Systems</title>
    <summary>  Adaptive video streaming over HTTP is becoming omnipresent in our daily life.
In the past, dozens of research papers have proposed novel approaches to
address different aspects of adaptive streaming and a decent amount of player
implementations (commercial and open source) are available. However, state of
the art evaluations are sometimes superficial as many proposals only
investigate a certain aspect of the problem or focus on a specific platform -
player implementations used in actual services are rarely considered. HTML5 is
now available on many platforms and foster the deployment of adaptive media
streaming applications. We propose a common evaluation framework for adaptive
HTML5 players and demonstrate its applicability by evaluating eight different
players which are actually deployed in real-world services.
</summary>
    <author>
      <name>Anatoliy Zabrovskiy</name>
    </author>
    <author>
      <name>Evgeny Petrov</name>
    </author>
    <author>
      <name>Evgeny Kuzmin</name>
    </author>
    <author>
      <name>Christian Timmerer</name>
    </author>
    <link href="http://arxiv.org/abs/1710.02459v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.02459v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03532v1</id>
    <updated>2017-10-10T12:25:25Z</updated>
    <published>2017-10-10T12:25:25Z</published>
    <title>Attribute Compression of 3D Point Clouds Using Laplacian Sparsity
  Optimized Graph Transform</title>
    <summary>  3D sensing and content capture have made significant progress in recent years
and the MPEG standardization organization is launching a new project on
immersive media with point cloud compression (PCC) as one key corner stone. In
this work, we introduce a new binary tree based point cloud content partition
and explore the graph signal processing tools, especially the graph transform
with optimized Laplacian sparsity, to achieve better energy compaction and
compression efficiency. The resulting rate-distortion operating points are
convex-hull optimized over the existing Lagrangian solutions. Simulation
results with the latest high quality point cloud content captured from the MPEG
PCC demonstrated the transform efficiency and rate-distortion (R-D) optimal
potential of the proposed solutions.
</summary>
    <author>
      <name>Yiting Shao</name>
    </author>
    <author>
      <name>Zhaobin Zhang</name>
    </author>
    <author>
      <name>Zhu Li</name>
    </author>
    <author>
      <name>Kui Fan</name>
    </author>
    <author>
      <name>Ge Li</name>
    </author>
    <link href="http://arxiv.org/abs/1710.03532v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03532v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.09919v2</id>
    <updated>2018-02-12T18:47:58Z</updated>
    <published>2017-10-26T21:28:57Z</published>
    <title>JND-Based Perceptual Video Coding for 4:4:4 Screen Content Data in HEVC</title>
    <summary>  The JCT-VC standardized Screen Content Coding (SCC) extension in the HEVC HM
RExt + SCM reference codec offers an impressive coding efficiency performance
when compared with HM RExt alone; however, it is not significantly perceptually
optimized. For instance, it does not include advanced HVS-based perceptual
coding methods, such as JND-based spatiotemporal masking schemes. In this
paper, we propose a novel JND-based perceptual video coding technique for HM
RExt + SCM. The proposed method is designed to further improve the compression
performance of HM RExt + SCM when applied to YCbCr 4:4:4 SC video data. In the
proposed technique, luminance masking and chrominance masking are exploited to
perceptually adjust the Quantization Step Size (QStep) at the Coding Block (CB)
level. Compared with HM RExt 16.10 + SCM 8.0, the proposed method considerably
reduces bitrates (Kbps), with a maximum reduction of 48.3%. In addition to
this, the subjective evaluations reveal that SC-PAQ achieves visually lossless
coding at very low bitrates.
</summary>
    <author>
      <name>Lee Prangnell</name>
    </author>
    <author>
      <name>Victor Sanchez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint: 2018 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP 2018)</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.09919v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.09919v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.11090v1</id>
    <updated>2017-10-30T17:37:29Z</updated>
    <published>2017-10-30T17:37:29Z</published>
    <title>Prediction of Satisfied User Ratio for Compressed Video</title>
    <summary>  A large-scale video quality dataset called the VideoSet has been constructed
recently to measure human subjective experience of H.264 coded video in terms
of the just-noticeable-difference (JND). It measures the first three JND points
of 5-second video of resolution 1080p, 720p, 540p and 360p. Based on the
VideoSet, we propose a method to predict the satisfied-user-ratio (SUR) curves
using a machine learning framework. First, we partition a video clip into local
spatial-temporal segments and evaluate the quality of each segment using the
VMAF quality index. Then, we aggregate these local VMAF measures to derive a
global one. Finally, the masking effect is incorporated and the support vector
regression (SVR) is used to predict the SUR curves, from which the JND points
can be derived. Experimental results are given to demonstrate the performance
of the proposed SUR prediction method.
</summary>
    <author>
      <name>Haiqiang Wang</name>
    </author>
    <author>
      <name>Ioannis Katsavounidis</name>
    </author>
    <author>
      <name>Qin Huang</name>
    </author>
    <author>
      <name>Xin Zhou</name>
    </author>
    <author>
      <name>C. -C. Jay Kuo</name>
    </author>
    <link href="http://arxiv.org/abs/1710.11090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.11090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00043v1</id>
    <updated>2017-11-30T19:38:13Z</updated>
    <published>2017-11-30T19:38:13Z</published>
    <title>A Color Intensity Invariant Low Level Feature Optimization Framework for
  Image Quality Assessment</title>
    <summary>  Image Quality Assessment (IQA) algorithms evaluate the perceptual quality of
an image using evaluation scores that assess the similarity or difference
between two images. We propose a new low-level feature based IQA technique,
which applies filter-bank decomposition and center-surround methodology.
Differing from existing methods, our model incorporates color intensity
adaptation and frequency scaling optimization at each filter-bank level and
spatial orientation to extract and enhance perceptually significant features.
Our computational model exploits the concept of object detection and
encapsulates characteristics proposed in other IQA algorithms in a unified
architecture. We also propose a systematic approach to review the evolution of
IQA algorithms using unbiased test datasets, instead of looking at individual
scores in isolation. Experimental results demonstrate the feasibility of our
approach.
</summary>
    <author>
      <name>Navaneeth K. Kottayil</name>
    </author>
    <author>
      <name>Irene Cheng</name>
    </author>
    <author>
      <name>Frederic Dufaux</name>
    </author>
    <author>
      <name>Anup Basu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11760-016-0873-x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11760-016-0873-x" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Signal, Image and Video Processing 10.6 (2016):1169-1176</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1712.00043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.02313v1</id>
    <updated>2017-12-06T18:30:31Z</updated>
    <published>2017-12-06T18:30:31Z</published>
    <title>DCT-domain Deep Convolutional Neural Networks for Multiple JPEG
  Compression Classification</title>
    <summary>  With the rapid advancements in digital imaging systems and networking,
low-cost hand-held image capture devices equipped with network connectivity are
becoming ubiquitous. This ease of digital image capture and sharing is also
accompanied by widespread usage of user-friendly image editing software. Thus,
we are in an era where digital images can be very easily used for the massive
spread of false information and their integrity need to be seriously
questioned. Application of multiple lossy compressions on images is an
essential part of any image editing pipeline involving lossy compressed images.
This paper aims to address the problem of classifying images based on the
number of JPEG compressions they have undergone, by utilizing deep
convolutional neural networks in DCT domain. The proposed system incorporates a
well designed pre-processing step before feeding the image data to CNN to
capture essential characteristics of compression artifacts and make the system
image content independent. Detailed experiments are performed to optimize
different aspects of the system, such as depth of CNN, number of DCT
frequencies, and execution time. Results on the standard UCID dataset
demonstrate that the proposed system outperforms existing systems for multiple
JPEG compression detection and is capable of classifying more number of
re-compression cycles then existing systems.
</summary>
    <author>
      <name>Vinay Verma</name>
    </author>
    <author>
      <name>Nikita Agarwal</name>
    </author>
    <author>
      <name>Nitin Khanna</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.image.2018.04.014</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.image.2018.04.014" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.02313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.02313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.06204v2</id>
    <updated>2018-08-22T02:03:27Z</updated>
    <published>2017-12-17T23:11:28Z</published>
    <title>Probabilistic Semantic Retrieval for Surveillance Videos with Activity
  Graphs</title>
    <summary>  We present a novel framework for finding complex activities matching
user-described queries in cluttered surveillance videos. The wide diversity of
queries coupled with unavailability of annotated activity data limits our
ability to train activity models. To bridge the semantic gap we propose to let
users describe an activity as a semantic graph with object attributes and
inter-object relationships associated with nodes and edges, respectively. We
learn node/edge-level visual predictors during training and, at test-time,
propose to retrieve activity by identifying likely locations that match the
semantic graph. We formulate a novel CRF based probabilistic activity
localization objective that accounts for mis-detections, mis-classifications
and track-losses, and outputs a likelihood score for a candidate grounded
location of the query in the video. We seek groundings that maximize overall
precision and recall. To handle the combinatorial search over all
high-probability groundings, we propose a highest precision subgraph matching
algorithm. Our method outperforms existing retrieval methods on benchmarked
datasets.
</summary>
    <author>
      <name>Yuting Chen</name>
    </author>
    <author>
      <name>Joseph Wang</name>
    </author>
    <author>
      <name>Yannan Bai</name>
    </author>
    <author>
      <name>Gregory Casta√±√≥n</name>
    </author>
    <author>
      <name>Venkatesh Saligrama</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2018.2865860</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2018.2865860" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1520-9210 (c) 2018 IEEE. This paper has been accepted by IEEE
  Transactions on Multimedia. Print ISSN: 1520-9210. Online ISSN: 1941-0077.
  Preprint link is https://ieeexplore.ieee.org/document/8438958/</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.06204v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.06204v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.06240v1</id>
    <updated>2017-12-18T03:37:36Z</updated>
    <published>2017-12-18T03:37:36Z</published>
    <title>Minimizing Embedding Distortion with Weighted Bigraph Matching in
  Reversible Data Hiding</title>
    <summary>  For a required payload, the existing reversible data hiding (RDH) methods
always expect to reduce the embedding distortion as much as possible, such as
by utilizing a well-designed predictor, taking into account the carrier-content
characteristics, and/or improving modification efficiency etc. However, due to
the diversity of natural images, it is actually very hard to accurately model
the statistical characteristics of natural images, which has limited the
practical use of traditional RDH methods that rely heavily on the content
characteristics. Based on this perspective, instead of directly exploiting the
content characteristics, in this paper, we model the embedding operation on a
weighted bipartite graph to reduce the introduced distortion due to data
embedding, which is proved to be equivalent to a graph problem called as
\emph{minimum weight maximum matching (MWMM)}. By solving the MWMM problem, we
can find the optimal histogram shifting strategy under the given condition.
Since the proposed method is essentially a general embedding model for the RDH,
it can be utilized for designing an RDH scheme. In our experiments, we
incorporate the proposed method into some related works, and, our experimental
results have shown that the proposed method can significantly improve the
payload-distortion performance, indicating that the proposed method could be
desirable and promising for practical use and the design of RDH schemes.
</summary>
    <author>
      <name>Hanzhou Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2352/ISSN.2470-1173.2020.4.MWSF-021</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2352/ISSN.2470-1173.2020.4.MWSF-021" rel="related"/>
    <link href="http://arxiv.org/abs/1712.06240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.06240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.07269v1</id>
    <updated>2017-12-20T00:00:57Z</updated>
    <published>2017-12-20T00:00:57Z</published>
    <title>Blind High Dynamic Range Quality estimation by disentangling perceptual
  and noise features in images</title>
    <summary>  Assessing the visual quality of High Dynamic Range (HDR) images is an
unexplored and an interesting research topic that has become relevant with the
current boom in HDR technology. We propose a new convolutional neural network
based model for No reference image quality assessment(NR-IQA) on HDR data. This
model predicts the amount and location of noise, perceptual influence of image
pixels on the noise, and the perceived quality, of a distorted image without
any reference image. The proposed model extracts numerical values corresponding
to the noise present in any given distorted image, and the perceptual effects
exhibited by a human eye when presented with the same. These two measures are
extracted separately yet sequentially and combined in a mixing function to
compute the quality of the distorted image perceived by a human eye. Our
training process derives the the component that computes perceptual effects
from a real world image quality dataset, rather than using results of
psycovisual experiments. With the proposed model, we demonstrate state of the
art performance for HDR NR-IQA and our results show performance similar to HDR
Full Reference Image Quality Assessment algorithms (FR-IQA).
</summary>
    <author>
      <name>Navaneeth Kamballur Kottayil</name>
    </author>
    <author>
      <name>Giuseppe Valenzise</name>
    </author>
    <author>
      <name>Frederic Dufaux</name>
    </author>
    <author>
      <name>Irene Cheng</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2017.2778570</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2017.2778570" rel="related"/>
    <link href="http://arxiv.org/abs/1712.07269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.07269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.02768v2</id>
    <updated>2018-01-14T13:33:38Z</updated>
    <published>2018-01-09T02:58:45Z</published>
    <title>Fake Colorized Image Detection</title>
    <summary>  Image forensics aims to detect the manipulation of digital images. Currently,
splicing detection, copy-move detection and image retouching detection are
drawing much attentions from researchers. However, image editing techniques
develop with time goes by. One emerging image editing technique is
colorization, which can colorize grayscale images with realistic colors.
Unfortunately, this technique may also be intentionally applied to certain
images to confound object recognition algorithms. To the best of our knowledge,
no forensic technique has yet been invented to identify whether an image is
colorized. We observed that, compared to natural images, colorized images,
which are generated by three state-of-the-art methods, possess statistical
differences for the hue and saturation channels. Besides, we also observe
statistical inconsistencies in the dark and bright channels, because the
colorization process will inevitably affect the dark and bright channel values.
Based on our observations, i.e., potential traces in the hue, saturation, dark
and bright channels, we propose two simple yet effective detection methods for
fake colorized images: Histogram based Fake Colorized Image Detection
(FCID-HIST) and Feature Encoding based Fake Colorized Image Detection
(FCID-FE). Experimental results demonstrate that both proposed methods exhibit
a decent performance against multiple state-of-the-art colorization approaches.
</summary>
    <author>
      <name>Yuanfang Guo</name>
    </author>
    <author>
      <name>Xiaochun Cao</name>
    </author>
    <author>
      <name>Wei Zhang</name>
    </author>
    <author>
      <name>Rui Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIFS.2018.2806926</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIFS.2018.2806926" rel="related"/>
    <link href="http://arxiv.org/abs/1801.02768v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.02768v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.04752v1</id>
    <updated>2018-01-15T11:48:06Z</updated>
    <published>2018-01-15T11:48:06Z</published>
    <title>Reversible Embedding to Covers Full of Boundaries</title>
    <summary>  In reversible data embedding, to avoid overflow and underflow problem, before
data embedding, boundary pixels are recorded as side information, which may be
losslessly compressed. The existing algorithms often assume that a natural
image has little boundary pixels so that the size of side information is small.
Accordingly, a relatively high pure payload could be achieved. However, there
actually may exist a lot of boundary pixels in a natural image, implying that,
the size of side information could be very large. Therefore, when to directly
use the existing algorithms, the pure embedding capacity may be not sufficient.
In order to address this problem, in this paper, we present a new and efficient
framework to reversible data embedding in images that have lots of boundary
pixels. The core idea is to losslessly preprocess boundary pixels so that it
can significantly reduce the side information. Experimental results have shown
the superiority and applicability of our work.
</summary>
    <author>
      <name>Hanzhou Wu</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <author>
      <name>Jing Dong</name>
    </author>
    <author>
      <name>Yanli Chen</name>
    </author>
    <author>
      <name>Hongxia Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-00015-8_35</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-00015-8_35" rel="related"/>
    <link href="http://arxiv.org/abs/1801.04752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.04752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.05264v2</id>
    <updated>2018-05-28T17:37:10Z</updated>
    <published>2018-01-08T05:17:11Z</published>
    <title>Adaptive Reversible Watermarking Based on Linear Prediction for Medical
  Videos</title>
    <summary>  Reversible video watermarking can guarantee that the watermark logo and the
original frame can be recovered from the watermarked frame without any
distortion. Although reversible video watermarking has successfully been
applied in multimedia, its application has not been extensively explored in
medical videos. Reversible watermarking in medical videos is still a
challenging problem. The existing reversible video watermarking algorithms,
which are based on error prediction expansion, use motion vectors for
prediction. In this study, we propose an adaptive reversible watermarking
method for medical videos. We suggest using temporal correlations for improving
the prediction accuracy. Hence, two temporal neighbor pixels in upcoming frames
are used alongside the four spatial rhombus neighboring pixels to minimize the
prediction error. To the best of our knowledge, this is the first time this
method is applied to medical videos. The method helps to protect patients'
personal and medical information by watermarking, i.e., increase the security
of Health Information Systems (HIS). Experimental results demonstrate the high
quality of the proposed watermarking method based on PSNR metric and a large
capacity for data hiding in medical videos.
</summary>
    <author>
      <name>Hamidreza Zarrabi</name>
    </author>
    <author>
      <name>Ali Emami</name>
    </author>
    <author>
      <name>Nader Karimi</name>
    </author>
    <author>
      <name>Shadrokh Samavi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Algorithms are now presented in a standard format</arxiv:comment>
    <link href="http://arxiv.org/abs/1801.05264v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.05264v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06030v1</id>
    <updated>2017-12-04T09:08:45Z</updated>
    <published>2017-12-04T09:08:45Z</published>
    <title>Multi-measures fusion based on multi-objective genetic programming for
  full-reference image quality assessment</title>
    <summary>  In this paper, we exploit the flexibility of multi-objective fitness
functions, and the efficiency of the model structure selection ability of a
standard genetic programming (GP) with the parameter estimation power of
classical regression via multi-gene genetic programming (MGGP), to propose a
new fusion technique for image quality assessment (IQA) that is called
Multi-measures Fusion based on Multi-Objective Genetic Programming (MFMOGP).
This technique can automatically select the most significant suitable measures,
from 16 full-reference IQA measures, used in aggregation and finds weights in a
weighted sum of their outputs while simultaneously optimizing for both accuracy
and complexity. The obtained well-performing fusion of IQA measures are
evaluated on four largest publicly available image databases and compared
against state-of-the-art full-reference IQA approaches. Results of comparison
reveal that the proposed approach outperforms other state-of-the-art recently
developed fusion approaches.
</summary>
    <author>
      <name>Naima Merzougui</name>
    </author>
    <author>
      <name>Naima Merzougui</name>
    </author>
    <link href="http://arxiv.org/abs/1801.06030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02774v3</id>
    <updated>2018-07-28T08:31:18Z</updated>
    <published>2018-02-08T09:53:56Z</published>
    <title>Learning to score the figure skating sports videos</title>
    <summary>  This paper targets at learning to score the figure skating sports videos. To
address this task, we propose a deep architecture that includes two
complementary components, i.e., Self-Attentive LSTM and Multi-scale
Convolutional Skip LSTM. These two components can efficiently learn the local
and global sequential information in each video. Furthermore, we present a
large-scale figure skating sports video dataset -- FisV dataset. This dataset
includes 500 figure skating videos with the average length of 2 minutes and 50
seconds. Each video is annotated by two scores of nine different referees,
i.e., Total Element Score(TES) and Total Program Component Score (PCS). Our
proposed model is validated on FisV and MIT-skate datasets. The experimental
results show the effectiveness of our models in learning to score the figure
skating videos.
</summary>
    <author>
      <name>Chengming Xu</name>
    </author>
    <author>
      <name>Yanwei Fu</name>
    </author>
    <author>
      <name>Bing Zhang</name>
    </author>
    <author>
      <name>Zitian Chen</name>
    </author>
    <author>
      <name>Yu-Gang Jiang</name>
    </author>
    <author>
      <name>Xiangyang Xue</name>
    </author>
    <link href="http://arxiv.org/abs/1802.02774v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02774v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.03937v1</id>
    <updated>2018-02-12T08:51:04Z</updated>
    <published>2018-02-12T08:51:04Z</published>
    <title>Compression for Multiple Reconstructions</title>
    <summary>  In this work we propose a method for optimizing the lossy compression for a
network of diverse reconstruction systems. We focus on adapting a standard
image compression method to a set of candidate displays, presenting the
decompressed signals to viewers. Each display is modeled as a linear operator
applied after decompression, and its probability to serve a network user. We
formulate a complicated operational rate-distortion optimization trading-off
the network's expected mean-squared reconstruction error and the compression
bit-cost. Using the alternating direction method of multipliers (ADMM) we
develop an iterative procedure where the network structure is separated from
the compression method, enabling the reliance on standard compression
techniques. We present experimental results showing our method to be the best
approach for adjusting high bit-rate image compression (using the
state-of-the-art HEVC standard) to a set of displays modeled as blur
degradations.
</summary>
    <author>
      <name>Yehuda Dar</name>
    </author>
    <author>
      <name>Michael Elad</name>
    </author>
    <author>
      <name>Alfred M. Bruckstein</name>
    </author>
    <link href="http://arxiv.org/abs/1802.03937v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.03937v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05178v1</id>
    <updated>2018-02-14T16:08:09Z</updated>
    <published>2018-02-14T16:08:09Z</published>
    <title>Similarity measures for vocal-based drum sample retrieval using deep
  convolutional auto-encoders</title>
    <summary>  The expressive nature of the voice provides a powerful medium for
communicating sonic ideas, motivating recent research on methods for query by
vocalisation. Meanwhile, deep learning methods have demonstrated
state-of-the-art results for matching vocal imitations to imitated sounds, yet
little is known about how well learned features represent the perceptual
similarity between vocalisations and queried sounds. In this paper, we address
this question using similarity ratings between vocal imitations and imitated
drum sounds. We use a linear mixed effect regression model to show how features
learned by convolutional auto-encoders (CAEs) perform as predictors for
perceptual similarity between sounds. Our experiments show that CAEs outperform
three baseline feature sets (spectrogram-based representations, MFCCs, and
temporal features) at predicting the subjective similarity ratings. We also
investigate how the size and shape of the encoded layer effects the predictive
power of the learned features. The results show that preservation of temporal
information is more important than spectral resolution for this application.
</summary>
    <author>
      <name>Adib Mehrabi</name>
    </author>
    <author>
      <name>Keunwoo Choi</name>
    </author>
    <author>
      <name>Simon Dixon</name>
    </author>
    <author>
      <name>Mark Sandler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICASSP 2018 camera-ready</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.05178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.00303v2</id>
    <updated>2018-05-29T14:33:33Z</updated>
    <published>2018-03-01T10:59:20Z</published>
    <title>Classifying flows and buffer state for YouTube's HTTP adaptive streaming
  service in mobile networks</title>
    <summary>  Accurate cross-layer information is very useful to optimize mobile networks
for specific applications. However, providing application-layer information to
lower protocol layers has become very difficult due to the wide adoption of
end-to-end encryption and due to the absence of cross-layer signaling
standards. As an alternative, this paper presents a traffic profiling solution
to passively estimate parameters of HTTP Adaptive Streaming (HAS) applications
at the lower layers. By observing IP packet arrivals, our machine learning
system identifies video flows and detects the state of an HAS client's
play-back buffer in real time. Our experiments with YouTube's mobile client
show that Random Forests achieve very high accuracy even with a strong
variation of link quality. Since this high performance is achieved at IP level
with a small, generic feature set, our approach requires no Deep Packet
Inspection (DPI), comes at low complexity, and does not interfere with
end-to-end encryption. Traffic profiling is, thus, a powerful new tool for
monitoring and managing even encrypted HAS traffic in mobile networks.
</summary>
    <author>
      <name>Dimitrios Tsilimantos</name>
    </author>
    <author>
      <name>Theodoros Karagkioules</name>
    </author>
    <author>
      <name>Stefan Valentin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 12 figures. Accepted for publication in the proceedings of
  ACM Multimedia Systems Conference (MMSys) 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.00303v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.00303v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.02280v1</id>
    <updated>2018-03-06T16:19:08Z</updated>
    <published>2018-03-06T16:19:08Z</published>
    <title>ART-UP: A Novel Method for Generating Scanning-robust Aesthetic QR codes</title>
    <summary>  QR codes are usually scanned in different environments, so they must be
robust to variations in illumination, scale, coverage, and camera angles.
Aesthetic QR codes improve the visual quality, but subtle changes in their
appearance may cause scanning failure. In this paper, a new method to generate
scanning-robust aesthetic QR codes is proposed, which is based on a
module-based scanning probability estimation model that can effectively balance
the tradeoff between visual quality and scanning robustness. Our method locally
adjusts the luminance of each module by estimating the probability of
successful sampling. The approach adopts the hierarchical, coarse-to-fine
strategy to enhance the visual quality of aesthetic QR codes, which
sequentially generate the following three codes: a binary aesthetic QR code, a
grayscale aesthetic QR code, and the final color aesthetic QR code. Our
approach also can be used to create QR codes with different visual styles by
adjusting some initialization parameters. User surveys and decoding experiments
were adopted for evaluating our method compared with state-of-the-art
algorithms, which indicates that the proposed approach has excellent
performance in terms of both visual quality and scanning robustness.
</summary>
    <author>
      <name>Mingliang Xu</name>
    </author>
    <author>
      <name>Qingfeng Li</name>
    </author>
    <author>
      <name>Jianwei Niu</name>
    </author>
    <author>
      <name>Xiting Liu</name>
    </author>
    <author>
      <name>Weiwei Xu</name>
    </author>
    <author>
      <name>Pei Lv</name>
    </author>
    <author>
      <name>Bing Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.02280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.02280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.04053v1</id>
    <updated>2018-03-11T22:01:37Z</updated>
    <published>2018-03-11T22:01:37Z</published>
    <title>Learning Local Distortion Visibility From Image Quality Data-sets</title>
    <summary>  Accurate prediction of local distortion visibility thresholds is critical in
many image and video processing applications. Existing methods require an
accurate modeling of the human visual system, and are derived through
pshycophysical experiments with simple, artificial stimuli. These approaches,
however, are difficult to generalize to natural images with complex types of
distortion. In this paper, we explore a different perspective, and we
investigate whether it is possible to learn local distortion visibility from
image quality scores. We propose a convolutional neural network based
optimization framework to infer local detection thresholds in a distorted
image. Our model is trained on multiple quality datasets, and the results are
correlated with empirical visibility thresholds collected on complex stimuli in
a recent study. Our results are comparable to state-of-the-art mathematical
models that were trained on phsycovisual data directly. This suggests that it
is possible to predict psychophysical phenomena from visibility information
embedded in image quality scores.
</summary>
    <author>
      <name>Navaneeth Kamballur Kottayil</name>
    </author>
    <author>
      <name>Giuseppe Valenzise</name>
    </author>
    <author>
      <name>Frederic Dufaux</name>
    </author>
    <author>
      <name>Irene Cheng</name>
    </author>
    <link href="http://arxiv.org/abs/1803.04053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.04749v3</id>
    <updated>2019-02-08T15:32:33Z</updated>
    <published>2018-03-13T12:32:52Z</published>
    <title>Robust Contrast Enhancement Forensics Using Pixel and Histogram Domain
  CNNs</title>
    <summary>  Contrast enhancement (CE) forensics has always been ofconcern to image
forensics community. It can provide aneffective tool for recovering image
history and identifyingtampered images. Although several CE forensic
algorithmshave been proposed, their robustness against some processingis still
unsatisfactory, such as JPEG compression and anti-forensic attacks. In order to
attenuate such deficiency, inthis paper we first present a discriminability
analysis of CEforensics in pixel and gray level histogram domains. Then, insuch
two domains, two end-to-end methods based on convo-lutional neural networks
(P-CNN, H-CNN) are proposed toachieve robust CE forensics against pre-JPEG
compressionand anti-forensics attacks. Experimental results show that
theproposed methods achieve much better performance than thestate-of-the-art
schemes for CE detection in the case of noother operation and comparable
performance when pre-JPEGcompression and anti-foresics attacks is used.
</summary>
    <author>
      <name>Pengpeng Yang</name>
    </author>
    <author>
      <name>Rongrong Ni</name>
    </author>
    <author>
      <name>Yao Zhao</name>
    </author>
    <author>
      <name>Gang Cao</name>
    </author>
    <author>
      <name>Wei Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, submitted to ICME2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.04749v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04749v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.04966v1</id>
    <updated>2018-03-13T06:23:39Z</updated>
    <published>2018-03-13T06:23:39Z</published>
    <title>An Improvement Technique based on Structural Similarity Thresholding for
  Digital Watermarking</title>
    <summary>  Digital watermarking is extensively used in ownership authentication and
copyright protection. In this paper, we propose an efficient thresholding
scheme to improve the watermark embedding procedure in an image. For the
proposed algorithm, watermark casting is performed separately in each block of
an image, and embedding in each block continues until a certain structural
similarity threshold is reached. Numerical evaluations demonstrate that our
scheme improves the imperceptibility of the watermark when the capacity remains
fix, and at the same time, robustness against attacks is assured. The proposed
method is applicable to most image watermarking algorithms. We verify this
issue on watermarking schemes in Discrete Cosine Transform (DCT), wavelet, and
spatial domain.
</summary>
    <author>
      <name>Amin Banitalebi-Dehkordi</name>
    </author>
    <author>
      <name>Mehdi Banitalebi-Dehkordi</name>
    </author>
    <author>
      <name>Jamshid Abouei</name>
    </author>
    <author>
      <name>Said Nader-Esfahani</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACENG, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1803.04966v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.04966v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.05747v1</id>
    <updated>2018-03-15T13:50:04Z</updated>
    <published>2018-03-15T13:50:04Z</published>
    <title>Joint Rate Allocation with Both Look-ahead And Feedback Model For High
  Efficiency Video Coding</title>
    <summary>  The objective of joint rate allocation among multiple coded video streams is
to share the bandwidth to meet the demands of minimum average distortion
(minAVE) or minimum distortion variance (minVAR). In previous works on minVAR
problems, bits are directly assigned in proportion to their complexity measures
and we call it look-ahead allocation model (LAM), which leads to the fact that
the performance will totally depend on the accuracy of the complexity measures.
This paper proposes a look-ahead and feedback allocation model (LFAM) for joint
rate allocation for High Efficiency Video Coding (HEVC) platform which requires
negligible computational cost. We derive the model from the target function of
minVAR theoretically. The bits are assigned according to the complexity
measures, the distortion and bitrate values fed back by the encoder together.
We integrated the proposed allocation model in HEVC reference software HM16.0
and several complexity measures were applied to our allocation model. Results
demonstrate that our proposed LFAM performs better than LAM, and an average of
65.94% variance of mean square error (MSE) is saved with different complexity
measures.
</summary>
    <author>
      <name>Hongfei Fan</name>
    </author>
    <author>
      <name>Lin Ding</name>
    </author>
    <author>
      <name>Xiaodong Xie</name>
    </author>
    <author>
      <name>Huizhu Jia</name>
    </author>
    <author>
      <name>Wen Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1803.05747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.05747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.06874v1</id>
    <updated>2018-03-19T11:32:36Z</updated>
    <published>2018-03-19T11:32:36Z</published>
    <title>Multi-Codec DASH Dataset</title>
    <summary>  The number of bandwidth-hungry applications and services is constantly
growing. HTTP adaptive streaming of audio-visual content accounts for the
majority of today's internet traffic. Although the internet bandwidth increases
also constantly, audio-visual compression technology is inevitable and we are
currently facing the challenge to be confronted with multiple video codecs.
This paper proposes a multi-codec DASH dataset comprising AVC, HEVC, VP9, and
AV1 in order to enable interoperability testing and streaming experiments for
the efficient usage of these codecs under various conditions. We adopt state of
the art encoding and packaging options and also provide basic quality metrics
along with the DASH segments. Additionally, we briefly introduce a multi-codec
DASH scheme and possible usage scenarios. Finally, we provide a preliminary
evaluation of the encoding efficiency in the context of HTTP adaptive streaming
services and applications.
</summary>
    <author>
      <name>Anatoliy Zabrovskiy</name>
    </author>
    <author>
      <name>Christian Feldmann</name>
    </author>
    <author>
      <name>Christian Timmerer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, submitted to ACM MMSys'18 (dataset track)</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.06874v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.06874v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.07789v1</id>
    <updated>2018-03-21T08:07:12Z</updated>
    <published>2018-03-21T08:07:12Z</published>
    <title>QoE-Oriented Resource Allocation for 360-degree Video Transmission over
  Heterogeneous Networks</title>
    <summary>  Immersive media streaming, especially virtual reality (VR)/360-degree video
streaming which is very bandwidth demanding, has become more and more popular
due to the rapid growth of the multimedia and networking deployments. To better
explore the usage of resource and achieve better quality of experience (QoE)
perceived by users, this paper develops an application-layer scheme to jointly
exploit the available bandwidth from the LTE and Wi-Fi networks in 360-degree
video streaming. This newly proposed scheme and the corresponding solution
algorithms utilize the saliency of video, prediction of users' view and the
status information of users to obtain an optimal association of the users with
different Wi-Fi access points (APs) for maximizing the system's utility.
Besides, a novel buffer strategy is proposed to mitigate the influence of
short-time prediction problem for transmitting 360-degree videos in
time-varying networks. The promising performance and low complexity of the
proposed scheme and algorithms are validated in simulations with various
360-degree videos.
</summary>
    <author>
      <name>Wei Huang</name>
    </author>
    <author>
      <name>Lianghui Ding</name>
    </author>
    <author>
      <name>Hung-Yu Wei</name>
    </author>
    <author>
      <name>Jenq-Neng Hwang</name>
    </author>
    <author>
      <name>Yiling Xu</name>
    </author>
    <author>
      <name>Wenjun Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to Digital Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.07789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.07789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.09219v2</id>
    <updated>2018-05-11T03:31:40Z</updated>
    <published>2018-03-25T09:22:03Z</published>
    <title>Digital Cardan Grille: A Modern Approach for Information Hiding</title>
    <summary>  In this paper, a new framework for construction of Cardan grille for
information hiding is proposed. Based on the semantic image inpainting
technique, the stego image are driven by secret messages directly. A mask
called Digital Cardan Grille (DCG) for determining the hidden location is
introduced to hide the message. The message is written to the corrupted region
that needs to be filled in the corrupted image in advance. Then the corrupted
image with secret message is feeded into a Generative Adversarial Network (GAN)
for semantic completion. The adversarial game not only reconstruct the
corrupted image , but also generate a stego image which contains the logic
rationality of image content. The experimental results verify the feasibility
of the proposed method.
</summary>
    <author>
      <name>Jia Liu</name>
    </author>
    <author>
      <name>Tanping Zhou</name>
    </author>
    <author>
      <name>Zhuo Zhang</name>
    </author>
    <author>
      <name>Yan Ke</name>
    </author>
    <author>
      <name>Yu Lei</name>
    </author>
    <author>
      <name>Minqing Zhang</name>
    </author>
    <author>
      <name>Xiaoyuan Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1803.09219v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09219v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.11286v1</id>
    <updated>2018-03-29T23:25:15Z</updated>
    <published>2018-03-29T23:25:15Z</published>
    <title>High Capacity Image Data Hiding of Scanned Text Documents Using Improved
  Quadtree</title>
    <summary>  In this paper, an effective method was introduced to steganography of text
document in the host image. In the available steganography methods, the message
has a random form. Therefore, the embedding capacity is generally low. In the
proposed method, the main underlying idea was the sparse property of scanned
documents. The scanned documents were converted from gray-level form to binary
values by halftoning idea and then the information-included parts were
extracted using the improved quadtree and separated from document context.
Next, in order to compress the extracted parts, an algorithm was proposed based
on reading the binary string bits, ignoring the zero behind the number, and
converting them to decimal values. Embedding capacity of the proposed method is
higher than that of other available methods with a random-based message.
Therefore, the proposed method can be used in the secure and intangible
transfer of text documents in the host image.
</summary>
    <author>
      <name>Seyyed Hossein Soleymani</name>
    </author>
    <author>
      <name>Amir Hossein Taherinia</name>
    </author>
    <link href="http://arxiv.org/abs/1803.11286v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.11286v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.00589v1</id>
    <updated>2018-03-15T12:07:25Z</updated>
    <published>2018-03-15T12:07:25Z</published>
    <title>Image Compression Using Proposed Enhanced Run Length Encoding Algorithm</title>
    <summary>  In this paper, we will present p roposed enhance process of image compression
by using RLE algorithm. This proposed yield to decrease the size of compressing
image, but the original method used primarily for compressing a binary images
[1].Which will yield increasing the size of an original image mostly when used
for color images. The test of an enhanced algorithm is performed on sample
consists of ten BMP 24-bit true color images, building an application by using
visual basic 6.0 to show the size after and before compression process and
computing the compression ratio for RLE and for the enhanced RLE algorithm.
</summary>
    <author>
      <name>Ali H. Husseen Al-nuaimi</name>
    </author>
    <author>
      <name>Shyamaa Shakir Al-juboori</name>
    </author>
    <author>
      <name>R. J. Mohammed</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IBN AL- HAITHAM J. FOR PURE &amp; APPL. SCI. VOL.24 (1) 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1804.00589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.00589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02680v1</id>
    <updated>2018-04-08T12:18:14Z</updated>
    <published>2018-04-08T12:18:14Z</published>
    <title>Semi-fragile Tamper Detection and Recovery based on Region
  Categorization and Two-Sided Circular Block Dependency</title>
    <summary>  This paper presents a new semi-fragile algorithm for image tamper detection
and recovery, which is based on region attention and two-sided circular block
dependency. This method categorizes the image blocks into three categories
according to their texture. In this method, less information is extracted from
areas with the smooth texture, and more information is extracted from areas
with the rough texture. Also, the extracted information for each type of blocks
is embedded in another block with the same type. So, changes in the smooth
areas are invisible to Human Visual System. To increase the localization power
a two-sided circular block dependency is proposed, which is able to distinguish
partially destroyed blocks. Pairwise block dependency and circular block
dependency, which are common methods in the block-based tamper detection, are
not able to distinguish the partially destroyed blocks. Cubic interpolation is
used in order to decrease the blocking effects in the recovery phase. The
results of the proposed method for regions with different texture show that the
proposed method is superior to non-region-attention based methods.
</summary>
    <author>
      <name>Seyyed Hossein Soleymani</name>
    </author>
    <link href="http://arxiv.org/abs/1804.02680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.02691v3</id>
    <updated>2019-06-01T12:31:31Z</updated>
    <published>2018-04-08T13:58:40Z</published>
    <title>Adaptive Spatial Steganography Based on Probability-Controlled
  Adversarial Examples</title>
    <summary>  Explanation from Sai Ma: The experiments in this paper are conducted on Caffe
framework. In Caffe, there is an API to directly set the gradient in Matlab. I
wrongly use it to control the 'probability', in fact, I modify the gradient
directly. The misusage of API leads to wrong experiment results, and wrong
theoretical analysis.
  Apologize to readers who have read this paper. We have submitted a correct
version of this paper to Multimedia Tools and Applications and it is under
revision.
  Thanks to Dr. Patrick Bas, who is the Associate Editor of TIFS and the
anonymous reviewers of this paper.
  Thanks to Tingting Song from Sun Yat-sen University. We discussed some
problems of this paper. Her advice helps me to improve the submitted paper to
Multimedia Tools and Applications.
</summary>
    <author>
      <name>Sai Ma</name>
    </author>
    <author>
      <name>Qingxiao Guan</name>
    </author>
    <author>
      <name>Xianfeng Zhao</name>
    </author>
    <author>
      <name>Yaqi Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper contains some problems in theoretical analysis and
  experiment</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.02691v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.02691v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03481v1</id>
    <updated>2018-04-10T12:31:44Z</updated>
    <published>2018-04-10T12:31:44Z</published>
    <title>DeepQoE: A unified Framework for Learning to Predict Video QoE</title>
    <summary>  Motivated by the prowess of deep learning (DL) based techniques in
prediction, generalization, and representation learning, we develop a novel
framework called DeepQoE to predict video quality of experience (QoE). The
end-to-end framework first uses a combination of DL techniques (e.g., word
embeddings) to extract generalized features. Next, these features are combined
and fed into a neural network for representation learning. Such representations
serve as inputs for classification or regression tasks. Evaluating the
performance of DeepQoE with two datasets, we show that for the small dataset,
the accuracy of all shallow learning algorithm is improved by using the
representation derived from DeepQoE. For the large dataset, our DeepQoE
framework achieves significant performance improvement in comparison to the
best baseline method (90.94% vs. 82.84%). Moreover, DeepQoE, also released as
an open source tool, provides video QoE research much-needed flexibility in
fitting different datasets, extracting generalized features, and learning
representations.
</summary>
    <author>
      <name>Huaizheng Zhang</name>
    </author>
    <author>
      <name>Han Hu</name>
    </author>
    <author>
      <name>Guanyu Gao</name>
    </author>
    <author>
      <name>Yonggang Wen</name>
    </author>
    <author>
      <name>Kyle Guan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures, ICME2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.03481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.03809v1</id>
    <updated>2018-04-11T04:51:25Z</updated>
    <published>2018-04-11T04:51:25Z</published>
    <title>Demoir√©ing of Camera-Captured Screen Images Using Deep Convolutional
  Neural Network</title>
    <summary>  Taking photos of optoelectronic displays is a direct and spontaneous way of
transferring data and keeping records, which is widely practiced. However, due
to the analog signal interference between the pixel grids of the display screen
and camera sensor array, objectionable moir\'e (alias) patterns appear in
captured screen images. As the moir\'e patterns are structured and highly
variant, they are difficult to be completely removed without affecting the
underneath latent image. In this paper, we propose an approach of deep
convolutional neural network for demoir\'eing screen photos. The proposed DCNN
consists of a coarse-scale network and a fine-scale network. In the
coarse-scale network, the input image is first downsampled and then processed
by stacked residual blocks to remove the moir\'e artifacts. After that, the
fine-scale network upsamples the demoir\'ed low-resolution image back to the
original resolution. Extensive experimental results have demonstrated that the
proposed technique can efficiently remove the moir\'e patterns for camera
acquired screen images; the new technique outperforms the existing ones.
</summary>
    <author>
      <name>Bolin Liu</name>
    </author>
    <author>
      <name>Xiao Shu</name>
    </author>
    <author>
      <name>Xiaolin Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1804.03809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.03809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04866v1</id>
    <updated>2018-04-13T09:59:54Z</updated>
    <published>2018-04-13T09:59:54Z</published>
    <title>The PS-Battles Dataset - an Image Collection for Image Manipulation
  Detection</title>
    <summary>  The boost of available digital media has led to a significant increase in
derivative work. With tools for manipulating objects becoming more and more
mature, it can be very difficult to determine whether one piece of media was
derived from another one or tampered with. As derivations can be done with
malicious intent, there is an urgent need for reliable and easily usable
tampering detection methods. However, even media considered semantically
untampered by humans might have already undergone compression steps or light
post-processing, making automated detection of tampering susceptible to false
positives. In this paper, we present the PS-Battles dataset which is gathered
from a large community of image manipulation enthusiasts and provides a basis
for media derivation and manipulation detection in the visual domain. The
dataset consists of 102'028 images grouped into 11'142 subsets, each containing
the original image as well as a varying number of manipulated derivatives.
</summary>
    <author>
      <name>Silvan Heller</name>
    </author>
    <author>
      <name>Luca Rossetto</name>
    </author>
    <author>
      <name>Heiko Schuldt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The dataset introduced in this paper can be found on
  https://github.com/dbisUnibas/PS-Battles</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.04866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.05490v1</id>
    <updated>2018-04-16T03:17:29Z</updated>
    <published>2018-04-16T03:17:29Z</published>
    <title>A survey of comics research in computer science</title>
    <summary>  Graphical novels such as comics and mangas are well known all over the world.
The digital transition started to change the way people are reading comics,
more and more on smartphones and tablets and less and less on paper. In the
recent years, a wide variety of research about comics has been proposed and
might change the way comics are created, distributed and read in future years.
Early work focuses on low level document image analysis: indeed comic books are
complex, they contains text, drawings, balloon, panels, onomatopoeia, etc.
Different fields of computer science covered research about user interaction
and content generation such as multimedia, artificial intelligence,
human-computer interaction, etc. with different sets of values. We propose in
this paper to review the previous research about comics in computer science, to
state what have been done and to give some insights about the main outlooks.
</summary>
    <author>
      <name>Olivier Augereau</name>
    </author>
    <author>
      <name>Motoi Iwata</name>
    </author>
    <author>
      <name>Koichi Kise</name>
    </author>
    <link href="http://arxiv.org/abs/1804.05490v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.05490v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06057v1</id>
    <updated>2018-04-17T06:03:14Z</updated>
    <published>2018-04-17T06:03:14Z</published>
    <title>Multimodal Co-Training for Selecting Good Examples from Webly Labeled
  Video</title>
    <summary>  We tackle the problem of learning concept classifiers from videos on the web
without using manually labeled data. Although metadata attached to videos
(e.g., video titles, descriptions) can be of help collecting training data for
the target concept, the collected data is often very noisy. The main challenge
is therefore how to select good examples from noisy training data. Previous
approaches firstly learn easy examples that are unlikely to be noise and then
gradually learn more complex examples. However, hard examples that are much
different from easy ones are never learned. In this paper, we propose an
approach called multimodal co-training (MMCo) for selecting good examples from
noisy training data. MMCo jointly learns classifiers for multiple modalities
that complement each other to select good examples. Since MMCo selects examples
by consensus of multimodal classifiers, a hard example for one modality can
still be used as a training example by exploiting the power of the other
modalities. The algorithm is very simple and easily implemented but yields
consistent and significant boosts in example selection and classification
performance on the FCVID and YouTube8M benchmarks.
</summary>
    <author>
      <name>Ryota Hinami</name>
    </author>
    <author>
      <name>Junwei Liang</name>
    </author>
    <author>
      <name>Shin'ichi Satoh</name>
    </author>
    <author>
      <name>Alexander Hauptmann</name>
    </author>
    <link href="http://arxiv.org/abs/1804.06057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06628v2</id>
    <updated>2019-04-28T13:26:31Z</updated>
    <published>2018-04-18T10:04:40Z</published>
    <title>Reversible Video Data Hiding Using Zero QDCT Coefficient-Pairs</title>
    <summary>  H.264/Advanced Video Coding (AVC) is one of the most commonly used video
compression standard currently. In this paper, we propose a Reversible Data
Hiding (RDH) method based on H.264/AVC videos. In the proposed method, the
macroblocks with intra-frame $4\times 4$ prediction modes in intra frames are
first selected as embeddable blocks. Then, the last zero Quantized Discrete
Cosine Transform (QDCT) coefficients in all $4\times 4$ blocks of the
embeddable macroblocks are paired. In the following, a modification mapping
rule based on making full use of modification directions are given. Finally,
each zero coefficient-pair is changed by combining the given mapping rule with
the to-be-embedded information bits. Since most of last QDCT coefficients in
all $4\times 4$ blocks are zero and they are located in high frequency area.
Therefore, the proposed method can obtain high embedding capacity and low
distortion.
</summary>
    <author>
      <name>Yi Chen</name>
    </author>
    <author>
      <name>Hongxia Wang</name>
    </author>
    <author>
      <name>Hanzhou Wu</name>
    </author>
    <author>
      <name>Yong Liu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11042-019-7635-z</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11042-019-7635-z" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 10 figures, accepted by Multimedia Tools and Applications</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Multimedia Tools and Applications, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1804.06628v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06628v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.07939v1</id>
    <updated>2018-04-21T10:35:33Z</updated>
    <published>2018-04-21T10:35:33Z</published>
    <title>Spatial Image Steganography Based on Generative Adversarial Network</title>
    <summary>  With the recent development of deep learning on steganalysis, embedding
secret information into digital images faces great challenges. In this paper, a
secure steganography algorithm by using adversarial training is proposed. The
architecture contain three component modules: a generator, an embedding
simulator and a discriminator. A generator based on U-NET to translate a cover
image into an embedding change probability is proposed. To fit the optimal
embedding simulator and propagate the gradient, a function called
Tanh-simulator is proposed. As for the discriminator, the selection-channel
awareness (SCA) is incorporated to resist the SCA based steganalytic methods.
Experimental results have shown that the proposed framework can increase the
security performance dramatically over the recently reported method ASDL-GAN,
while the training time is only 30% of that used by ASDL-GAN. Furthermore, it
also performs better than the hand-crafted steganographic algorithm S-UNIWARD.
</summary>
    <author>
      <name>Jianhua Yang</name>
    </author>
    <author>
      <name>Kai Liu</name>
    </author>
    <author>
      <name>Xiangui Kang</name>
    </author>
    <author>
      <name>Edward K. Wong</name>
    </author>
    <author>
      <name>Yun-Qing Shi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.07939v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.07939v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.09539v1</id>
    <updated>2018-04-25T13:22:38Z</updated>
    <published>2018-04-25T13:22:38Z</published>
    <title>Cross-media Multi-level Alignment with Relation Attention Network</title>
    <summary>  With the rapid growth of multimedia data, such as image and text, it is a
highly challenging problem to effectively correlate and retrieve the data of
different media types. Naturally, when correlating an image with textual
description, people focus on not only the alignment between discriminative
image regions and key words, but also the relations lying in the visual and
textual context. Relation understanding is essential for cross-media
correlation learning, which is ignored by prior cross-media retrieval works. To
address the above issue, we propose Cross-media Relation Attention Network
(CRAN) with multi-level alignment. First, we propose visual-language relation
attention model to explore both fine-grained patches and their relations of
different media types. We aim to not only exploit cross-media fine-grained
local information, but also capture the intrinsic relation information, which
can provide complementary hints for correlation learning. Second, we propose
cross-media multi-level alignment to explore global, local and relation
alignments across different media types, which can mutually boost to learn more
precise cross-media correlation. We conduct experiments on 2 cross-media
datasets, and compare with 10 state-of-the-art methods to verify the
effectiveness of proposed approach.
</summary>
    <author>
      <name>Jinwei Qi</name>
    </author>
    <author>
      <name>Yuxin Peng</name>
    </author>
    <author>
      <name>Yuxin Yuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, accepted by International Joint Conference on Artificial
  Intelligence (IJCAI) 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.09539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.09539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.09869v2</id>
    <updated>2019-01-09T09:49:58Z</updated>
    <published>2018-04-26T03:16:48Z</published>
    <title>Learning for Video Compression</title>
    <summary>  One key challenge to learning-based video compression is that motion
predictive coding, a very effective tool for video compression, can hardly be
trained into a neural network. In this paper we propose the concept of
PixelMotionCNN (PMCNN) which includes motion extension and hybrid prediction
networks. PMCNN can model spatiotemporal coherence to effectively perform
predictive coding inside the learning network. On the basis of PMCNN, we
further explore a learning-based framework for video compression with
additional components of iterative analysis/synthesis, binarization, etc.
Experimental results demonstrate the effectiveness of the proposed scheme.
Although entropy coding and complex configurations are not employed in this
paper, we still demonstrate superior performance compared with MPEG-2 and
achieve comparable results with H.264 codec. The proposed learning-based scheme
provides a possible new direction to further improve compression efficiency and
functionalities of future video coding.
</summary>
    <author>
      <name>Zhibo Chen</name>
    </author>
    <author>
      <name>Tianyu He</name>
    </author>
    <author>
      <name>Xin Jin</name>
    </author>
    <author>
      <name>Feng Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCSVT.2019.2892608</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCSVT.2019.2892608" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by IEEE Transactions on Circuits and Systems for Video
  Technology (TCSVT)</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.09869v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.09869v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.10878v2</id>
    <updated>2019-04-08T22:49:22Z</updated>
    <published>2018-04-29T05:54:55Z</published>
    <title>Dynamic Adaptive Point Cloud Streaming</title>
    <summary>  High-quality point clouds have recently gained interest as an emerging form
of representing immersive 3D graphics. Unfortunately, these 3D media are bulky
and severely bandwidth intensive, which makes it difficult for streaming to
resource-limited and mobile devices. This has called researchers to propose
efficient and adaptive approaches for streaming of high-quality point clouds.
  In this paper, we run a pilot study towards dynamic adaptive point cloud
streaming, and extend the concept of dynamic adaptive streaming over HTTP
(DASH) towards DASH-PC, a dynamic adaptive bandwidth-efficient and view-aware
point cloud streaming system. DASH-PC can tackle the huge bandwidth demands of
dense point cloud streaming while at the same time can semantically link to
human visual acuity to maintain high visual quality when needed. In order to
describe the various quality representations, we propose multiple thinning
approaches to spatially sub-sample point clouds in the 3D space, and design a
DASH Media Presentation Description manifest specific for point cloud
streaming. Our initial evaluations show that we can achieve significant
bandwidth and performance improvement on dense point cloud streaming with minor
negative quality impacts compared to the baseline scenario when no adaptations
is applied.
</summary>
    <author>
      <name>Mohammad Hosseini</name>
    </author>
    <author>
      <name>Christian Timmerer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3210424.3210429</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3210424.3210429" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 23rd ACM Packet Video (PV'18) Workshop, June 12--15, 2018,
  Amsterdam, Netherlands</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.10878v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10878v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.11240v1</id>
    <updated>2018-03-30T13:13:04Z</updated>
    <published>2018-03-30T13:13:04Z</published>
    <title>A blind robust watermarking method based on Arnold Cat map and amplified
  pseudo-noise strings with weak correlation</title>
    <summary>  In this paper, a robust and blind watermarking method is proposed, which is
highly resistant to the common image watermarking attacks, such as noises,
compression, and image quality enhancement processing. In this method, Arnold
Cat map is used as a pre-processing on the host image, which increases the
security and imperceptibility of embedding watermark bits with a strong gain
factor. Moreover, two pseudo-noise strings with weak correlation are used as
the symbol of each 0 or 1 bit of the watermark, which increases the accuracy in
detecting the state of watermark bits at extraction phase in comparison to
using two random pseudo-noise strings. In this method, to increase the
robustness and further imperceptibility of the embedding, the Arnold Cat mapped
image is subjected to non-overlapping blocking, and then the high frequency
coefficients of the approximation sub-band of the FDCuT transform are used as
the embedding location for each block. Comparison of the proposed method with
recent robust methods under the same experimental conditions indicates the
superiority of the proposed method.
</summary>
    <author>
      <name>Seyyed Hossein Soleymani</name>
    </author>
    <author>
      <name>Amir Hossein Taherinia</name>
    </author>
    <author>
      <name>Amir Hossein Mohajerzadeh</name>
    </author>
    <link href="http://arxiv.org/abs/1804.11240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.11240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00263v2</id>
    <updated>2018-07-20T16:04:05Z</updated>
    <published>2018-06-01T10:06:51Z</published>
    <title>A Revision Control System for Image Editing in Collaborative Multimedia
  Design</title>
    <summary>  Revision control is a vital component in the collaborative development of
artifacts such as software code and multimedia. While revision control has been
widely deployed for text files, very few attempts to control the versioning of
binary files can be found in the literature. This can be inconvenient for
graphics applications that use a significant amount of binary data, such as
images, videos, meshes, and animations. Existing strategies such as storing
whole files for individual revisions or simple binary deltas, respectively
consume significant storage and obscure semantic information. To overcome these
limitations, in this paper we present a revision control system for digital
images that stores revisions in form of graphs. Besides, being integrated with
Git, our revision control system also facilitates artistic creation processes
in common image editing and digital painting workflows. A preliminary user
study demonstrates the usability of the proposed system.
</summary>
    <author>
      <name>Fabio Calefato</name>
    </author>
    <author>
      <name>Giovanna Castellano</name>
    </author>
    <author>
      <name>Veronica Rossano</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/iV.2018.00095</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/iV.2018.00095" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">pp. 512-517 (6 pages)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 22nd Int'l Conf. on Information Visualisation (iV2018),
  Salerno, Italy, 10-13 July 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.00263v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00263v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01571v1</id>
    <updated>2018-06-05T09:20:40Z</updated>
    <published>2018-06-05T09:20:40Z</published>
    <title>Double JPEG Compression Detection by Exploring the Correlations in DCT
  Domain</title>
    <summary>  In the field of digital image processing, JPEG image compression technique
has been widely applied. And numerous image processing software suppose this.
It is likely for the images undergoing double JPEG compression to be tampered.
Therefore, double JPEG compression detection schemes can provide an important
clue for image forgery detection. In this paper, we propose an effective
algorithm to detect double JPEG compression with different quality factors.
Firstly, the quantized DCT coefficients with same frequency are extracted to
build the new data matrices. Then, considering the direction effect on the
correlation between the adjacent positions in DCT domain, twelve kinds of
high-pass filter templates with different directions are executed and the
translation probability matrix is calculated for each filtered data.
Furthermore, principal component analysis and support vector machine technique
are applied to reduce the feature dimension and train a classifier,
respectively. Experimental results have demonstrated that the proposed method
is effective and has comparable performance.
</summary>
    <author>
      <name>Pengpeng Yang</name>
    </author>
    <author>
      <name>Rongrong Ni</name>
    </author>
    <author>
      <name>Yao Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, submitted to APSIPA ASC 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03483v2</id>
    <updated>2018-08-15T07:53:15Z</updated>
    <published>2018-06-09T14:53:07Z</published>
    <title>Hierarchical Information Quadtree: Efficient Spatial Temporal Image
  Search for Multimedia Stream</title>
    <summary>  Massive amount of multimedia data that contain times- tamps and geographical
information are being generated at an unprecedented scale in many emerging
applications such as photo sharing web site and social networks applications.
Due to their importance, a large body of work has focused on efficiently
computing various spatial image queries. In this paper,we study the spatial
temporal image query which considers three important constraints during the
search including time recency, spatial proximity and visual relevance. A novel
index structure, namely Hierarchical Information Quadtree(\hiq), to efficiently
insert/delete spatial temporal images with high arrive rates. Base on \hiq an
efficient algorithm is developed to support spatial temporal image query. We
show via extensive experimentation with real spatial databases clearly
demonstrate the efficiency of our methods.
</summary>
    <author>
      <name>Chengyuan Zhang</name>
    </author>
    <author>
      <name>Ruipeng Chen</name>
    </author>
    <author>
      <name>Lei Zhu</name>
    </author>
    <author>
      <name>Anfeng Liu</name>
    </author>
    <author>
      <name>Yunwu Lin</name>
    </author>
    <author>
      <name>Fang Huang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11042-018-6284-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11042-018-6284-y" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at Multimedia Tools and Applications. arXiv admin note:
  text overlap with arXiv:1805.02009</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.03483v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03483v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.03618v2</id>
    <updated>2018-10-16T11:15:23Z</updated>
    <published>2018-06-10T09:23:10Z</published>
    <title>Steganography Security: Principle and Practice</title>
    <summary>  This paper focuses on several theoretical issues and principles in
steganography security, and defines four security levels by analyzing the
corresponding algorithm instances. In the theoretical analysis, we discuss the
differences between steganography security and watermarking security. The two
necessary conditions for the steganography security are obtained. Under the
current technology situation, we then analyze the indistinguishability of the
cover and stego-cover, and consider that the steganography security should rely
on the key secrecy with algorithms open. By specifying the role of key in
steganography, the necessary conditions for a secure steganography algorithm in
theory are formally presented. When analyzing the security instances, we have
classified the steganalysis attacks according to their variable access to the
steganography system, and then defined the four security levels. The higher
level security one has, the higher level attacks one can resist. We have also
presented algorithm instances based on current technical conditions, and
analyzed their data hiding process, security level, and practice requirements.
</summary>
    <author>
      <name>Yan Ke</name>
    </author>
    <author>
      <name>Jia Liu</name>
    </author>
    <author>
      <name>Min-qing Zhang</name>
    </author>
    <author>
      <name>Ting-ting Su</name>
    </author>
    <author>
      <name>Xiao-yuan Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1806.03618v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.03618v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.07008v1</id>
    <updated>2018-06-19T01:58:09Z</updated>
    <published>2018-06-19T01:58:09Z</published>
    <title>A Group Variational Transformation Neural Network for Fractional
  Interpolation of Video Coding</title>
    <summary>  Motion compensation is an important technology in video coding to remove the
temporal redundancy between coded video frames. In motion compensation,
fractional interpolation is used to obtain more reference blocks at sub-pixel
level. Existing video coding standards commonly use fixed interpolation filters
for fractional interpolation, which are not efficient enough to handle diverse
video signals well. In this paper, we design a group variational transformation
convolutional neural network (GVTCNN) to improve the fractional interpolation
performance of the luma component in motion compensation. GVTCNN infers samples
at different sub-pixel positions from the input integer-position sample. It
first extracts a shared feature map from the integer-position sample to infer
various sub-pixel position samples. Then a group variational transformation
technique is used to transform a group of copied shared feature maps to samples
at different sub-pixel positions. Experimental results have identified the
interpolation efficiency of our GVTCNN. Compared with the interpolation method
of High Efficiency Video Coding, our method achieves 1.9% bit saving on average
and up to 5.6% bit saving under low-delay P configuration.
</summary>
    <author>
      <name>Sifeng Xia</name>
    </author>
    <author>
      <name>Wenhan Yang</name>
    </author>
    <author>
      <name>Yueyu Hu</name>
    </author>
    <author>
      <name>Siwei Ma</name>
    </author>
    <author>
      <name>Jiaying Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">DCC 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.07008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.07008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00681v1</id>
    <updated>2018-06-28T22:34:37Z</updated>
    <published>2018-06-28T22:34:37Z</published>
    <title>Analysis and prediction of JND-based video quality model</title>
    <summary>  The just-noticeable-difference (JND) visual perception property has received
much attention in characterizing human subjective viewing experience of
compressed video. In this work, we quantify the JND-based video quality
assessment model using the satisfied user ratio (SUR) curve, and show that the
SUR model can be greatly simplified since the JND points of multiple subjects
for the same content in the VideoSet can be well modeled by the normal
distribution. Then, we design an SUR prediction method with video quality
degradation features and masking features and use them to predict the first,
second and the third JND points and their corresponding SUR curves. Finally, we
verify the performance of the proposed SUR prediction method with different
configurations on the VideoSet. The experimental results demonstrate that the
proposed SUR prediction method achieves good performance in various resolutions
with the mean absolute error (MAE) of the SUR smaller than 0.05 on average.
</summary>
    <author>
      <name>Haiqiang Wang</name>
    </author>
    <author>
      <name>Xinfeng Zhang</name>
    </author>
    <author>
      <name>Chao Yang</name>
    </author>
    <author>
      <name>C. -C. Jay Kuo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PCS 2018. arXiv admin note: text overlap with arXiv:1710.11090</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00681v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00681v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00920v1</id>
    <updated>2018-07-02T23:17:07Z</updated>
    <published>2018-07-02T23:17:07Z</published>
    <title>A JND-based Video Quality Assessment Model and Its Application</title>
    <summary>  Based on the Just-Noticeable-Difference (JND) criterion, a subjective video
quality assessment (VQA) dataset, called the VideoSet, was constructed
recently. In this work, we propose a JND-based VQA model using a probabilistic
framework to analyze and clean collected subjective test data. While most
traditional VQA models focus on content variability, our proposed VQA model
takes both subject and content variabilities into account. The model parameters
used to describe subject and content variabilities are jointly optimized by
solving a maximum likelihood estimation (MLE) problem. As an application, the
new subjective VQA model is used to filter out unreliable video quality scores
collected in the VideoSet. Experiments are conducted to demonstrate the
effectiveness of the proposed model.
</summary>
    <author>
      <name>Haiqiang Wang</name>
    </author>
    <author>
      <name>Xinfeng Zhang</name>
    </author>
    <author>
      <name>Chao Yang</name>
    </author>
    <author>
      <name>C. -C. Jay Kuo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v3</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.00920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.02895v1</id>
    <updated>2018-07-08T23:27:12Z</updated>
    <published>2018-07-08T23:27:12Z</published>
    <title>A Filter of Minhash for Image Similarity Measures</title>
    <summary>  Image similarity measures play an important role in nearest neighbor search
and duplicate detection for large-scale image datasets. Recently, Minwise
Hashing (or Minhash) and its related hashing algorithms have achieved great
performances in large-scale image retrieval systems. However, there are a large
number of comparisons for image pairs in these applications, which may spend a
lot of computation time and affect the performance. In order to quickly obtain
the pairwise images that theirs similarities are higher than the specific
threshold T (e.g., 0.5), we propose a dynamic threshold filter of Minwise
Hashing for image similarity measures. It greatly reduces the calculation time
by terminating the unnecessary comparisons in advance. We also find that the
filter can be extended to other hashing algorithms, on when the estimator
satisfies the binomial distribution, such as b-Bit Minwise Hashing, One
Permutation Hashing, etc. In this pager, we use the Bag-of-Visual-Words (BoVW)
model based on the Scale Invariant Feature Transform (SIFT) to represent the
image features. We have proved that the filter is correct and effective through
the experiment on real image datasets.
</summary>
    <author>
      <name>Jun Long</name>
    </author>
    <author>
      <name>Qunfeng Liu</name>
    </author>
    <author>
      <name>Xinpan Yuan</name>
    </author>
    <author>
      <name>Chengyuan Zhang</name>
    </author>
    <author>
      <name>Junfeng Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.02895v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.02895v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.03773v1</id>
    <updated>2018-06-25T02:06:22Z</updated>
    <published>2018-06-25T02:06:22Z</published>
    <title>EAST Real-Time VOD System Based on MDSplus</title>
    <summary>  As with EAST (Experimental Advanced Superconducting Tokamak) experimental
data analyzed by more and more collaborators, the experimental videos which
directly reflect the real status of vacuum attract more and more researchers'
attention. The real time VOD (Video On Demand) system based on MDSplus allows
users reading the video frames in real time as same as the signal data which is
also stored in the MDSplus database. User can display the plasma discharge
videos and analyze videos frame by frame through jScope or our VOD web station.
The system mainly includes the frames storing and frames displaying. The frames
storing application accepts shot information by using socket TCP communication
firstly, then reads video frames through disk mapping, finally stores them into
MDSplus. The displaying process is implemented through B/S (Browser/Server)
framework, it uses PHP and JavaScript to realize VOD function and read frames
information from MDSplus. The system offers a unit way to access and backup
experimental data and video during the EAST experiment, which is of great
benefit to EAST experimenter than the formal VOD system in VOD function and
real time performance.
</summary>
    <author>
      <name>J. Y. Xia</name>
    </author>
    <author>
      <name>B. J. Xiao</name>
    </author>
    <author>
      <name>Fei Yang</name>
    </author>
    <author>
      <name>Dan Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21st IEEE Real Time Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.03773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.03773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05323v1</id>
    <updated>2018-07-14T02:32:29Z</updated>
    <published>2018-07-14T02:32:29Z</published>
    <title>A Bayesian Approach to Block Structure Inference in AV1-based Multi-rate
  Video Encoding</title>
    <summary>  Due to differences in frame structure, existing multi-rate video encoding
algorithms cannot be directly adapted to encoders utilizing special reference
frames such as AV1 without introducing substantial rate-distortion loss. To
tackle this problem, we propose a novel bayesian block structure inference
model inspired by a modification to an HEVC-based algorithm. It estimates the
posterior probabilistic distributions of block partitioning, and adapts early
terminations in the RDO procedure accordingly. Experimental results show that
the proposed method provides flexibility for controlling the tradeoff between
speed and coding efficiency, and can achieve an average time saving of 36.1%
(up to 50.6%) with negligible bitrate cost.
</summary>
    <author>
      <name>Bichuan Guo</name>
    </author>
    <author>
      <name>Xinyao Chen</name>
    </author>
    <author>
      <name>Jiawen Gu</name>
    </author>
    <author>
      <name>Yuxing Han</name>
    </author>
    <author>
      <name>Jiangtao Wen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DCC.2018.00047</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DCC.2018.00047" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">published in IEEE Data Compression Conference, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.05323v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05323v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05364v1</id>
    <updated>2018-07-14T09:19:49Z</updated>
    <published>2018-07-14T09:19:49Z</published>
    <title>Convex Optimization Based Bit Allocation for Light Field Compression
  under Weighting and Consistency Constraints</title>
    <summary>  Compared with conventional image and video, light field images introduce the
weight channel, as well as the visual consistency of rendered view, information
that has to be taken into account when compressing the pseudo-temporal-sequence
(PTS) created from light field images. In this paper, we propose a novel frame
level bit allocation framework for PTS coding. A joint model that measures
weighted distortion and visual consistency, combined with an iterative encoding
system, yields the optimal bit allocation for each frame by solving a convex
optimization problem. Experimental results show that the proposed framework is
effective in producing desired distortion distribution based on weights, and
achieves up to 24.7% BD-rate reduction comparing to the default rate control
algorithm.
</summary>
    <author>
      <name>Bichuan Guo</name>
    </author>
    <author>
      <name>Yuxing Han</name>
    </author>
    <author>
      <name>Jiangtao Wen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DCC.2018.00019</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DCC.2018.00019" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">published in IEEE Data Compression Conference, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.05364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05365v1</id>
    <updated>2018-07-14T09:29:57Z</updated>
    <published>2018-07-14T09:29:57Z</published>
    <title>Fast Block Structure Determination in AV1-based Multiple Resolutions
  Video Encoding</title>
    <summary>  The widely used adaptive HTTP streaming requires an efficient algorithm to
encode the same video to different resolutions. In this paper, we propose a
fast block structure determination algorithm based on the AV1 codec that
accelerates high resolution encoding, which is the bottle-neck of multiple
resolutions encoding. The block structure similarity across resolutions is
modeled by the fineness of frame detail and scale of object motions, this
enables us to accelerate high resolution encoding based on low resolution
encoding results. The average depth of a block's co-located neighborhood is
used to decide early termination in the RDO process. Encoding results show that
our proposed algorithm reduces encoding time by 30.1%-36.8%, while keeping
BD-rate low at 0.71%-1.04%. Comparing to the state-of-the-art, our method
halves performance loss without sacrificing time savings.
</summary>
    <author>
      <name>Bichuan Guo</name>
    </author>
    <author>
      <name>Yuxing Han</name>
    </author>
    <author>
      <name>Jiangtao Wen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICME.2018.8486492</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICME.2018.8486492" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">published in IEEE International Conference on Multimedia and Expo,
  2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.05365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07203v1</id>
    <updated>2018-07-19T00:58:33Z</updated>
    <published>2018-07-19T00:58:33Z</published>
    <title>Few-Shot Adaptation for Multimedia Semantic Indexing</title>
    <summary>  We propose a few-shot adaptation framework, which bridges zero-shot learning
and supervised many-shot learning, for semantic indexing of image and video
data. Few-shot adaptation provides robust parameter estimation with few
training examples, by optimizing the parameters of zero-shot learning and
supervised many-shot learning simultaneously. In this method, first we build a
zero-shot detector, and then update it by using the few examples. Our
experiments show the effectiveness of the proposed framework on three datasets:
TRECVID Semantic Indexing 2010, 2014, and ImageNET. On the ImageNET dataset, we
show that our method outperforms recent few-shot learning methods. On the
TRECVID 2014 dataset, we achieve 15.19% and 35.98% in Mean Average Precision
under the zero-shot condition and the supervised condition, respectively. To
the best of our knowledge, these are the best results on this dataset.
</summary>
    <author>
      <name>Nakamasa Inoue</name>
    </author>
    <author>
      <name>Koichi Shinoda</name>
    </author>
    <link href="http://arxiv.org/abs/1807.07203v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07203v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08176v1</id>
    <updated>2018-07-21T16:49:25Z</updated>
    <published>2018-07-21T16:49:25Z</published>
    <title>A Convolutional Neural Networks Denoising Approach for Salt and Pepper
  Noise</title>
    <summary>  The salt and pepper noise, especially the one with extremely high percentage
of impulses, brings a significant challenge to image denoising. In this paper,
we propose a non-local switching filter convolutional neural network denoising
algorithm, named NLSF-CNN, for salt and pepper noise. As its name suggested,
our NLSF-CNN consists of two steps, i.e., a NLSF processing step and a CNN
training step. First, we develop a NLSF pre-processing step for noisy images
using non-local information. Then, the pre-processed images are divided into
patches and used for CNN training, leading to a CNN denoising model for future
noisy images. We conduct a number of experiments to evaluate the effectiveness
of NLSF-CNN. Experimental results show that NLSF-CNN outperforms the
state-of-the-art denoising algorithms with a few training images.
</summary>
    <author>
      <name>Bo Fu</name>
    </author>
    <author>
      <name>Xiao-Yang Zhao</name>
    </author>
    <author>
      <name>Yi Li</name>
    </author>
    <author>
      <name>Xiang-Hai Wang</name>
    </author>
    <author>
      <name>Yong-Gong Ren</name>
    </author>
    <link href="http://arxiv.org/abs/1807.08176v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08176v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09560v1</id>
    <updated>2018-07-25T12:55:59Z</updated>
    <published>2018-07-25T12:55:59Z</published>
    <title>Who is the director of this movie? Automatic style recognition based on
  shot features</title>
    <summary>  We show how low-level formal features, such as shot duration, meant as length
of camera takes, and shot scale, i.e. the distance between the camera and the
subject, are distinctive of a director's style in art movies. So far such
features were thought of not having enough varieties to become distinctive of
an author. However our investigation on the full filmographies of six different
authors (Scorsese, Godard, Tarr, Fellini, Antonioni, and Bergman) for a total
number of 120 movies analysed second by second, confirms that these
shot-related features do not appear as random patterns in movies from the same
director. For feature extraction we adopt methods based on both conventional
and deep learning techniques. Our findings suggest that feature sequential
patterns, i.e. how features evolve in time, are at least as important as the
related feature distributions. To the best of our knowledge this is the first
study dealing with automatic attribution of movie authorship, which opens up
interesting lines of cross-disciplinary research on the impact of style on the
aesthetic and emotional effects on the viewers.
</summary>
    <author>
      <name>Michele Svanera</name>
    </author>
    <author>
      <name>Mattia Savardi</name>
    </author>
    <author>
      <name>Alberto Signoroni</name>
    </author>
    <author>
      <name>Andr√°s B√°lint Kov√°cs</name>
    </author>
    <author>
      <name>Sergio Benini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MMUL.2019.2940004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MMUL.2019.2940004" rel="related"/>
    <link href="http://arxiv.org/abs/1807.09560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00163v1</id>
    <updated>2018-08-01T04:43:16Z</updated>
    <published>2018-08-01T04:43:16Z</published>
    <title>An Advert Creation System for Next-Gen Publicity</title>
    <summary>  With the rapid proliferation of multimedia data in the internet, there has
been a fast rise in the creation of videos for the viewers. This enables the
viewers to skip the advertisement breaks in the videos, using ad blockers and
'skip ad' buttons -- bringing online marketing and publicity to a stall. In
this paper, we demonstrate a system that can effectively integrate a new
advertisement into a video sequence. We use state-of-the-art techniques from
deep learning and computational photogrammetry, for effective detection of
existing adverts, and seamless integration of new adverts into video sequences.
This is helpful for targeted advertisement, paving the path for next-gen
publicity.
</summary>
    <author>
      <name>Atul Nautiyal</name>
    </author>
    <author>
      <name>Killian McCabe</name>
    </author>
    <author>
      <name>Murhaf Hossari</name>
    </author>
    <author>
      <name>Soumyabrata Dev</name>
    </author>
    <author>
      <name>Matthew Nicholson</name>
    </author>
    <author>
      <name>Clare Conran</name>
    </author>
    <author>
      <name>Declan McKibben</name>
    </author>
    <author>
      <name>Jian Tang</name>
    </author>
    <author>
      <name>Xu Wei</name>
    </author>
    <author>
      <name>Francois Pitie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in 2018 European Conference on Machine Learning and
  Principles and Practice of Knowledge Discovery in Databases (ECML-PKDD)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.05941v1</id>
    <updated>2018-08-17T17:49:39Z</updated>
    <published>2018-08-17T17:49:39Z</published>
    <title>First Steps Toward CNN based Source Classification of Document Images
  Shared Over Messaging App</title>
    <summary>  Knowledge of source smartphone corresponding to a document image can be
helpful in a variety of applications including copyright infringement,
ownership attribution, leak identification and usage restriction. In this
letter, we investigate a convolutional neural network-based approach to solve
source smartphone identification problem for printed text documents which have
been captured by smartphone cameras and shared over messaging platform. In
absence of any publicly available dataset addressing this problem, we introduce
a new image dataset consisting of 315 images of documents printed in three
different fonts, captured using 21 smartphones and shared over WhatsApp.
Experiments conducted on this dataset demonstrate that, in all scenarios, the
proposed system performs as well as or better than the state-of-the-art system
based on handcrafted features and classification of letters extracted from
document images. The new dataset and code of the proposed system will be made
publicly available along with this letter's publication, presently they are
submitted for review.
</summary>
    <author>
      <name>Sharad Joshi</name>
    </author>
    <author>
      <name>Suraj Saxena</name>
    </author>
    <author>
      <name>Nitin Khanna</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.image.2019.05.020</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.image.2019.05.020" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.05941v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.05941v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.06686v1</id>
    <updated>2018-08-20T20:47:56Z</updated>
    <published>2018-08-20T20:47:56Z</published>
    <title>Deep Multimodal Image-Repurposing Detection</title>
    <summary>  Nefarious actors on social media and other platforms often spread rumors and
falsehoods through images whose metadata (e.g., captions) have been modified to
provide visual substantiation of the rumor/falsehood. This type of modification
is referred to as image repurposing, in which often an unmanipulated image is
published along with incorrect or manipulated metadata to serve the actor's
ulterior motives. We present the Multimodal Entity Image Repurposing (MEIR)
dataset, a substantially challenging dataset over that which has been
previously available to support research into image repurposing detection. The
new dataset includes location, person, and organization manipulations on
real-world data sourced from Flickr. We also present a novel, end-to-end, deep
multimodal learning model for assessing the integrity of an image by combining
information extracted from the image with related information from a knowledge
base. The proposed method is compared against state-of-the-art techniques on
existing datasets as well as MEIR, where it outperforms existing methods across
the board, with AUC improvement up to 0.23.
</summary>
    <author>
      <name>Ekraam Sabir</name>
    </author>
    <author>
      <name>Wael AbdAlmageed</name>
    </author>
    <author>
      <name>Yue Wu</name>
    </author>
    <author>
      <name>Prem Natarajan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3240508.3240707</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3240508.3240707" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published at ACM Multimeda 2018 (orals)</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.06686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.06686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.08567v1</id>
    <updated>2018-08-26T15:02:34Z</updated>
    <published>2018-08-26T15:02:34Z</published>
    <title>Patch-based Contour Prior Image Denoising for Salt and Pepper Noise</title>
    <summary>  The salt and pepper noise brings a significant challenge to image denoising
technology, i.e. how to removal the noise clearly and retain the details
effectively? In this paper, we propose a patch-based contour prior denoising
approach for salt and pepper noise. First, noisy image is cut into patches as
basic representation unit, a discrete total variation model is designed to
extract contour structures; Second, a weighted Euclidean distance is designed
to search the most similar patches, then, corresponding contour stencils are
extracted from these similar patches; At the last, we build filter from contour
stencils in the framework of regression. Numerical results illustrate that the
proposed method is competitive with the state-of-the-art methods in terms of
the peak signal-to-noise (PSNR) and visual effects.
</summary>
    <author>
      <name>Bo Fu</name>
    </author>
    <author>
      <name>XiaoYang Zhao</name>
    </author>
    <author>
      <name>Yi Li</name>
    </author>
    <author>
      <name>XiangHai Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.08567v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.08567v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09198v2</id>
    <updated>2018-08-29T04:37:46Z</updated>
    <published>2018-08-28T09:47:53Z</published>
    <title>Representation Learning for Image-based Music Recommendation</title>
    <summary>  Image perception is one of the most direct ways to provide contextual
information about a user concerning his/her surrounding environment; hence
images are a suitable proxy for contextual recommendation. We propose a novel
representation learning framework for image-based music recommendation that
bridges the heterogeneity gap between music and image data; the proposed method
is a key component for various contextual recommendation tasks. Preliminary
experiments show that for an image-to-song retrieval task, the proposed method
retrieves relevant or conceptually similar songs for input images.
</summary>
    <author>
      <name>Chih-Chun Hsia</name>
    </author>
    <author>
      <name>Kwei-Herng Lai</name>
    </author>
    <author>
      <name>Yian Chen</name>
    </author>
    <author>
      <name>Chuan-Ju Wang</name>
    </author>
    <author>
      <name>Ming-Feng Tsai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, LBRS@RecSys'18</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09198v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09198v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09610v1</id>
    <updated>2018-08-29T02:30:54Z</updated>
    <published>2018-08-29T02:30:54Z</published>
    <title>Efficient Region of Visual Interests Search for Geo-multimedia Data</title>
    <summary>  With the proliferation of online social networking services and mobile smart
devices equipped with mobile communications module and position sensor module,
massive amount of multimedia data has been collected, stored and shared. This
trend has put forward higher request on massive multimedia data retrieval. In
this paper, we investigate a novel spatial query named region of visual
interests query (RoVIQ), which aims to search users containing geographical
information and visual words. Three baseline methods are presented to introduce
how to exploit existing techniques to address this problem. Then we propose the
definition of this query and related notions at the first time. To improve the
performance of query, we propose a novel spatial indexing structure called
quadtree based inverted visual index which is a combination of quadtree,
inverted index and visual words. Based on it, we design a efficient search
algorithm named region of visual interests search to support RoVIQ.
Experimental evaluations on real geo-image datasets demonstrate that our
solution outperforms state-of-the-art method.
</summary>
    <author>
      <name>Chengyuan Zhang</name>
    </author>
    <author>
      <name>Yunwu Lin</name>
    </author>
    <author>
      <name>Lei Zhu</name>
    </author>
    <author>
      <name>Zuping Zhang</name>
    </author>
    <author>
      <name>Yan Tang</name>
    </author>
    <author>
      <name>Fang Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.09640v1</id>
    <updated>2018-08-29T05:04:58Z</updated>
    <published>2018-08-29T05:04:58Z</published>
    <title>Wavelet Video Coding Algorithm Based on Energy Weighted Significance
  Probability Balancing Tree</title>
    <summary>  This work presents a 3-D wavelet video coding algorithm. By analyzing the
contribution of each biorthogonal wavelet basis to reconstructed signal's
energy, we weight each wavelet subband according to its basis energy. Based on
distribution of weighted coefficients, we further discuss a 3-D wavelet tree
structure named \textbf{significance probability balancing tree}, which places
the coefficients with similar probabilities of being significant on the same
layer. It is implemented by using hybrid spatial orientation tree and
temporal-domain block tree. Subsequently, a novel 3-D wavelet video coding
algorithm is proposed based on the energy-weighted significance probability
balancing tree. Experimental results illustrate that our algorithm always
achieves good reconstruction quality for different classes of video sequences.
Compared with asymmetric 3-D orientation tree, the average peak signal-to-noise
ratio (PSNR) gain of our algorithm are 1.24dB, 2.54dB and 2.57dB for luminance
(Y) and chrominance (U,V) components, respectively. Compared with
temporal-spatial orientation tree algorithm, our algorithm gains 0.38dB, 2.92dB
and 2.39dB higher PSNR separately for Y, U, and V components. In addition, the
proposed algorithm requires lower computation cost than those of the above two
algorithms.
</summary>
    <author>
      <name>Chuan-Ming Song</name>
    </author>
    <author>
      <name>Bo Fu</name>
    </author>
    <author>
      <name>Xiang-Hai Wang</name>
    </author>
    <author>
      <name>Ming-Zhe Fu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 2 figures, submission to Multimedia Tools and Applications</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.09640v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.09640v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.03916v1</id>
    <updated>2018-10-09T11:23:13Z</updated>
    <published>2018-10-09T11:23:13Z</published>
    <title>3D Holoscopic Imaging for Cultural Heritage Digitalisation</title>
    <summary>  The growing interest in archaeology has enabled the discovery of an immense
number of cultural heritage assets and historical sites. Hence, preservation of
CH through digitalisation is becoming a primordial requirement for many
countries as a part of national cultural programs. However, CH digitalisation
is still posing serious challenges such as cost and time-consumption. In this
manuscript, 3D holoscopic (H3D) technology is applied to capture small sized CH
assets. The H3D camera utilises micro lens array within a single aperture lens
and typical 2D sensor to acquire 3D information. This technology allows 3D
autostereoscopic visualisation with full motion parallax if convenient
Microlens Array (MLA)is used on the display side. Experimental works have shown
easiness and simplicity of H3D acquisition compared to existing technologies.
In fact, H3D capture process took an equal time of shooting a standard 2D
image. These advantages qualify H3D technology to be cost effective and
time-saving technology for cultural heritage 3D digitisation.
</summary>
    <author>
      <name>Taha Alfaqheri</name>
    </author>
    <author>
      <name>Seif Allah El Mesloul Nasri</name>
    </author>
    <author>
      <name>Abdul Hamid Sadka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.03916v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.03916v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.04401v2</id>
    <updated>2018-10-11T19:33:04Z</updated>
    <published>2018-10-10T07:50:58Z</published>
    <title>V3C - a Research Video Collection</title>
    <summary>  With the widespread use of smartphones as recording devices and the massive
growth in bandwidth, the number and volume of video collections has increased
significantly in the last years. This poses novel challenges to the management
of these large-scale video data and especially to the analysis of and retrieval
from such video collections. At the same time, existing video datasets used for
research and experimentation are either not large enough to represent current
collections or do not reflect the properties of video commonly found on the
Internet in terms of content, length, or resolution. In this paper, we
introduce the Vimeo Creative Commons Collection, in short V3C, a collection of
28'450 videos (with overall length of about 3'800 hours) published under
creative commons license on Vimeo. V3C comes with a shot segmentation for each
video, together with the resulting keyframes in original as well as reduced
resolution and additional metadata. It is intended to be used from 2019 at the
International large-scale TREC Video Retrieval Evaluation campaign (TRECVid).
</summary>
    <author>
      <name>Luca Rossetto</name>
    </author>
    <author>
      <name>Heiko Schuldt</name>
    </author>
    <author>
      <name>George Awad</name>
    </author>
    <author>
      <name>Asad A. Butt</name>
    </author>
    <link href="http://arxiv.org/abs/1810.04401v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.04401v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.04547v1</id>
    <updated>2018-10-10T14:27:02Z</updated>
    <published>2018-10-10T14:27:02Z</published>
    <title>Temporal Cross-Media Retrieval with Soft-Smoothing</title>
    <summary>  Multimedia information have strong temporal correlations that shape the way
modalities co-occur over time. In this paper we study the dynamic nature of
multimedia and social-media information, where the temporal dimension emerges
as a strong source of evidence for learning the temporal correlations across
visual and textual modalities. So far, cross-media retrieval models, explored
the correlations between different modalities (e.g. text and image) to learn a
common subspace, in which semantically similar instances lie in the same
neighbourhood. Building on such knowledge, we propose a novel temporal
cross-media neural architecture, that departs from standard cross-media
methods, by explicitly accounting for the temporal dimension through temporal
subspace learning. The model is softly-constrained with temporal and
inter-modality constraints that guide the new subspace learning task by
favouring temporal correlations between semantically similar and temporally
close instances. Experiments on three distinct datasets show that accounting
for time turns out to be important for cross-media retrieval. Namely, the
proposed method outperforms a set of baselines on the task of temporal
cross-media retrieval, demonstrating its effectiveness for performing temporal
subspace learning.
</summary>
    <author>
      <name>David Semedo</name>
    </author>
    <author>
      <name>Jo√£o Magalh√£es</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3240508.3240665</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3240508.3240665" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ACM MM 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.04547v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.04547v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.06030v1</id>
    <updated>2018-10-14T13:00:39Z</updated>
    <published>2018-10-14T13:00:39Z</published>
    <title>CNN-VWII: An Efficient Approach for Large-Scale Video Retrieval by Image
  Queries</title>
    <summary>  This paper aims to solve the problem of large-scale video retrieval by a
query image. Firstly, we define the problem of top-$k$ image to video query.
Then, we combine the merits of convolutional neural networks(CNN for short) and
Bag of Visual Word(BoVW for short) module to design a model for video frames
information extraction and representation. In order to meet the requirements of
large-scale video retrieval, we proposed a visual weighted inverted index(VWII
for short) and related algorithm to improve the efficiency and accuracy of
retrieval process. Comprehensive experiments show that our proposed technique
achieves substantial improvements (up to an order of magnitude speed up) over
the state-of-the-art techniques with similar accuracy.
</summary>
    <author>
      <name>Chengyuan Zhang</name>
    </author>
    <author>
      <name>Yunwu Lin</name>
    </author>
    <author>
      <name>Lei Zhu</name>
    </author>
    <author>
      <name>Anfeng Liu</name>
    </author>
    <author>
      <name>Zuping Zhang</name>
    </author>
    <author>
      <name>Fang Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to Pattern Recognition Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.06030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.06030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.08339v2</id>
    <updated>2018-10-22T19:21:18Z</updated>
    <published>2018-10-19T03:00:19Z</published>
    <title>Quality Assessment for Tone-Mapped HDR Images Using Multi-Scale and
  Multi-Layer Information</title>
    <summary>  Tone mapping operators and multi-exposure fusion methods allow us to enjoy
the informative contents of high dynamic range (HDR) images with standard
dynamic range devices, but also introduce distortions into HDR contents.
Therefore methods are needed to evaluate tone-mapped image quality. Due to the
complexity of possible distortions in a tone-mapped image, information from
different scales and different levels should be considered when predicting
tone-mapped image quality. So we propose a new no-reference method of
tone-mapped image quality assessment based on multi-scale and multi-layer
features that are extracted from a pre-trained deep convolutional neural
network model. After being aggregated, the extracted features are mapped to
quality predictions by regression. The proposed method is tested on the largest
public database for TMIQA and compared to existing no-reference methods. The
experimental results show that the proposed method achieves better performance.
</summary>
    <author>
      <name>Qin He</name>
    </author>
    <author>
      <name>Dingquan Li</name>
    </author>
    <author>
      <name>Tingting Jiang</name>
    </author>
    <author>
      <name>Ming Jiang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICMEW.2018.8551502</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICMEW.2018.8551502" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has 6 pages, 3 tables and 2 figures in total, corrects a
  typo in the accepted version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 2018 IEEE International Conference on Multimedia
  and Expo Workshops (ICMEW)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1810.08339v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.08339v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.13151v3</id>
    <updated>2019-06-12T10:35:01Z</updated>
    <published>2018-10-31T08:24:15Z</published>
    <title>Semantic Modeling of Textual Relationships in Cross-Modal Retrieval</title>
    <summary>  Feature modeling of different modalities is a basic problem in current
research of cross-modal information retrieval. Existing models typically
project texts and images into one embedding space, in which semantically
similar information will have a shorter distance. Semantic modeling of textural
relationships is notoriously difficult. In this paper, we propose an approach
to model texts using a featured graph by integrating multi-view textual
relationships including semantic relations, statistical co-occurrence, and
prior relations in the knowledge base. A dual-path neural network is adopted to
learn multi-modal representations of information and cross-modal similarity
measure jointly. We use a Graph Convolutional Network (GCN) for generating
relation-aware text representations, and use a Convolutional Neural Network
(CNN) with non-linearities for image representations. The cross-modal
similarity measure is learned by distance metric learning. Experimental results
show that, by leveraging the rich relational semantics in texts, our model can
outperform the state-of-the-art models by 3.4% and 6.3% on accuracy on two
benchmark datasets.
</summary>
    <author>
      <name>Jing Yu</name>
    </author>
    <author>
      <name>Chenghao Yang</name>
    </author>
    <author>
      <name>Zengchang Qin</name>
    </author>
    <author>
      <name>Zhuoqian Yang</name>
    </author>
    <author>
      <name>Yue Hu</name>
    </author>
    <author>
      <name>Weifeng Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in KSEM 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.13151v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.13151v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.00818v1</id>
    <updated>2018-11-02T11:03:28Z</updated>
    <published>2018-11-02T11:03:28Z</published>
    <title>Listen to Dance: Music-driven choreography generation using
  Autoregressive Encoder-Decoder Network</title>
    <summary>  Automatic choreography generation is a challenging task because it often
requires an understanding of two abstract concepts - music and dance - which
are realized in the two different modalities, namely audio and video,
respectively. In this paper, we propose a music-driven choreography generation
system using an auto-regressive encoder-decoder network. To this end, we first
collect a set of multimedia clips that include both music and corresponding
dance motion. We then extract the joint coordinates of the dancer from video
and the mel-spectrogram of music from audio, and train our network using
music-choreography pairs as input. Finally, a novel dance motion is generated
at the inference time when only music is given as an input. We performed a user
study for a qualitative evaluation of the proposed method, and the results show
that the proposed model is able to generate musically meaningful and natural
dance movements given an unheard song.
</summary>
    <author>
      <name>Juheon Lee</name>
    </author>
    <author>
      <name>Seohyun Kim</name>
    </author>
    <author>
      <name>Kyogu Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.00818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.00818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.03713v1</id>
    <updated>2018-11-08T23:15:50Z</updated>
    <published>2018-11-08T23:15:50Z</published>
    <title>Performance Comparison of Contemporary DNN Watermarking Techniques</title>
    <summary>  DNNs shall be considered as the intellectual property (IP) of the model
builder due to the impeding cost of designing/training a highly accurate model.
Research attempts have been made to protect the authorship of the trained model
and prevent IP infringement using DNN watermarking techniques. In this paper,
we provide a comprehensive performance comparison of the state-of-the-art DNN
watermarking methodologies according to the essential requisites for an
effective watermarking technique. We identify the pros and cons of each scheme
and provide insights into the underlying rationale. Empirical results
corroborate that DeepSigns framework proposed in [4] has the best overall
performance in terms of the evaluation metrics. Our comparison facilitates the
development of pending watermarking approaches and enables the model owner to
deploy the watermarking scheme that satisfying her requirements.
</summary>
    <author>
      <name>Huili Chen</name>
    </author>
    <author>
      <name>Bita Darvish Rouhani</name>
    </author>
    <author>
      <name>Xinwei Fan</name>
    </author>
    <author>
      <name>Osman Cihan Kilinc</name>
    </author>
    <author>
      <name>Farinaz Koushanfar</name>
    </author>
    <link href="http://arxiv.org/abs/1811.03713v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.03713v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.03732v3</id>
    <updated>2020-11-20T12:36:34Z</updated>
    <published>2018-11-09T01:34:59Z</published>
    <title>Distribution-Preserving Steganography Based on Text-to-Speech Generative
  Models</title>
    <summary>  Steganography is the art and science of hiding secret messages in public
communication so that the presence of the secret messages cannot be detected.
There are two distribution-preserving steganographic frameworks, one is
sampling-based and the other is compression-based. The former requires a
perfect sampler which yields data following the same distribution, and the
latter needs explicit distribution of generative objects. However, these two
conditions are too strict even unrealistic in the traditional data environment,
e.g. the distribution of natural images is hard to seize. Fortunately,
generative models bring new vitality to distribution-preserving steganography,
which can serve as the perfect sampler or provide the explicit distribution of
generative media. Take text-to-speech generation task as an example, we propose
distribution-preserving steganography based on WaveGlow and WaveNet, which
corresponds to the former two categories. Steganalysis experiments and
theoretical analysis are conducted to demonstrate that the proposed methods can
preserve the distribution.
</summary>
    <author>
      <name>Kejiang Chen</name>
    </author>
    <author>
      <name>Hang Zhou</name>
    </author>
    <author>
      <name>Hanqing Zhao</name>
    </author>
    <author>
      <name>Dongdong Chen</name>
    </author>
    <author>
      <name>Weiming Zhang</name>
    </author>
    <author>
      <name>Nenghai Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1811.03732v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.03732v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.04115v1</id>
    <updated>2018-11-09T19:41:56Z</updated>
    <published>2018-11-09T19:41:56Z</published>
    <title>ADNet: A Deep Network for Detecting Adverts</title>
    <summary>  Online video advertising gives content providers the ability to deliver
compelling content, reach a growing audience, and generate additional revenue
from online media. Recently, advertising strategies are designed to look for
original advert(s) in a video frame, and replacing them with new adverts. These
strategies, popularly known as product placement or embedded marketing, greatly
help the marketing agencies to reach out to a wider audience. However, in the
existing literature, such detection of candidate frames in a video sequence for
the purpose of advert integration, is done manually. In this paper, we propose
a deep-learning architecture called ADNet, that automatically detects the
presence of advertisements in video frames. Our approach is the first of its
kind that automatically detects the presence of adverts in a video frame, and
achieves state-of-the-art results on a public dataset.
</summary>
    <author>
      <name>Murhaf Hossari</name>
    </author>
    <author>
      <name>Soumyabrata Dev</name>
    </author>
    <author>
      <name>Matthew Nicholson</name>
    </author>
    <author>
      <name>Killian McCabe</name>
    </author>
    <author>
      <name>Atul Nautiyal</name>
    </author>
    <author>
      <name>Clare Conran</name>
    </author>
    <author>
      <name>Jian Tang</name>
    </author>
    <author>
      <name>Wei Xu</name>
    </author>
    <author>
      <name>Fran√ßois Piti√©</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Proc. 26th Irish Conference on Artificial Intelligence
  and Cognitive Science (AICS 2018), First two authors contributed equally to
  this work</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.04115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.04115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.04193v2</id>
    <updated>2019-06-12T17:59:20Z</updated>
    <published>2018-11-10T04:22:28Z</published>
    <title>A Ginga-enabled Digital Radio Mondiale Broadcasting chain: Signaling and
  Definitions</title>
    <summary>  ISDB-T International standard is currently adopted by most Latin America
countries and is already installed in most TV sets sold in recent years in the
region. To support interactive applications in Digital TV receivers, ISDB-T
defines the middleware Ginga. Similar to Digital TV, Digital Radio standards
also provide the means to carry interactive applications; however, their
specifications for interactive applications are usually more restricted than
the ones used in Digital TV. Also, interactive applications for Digital TV and
Digital Radio are usually incompatible. Motivated by such observations, this
report considers the importance of interactive applications for both TV and
Radio Broadcasting and the advantages of using the same middleware and
languages specification for Digital TV and Radio. More specifically, it
establishes the signaling and definitions on how to transport and execute
Ginga-NCL and Ginga-HTML5 applications over DRM (Digital Radio Mondiale)
transmission. Ministry of Science, Technology, Innovation and Communication of
Brazil is carrying trials with Digital Radio Mondiale standard in order to
define the reference model of the Brazilian Digital Radio System (Portuguese:
Sistema Brasileiro de R\'adio Digital - SBRD).
</summary>
    <author>
      <name>Rafael Diniz</name>
    </author>
    <author>
      <name>Alan L. V. Guedes</name>
    </author>
    <author>
      <name>Sergio Colcher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.04193v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.04193v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.06166v3</id>
    <updated>2019-05-02T14:40:30Z</updated>
    <published>2018-11-15T04:29:49Z</published>
    <title>Tiyuntsong: A Self-Play Reinforcement Learning Approach for ABR Video
  Streaming</title>
    <summary>  Existing reinforcement learning~(RL)-based adaptive bitrate~(ABR) approaches
outperform the previous fixed control rules based methods by improving the
Quality of Experience~(QoE) score, as the QoE metric can hardly provide clear
guidance for optimization, finally resulting in the unexpected strategies. In
this paper, we propose \emph{Tiyuntsong}, a self-play reinforcement learning
approach with generative adversarial network~(GAN)-based method for ABR video
streaming. Tiyuntsong learns strategies automatically by training two agents
who are competing against each other. Note that the competition results are
determined by a set of rules rather than a numerical QoE score that allows
clearer optimization objectives. Meanwhile, we propose GAN Enhancement Module
to extract hidden features from the past status for preserving the information
without the limitations of sequence lengths. Using testbed experiments, we show
that the utilization of GAN significantly improves the Tiyuntsong's
performance. By comparing the performance of ABRs, we observe that Tiyuntsong
also betters existing ABR algorithms in the underlying metrics.
</summary>
    <author>
      <name>Tianchi Huang</name>
    </author>
    <author>
      <name>Xin Yao</name>
    </author>
    <author>
      <name>Chenglei Wu</name>
    </author>
    <author>
      <name>Rui-Xiao Zhang</name>
    </author>
    <author>
      <name>Zhangyuan Pang</name>
    </author>
    <author>
      <name>Lifeng Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in ICME 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.06166v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.06166v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.10826v1</id>
    <updated>2018-11-27T06:10:51Z</updated>
    <published>2018-11-27T06:10:51Z</published>
    <title>VECTORS: Video communication through opportunistic relays and scalable
  video coding</title>
    <summary>  Crowd-sourced video distribution is frequently of interest in the local
vicinity. In this paper, we propose a novel design to transfer such content
over opportunistic networks with adaptive quality encoding to achieve
reasonable delay bounds. The video segments are transmitted between source and
destination in a delay tolerant manner using the Nearby Connections Android
library. This implementation can be applied to multiple domains, including farm
monitoring, wildlife, and environmental tracking, disaster response scenarios,
etc. In this work, we present the design of an opportunistic contact based
system, and we discuss basic results for the trial runs within our institute.
</summary>
    <author>
      <name>Abhishek Thakur</name>
    </author>
    <author>
      <name>Arnav Dhamija</name>
    </author>
    <author>
      <name>Tejeshwar Reddy G</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures, and under 3000 words for submission to the
  SoftwareX journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.10826v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.10826v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.12687v1</id>
    <updated>2018-11-30T09:46:29Z</updated>
    <published>2018-11-30T09:46:29Z</published>
    <title>Hybrid Distortion Aggregated Visual Comfort Assessment for Stereoscopic
  Image Retargeting</title>
    <summary>  Visual comfort is a quite important factor in 3D media service. Few research
efforts have been carried out in this area especially in case of 3D content
retargeting which may introduce more complicated visual distortions. In this
paper, we propose a Hybrid Distortion Aggregated Visual Comfort Assessment
(HDA-VCA) scheme for stereoscopic retargeted images (SRI), considering
aggregation of hybrid distortions including structure distortion, information
loss, binocular incongruity and semantic distortion. Specifically, a Local-SSIM
feature is proposed to reflect the local structural distortion of SRI, and
information loss is represented by Dual Natural Scene Statistics (D-NSS)
feature extracted from the binocular summation and difference channels.
Regarding binocular incongruity, visual comfort zone, window violation,
binocular rivalry, and accommodation-vergence conflict of human visual system
(HVS) are evaluated. Finally, the semantic distortion is represented by the
correlation distance of paired feature maps extracted from original
stereoscopic image and its retargeted image by using trained deep neural
network. We validate the effectiveness of HDA-VCA on published Stereoscopic
Image Retargeting Database (SIRD) and two stereoscopic image databases IEEE-SA
and NBU 3D-VCA. The results demonstrate HDA-VCA's superior performance in
handling hybrid distortions compared to state-of-the-art VCA schemes.
</summary>
    <author>
      <name>Ya Zhou</name>
    </author>
    <author>
      <name>Zhibo Chen</name>
    </author>
    <author>
      <name>Weiping Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 11 figures, 4 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.12687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.12687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.05712v1</id>
    <updated>2019-01-17T10:10:30Z</updated>
    <published>2019-01-17T10:10:30Z</published>
    <title>CBA: Contextual Quality Adaptation for Adaptive Bitrate Video Streaming
  (Extended Version)</title>
    <summary>  Recent advances in quality adaptation algorithms leave adaptive bitrate (ABR)
streaming architectures at a crossroads: When determining the sustainable video
quality one may either rely on the information gathered at the client vantage
point or on server and network assistance. The fundamental problem here is to
determine how valuable either information is for the adaptation decision. This
problem becomes particularly hard in future Internet settings such as Named
Data Networking (NDN) where the notion of a network connection does not exist.
  In this paper, we provide a fresh view on ABR quality adaptation for QoE
maximization, which we formalize as a decision problem under uncertainty, and
for which we contribute a sparse Bayesian contextual bandit algorithm denoted
CBA. This allows taking high-dimensional streaming context information,
including client-measured variables and network assistance, to find online the
most valuable information for the quality adaptation. Since sparse Bayesian
estimation is computationally expensive, we develop a fast new inference scheme
to support online video adaptation. We perform an extensive evaluation of our
adaptation algorithm in the particularly challenging setting of NDN, where we
use an emulation testbed to demonstrate the efficacy of CBA compared to
state-of-the-art algorithms.
</summary>
    <author>
      <name>Bastian Alt</name>
    </author>
    <author>
      <name>Trevor Ballard</name>
    </author>
    <author>
      <name>Ralf Steinmetz</name>
    </author>
    <author>
      <name>Heinz Koeppl</name>
    </author>
    <author>
      <name>Amr Rizk</name>
    </author>
    <link href="http://arxiv.org/abs/1901.05712v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.05712v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.06279v1</id>
    <updated>2019-01-18T14:49:37Z</updated>
    <published>2019-01-18T14:49:37Z</published>
    <title>Video Multimethod Assessment Fusion (VMAF) on 360VR contents</title>
    <summary>  This paper describes the subjective experiments and subsequent analysis
carried out to validate the application of one of the most robust and
influential video quality metrics, Video Multimethod Assessment Fusion (VMAF),
to 360VR contents. VMAF is a full reference metric initially designed to work
with traditional 2D contents. Hence, at first, it cannot be assumed to be
compatible with the particularities of the scenario where omnidirectional
content is visualized using a Head-Mounted Display (HMD). Therefore, through a
complete set of tests, we prove that this metric can be successfully used
without any specific training or adjustments to obtain the quality of 360VR
sequences actually perceived by users.
</summary>
    <author>
      <name>Marta Orduna</name>
    </author>
    <author>
      <name>C√©sar D√≠az</name>
    </author>
    <author>
      <name>Lara Mu√±oz</name>
    </author>
    <author>
      <name>Pablo P√©rez</name>
    </author>
    <author>
      <name>Ignacio Benito</name>
    </author>
    <author>
      <name>Narciso Garc√≠a</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCE.2019.2957987</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCE.2019.2957987" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Trans. Consumer Electronics, vol. 66, no. 1, pp. 22-31, Feb.
  2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1901.06279v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.06279v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.07840v1</id>
    <updated>2019-01-23T12:10:39Z</updated>
    <published>2019-01-23T12:10:39Z</published>
    <title>On basis images for the digital image representation</title>
    <summary>  Digital array orthogonal transformations that can be presented as a
decomposition over basis items or basis images are considered. The orthogonal
transform provides digital data scattering, a process of pixel energy
redistributing, that is illustrated with the help of basis images. Data
scattering plays important role for applications as image coding and
watermarking. We established a simple quantum analogues of basis images. They
are representations of quantum operators that describe transition of single
particle between its states.
  Considering basis images as items of a matrix, we introduced a block matrix
that is suitable for orthogonal transforms of multi-dimensional arrays such as
block vector, components of which are matrices. We present an orthogonal
transform that produces correlation between arrays. Due to correlation new
feature of data scattering was found. A presented detection algorithm is an
example of how it can be used in frequency domain watermarking.
</summary>
    <author>
      <name>V. N. Gorbachev</name>
    </author>
    <author>
      <name>L. A. Denisov</name>
    </author>
    <author>
      <name>E. M. Kaynarova</name>
    </author>
    <author>
      <name>I. K. Metelev</name>
    </author>
    <author>
      <name>E. S. Yakovleva</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.07840v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.07840v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="MM cs" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.10744v3</id>
    <updated>2019-02-13T11:36:43Z</updated>
    <published>2019-01-30T10:17:52Z</published>
    <title>A study for Image compression using Re-Pair algorithm</title>
    <summary>  The compression is an important topic in computer science which allows we to
storage more amount of data on our data storage. There are several techniques
to compress any file. In this manuscript will be described the most important
algorithm to compress images such as JPEG and it will be compared with another
method to retrieve good reason to not use this method on images. So to compress
the text the most encoding technique known is the Huffman Encoding which it
will be explained in exhaustive way. In this manuscript will showed how to
compute a text compression method on images in particular the method and the
reason to choice a determinate image format against the other. The method
studied and analyzed in this manuscript is the Re-Pair algorithm which is
purely for grammatical context to be compress. At the and it will be showed the
good result of this application.
</summary>
    <author>
      <name>Pasquale De Luca</name>
    </author>
    <author>
      <name>Vincenzo Maria Russiello</name>
    </author>
    <author>
      <name>Raffaele Ciro Sannino</name>
    </author>
    <author>
      <name>Lorenzo Valente</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.10744v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.10744v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.00680v1</id>
    <updated>2019-02-02T09:35:37Z</updated>
    <published>2019-02-02T09:35:37Z</published>
    <title>Data Driven Analysis of Tiny Touchscreen Performance with MicroJam</title>
    <summary>  The widespread adoption of mobile devices, such as smartphones and tablets,
has made touchscreens a common interface for musical performance. New mobile
musical instruments have been designed that embrace collaborative creation and
that explore the affordances of mobile devices, as well as their constraints.
While these have been investigated from design and user experience
perspectives, there is little examination of the performers' musical outputs.
In this work, we introduce a constrained touchscreen performance app, MicroJam,
designed to enable collaboration between performers, and engage in a novel
data-driven analysis of more than 1600 performances using the app. MicroJam
constrains performances to five seconds, and emphasises frequent and casual
music making through a social media-inspired interface. Performers collaborate
by replying to performances, adding new musical layers that are played back at
the same time. Our analysis shows that users tend to focus on the centre and
diagonals of the touchscreen area, and tend to swirl or swipe rather than tap.
We also observe that while long swipes dominate the visual appearance of
performances, the majority of interactions are short with limited expressive
possibilities. Our findings are summarised into a set of design recommendations
for MicroJam and other touchscreen apps for social musical interaction.
</summary>
    <author>
      <name>Charles P Martin</name>
    </author>
    <author>
      <name>Jim Torresen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/comj_a_00536</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/comj_a_00536" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Music Journal, 43(4), 41-57 (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1902.00680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.00680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.01372v1</id>
    <updated>2019-02-04T18:39:44Z</updated>
    <published>2019-02-04T18:39:44Z</published>
    <title>Vignette: Perceptual Compression for Video Storage and Processing
  Systems</title>
    <summary>  Compressed videos constitute 70% of Internet traffic, and video upload growth
rates far outpace compute and storage improvement trends. Past work in
leveraging perceptual cues like saliency, i.e., regions where viewers focus
their perceptual attention, reduces compressed video size while maintaining
perceptual quality, but requires significant changes to video codecs and
ignores the data management of this perceptual information.
  In this paper, we propose Vignette, a compression technique and storage
manager for perception-based video compression. Vignette complements
off-the-shelf compression software and hardware codec implementations.
Vignette's compression technique uses a neural network to predict saliency
information used during transcoding, and its storage manager integrates
perceptual information into the video storage system to support a perceptual
compression feedback loop. Vignette's saliency-based optimizations reduce
storage by up to 95% with minimal quality loss, and Vignette videos lead to
power savings of 50% on mobile phones during video playback. Our results
demonstrate the benefit of embedding information about the human visual system
into the architecture of video storage systems.
</summary>
    <author>
      <name>Amrita Mazumdar</name>
    </author>
    <author>
      <name>Brandon Haynes</name>
    </author>
    <author>
      <name>Magdalena Balazinska</name>
    </author>
    <author>
      <name>Luis Ceze</name>
    </author>
    <author>
      <name>Alvin Cheung</name>
    </author>
    <author>
      <name>Mark Oskin</name>
    </author>
    <link href="http://arxiv.org/abs/1902.01372v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.01372v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.04169v1</id>
    <updated>2019-02-11T22:20:45Z</updated>
    <published>2019-02-11T22:20:45Z</published>
    <title>Occupancy-map-based rate distortion optimization for video-based point
  cloud compression</title>
    <summary>  The state-of-the-art video-based point cloud compression scheme projects the
3D point cloud to 2D patch by patch and organizes the patches into frames to
compress them using the efficient video compression scheme. Such a scheme shows
a good trade-off between the number of points projected and the video
continuity to utilize the video compression scheme. However, some unoccupied
pixels between different patches are compressed using almost the same quality
with the occupied pixels, which will lead to the waste of lots of bits since
the unoccupied pixels are useless for the reconstructed point cloud. In this
paper, we propose to consider only the rate instead of the rate distortion cost
for the unoccupied pixels during the rate distortion optimization process. The
proposed scheme can be applied to both the geometry and attribute frames. The
experimental results show that the proposed algorithm can achieve an average of
11.9% and 15.4% bitrate savings for the geometry and attribute, respectively.
</summary>
    <author>
      <name>Li Li</name>
    </author>
    <author>
      <name>Zhu Li</name>
    </author>
    <author>
      <name>Shan Liu</name>
    </author>
    <author>
      <name>Houqiang Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.04169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.04169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.04985v1</id>
    <updated>2019-02-13T16:24:46Z</updated>
    <published>2019-02-13T16:24:46Z</published>
    <title>Development of Video Frame Enhancement Technique Using Pixel Intensity
  Analysis</title>
    <summary>  This paper developed a brightness enhancement technique for video frame pixel
intensity improvement. Frames extracted from the six sample video data used in
this work were stored in the form of images in a buffer. Noise was added to the
extracted image frames to vary the intensity of their pixels so that the pixel
values of the noisy images differ from their true values in order to determine
the efficiency of the developed technique. Simulation results showed that, the
developed technique was efficient with an improved pixel intensity and
histogram distribution. The Peak to Signal Noise Ratio evaluation showed that
the efficiency of the developed technique for both grayscale and coloured video
frames were improved by PSNR of 12.45%, 16.32%, 27.57% and 19.83% over the grey
level colour (black and white) for the NAELS1.avi, NAELS2.avi, NTA1.avi and
NTA2.avi respectively. Also, a percentage improvement of 28.93% and 31.68% were
obtained for the coloured image over the grey level image for Akiyo.avi and
Forman.avi benchmark video frame, respectively.
</summary>
    <author>
      <name>H. A. Abdulkareem</name>
    </author>
    <author>
      <name>A. M. S. Tekanyi</name>
    </author>
    <author>
      <name>I. Yau</name>
    </author>
    <author>
      <name>B. O. Sadiq</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.04985v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.04985v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.05179v2</id>
    <updated>2019-05-15T21:06:23Z</updated>
    <published>2019-02-14T01:28:17Z</published>
    <title>Multi-task learning with compressible features for Collaborative
  Intelligence</title>
    <summary>  A promising way to deploy Artificial Intelligence (AI)-based services on
mobile devices is to run a part of the AI model (a deep neural network) on the
mobile itself, and the rest in the cloud. This is sometimes referred to as
collaborative intelligence. In this framework, intermediate features from the
deep network need to be transmitted to the cloud for further processing. We
study the case where such features are used for multiple purposes in the cloud
(multi-tasking) and where they need to be compressible in order to allow
efficient transmission to the cloud. To this end, we introduce a new loss
function that encourages feature compressibility while improving system
performance on multiple tasks. Experimental results show that with the
compression-friendly loss, one can achieve around 20% bitrate reduction without
sacrificing the performance on several vision-related tasks.
</summary>
    <author>
      <name>Saeed Ranjbar Alvar</name>
    </author>
    <author>
      <name>Ivan V. Bajiƒá</name>
    </author>
    <link href="http://arxiv.org/abs/1902.05179v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.05179v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.05347v1</id>
    <updated>2019-02-14T13:36:01Z</updated>
    <published>2019-02-14T13:36:01Z</published>
    <title>Multimodal music information processing and retrieval: survey and future
  challenges</title>
    <summary>  Towards improving the performance in various music information processing
tasks, recent studies exploit different modalities able to capture diverse
aspects of music. Such modalities include audio recordings, symbolic music
scores, mid-level representations, motion, and gestural data, video recordings,
editorial or cultural tags, lyrics and album cover arts. This paper critically
reviews the various approaches adopted in Music Information Processing and
Retrieval and highlights how multimodal algorithms can help Music Computing
applications. First, we categorize the related literature based on the
application they address. Subsequently, we analyze existing information fusion
approaches, and we conclude with the set of challenges that Music Information
Retrieval and Sound and Music Computing research communities should focus in
the next years.
</summary>
    <author>
      <name>Federico Simonetta</name>
    </author>
    <author>
      <name>Stavros Ntalampiras</name>
    </author>
    <author>
      <name>Federico Avanzini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MMRP.2019.00012</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MMRP.2019.00012" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">in 2019 International Workshop on Multilayer Music Representation
  and Processing, Milano, IEEE 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1902.05347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.05347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; H.3.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.10102v2</id>
    <updated>2021-04-29T09:07:39Z</updated>
    <published>2019-02-26T18:30:50Z</published>
    <title>A multimodal movie review corpus for fine-grained opinion mining</title>
    <summary>  In this paper, we introduce a set of opinion annotations for the POM movie
review dataset, composed of 1000 videos. The annotation campaign is motivated
by the development of a hierarchical opinion prediction framework allowing one
to predict the different components of the opinions (e.g. polarity and aspect)
and to identify the corresponding textual spans. The resulting annotations have
been gathered at two granularity levels: a coarse one (opinionated span) and a
finer one (span of opinion components). We introduce specific categories in
order to make the annotation of opinions easier for movie reviews. For example,
some categories allow the discovery of user recommendation and preference in
movie reviews. We provide a quantitative analysis of the annotations and report
the inter-annotator agreement under the different levels of granularity. We
provide thus the first set of ground-truth annotations which can be used for
the task of fine-grained multimodal opinion prediction. We provide an analysis
of the data gathered through an inter-annotator study and show that a linear
structured predictor learns meaningful features even for the prediction of
scarce labels. Both the annotations and the baseline system are made publicly
available. https://github.com/eusip/POM/
</summary>
    <author>
      <name>Alexandre Garcia</name>
    </author>
    <author>
      <name>Slim Essid</name>
    </author>
    <author>
      <name>Florence d'Alch√©-Buc</name>
    </author>
    <author>
      <name>Chlo√© Clavel</name>
    </author>
    <link href="http://arxiv.org/abs/1902.10102v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.10102v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.11100v1</id>
    <updated>2018-12-25T12:34:21Z</updated>
    <published>2018-12-25T12:34:21Z</published>
    <title>Usage of analytic hierarchy process for steganographic inserts detection
  in images</title>
    <summary>  This article presents the method of steganography detection, which is formed
by replacing the least significant bit (LSB). Detection is performed by
dividing the image into layers and making an analysis of zero-layer of adjacent
bits for every bit. First-layer and second-layer are analyzed too. Hierarchies
analysis method is used for making decision if current bit is changed.
Weighting coefficients as part of the analytic hierarchy process are formed on
the values of bits. Then a matrix of corrupted pixels is generated.
Visualization of matrix with corrupted pixels allows to determine size,
location and presence of the embedded message. Computer experiment was
performed. Message was embedded in a bounded rectangular area of the image.
This method demonstrated efficiency even at low filling container, less than
10\%. Widespread statistical methods are unable to detect this steganographic
insert. The location and size of the embedded message can be determined with an
error which is not exceeding to five pixels.
</summary>
    <author>
      <name>S. V. Belim</name>
    </author>
    <author>
      <name>D. E. Vilkhovskiy</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2016 Dynamics of Systems, Mechanisms and Machines (Dynamics),
  Omsk, Russia, pp. 1-5</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1902.11100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.11100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.03247v1</id>
    <updated>2019-03-08T02:02:51Z</updated>
    <published>2019-03-08T02:02:51Z</published>
    <title>HoloCast: Graph Signal Processing for Graceful Point Cloud Delivery</title>
    <summary>  In conventional point cloud delivery, a sender uses octree-based digital
video compression to stream three-dimensional (3D) points and the corresponding
color attributes over band-limited links, e.g., wireless channels, for 3D scene
reconstructions. However, the digital-based delivery schemes have an issue
called cliff effect, where the 3D reconstruction quality is a step function in
terms of wireless channel quality. We propose a novel scheme of point cloud
delivery, called HoloCast, to gracefully improve the reconstruction quality
with the improvement of wireless channel quality. HoloCast regards the 3D
points and color components as graph signals and directly transmits
linear-transformed signals based on graph Fourier transform (GFT), without
digital quantization and entropy coding operations. One of main contributions
in HoloCast is that the use of GFT can deal with non-ordered and non-uniformly
distributed multi-dimensional signals such as holographic data unlike
conventional delivery schemes. Performance results with point cloud data show
that HoloCast yields better 3D reconstruction quality compared to digital-based
methods in noisy wireless environment.
</summary>
    <author>
      <name>Takuya Fujihashi</name>
    </author>
    <author>
      <name>Toshiaki Koike-Akino</name>
    </author>
    <author>
      <name>Takashi Watanabe</name>
    </author>
    <author>
      <name>Philip V. Orlik</name>
    </author>
    <link href="http://arxiv.org/abs/1903.03247v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.03247v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.07428v1</id>
    <updated>2019-03-18T13:33:34Z</updated>
    <published>2019-03-18T13:33:34Z</published>
    <title>Scene Segmentation-Based Luminance Adjustment for Multi-Exposure Image
  Fusion</title>
    <summary>  We propose a novel method for adjusting luminance for multi-exposure image
fusion. For the adjustment, two novel scene segmentation approaches based on
luminance distribution are also proposed. Multi-exposure image fusion is a
method for producing images that are expected to be more informative and
perceptually appealing than any of the input ones, by directly fusing photos
taken with different exposures. However, existing fusion methods often produce
unclear fused images when input images do not have a sufficient number of
different exposure levels. In this paper, we point out that adjusting the
luminance of input images makes it possible to improve the quality of the final
fused images. This insight is the basis of the proposed method. The proposed
method enables us to produce high-quality images, even when undesirable inputs
are given. Visual comparison results show that the proposed method can produce
images that clearly represent a whole scene. In addition, multi-exposure image
fusion with the proposed method outperforms state-of-the-art fusion methods in
terms of MEF-SSIM, discrete entropy, tone mapped image quality index, and
statistical naturalness.
</summary>
    <author>
      <name>Yuma Kinoshita</name>
    </author>
    <author>
      <name>Hitoshi Kiya</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2019.2906501</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2019.2906501" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">will be published in IEEE Transactions on Image Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.07428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.07428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.09884v1</id>
    <updated>2019-03-23T21:28:43Z</updated>
    <published>2019-03-23T21:28:43Z</published>
    <title>Detecting the Presence of ENF Signal in Digital Videos: a Superpixel
  based Approach</title>
    <summary>  ENF (Electrical Network Frequency) instantaneously fluctuates around its
nominal value (50/60 Hz) due to a continuous disparity between generated power
and consumed power. Consequently, luminous intensity of a mains-powered light
source varies depending on ENF fluctuations in the grid network. Variations in
the luminance over time can be captured from video recordings and ENF can be
estimated through content analysis of these recordings. In ENF based video
forensics, it is critical to check whether a given video file is appropriate
for this type of analysis. That is, if ENF signal is not present in a given
video, it would be useless to apply ENF based forensic analysis. In this work,
an ENF signal presence detection method is introduced for videos. The proposed
method is based on multiple ENF signal estimations from steady superpixels,
i.e. pixels that are most likely uniform in color, brightness, and texture, and
intraclass similarity of the estimated signals. Subsequently, consistency among
these estimates is then used to determine the presence or absence of an ENF
signal in a given video. The proposed technique can operate on video clips as
short as 2 minutes and is independent of the camera sensor type, i.e. CCD or
CMOS.
</summary>
    <author>
      <name>Saffet Vatansever</name>
    </author>
    <author>
      <name>Ahmet Emir Dirik</name>
    </author>
    <author>
      <name>Nasir Memon</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2017.2741440</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2017.2741440" rel="related"/>
    <link href="http://arxiv.org/abs/1903.09884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.09884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.10195v1</id>
    <updated>2019-03-25T09:27:44Z</updated>
    <published>2019-03-25T09:27:44Z</published>
    <title>Wav2Pix: Speech-conditioned Face Generation using Generative Adversarial
  Networks</title>
    <summary>  Speech is a rich biometric signal that contains information about the
identity, gender and emotional state of the speaker. In this work, we explore
its potential to generate face images of a speaker by conditioning a Generative
Adversarial Network (GAN) with raw speech input. We propose a deep neural
network that is trained from scratch in an end-to-end fashion, generating a
face directly from the raw speech waveform without any additional identity
information (e.g reference image or one-hot encoding). Our model is trained in
a self-supervised approach by exploiting the audio and visual signals naturally
aligned in videos. With the purpose of training from video data, we present a
novel dataset collected for this work, with high-quality videos of youtubers
with notable expressiveness in both the speech and visual signals.
</summary>
    <author>
      <name>Amanda Duarte</name>
    </author>
    <author>
      <name>Francisco Roldan</name>
    </author>
    <author>
      <name>Miquel Tubau</name>
    </author>
    <author>
      <name>Janna Escur</name>
    </author>
    <author>
      <name>Santiago Pascual</name>
    </author>
    <author>
      <name>Amaia Salvador</name>
    </author>
    <author>
      <name>Eva Mohedano</name>
    </author>
    <author>
      <name>Kevin McGuinness</name>
    </author>
    <author>
      <name>Jordi Torres</name>
    </author>
    <author>
      <name>Xavier Giro-i-Nieto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICASSP 2019. Projevct website at
  https://imatge-upc.github.io/wav2pix/</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.10195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.10195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.11722v1</id>
    <updated>2019-03-27T23:00:07Z</updated>
    <published>2019-03-27T23:00:07Z</published>
    <title>Resource Allocation Mechanism for Media Handling Services in Cloud
  Multimedia Conferencing</title>
    <summary>  Multimedia conferencing is the conversational exchange of multimedia content
between multiple parties. It has a wide range of applications (e.g., Massively
Multiplayer Online Games (MMOGs) and distance learning). Media handling
services (e.g., video mixing, transcoding, and compressing) are critical to
multimedia conferencing. However, efficient resource usage and scalability
still remain important challenges. Unfortunately, the cloud-based approaches
proposed so far have several deficiencies in terms of efficiency in resource
usage and scaling, while meeting Quality of Service (QoS) requirements. This
paper proposes a solution which optimizes resource allocation and scales in
terms of the number of participants while guaranteeing QoS. Moreover, our
solution composes different media handling services to support the
participants' demands. We formulate the resource allocation problem
mathematically as an Integer Linear Programming (ILP) problem and design a
heuristic for it. We evaluate our proposed solution for different numbers of
participants and different participants' geographical distributions. Simulation
results show that our resource allocation mechanism can compose the media
handling services and allocate the required resources in an optimal manner
while honoring the QoS in terms of end-to-end delay.
</summary>
    <author>
      <name>Abbas Soltanian</name>
    </author>
    <author>
      <name>Diala Naboulsi</name>
    </author>
    <author>
      <name>Roch Glitho</name>
    </author>
    <author>
      <name>Halima Elbiaze</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JSAC.2019.2906806</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JSAC.2019.2906806" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 14 figures, IEEE Journal on Selected Areas in
  Communications (JSAC)</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.11722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.11722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.11987v1</id>
    <updated>2019-03-28T14:09:40Z</updated>
    <published>2019-03-28T14:09:40Z</published>
    <title>Universal chosen-ciphertext attack for a family of image encryption
  schemes</title>
    <summary>  During the past decades, there is a great popularity employing nonlinear
dynamics and permutation-substitution architecture for image encryption. There
are three primary procedures in such encryption schemes, the key schedule
module for producing encryption factors, permutation for image scrambling and
substitution for pixel modification. Under the assumption of chosen-ciphertext
attack, we evaluate the security of a class of image ciphers which adopts
pixel-level permutation and modular addition for substitution. It is
mathematically revealed that the mapping from differentials of ciphertexts to
those of plaintexts are linear and has nothing to do with the key schedules,
permutation techniques and encryption rounds. Moreover, a universal
chosen-ciphertext attack is proposed and validated. Experimental results
demonstrate that the plaintexts can be directly reconstructed without any
security key or encryption elements. Related cryptographic discussions are also
given.
</summary>
    <author>
      <name>Junxin Chen</name>
    </author>
    <author>
      <name>Lei Chen</name>
    </author>
    <author>
      <name>Yicong Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.11987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.11987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.12088v1</id>
    <updated>2019-03-28T16:11:42Z</updated>
    <published>2019-03-28T16:11:42Z</published>
    <title>GANs-NQM: A Generative Adversarial Networks based No Reference Quality
  Assessment Metric for RGB-D Synthesized Views</title>
    <summary>  In this paper, we proposed a no-reference (NR) quality metric for RGB plus
image-depth (RGB-D) synthesis images based on Generative Adversarial Networks
(GANs), namely GANs-NQM. Due to the failure of the inpainting on dis-occluded
regions in RGB-D synthesis process, to capture the non-uniformly distributed
local distortions and to learn their impact on perceptual quality are
challenging tasks for objective quality metrics. In our study, based on the
characteristics of GANs, we proposed i) a novel training strategy of GANs for
RGB-D synthesis images using existing large-scale computer vision datasets
rather than RGB-D dataset; ii) a referenceless quality metric based on the
trained discriminator by learning a `Bag of Distortion Word' (BDW) codebook and
a local distortion regions selector; iii) a hole filling inpainter, i.e., the
generator of the trained GANs, for RGB-D dis-occluded regions as a side
outcome. According to the experimental results on IRCCyN/IVC DIBR database, the
proposed model outperforms the state-of-the-art quality metrics, in addition,
is more applicable in real scenarios. The corresponding context inpainter also
shows appealing results over other inpainting algorithms.
</summary>
    <author>
      <name>Suiyi Ling</name>
    </author>
    <author>
      <name>Jing Li</name>
    </author>
    <author>
      <name>Junle Wang</name>
    </author>
    <author>
      <name>Patrick Le Callet</name>
    </author>
    <link href="http://arxiv.org/abs/1903.12088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.12088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.12399v1</id>
    <updated>2019-03-29T08:56:50Z</updated>
    <published>2019-03-29T08:56:50Z</published>
    <title>A Study on the Characteristics of Douyin Short Videos and Implications
  for Edge Caching</title>
    <summary>  Douyin, internationally known as TikTok, has become one of the most
successful short-video platforms. To maintain its popularity, Douyin has to
provide better Quality of Experience (QoE) to its growing user base.
Understanding the characteristics of Douyin videos is thus critical to its
service improvement and system design. In this paper, we present an initial
study on the fundamental characteristics of Douyin videos based on a dataset of
over 260 thousand short videos collected across three months. The
characteristics of Douyin videos are found to be significantly different from
traditional online videos, ranging from video bitrate, size, to popularity. In
particular, the distributions of the bitrate and size of videos follow Weibull
distribution. We further observe that the most popular Douyin videos follow
Zifp's law on video popularity, but the rest of the videos do not. We also
investigate the correlation between popularity metrics used for Douyin videos.
It is found that the correlation between the number of views and the number of
likes are strong, while other correlations are relatively low. Finally, by
using a case study, we demonstrate that the above findings can provide
important guidance on designing an efficient edge caching system.
</summary>
    <author>
      <name>Zhuang Chen</name>
    </author>
    <author>
      <name>Qian He</name>
    </author>
    <author>
      <name>Zhifei Mao</name>
    </author>
    <author>
      <name>Hwei-Ming Chung</name>
    </author>
    <author>
      <name>Sabita Maharjan</name>
    </author>
    <link href="http://arxiv.org/abs/1903.12399v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.12399v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.00150v2</id>
    <updated>2019-04-17T03:27:35Z</updated>
    <published>2019-03-30T05:17:27Z</published>
    <title>Learning Affective Correspondence between Music and Image</title>
    <summary>  We introduce the problem of learning affective correspondence between audio
(music) and visual data (images). For this task, a music clip and an image are
considered similar (having true correspondence) if they have similar emotion
content. In order to estimate this crossmodal, emotion-centric similarity, we
propose a deep neural network architecture that learns to project the data from
the two modalities to a common representation space, and performs a binary
classification task of predicting the affective correspondence (true or false).
To facilitate the current study, we construct a large scale database containing
more than $3,500$ music clips and $85,000$ images with three emotion classes
(positive, neutral, negative). The proposed approach achieves $61.67\%$
accuracy for the affective correspondence prediction task on this database,
outperforming two relevant and competitive baselines. We also demonstrate that
our network learns modality-specific representations of emotion (without
explicitly being trained with emotion labels), which are useful for emotion
recognition in individual modalities.
</summary>
    <author>
      <name>Gaurav Verma</name>
    </author>
    <author>
      <name>Eeshan Gunesh Dhekane</name>
    </author>
    <author>
      <name>Tanaya Guha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, International Conference on Acoustics, Speech and Signal
  Processing (ICASSP) 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.00150v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.00150v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.00553v1</id>
    <updated>2019-04-01T04:12:57Z</updated>
    <published>2019-04-01T04:12:57Z</published>
    <title>Layered Image Compression using Scalable Auto-encoder</title>
    <summary>  This paper presents a novel convolutional neural network (CNN) based image
compression framework via scalable auto-encoder (SAE). Specifically, our SAE
based deep image codec consists of hierarchical coding layers, each of which is
an end-to-end optimized auto-encoder. The coarse image content and texture are
encoded through the first (base) layer while the consecutive (enhance) layers
iteratively code the pixel-level reconstruction errors between the original and
former reconstructed images. The proposed SAE structure alleviates the need to
train multiple models for different bit-rate points by recently proposed
auto-encoder based codecs. The SAE layers can be combined to realize multiple
rate points, or to produce a scalable stream. The proposed method has similar
rate-distortion performance in the low-to-medium rate range as the
state-of-the-art CNN based image codec (which uses different optimized networks
to realize different bit rates) over a standard public image dataset.
Furthermore, the proposed codec generates better perceptual quality in this bit
rate range.
</summary>
    <author>
      <name>Chuanmin Jia</name>
    </author>
    <author>
      <name>Zhaoyi Liu</name>
    </author>
    <author>
      <name>Yao Wang</name>
    </author>
    <author>
      <name>Siwei Ma</name>
    </author>
    <author>
      <name>Wen Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted by IEEE MIPR 2019 as conference paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.00553v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.00553v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.01417v1</id>
    <updated>2019-04-01T09:15:14Z</updated>
    <published>2019-04-01T09:15:14Z</published>
    <title>The bilateral solver for quality estimation based multi-focus image
  fusion</title>
    <summary>  In this work, a fast Bilateral Solver for Quality Estimation Based
multi-focus Image Fusion method (BS-QEBIF) is proposed. The all-in-focus image
is generated by pixel-wise summing up the multi-focus source images with their
focus-levels maps as weights. Since the visual quality of an image patch is
highly correlated with its focus level, the focus-level maps are preliminarily
obtained based on visual quality scores, as pre-estimations. However, the
pre-estimations are not ideal. Thus the fast bilateral solver is then adopted
to smooth the pre-estimations, and edges in the multi-focus source images can
be preserved simultaneously. The edge-preserving smoothed results are utilized
as final focus-level maps. Moreover, this work provides a confidence-map
solution for the unstable fusion in the focus-level-changed boundary regions.
Experiments were conducted on $25$ pairs of source images. The proposed
BS-QEBIF outperforms the other $13$ fusion methods objectively and
subjectively. The all-in-focus image produced by the proposed method can well
maintain the details in the multi-focus source images and does not suffer from
any residual errors. Experimental results show that BS-QEBIF can handle the
focus-level-changed boundary regions without any blocking, ringing and blurring
artifacts.
</summary>
    <author>
      <name>Jingwei Guan</name>
    </author>
    <author>
      <name>Yibo Chen</name>
    </author>
    <author>
      <name>Wai-kuen Cham</name>
    </author>
    <link href="http://arxiv.org/abs/1904.01417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.01417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.02354v2</id>
    <updated>2019-04-09T12:05:49Z</updated>
    <published>2019-04-04T05:27:10Z</published>
    <title>MMED: A Multi-domain and Multi-modality Event Dataset</title>
    <summary>  In this work, we construct and release a multi-domain and multi-modality
event dataset (MMED), containing 25,165 textual news articles collected from
hundreds of news media sites (e.g., Yahoo News, Google News, CNN News.) and
76,516 image posts shared on Flickr social media, which are annotated according
to 412 real-world events. The dataset is collected to explore the problem of
organizing heterogeneous data contributed by professionals and amateurs in
different data domains, and the problem of transferring event knowledge
obtained from one data domain to heterogeneous data domain, thus summarizing
the data with different contributors. We hope that the release of the MMED
dataset can stimulate innovate research on related challenging problems, such
as event discovery, cross-modal (event) retrieval, and visual question
answering, etc.
</summary>
    <author>
      <name>Zhenguo Yang</name>
    </author>
    <author>
      <name>Zehang Lin</name>
    </author>
    <author>
      <name>Min Cheng</name>
    </author>
    <author>
      <name>Qing Li</name>
    </author>
    <author>
      <name>Wenyin Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1904.02354v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.02354v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.06457v2</id>
    <updated>2019-08-01T23:24:57Z</updated>
    <published>2019-04-13T00:39:18Z</published>
    <title>YouTube UGC Dataset for Video Compression Research</title>
    <summary>  Non-professional video, commonly known as User Generated Content (UGC) has
become very popular in today's video sharing applications. However, traditional
metrics used in compression and quality assessment, like BD-Rate and PSNR, are
designed for pristine originals. Thus, their accuracy drops significantly when
being applied on non-pristine originals (the majority of UGC). Understanding
difficulties for compression and quality assessment in the scenario of UGC is
important, but there are few public UGC datasets available for research. This
paper introduces a large scale UGC dataset (1500 20 sec video clips) sampled
from millions of YouTube videos. The dataset covers popular categories like
Gaming, Sports, and new features like High Dynamic Range (HDR). Besides a novel
sampling method based on features extracted from encoding, challenges for UGC
compression and quality evaluation are also discussed. Shortcomings of
traditional reference-based metrics on UGC are addressed. We demonstrate a
promising way to evaluate UGC quality by no-reference objective quality
metrics, and evaluate the current dataset with three no-reference metrics
(Noise, Banding, and SLEEQ).
</summary>
    <author>
      <name>Yilin Wang</name>
    </author>
    <author>
      <name>Sasi Inguva</name>
    </author>
    <author>
      <name>Balu Adsumilli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MMSP.2019.8901772</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MMSP.2019.8901772" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2019 IEEE 21st International Workshop on Multimedia Signal
  Processing (MMSP)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1904.06457v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.06457v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.07554v1</id>
    <updated>2019-04-16T09:34:54Z</updated>
    <published>2019-04-16T09:34:54Z</published>
    <title>Steganographer Identification</title>
    <summary>  Conventional steganalysis detects the presence of steganography within single
objects. In the real-world, we may face a complex scenario that one or some of
multiple users called actors are guilty of using steganography, which is
typically defined as the Steganographer Identification Problem (SIP). One might
use the conventional steganalysis algorithms to separate stego objects from
cover objects and then identify the guilty actors. However, the guilty actors
may be lost due to a number of false alarms. To deal with the SIP, most of the
state-of-the-arts use unsupervised learning based approaches. In their
solutions, each actor holds multiple digital objects, from which a set of
feature vectors can be extracted. The well-defined distances between these
feature sets are determined to measure the similarity between the corresponding
actors. By applying clustering or outlier detection, the most suspicious
actor(s) will be judged as the steganographer(s). Though the SIP needs further
study, the existing works have good ability to identify the steganographer(s)
when non-adaptive steganographic embedding was applied. In this chapter, we
will present foundational concepts and review advanced methodologies in SIP.
This chapter is self-contained and intended as a tutorial introducing the SIP
in the context of media steganography.
</summary>
    <author>
      <name>Hanzhou Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/B978-0-12-819438-6.00021-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/B978-0-12-819438-6.00021-9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A tutorial with 30 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.07554v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.07554v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.08042v1</id>
    <updated>2019-04-17T01:19:51Z</updated>
    <published>2019-04-17T01:19:51Z</published>
    <title>Adversarial Cross-Modal Retrieval via Learning and Transferring
  Single-Modal Similarities</title>
    <summary>  Cross-modal retrieval aims to retrieve relevant data across different
modalities (e.g., texts vs. images). The common strategy is to apply
element-wise constraints between manually labeled pair-wise items to guide the
generators to learn the semantic relationships between the modalities, so that
the similar items can be projected close to each other in the common
representation subspace. However, such constraints often fail to preserve the
semantic structure between unpaired but semantically similar items (e.g. the
unpaired items with the same class label are more similar than items with
different labels). To address the above problem, we propose a novel cross-modal
similarity transferring (CMST) method to learn and preserve the semantic
relationships between unpaired items in an unsupervised way. The key idea is to
learn the quantitative similarities in single-modal representation subspace,
and then transfer them to the common representation subspace to establish the
semantic relationships between unpaired items across modalities. Experiments
show that our method outperforms the state-of-the-art approaches both in the
class-based and pair-based retrieval tasks.
</summary>
    <author>
      <name>Xin Wen</name>
    </author>
    <author>
      <name>Zhizhong Han</name>
    </author>
    <author>
      <name>Xinyu Yin</name>
    </author>
    <author>
      <name>Yu-Shen Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1904.08042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.08042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.08418v1</id>
    <updated>2019-04-17T12:30:54Z</updated>
    <published>2019-04-17T12:30:54Z</published>
    <title>An efficient multi-language Video Search Engine to facilitate the HADJ
  and the UMRA</title>
    <summary>  Videos clips became the most important and prominent multimedia document to
illustrate the rituals process of Hajj and Umrah. Therefore, it is necessary to
develop a system to facilitate access to information related to the duties, the
pillars, the stages and the prayers. In this paper present a new project
accomplishing a search engine in a large video database enabling any pilgrims
to get the information that he care about as fast, accurate. This project is
based on two techniques: (a) the weighting method to determine the degree of
affiliation of a video clip to a particular topic (b) organizing data using
several layers.
</summary>
    <author>
      <name>Mohamed Hamroun</name>
    </author>
    <author>
      <name>Sonia Lajmi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1308.3225</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Conference: 17th Scientific Forum for the Research of Hajj, Umrah
  and Madinah Visit At: Madinah, Saudi Arabia Volume: 17 , May 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1904.08418v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.08418v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.10961v1</id>
    <updated>2019-04-24T09:42:33Z</updated>
    <published>2019-04-24T09:42:33Z</published>
    <title>A Noise-aware Enhancement Method for Underexposed Images</title>
    <summary>  A novel method of contrast enhancement is proposed for underexposed images,
in which heavy noise is hidden. Under low light conditions, images taken by
digital cameras have low contrast in dark or bright regions. This is due to a
limited dynamic range that imaging sensors have. For these reasons, various
contrast enhancement methods have been proposed so far. These methods, however,
have two problems: (1) The loss of details in bright regions due to
over-enhancement of contrast. (2) The noise is amplified in dark regions
because conventional enhancement methods do not consider noise included in
images. The proposed method aims to overcome these problems. In the proposed
method, a shadow-up function is applied to adaptive gamma correction with
weighting distribution, and a denoising filter is also used to avoid noise
being amplified in dark regions. As a result, the proposed method allows us not
only to enhance contrast of dark regions, but also to avoid amplifying noise,
even under strong noise environments.
</summary>
    <author>
      <name>Chien-Cheng Chien</name>
    </author>
    <author>
      <name>Yuma Kinoshita</name>
    </author>
    <author>
      <name>Hitoshi Kiya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1811.03280</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.10961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.10961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.00579v1</id>
    <updated>2019-05-02T05:55:53Z</updated>
    <published>2019-05-02T05:55:53Z</published>
    <title>Herding Effect based Attention for Personalized Time-Sync Video
  Recommendation</title>
    <summary>  Time-sync comment (TSC) is a new form of user-interaction review associated
with real-time video contents, which contains a user's preferences for videos
and therefore well suited as the data source for video recommendations.
However, existing review-based recommendation methods ignore the
context-dependent (generated by user-interaction), real-time, and
time-sensitive properties of TSC data. To bridge the above gaps, in this paper,
we use video images and users' TSCs to design an Image-Text Fusion model with a
novel Herding Effect Attention mechanism (called ITF-HEA), which can predict
users' favorite videos with model-based collaborative filtering. Specifically,
in the HEA mechanism, we weight the context information based on the semantic
similarities and time intervals between each TSC and its context, thereby
considering influences of the herding effect in the model. Experiments show
that ITF-HEA is on average 3.78\% higher than the state-of-the-art method upon
F1-score in baselines.
</summary>
    <author>
      <name>Wenmian Yang</name>
    </author>
    <author>
      <name>Wenyuan Gao</name>
    </author>
    <author>
      <name>Xiaojie Zhou</name>
    </author>
    <author>
      <name>Weijia Jia</name>
    </author>
    <author>
      <name>Shaohua Zhang</name>
    </author>
    <author>
      <name>Yutao Luo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICME.2019.00085</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICME.2019.00085" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACCEPTED for ORAL presentation at IEEE ICME 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.00579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.00579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.01674v1</id>
    <updated>2019-05-05T12:51:36Z</updated>
    <published>2019-05-05T12:51:36Z</published>
    <title>Game-theoretic Analysis to Content-adaptive Reversible Watermarking</title>
    <summary>  While many games were designed for steganography and robust watermarking, few
focused on reversible watermarking. We present a two-encoder game related to
the rate-distortion optimization of content-adaptive reversible watermarking.
In the game, Alice first hides a payload into a cover. Then, Bob hides another
payload into the modified cover. The embedding strategy of Alice affects the
embedding capacity of Bob. The embedding strategy of Bob may produce
data-extraction errors to Alice. Both want to embed as many pure secret bits as
possible, subjected to an upper-bounded distortion. We investigate
non-cooperative game and cooperative game between Alice and Bob. When they
cooperate with each other, one may consider them as a whole, i.e., an encoder
uses a cover for data embedding with two times. When they do not cooperate with
each other, the game corresponds to a separable system, i.e., both want to
independently hide a payload within the cover, but recovering the cover may
need cooperation. We find equilibrium strategies for both players under
constraints.
</summary>
    <author>
      <name>Hanzhou Wu</name>
    </author>
    <author>
      <name>Xinpeng Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/02564602.2020.1739574</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/02564602.2020.1739574" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.01674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.01674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.01790v2</id>
    <updated>2019-05-11T00:11:38Z</updated>
    <published>2019-05-06T02:08:46Z</published>
    <title>A multimodal lossless coding method for skeletons in videos</title>
    <summary>  Nowadays, skeleton information in videos plays an important role in
human-centric video analysis but effective coding such massive skeleton
information has never been addressed in previous work. In this paper, we make
the first attempt to solve this problem by proposing a multimodal skeleton
coding tool containing three different coding schemes, namely, spatial
differential-coding scheme, motionvector-based differential-coding scheme and
inter prediction scheme, thus utilizing both spatial and temporal redundancy to
losslessly compress skeleton data. More importantly, these schemes are switched
properly for different types of skeletons in video frames, hence achieving
further improvement of compression rate. Experimental results show that our
approach leads to 74.4% and 54.7% size reduction on our surveillance sequences
and overall test sequences respectively, which demonstrates the effectiveness
of our skeleton coding tool.
</summary>
    <author>
      <name>Mingzhou Liu</name>
    </author>
    <author>
      <name>Xiaoyi He</name>
    </author>
    <author>
      <name>Weiyao Lin</name>
    </author>
    <author>
      <name>Xintong Han</name>
    </author>
    <author>
      <name>Yanmin Zhu</name>
    </author>
    <author>
      <name>Hongtao Lu</name>
    </author>
    <author>
      <name>Hongkai Xiong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This manuscript is the accepted version for ICMEW (IEEE Intl. Conf.
  Multimedia &amp; Expo Workshop), IEEE Intl. Conf. Multimedia &amp; Expo Workshop
  (ICME), 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.01790v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.01790v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.03508v1</id>
    <updated>2019-05-09T09:54:15Z</updated>
    <published>2019-05-09T09:54:15Z</published>
    <title>Methodology for accurately assessing the quality perceived by users on
  360VR contents</title>
    <summary>  To properly evaluate the performance of 360VR-specific encoding and
transmission schemes, and particularly of the solutions based on viewport
adaptation, it is necessary to consider not only the bandwidth saved, but also
the quality of the portion of the scene actually seen by users over time. With
this motivation, we propose a robust, yet flexible methodology for accurately
assessing the quality within the viewport along the visualization session. This
procedure is based on a complete analysis of the geometric relations involved.
Moreover, the designed methodology allows for both offline and online usage
thanks to the use of different approximations. In this way, our methodology can
be used regardless of the approach to properly evaluate the implemented
strategy, obtaining a fairer comparison between them.
</summary>
    <author>
      <name>Lara Mu√±oz</name>
    </author>
    <author>
      <name>C√©sar D√≠az</name>
    </author>
    <author>
      <name>Marta Orduna</name>
    </author>
    <author>
      <name>Jos√© Ignacio Ronda</name>
    </author>
    <author>
      <name>Pablo P√©rez</name>
    </author>
    <author>
      <name>Ignacio Benito</name>
    </author>
    <author>
      <name>Narciso Garc√≠a</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.dsp.2020.102706</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.dsp.2020.102706" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Digital Signal Processing, vol. 100, article 102706, pp. 1-16, May
  2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1905.03508v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.03508v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.03533v1</id>
    <updated>2019-05-09T11:13:16Z</updated>
    <published>2019-05-09T11:13:16Z</published>
    <title>Reversible Data Hiding in JPEG Images with Multi-objective Optimization</title>
    <summary>  Among various methods of reversible data hiding (RDH) in JPEG images, the
consideration in designing is only the image quality, but the image quality and
the file size expansion are equally important in JPEG images. Based on this
situation, we propose a RDH scheme in JPEG images considering both the image
quality and the file size expansion while designing the algorithm. The
multi-objective optimization strategy is utilized to realize the balance of the
two objectives. Specifically, the cover is divided into several non-overlapping
signals firstly, and after that, the embedding costs of signals are calculated
using the knowledge of the JPEG compression. Next, the optimized combination of
signals for embedding data is gained by the multi-objective optimization.
Experimental results show the better performance of our proposed RDH compared
with state-of-the-art RDH in JPEG images.
</summary>
    <author>
      <name>Zhaoxia Yin</name>
    </author>
    <author>
      <name>Yuan Ji</name>
    </author>
    <author>
      <name>Bin Luo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCSVT.2020.2969463</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCSVT.2020.2969463" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Circuits and Systems for Video Technology,
  2020, 30(8): 2343-2352</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1905.03533v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.03533v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.04709v2</id>
    <updated>2019-05-14T09:06:34Z</updated>
    <published>2019-05-12T12:24:27Z</published>
    <title>Deep Vocoder: Low Bit Rate Compression of Speech with Deep Autoencoder</title>
    <summary>  Inspired by the success of deep neural networks (DNNs) in speech processing,
this paper presents Deep Vocoder, a direct end-to-end low bit rate speech
compression method with deep autoencoder (DAE). In Deep Vocoder, DAE is used
for extracting the latent representing features (LRFs) of speech, which are
then efficiently quantized by an analysis-by-synthesis vector quantization (AbS
VQ) method. AbS VQ aims to minimize the perceptual spectral reconstruction
distortion rather than the distortion of LRFs vector itself. Also, a suboptimal
codebook searching technique is proposed to further reduce the computational
complexity. Experimental results demonstrate that Deep Vocoder yields
substantial improvements in terms of frequency-weighted segmental SNR, STOI and
PESQ score when compared to the output of the conventional SQ- or VQ-based
codec. The yielded PESQ score over the TIMIT corpus is 3.34 and 3.08 for speech
coding at 2400 bit/s and 1200 bit/s, respectively.
</summary>
    <author>
      <name>Gang Min</name>
    </author>
    <author>
      <name>Changqing Zhang</name>
    </author>
    <author>
      <name>Xiongwei Zhang</name>
    </author>
    <author>
      <name>Wei Tan</name>
    </author>
    <link href="http://arxiv.org/abs/1905.04709v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.04709v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.05365v1</id>
    <updated>2019-05-14T02:51:32Z</updated>
    <published>2019-05-14T02:51:32Z</published>
    <title>Reversible data hiding based on reducing invalid shifting of pixels in
  histogram shifting</title>
    <summary>  In recent years, reversible data hiding (RDH), a new research hotspot in the
field of information security, has been paid more and more attention by
researchers. Most of the existing RDH schemes do not fully take it into account
that natural image's texture has influence on embedding distortion. The image
distortion caused by embedding data in the image's smooth region is much
smaller than that in the unsmooth region, essentially, it is because embedding
additional data in the smooth region corresponds to fewer invalid shifting
pixels (ISPs) in histogram shifting. Thus, we propose a RDH scheme based on the
images texture to reduce invalid shifting of pixels in histogram shifting.
Specifically, first, a cover image is divided into two sub-images by the
checkerboard pattern, and then each sub-image's fluctuation values are
calculated. Finally, additional data can be embedded into the region of
sub-images with smaller fluctuation value preferentially. The experimental
results demonstrate that the proposed method has higher capacity and better
stego-image quality than some existing RDH schemes.
</summary>
    <author>
      <name>Yujie Jia</name>
    </author>
    <author>
      <name>Zhaoxia Yin</name>
    </author>
    <author>
      <name>Xinpeng Zhang</name>
    </author>
    <author>
      <name>Yonglong Luo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.sigpro.2019.05.020</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.sigpro.2019.05.020" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 11 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Signal Processing, 2019, 163: 238-246</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1905.05365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.05365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.05627v2</id>
    <updated>2019-11-19T02:13:05Z</updated>
    <published>2019-05-14T14:12:12Z</published>
    <title>High Capacity Lossless Data Hiding in JPEG Bitstream Based on General
  VLC Mapping</title>
    <summary>  JPEG is the most popular image format, which is widely used in our daily
life. Therefore, reversible data hiding (RDH) for JPEG images is important.
Most of the RDH schemes for JPEG images will cause significant distortions and
large file size increments in the marked JPEG image. As a special case of RDH,
the lossless data hiding (LDH) technique can keep the visual quality of the
marked images no degradation. In this paper, a novel high capacity LDH scheme
is proposed. In the JPEG bitstream, not all the variable length codes (VLC) are
used to encode image data. By constructing the mapping between the used and
unused VLCs, the secret data can be embedded by replacing the used VLC with the
unused VLC. Different from the previous schemes, our mapping strategy allows
the lengths of unused and used VLCs in a mapping set to be unequal. We present
some basic insights into the construction of the mapping relationship.
Experimental results show that most of the JPEG images using the proposed
scheme obtain smaller file size increments than previous RDH schemes.
Furthermore, the proposed scheme can obtain high embedding capacity while
keeping the marked JPEG image with no distortion.
</summary>
    <author>
      <name>Yang Du</name>
    </author>
    <author>
      <name>Zhaoxia Yin</name>
    </author>
    <author>
      <name>Xinpeng Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TDSC.2020.3013326</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TDSC.2020.3013326" rel="related"/>
    <link href="http://arxiv.org/abs/1905.05627v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.05627v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.05925v1</id>
    <updated>2019-05-15T03:17:18Z</updated>
    <published>2019-05-15T03:17:18Z</published>
    <title>SmartBullets: A Cloud-Assisted Bullet Screen Filter based on Deep
  Learning</title>
    <summary>  Bullet-screen is a technique that enables the website users to send real-time
comment `bullet' cross the screen. Compared with the traditional review of a
video, bullet-screen provides new features of feeling expression to video
watching and more iterations between video viewers. However, since all the
comments from the viewers are shown on the screen publicly and simultaneously,
some low-quality bullets will reduce the watching enjoyment of the users.
Although the bullet-screen video websites have provided filter functions based
on regular expression, bad bullets can still easily pass the filter through
making a small modification.
  In this paper, we present SmartBullets, a user-centered bullet-screen filter
based on deep learning techniques. A convolutional neural network is trained as
the classifier to determine whether a bullet need to be removed according to
its quality. Moreover, to increase the scalability of the filter, we employ a
cloud-assisted framework by developing a backend cloud server and a front-end
browser extension. The evaluation of 40 volunteers shows that SmartBullets can
effectively remove the low-quality bullets and improve the overall watching
experience of viewers.
</summary>
    <author>
      <name>Haoran Niu</name>
    </author>
    <author>
      <name>Jiangnan Li</name>
    </author>
    <author>
      <name>Yu Zhao</name>
    </author>
    <link href="http://arxiv.org/abs/1905.05925v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.05925v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.06500v1</id>
    <updated>2019-05-16T02:16:19Z</updated>
    <published>2019-05-16T02:16:19Z</published>
    <title>EVSO: Environment-aware Video Streaming Optimization of Power
  Consumption</title>
    <summary>  Streaming services gradually support high-quality videos for better user
experience. However, streaming high-quality video on mobile devices consumes a
considerable amount of energy. This paper presents the design and prototype of
EVSO, which achieves power saving by applying adaptive frame rates to parts of
videos with a little degradation of the user experience. EVSO utilizes a novel
perceptual similarity measurement method based on human visual perception
specialized for a video encoder. We also extend the media presentation
description, in which the video content is selected based only on the network
bandwidth, to allow for additional consideration of the user's battery status.
EVSO's streaming server preprocesses the video into several processed videos
according to the similarity intensity of each part of the video and then
provides the client with the processed video suitable for the network bandwidth
and the battery status of the client's mobile device. The EVSO system was
implemented on the commonly used H.264/AVC encoder. We conduct various
experiments and a user study with nine videos. Our experimental results show
that EVSO effectively reduces energy consumption when mobile devices use
streaming services by 22% on average and up to 27% while maintaining the
quality of the user experience.
</summary>
    <author>
      <name>Kyoungjun Park</name>
    </author>
    <author>
      <name>Myungchul Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 9 figures, IEEE International Conference on Computer
  Communications (INFOCOM) 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.06500v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.06500v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.06650v1</id>
    <updated>2019-05-16T10:45:06Z</updated>
    <published>2019-05-16T10:45:06Z</published>
    <title>Reactive Video Caching via long-short-term fusion approach</title>
    <summary>  Video caching has been a basic network functionality in today's network
architectures. Although the abundance of caching replacement algorithms has
been proposed recently, these methods all suffer from a key limitation: due to
their immature rules, inaccurate feature engineering or unresponsive model
update, they cannot strike a balance between the long-term history and
short-term sudden events. To address this concern, we propose LA-E2, a
long-short-term fusion caching replacement approach, which is based on a
learning-aided exploration-exploitation process. Specifically, by effectively
combining the deep neural network (DNN) based prediction with the online
exploitation-exploration process through a \emph{top-k} method, LA-E2 can both
make use of the historical information and adapt to the constantly changing
popularity responsively. Through the extensive experiments in two real-world
datasets, we show that LA-E2 can achieve state-of-the-art performance and
generalize well. Especially when the cache size is small, our approach can
outperform the baselines by 17.5\%-68.7\% higher in total hit rate.
</summary>
    <author>
      <name>Rui-Xiao Zhang</name>
    </author>
    <author>
      <name>Tianchi Huang</name>
    </author>
    <author>
      <name>Chenglei Wu</name>
    </author>
    <author>
      <name>Lifeng Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.06650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.06650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.08967v3</id>
    <updated>2019-11-14T14:02:28Z</updated>
    <published>2019-05-22T06:13:19Z</published>
    <title>Multiple reconstruction compression framework based on PNG image</title>
    <summary>  It is shown that neural networks (NNs) achieve excellent performances in
image compression and reconstruction. However, there are still many
shortcomings in the practical application, which eventually lead to the loss of
neural network image processing ability. Based on this, this paper proposes a
joint framework based on neural network and zoom compression. The framework
first encodes the incoming PNG or JPEG image information, and then the image is
converted into binary input decoder to reconstruct the intermediate state
image, next we import the intermediate state image into the zooming compressor
and re-pressurize it, and reconstruct the final image. From the experimental
results, this method can better process the digital image and suppress the
reverse expansion problem, and the compression effect can be improved by 4 to
10 times as much as that of using RNN alone, showing better ability in
application. In this paper, the method is transmitted over a digital image, the
effect is far better than the existing compression method alone, the Human
visual system can not feel the change of the effect.
</summary>
    <author>
      <name>Zhiqing Lu</name>
    </author>
    <author>
      <name>Zhaoxia Yin</name>
    </author>
    <author>
      <name>Bin Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The experimental results cannot reproduced</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.08967v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.08967v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.09625v2</id>
    <updated>2019-11-25T08:37:52Z</updated>
    <published>2019-05-23T12:53:07Z</published>
    <title>An Improved Reversible Data Hiding in Encrypted Images using Parametric
  Binary Tree Labeling</title>
    <summary>  This work proposes an improved reversible data hiding scheme in encrypted
images using parametric binary tree labeling(IPBTL-RDHEI), which takes
advantage of the spatial correlation in the entire original image but not in
small image blocks to reserve room for hiding data. Then the original image is
encrypted with an encryption key and the parametric binary tree is used to
label encrypted pixels into two different categories. Finally, one of the two
categories of encrypted pixels can embed secret information by bit replacement.
According to the experimental results, compared with several state-of-the-art
methods, the proposed IPBTL-RDHEI method achieves higher embedding rate and
outperforms the competitors. Due to the reversibility of IPBTL-RDHEI, the
original plaintext image and the secret information can be restored and
extracted losslessly and separately.
</summary>
    <author>
      <name>Youqing Wu</name>
    </author>
    <author>
      <name>Youzhi Xiang</name>
    </author>
    <author>
      <name>Yutang Guo</name>
    </author>
    <author>
      <name>Jin Tang</name>
    </author>
    <author>
      <name>Zhaoxia Yin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2019.2952979</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2019.2952979" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Multimedia, 2019, 22(8): 1929-1938</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1905.09625v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.09625v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.11705v2</id>
    <updated>2019-11-07T14:36:21Z</updated>
    <published>2019-05-28T09:37:19Z</published>
    <title>Optimizing Adaptive Video Streaming in Mobile Networks via Online
  Learning</title>
    <summary>  In this paper, we propose a novel algorithm for video rate adaptation in HTTP
Adaptive Streaming (HAS), based on online learning. The proposed algorithm,
named Learn2Adapt (L2A), is shown to provide a robust rate adaptation strategy
which, unlike most of the state-of-the-art techniques, does not require
parameter tuning, channel model assumptions or application-specific
adjustments. These properties make it very suitable for mobile users, who
typically experience fast variations in channel characteristics. Simulations
show that L2A improves on the overall Quality of Experience (QoE) and in
particular the average streaming rate, a result obtained independently of the
channel and application scenarios.
</summary>
    <author>
      <name>Theodoros Karagkioules</name>
    </author>
    <author>
      <name>Georgios S. Paschos</name>
    </author>
    <author>
      <name>Nikolaos Liakopoulos</name>
    </author>
    <author>
      <name>Attilio Fiandrotti</name>
    </author>
    <author>
      <name>Dimitrios Tsilimantos</name>
    </author>
    <author>
      <name>Marco Cagnazzo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures, submitted to IEEE Transactions on Multimedia
  (under review)</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.11705v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.11705v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.12245v1</id>
    <updated>2019-05-29T06:57:09Z</updated>
    <published>2019-05-29T06:57:09Z</published>
    <title>Automatic Realistic Music Video Generation from Segments of Youtube
  Videos</title>
    <summary>  A Music Video (MV) is a video aiming at visually illustrating or extending
the meaning of its background music. This paper proposes a novel method to
automatically generate, from an input music track, a music video made of
segments of Youtube music videos which would fit this music. The system
analyzes the input music to find its genre (pop, rock, ...) and finds segmented
MVs with the same genre in the database. Then, a K-Means clustering is done to
group video segments by color histogram, meaning segments of MVs having the
same global distribution of colors. A few clusters are randomly selected, then
are assembled around music boundaries, which are moments where a significant
change in the music occurs (for instance, transitioning from verse to chorus).
This way, when the music changes, the video color mood changes as well. This
work aims at generating high-quality realistic MVs, which could be mistaken for
man-made MVs. By asking users to identify, in a batch of music videos
containing professional MVs, amateur-made MVs and generated MVs by our
algorithm, we show that our algorithm gives satisfying results, as 45% of
generated videos are mistaken for professional MVs and 21.6% are mistaken for
amateur-made MVs. More information can be found in the project website:
http://ml.cs.tsinghua.edu.cn/~sarah/
</summary>
    <author>
      <name>Sarah Gross</name>
    </author>
    <author>
      <name>Xingxing Wei</name>
    </author>
    <author>
      <name>Jun Zhu</name>
    </author>
    <link href="http://arxiv.org/abs/1905.12245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.12245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.13087v1</id>
    <updated>2019-05-30T15:00:11Z</updated>
    <published>2019-05-30T15:00:11Z</published>
    <title>TS-RNN: Text Steganalysis Based on Recurrent Neural Networks</title>
    <summary>  With the rapid development of natural language processing technologies, more
and more text steganographic methods based on automatic text generation
technology have appeared in recent years. These models use the powerful
self-learning and feature extraction ability of the neural networks to learn
the feature expression of massive normal texts. Then they can automatically
generate dense steganographic texts which conform to such statistical
distribution based on the learned statistical patterns. In this paper, we
observe that the conditional probability distribution of each word in the
automatically generated steganographic texts will be distorted after embedded
with hidden information. We use Recurrent Neural Networks (RNNs) to extract
these feature distribution differences and then classify those features into
cover text and stego text categories. Experimental results show that the
proposed model can achieve high detection accuracy. Besides, the proposed model
can even make use of the subtle differences of the feature distribution of
texts to estimate the amount of hidden information embedded in the generated
steganographic text.
</summary>
    <author>
      <name>Zhongliang Yang</name>
    </author>
    <author>
      <name>Ke Wang</name>
    </author>
    <author>
      <name>Jian Li</name>
    </author>
    <author>
      <name>Yongfeng Huang</name>
    </author>
    <author>
      <name>Yu-Jin Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2019.2920452</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2019.2920452" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Signal Processing Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.13087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.13087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.00042v1</id>
    <updated>2019-06-28T19:05:53Z</updated>
    <published>2019-06-28T19:05:53Z</published>
    <title>Rhythm Dungeon: A Blockchain-based Music Roguelike Game</title>
    <summary>  Rhythm Dungeon is a rhythm game which leverages the blockchain as a shared
open database. During the gaming session, the player explores a roguelike
dungeon by inputting specific sequences in time to music rhythm. By integrating
smart contract to the game program, the enemies through the venture are
generated from other games which share the identical blockchain. On the other
hand, the player may upload their characters at the end of their journey, so
that their own character may appear in other games and make an influence.
Rhythm Dungeon is designed and implemented to show the potential of
decentralized gaming experience, which utilizes the blockchain to provide
asynchronous interactions among massive players.
</summary>
    <author>
      <name>Tengfei Wang</name>
    </author>
    <author>
      <name>Shuyi Zhang</name>
    </author>
    <author>
      <name>Xiao Wu</name>
    </author>
    <author>
      <name>Wei Cai</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2019 Foundation of Digital Games Demos (FDG 2019 DEMO), San Luis
  Obispo, California, USA, August 26-30, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1907.00042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.00042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.01154v1</id>
    <updated>2019-07-02T04:10:02Z</updated>
    <published>2019-07-02T04:10:02Z</published>
    <title>Adaptive Music Composition for Games</title>
    <summary>  The generation of music that adapts dynamically to content and actions has an
important role in building more immersive, memorable and emotive game
experiences. To date, the development of adaptive music systems for video games
is limited by both the nature of algorithms used for real-time music generation
and the limited modelling of player action, game world context and emotion in
current games. We propose that these issues must be addressed in tandem for the
quality and flexibility of adaptive game music to significantly improve.
Cognitive models of knowledge organisation and emotional affect are integrated
with multi-modal, multi-agent composition techniques to produce a novel
Adaptive Music System (AMS). The system is integrated into two stylistically
distinct games. Gamers reported an overall higher immersion and correlation of
music with game-world concepts with the AMS than with the original game
soundtracks in both games.
</summary>
    <author>
      <name>Patrick Hutchings</name>
    </author>
    <author>
      <name>Jon McCormack</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TG.2019.2921979</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TG.2019.2921979" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint. Accepted for publication in IEEE Transactions on Games,
  2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.01154v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.01154v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1; H.5.5; D.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.01985v2</id>
    <updated>2019-07-04T15:38:50Z</updated>
    <published>2019-07-03T15:15:21Z</published>
    <title>Intrinsic Image Popularity Assessment</title>
    <summary>  The goal of research in automatic image popularity assessment (IPA) is to
develop computational models that can accurately predict the potential of a
social image to go viral on the Internet. Here, we aim to single out the
contribution of visual content to image popularity, i.e., intrinsic image
popularity. Specifically, we first describe a probabilistic method to generate
massive popularity-discriminable image pairs, based on which the first
large-scale image database for intrinsic IPA (I$^2$PA) is established. We then
develop computational models for I$^2$PA based on deep neural networks,
optimizing for ranking consistency with millions of popularity-discriminable
image pairs. Experiments on Instagram and other social platforms demonstrate
that the optimized model performs favorably against existing methods, exhibits
reasonable generalizability on different databases, and even surpasses
human-level performance on Instagram. In addition, we conduct a psychophysical
experiment to analyze various aspects of human behavior in I$^2$PA.
</summary>
    <author>
      <name>Keyan Ding</name>
    </author>
    <author>
      <name>Kede Ma</name>
    </author>
    <author>
      <name>Shiqi Wang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3343031.3351007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3343031.3351007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACM Multimedia 2019</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 27th ACM International Conference on
  Multimedia, 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1907.01985v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.01985v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.03842v2</id>
    <updated>2019-08-29T14:30:22Z</updated>
    <published>2019-07-08T20:07:16Z</published>
    <title>Barriers towards no-reference metrics application to compressed video
  quality analysis: on the example of no-reference metric NIQE</title>
    <summary>  This paper analyses the application of no-reference metric NIQE to the task
of video-codec comparison. A number of issues in the metric behaviour on videos
was detected and described. The metric has outlying scores on black and
solid-coloured frames. The proposed averaging technique for metric quality
scores helped to improve the results in some cases. Also, NIQE has low-quality
scores for videos with detailed textures and higher scores for videos of lower
bitrates due to the blurring of these textures after compression. Although NIQE
showed natural results for many tested videos, it is not universal and
currently can not be used for video-codec comparisons.
</summary>
    <author>
      <name>Anastasia Zvezdakova</name>
    </author>
    <author>
      <name>Dmitriy Kulikov</name>
    </author>
    <author>
      <name>Denis Kondranin</name>
    </author>
    <author>
      <name>Dmitriy Vatolin</name>
    </author>
    <link href="http://arxiv.org/abs/1907.03842v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.03842v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.04025v1</id>
    <updated>2019-07-09T07:31:23Z</updated>
    <published>2019-07-09T07:31:23Z</published>
    <title>On the Security and Applicability of Fragile Camera Fingerprints</title>
    <summary>  Camera sensor noise is one of the most reliable device characteristics in
digital image forensics, enabling the unique linkage of images to digital
cameras. This so-called camera fingerprint gives rise to different
applications, such as image forensics and authentication. However, if images
are publicly available, an adversary can estimate the fingerprint from her
victim and plant it into spurious images. The concept of fragile camera
fingerprints addresses this attack by exploiting asymmetries in data access:
While the camera owner will always have access to a full fingerprint from
uncompressed images, the adversary has typically access to compressed images
and thus only to a truncated fingerprint. The security of this defense,
however, has not been systematically explored yet. This paper provides the
first comprehensive analysis of fragile camera fingerprints under attack. A
series of theoretical and practical tests demonstrate that fragile camera
fingerprints allow a reliable device identification for common compression
levels in an adversarial environment.
</summary>
    <author>
      <name>Erwin Quiring</name>
    </author>
    <author>
      <name>Matthias Kirchner</name>
    </author>
    <author>
      <name>Konrad Rieck</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ESORICS 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.04025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.04986v1</id>
    <updated>2019-07-11T03:46:21Z</updated>
    <published>2019-07-11T03:46:21Z</published>
    <title>Heard More Than Heard: An Audio Steganography Method Based on GAN</title>
    <summary>  Audio steganography is a collection of techniques for concealing the
existence of information by embedding it within a non-secret audio, which is
referred to as carrier. Distinct from cryptography, the steganography put
emphasis on the hiding of the secret existence. The existing audio
steganography methods mainly depend on human handcraft, while we proposed an
audio steganography algorithm which automatically generated from adversarial
training. The method consists of three neural networks: encoder which embeds
the secret message in the carrier, decoder which extracts the message, and
discriminator which determine the carriers contain secret messages. All the
networks are simultaneously trained to create embedding, extracting and
discriminating process. The system is trained with different training settings
on two datasets. Competed the majority of audio steganographic schemes, the
proposed scheme could produce high fidelity steganographic audio which contains
secret audio. Besides, the additional experiments verify the robustness and
security of our algorithm.
</summary>
    <author>
      <name>Dengpan Ye</name>
    </author>
    <author>
      <name>Shunzhi Jiang</name>
    </author>
    <author>
      <name>Jiaqin Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1907.04986v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04986v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.01483v1</id>
    <updated>2019-08-05T06:22:39Z</updated>
    <published>2019-08-05T06:22:39Z</published>
    <title>Image Steganography using Gaussian Markov Random Field Model</title>
    <summary>  Recent advances on adaptive steganography show that the performance of image
steganographic communication can be improved by incorporating the non-additive
models that capture the dependences among adjacent pixels. In this paper, a
Gaussian Markov Random Field model (GMRF) with four-element cross neighborhood
is proposed to characterize the interactions among local elements of cover
images, and the problem of secure image steganography is formulated as the one
of minimization of KL-divergence in terms of a series of low-dimensional clique
structures associated with GMRF by taking advantages of the conditional
independence of GMRF. The adoption of the proposed GMRF tessellates the cover
image into two disjoint subimages, and an alternating iterative optimization
scheme is developed to effectively embed the given payload while minimizing the
total KL-divergence between cover and stego, i.e., the statistical
detectability. Experimental results demonstrate that the proposed GMRF
outperforms the prior arts of model based schemes, e.g., MiPOD, and rivals the
state-of-the-art HiLL for practical steganography, where the selection channel
knowledges are unavailable to steganalyzers.
</summary>
    <author>
      <name>Wenkang Su</name>
    </author>
    <author>
      <name>Jiangqun Ni</name>
    </author>
    <author>
      <name>Yuanfeng Pan</name>
    </author>
    <author>
      <name>Xianglei Hu</name>
    </author>
    <author>
      <name>Yun-Qing Shi</name>
    </author>
    <link href="http://arxiv.org/abs/1908.01483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.01483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.03505v1</id>
    <updated>2019-08-09T15:51:33Z</updated>
    <published>2019-08-09T15:51:33Z</published>
    <title>A Benchmark of Visual Storytelling in Social Media</title>
    <summary>  Media editors in the newsroom are constantly pressed to provide a "like-being
there" coverage of live events. Social media provides a disorganised collection
of images and videos that media professionals need to grasp before publishing
their latest news updated. Automated news visual storyline editing with social
media content can be very challenging, as it not only entails the task of
finding the right content but also making sure that news content evolves
coherently over time. To tackle these issues, this paper proposes a benchmark
for assessing social media visual storylines. The SocialStories benchmark,
comprised by total of 40 curated stories covering sports and cultural events,
provides the experimental setup and introduces novel quantitative metrics to
perform a rigorous evaluation of visual storytelling with social media data.
</summary>
    <author>
      <name>Gon√ßalo Marcelino</name>
    </author>
    <author>
      <name>David Semedo</name>
    </author>
    <author>
      <name>Andr√© Mour√£o</name>
    </author>
    <author>
      <name>Saverio Blasi</name>
    </author>
    <author>
      <name>Marta Mrak</name>
    </author>
    <author>
      <name>Jo√£o Magalh√£es</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3323873.3325047</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3323873.3325047" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ACM ICMR 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.03505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.03505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.03846v1</id>
    <updated>2019-08-11T03:59:18Z</updated>
    <published>2019-08-11T03:59:18Z</published>
    <title>Exploiting Temporal Relationships in Video Moment Localization with
  Natural Language</title>
    <summary>  We address the problem of video moment localization with natural language,
i.e. localizing a video segment described by a natural language sentence. While
most prior work focuses on grounding the query as a whole, temporal
dependencies and reasoning between events within the text are not fully
considered. In this paper, we propose a novel Temporal Compositional Modular
Network (TCMN) where a tree attention network first automatically decomposes a
sentence into three descriptions with respect to the main event, context event
and temporal signal. Two modules are then utilized to measure the visual
similarity and location similarity between each segment and the decomposed
descriptions. Moreover, since the main event and context event may rely on
different modalities (RGB or optical flow), we use late fusion to form an
ensemble of four models, where each model is independently trained by one
combination of the visual input. Experiments show that our model outperforms
the state-of-the-art methods on the TEMPO dataset.
</summary>
    <author>
      <name>Songyang Zhang</name>
    </author>
    <author>
      <name>Jinsong Su</name>
    </author>
    <author>
      <name>Jiebo Luo</name>
    </author>
    <link href="http://arxiv.org/abs/1908.03846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.03846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.05965v1</id>
    <updated>2019-08-16T13:23:02Z</updated>
    <published>2019-08-16T13:23:02Z</published>
    <title>Adaptive Embedding Pattern for Grayscale-Invariance Reversible Data
  Hiding</title>
    <summary>  In traditional reversible data hiding (RDH) methods, researchers pay
attention to enlarge the embedding capacity (EC) and to reduce the embedding
distortion (ED). Recently, a completely novel RDH algorithm was developed to
embed secret data into color image without changing the corresponding grayscale
[1], which largely expands the applications of RDH. In [1], for color image,
channel R and channel B are exploited to carry secret information, channel G is
adjusted for balancing the modifications of channel R and channel B to keep the
invariance of grayscale. However, we found that the embedding performance (EP)
of that method is still unsatisfied and could be further enhanced. To improve
the EP, an adaptive embedding pattern is introduced to enhance the competence
of algorithm for selectively embedding different bits of secret data into
pixels according to context information. Moreover, a novel two-level predictor
is designed by uniting two normal predictors for reducing the ED for embedding
more bits. Experimental results demonstrate that, compared to the previous
method, our scheme could significantly enhance the image fidelity while keeping
the grayscale invariant.
</summary>
    <author>
      <name>Erdun Gao</name>
    </author>
    <author>
      <name>Zhibin Pan</name>
    </author>
    <author>
      <name>Xinyi Gao</name>
    </author>
    <link href="http://arxiv.org/abs/1908.05965v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.05965v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.08505v1</id>
    <updated>2019-08-22T17:24:37Z</updated>
    <published>2019-08-22T17:24:37Z</published>
    <title>ColorNet -- Estimating Colorfulness in Natural Images</title>
    <summary>  Measuring the colorfulness of a natural or virtual scene is critical for many
applications in image processing field ranging from capturing to display. In
this paper, we propose the first deep learning-based colorfulness estimation
metric. For this purpose, we develop a color rating model which simultaneously
learns to extracts the pertinent characteristic color features and the mapping
from feature space to the ideal colorfulness scores for a variety of natural
colored images. Additionally, we propose to overcome the lack of adequate
annotated dataset problem by combining/aligning two publicly available
colorfulness databases using the results of a new subjective test which employs
a common subset of both databases. Using the obtained subjectively annotated
dataset with 180 colored images, we finally demonstrate the efficacy of our
proposed model over the traditional methods, both quantitatively and
qualitatively.
</summary>
    <author>
      <name>Emin Zerman</name>
    </author>
    <author>
      <name>Aakanksha Rana</name>
    </author>
    <author>
      <name>Aljosa Smolic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to IEEE International Conference on Image Processing (ICIP)
  2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.08505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.08505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.10818v1</id>
    <updated>2019-08-28T16:40:20Z</updated>
    <published>2019-08-28T16:40:20Z</published>
    <title>False News Detection on Social Media</title>
    <summary>  Social media has become a major information platform where people consume and
share news. However, it has also enabled the wide dissemination of false news,
i.e., news posts published on social media that are verifiably false, causing
significant negative effects on society. In order to help prevent further
propagation of false news on social media, we set up this competition to
motivate the development of automated real-time false news detection
approaches. Specifically, this competition includes three sub-tasks: false-news
text detection, false-news image detection and false-news multi-modal
detetcion, which aims to motivate participants to further explore the
efficiency of multiple modalities in detecting false news and reasonable fusion
approaches of multi-modal contents. To better support this competition, we also
construct and publicize a multi-modal data repository about False News on Weibo
Social platform(MCG-FNeWS}) to help evaluate the performance of different
approaches from participants.
</summary>
    <author>
      <name>Juan Cao</name>
    </author>
    <author>
      <name>Qiang Sheng</name>
    </author>
    <author>
      <name>Peng Qi</name>
    </author>
    <author>
      <name>Lei Zhong</name>
    </author>
    <author>
      <name>Yanyan Wang</name>
    </author>
    <author>
      <name>Xueyao Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1908.10818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.10818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.11331v1</id>
    <updated>2019-08-29T16:24:29Z</updated>
    <published>2019-08-29T16:24:29Z</published>
    <title>A Robust Image Watermarking System Based on Deep Neural Networks</title>
    <summary>  Digital image watermarking is the process of embedding and extracting
watermark covertly on a carrier image. Incorporating deep learning networks
with image watermarking has attracted increasing attention during recent years.
However, existing deep learning-based watermarking systems cannot achieve
robustness, blindness, and automated embedding and extraction simultaneously.
In this paper, a fully automated image watermarking system based on deep neural
networks is proposed to generalize the image watermarking processes. An
unsupervised deep learning structure and a novel loss computation are proposed
to achieve high capacity and high robustness without any prior knowledge of
possible attacks. Furthermore, a challenging application of watermark
extraction from camera-captured images is provided to validate the practicality
as well as the robustness of the proposed system. Experimental results show the
superiority performance of the proposed system as comparing against several
currently available techniques.
</summary>
    <author>
      <name>Xin Zhong</name>
    </author>
    <author>
      <name>Frank Y. Shih</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2020.3006415</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2020.3006415" rel="related"/>
    <link href="http://arxiv.org/abs/1908.11331v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.11331v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.11517v2</id>
    <updated>2019-09-12T03:28:36Z</updated>
    <published>2019-08-30T03:30:23Z</published>
    <title>UGC-VIDEO: perceptual quality assessment of user-generated videos</title>
    <summary>  Recent years have witnessed an ever-expandingvolume of user-generated content
(UGC) videos available on the Internet. Nevertheless, progress on perceptual
quality assessmentof UGC videos still remains quite limited. There are many
distinguished characteristics of UGC videos in the complete video production
and delivery chain, and one important property closely relevant to video
quality is that there does not exist the pristine source after they are
uploaded to the hosting platform,such that they often undergo multiple
compression stages before ultimately viewed. To facilitate the UGC video
quality assessment,we created a UGC video perceptual quality assessment
database. It contains 50 source videos collected from TikTok with diverse
content, along with multiple distortion versions generated bythe compression
with different quantization levels and coding standards. Subjective quality
assessment was conducted to evaluate the video quality. Furthermore, we
benchmark the database using existing quality assessment algorithms, and
potential roomis observed to future improve the accuracy of UGC video quality
measures.
</summary>
    <author>
      <name>Yang Li</name>
    </author>
    <author>
      <name>Shengbin Meng</name>
    </author>
    <author>
      <name>Xinfeng Zhang</name>
    </author>
    <author>
      <name>Shiqi Wang</name>
    </author>
    <author>
      <name>Yue Wang</name>
    </author>
    <author>
      <name>Siwei Ma</name>
    </author>
    <link href="http://arxiv.org/abs/1908.11517v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.11517v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.01795v2</id>
    <updated>2020-01-21T05:06:20Z</updated>
    <published>2019-10-04T04:10:43Z</published>
    <title>SMP Challenge: An Overview of Social Media Prediction Challenge 2019</title>
    <summary>  "SMP Challenge" aims to discover novel prediction tasks for numerous data on
social multimedia and seek excellent research teams. Making predictions via
social multimedia data (e.g. photos, videos or news) is not only helps us to
make better strategic decisions for the future, but also explores advanced
predictive learning and analytic methods on various problems and scenarios,
such as multimedia recommendation, advertising system, fashion analysis etc.
  In the SMP Challenge at ACM Multimedia 2019, we introduce a novel prediction
task Temporal Popularity Prediction, which focuses on predicting future
interaction or attractiveness (in terms of clicks, views or likes etc.) of new
online posts in social media feeds before uploading. We also collected and
released a large-scale SMPD benchmark with over 480K posts from 69K users. In
this paper, we define the challenge problem, give an overview of the dataset,
present statistics of rich information for data and annotation and design the
accuracy and correlation evaluation metrics for temporal popularity prediction
to the challenge.
</summary>
    <author>
      <name>Bo Wu</name>
    </author>
    <author>
      <name>Wen-Huang Cheng</name>
    </author>
    <author>
      <name>Peiye Liu</name>
    </author>
    <author>
      <name>Bei Liu</name>
    </author>
    <author>
      <name>Zhaoyang Zeng</name>
    </author>
    <author>
      <name>Jiebo Luo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ACM MM 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.01795v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.01795v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.02334v1</id>
    <updated>2019-10-05T22:05:43Z</updated>
    <published>2019-10-05T22:05:43Z</published>
    <title>Hate Speech in Pixels: Detection of Offensive Memes towards Automatic
  Moderation</title>
    <summary>  This work addresses the challenge of hate speech detection in Internet memes,
and attempts using visual information to automatically detect hate speech,
unlike any previous work of our knowledge. Memes are pixel-based multimedia
documents that contain photos or illustrations together with phrases which,
when combined, usually adopt a funny meaning. However, hate memes are also used
to spread hate through social networks, so their automatic detection would help
reduce their harmful societal impact. Our results indicate that the model can
learn to detect some of the memes, but that the task is far from being solved
with this simple architecture. While previous work focuses on linguistic hate
speech, our experiments indicate how the visual modality can be much more
informative for hate speech detection than the linguistic one in memes. In our
experiments, we built a dataset of 5,020 memes to train and evaluate a
multi-layer perceptron over the visual and language representations, whether
independently or fused. The source code and mode and models are available
https://github.com/imatge-upc/hate-speech-detection .
</summary>
    <author>
      <name>Benet Oriol Sabat</name>
    </author>
    <author>
      <name>Cristian Canton Ferrer</name>
    </author>
    <author>
      <name>Xavier Giro-i-Nieto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">AI for Social Good Workshop at NeurIPS 2019 (short paper)</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.02334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.06005v1</id>
    <updated>2019-10-14T09:24:46Z</updated>
    <published>2019-10-14T09:24:46Z</published>
    <title>Real-Time Visual Navigation in Huge Image Sets Using Similarity Graphs</title>
    <summary>  Nowadays stock photo agencies often have millions of images. Non-stop viewing
of 20 million images at a speed of 10 images per second would take more than
three weeks. This demonstrates the impossibility to inspect all images and the
difficulty to get an overview of the entire collection. Although there has been
a lot of effort to improve visual image search, there is little research and
support for visual image exploration. Typically, users start "exploring" an
image collection with a keyword search or an example image for a similarity
search. Both searches lead to long unstructured lists of result images. In
earlier publications, we introduced the idea of graph-based image navigation
and proposed an efficient algorithm for building hierarchical image similarity
graphs for dynamically changing image collections. In this demo we showcase
real-time visual exploration of millions of images with a standard web browser.
Subsets of images are successively retrieved from the graph and displayed as a
visually sorted 2D image map, which can be zoomed and dragged to explore
related concepts. Maintaining the positions of previously shown images creates
the impression of an "endless map". This approach allows an easy visual
image-based navigation, while preserving the complex image relationships of the
graph.
</summary>
    <author>
      <name>Kai Uwe Barthel</name>
    </author>
    <author>
      <name>Nico Hezel</name>
    </author>
    <author>
      <name>Konstantin Schall</name>
    </author>
    <author>
      <name>Klaus Jung</name>
    </author>
    <link href="http://arxiv.org/abs/1910.06005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.06043v1</id>
    <updated>2019-10-14T11:26:08Z</updated>
    <published>2019-10-14T11:26:08Z</published>
    <title>A Hybrid Control Scheme for Adaptive Live Streaming</title>
    <summary>  The live streaming is more challenging than on-demand streaming, because the
low latency is also a strong requirement in addition to the trade-off between
video quality and jitters in playback. To balance several inherently
conflicting performance metrics and improve the overall quality of experience
(QoE), many adaptation schemes have been proposed. Bitrate adaptation is one of
the major solutions for video streaming under time-varying network conditions,
which works even better combining with some latency control methods, such as
adaptive playback rate control and frame dropping. However, it still remains a
challenging problem to design an algorithm to combine these adaptation schemes
together. To tackle this problem, we propose a hybrid control scheme for
adaptive live streaming, namely HYSA, based on heuristic playback rate control,
latency-constrained bitrate control and QoE-oriented adaptive frame dropping.
The proposed scheme utilizes Kaufman's Adaptive Moving Average (KAMA) to
predict segment bitrates for better rate decisions. Extensive simulations
demonstrate that HYSA outperforms most of the existing adaptation schemes on
overall QoE.
</summary>
    <author>
      <name>Huan Peng</name>
    </author>
    <author>
      <name>Yuan Zhang</name>
    </author>
    <author>
      <name>Yongbei Yang</name>
    </author>
    <author>
      <name>Jinyao Yan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, ACMMM 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.06043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.07394v1</id>
    <updated>2019-10-16T14:59:59Z</updated>
    <published>2019-10-16T14:59:59Z</published>
    <title>A Study of Annotation and Alignment Accuracy for Performance Comparison
  in Complex Orchestral Music</title>
    <summary>  Quantitative analysis of commonalities and differences between recorded music
performances is an increasingly common task in computational musicology. A
typical scenario involves manual annotation of different recordings of the same
piece along the time dimension, for comparative analysis of, e.g., the musical
tempo, or for mapping other performance-related information between
performances. This can be done by manually annotating one reference
performance, and then automatically synchronizing other performances, using
audio-to-audio alignment algorithms. In this paper we address several questions
related to those tasks. First, we analyze different annotations of the same
musical piece, quantifying timing deviations between the respective human
annotators. A statistical evaluation of the marker time stamps will provide (a)
an estimate of the expected timing precision of human annotations and (b) a
ground truth for subsequent automatic alignment experiments. We then carry out
a systematic evaluation of different audio features for audio-to-audio
alignment, quantifying the degree of alignment accuracy that can be achieved,
and relate this to the results from the annotation study.
</summary>
    <author>
      <name>Thassilo Gadermaier</name>
    </author>
    <author>
      <name>Gerhard Widmer</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 20th International Society for Music
  Information Retrieval Conference, (ISMIR) 2019, Delft, The Netherlands,
  November 4-8, 2019, pages: 769--775</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.07394v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.07394v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.07992v1</id>
    <updated>2019-10-17T16:06:12Z</updated>
    <published>2019-10-17T16:06:12Z</published>
    <title>Dual-Domain Fusion Convolutional Neural Network for Contrast Enhancement
  Forensics</title>
    <summary>  Contrast enhancement (CE) forensics techniques have always been of great
interest for image forensics community, as they can be an effective tool for
recovering image history and identifying tampered images. Although several CE
forensic algorithms have been proposed, their accuracy and robustness against
some kinds of processing are still unsatisfactory. In order to attenuate such
deficiency, in this paper we propose a new framework based on dual-domain
fusion convolutional neural network to fuse the features of pixel and histogram
domains for CE forensics. Specifically, we first present a pixel-domain
convolutional neural network (P-CNN) to automatically capture the patterns of
contrast-enhanced images in the pixel domain. Then, we present a
histogram-domain convolutional neural network (H-CNN) to extract the features
in the histogram domain. The feature representations of pixel and histogram
domains are fused and fed into two fully connected layers for the
classification of contrast-enhanced images. Experimental results show that the
proposed method achieve better performance and is robust against pre-JPEG
compression and anti-forensics attacks. In addition, a strategy for performance
improvement of CNN-based forensics is explored, which could provide guidance
for the design of CNN-based forensics tools.
</summary>
    <author>
      <name>Pengpeng Yang</name>
    </author>
    <author>
      <name>Rongrong Ni</name>
    </author>
    <author>
      <name>Yao Zhao</name>
    </author>
    <author>
      <name>Gang Cao</name>
    </author>
    <author>
      <name>Wei Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.07992v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.07992v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.08341v1</id>
    <updated>2019-10-18T10:53:46Z</updated>
    <published>2019-10-18T10:53:46Z</published>
    <title>Hide the Image in FC-DenseNets to another Image</title>
    <summary>  In the past, steganography was to embed text in a carrier, the sender Alice
and the recipient Bob share the key, and the text is extracted by Bob through
the key. If more information is embedded, the image is easily distorted. In
contrast, if there is less embedded information, the image maintains good
visual integrity, but does not meet our requirements for steganographic
capacity. In this paper, we focus on tackling these challenges and limitations
to improve steganographic capacity. An image steganography method based on
Fully Convolutional Dense Network(FC-DenseNet) was proposed by us. The hidden
network and the extracted network are trained at the same time. The dataset of
the deep neural network is derived from various natural images of ImageNet. The
experimental results show that the stego-image after steganography and the
secret image extracted from stego-imge have a visually good effect, and the
stego-image has high capacity and high peak signal to noise ratio.
Image-to-image full size hiding is implemented.
</summary>
    <author>
      <name>Duan Xintao</name>
    </author>
    <author>
      <name>Liu Nao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages,7 figures,conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.08341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.08341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.11949v2</id>
    <updated>2021-01-19T12:26:15Z</updated>
    <published>2019-10-25T21:47:52Z</published>
    <title>Automatic Reminiscence Therapy for Dementia</title>
    <summary>  With people living longer than ever, the number of cases with dementia such
as Alzheimer's disease increases steadily. It affects more than 46 million
people worldwide, and it is estimated that in 2050 more than 100 million will
be affected. While there are not effective treatments for these terminal
diseases, therapies such as reminiscence, that stimulate memories from the past
are recommended. Currently, reminiscence therapy takes place in care homes and
is guided by a therapist or a carer. In this work, we present an AI-based
solution to automatize the reminiscence therapy, which consists in a dialogue
system that uses photos as input to generate questions. We run a usability case
study with patients diagnosed of mild cognitive impairment that shows they
found the system very entertaining and challenging. Overall, this paper
presents how reminiscence therapy can be automatized by using machine learning,
and deployed to smartphones and laptops, making the therapy more accessible to
every person affected by dementia.
</summary>
    <author>
      <name>Mariona Caros</name>
    </author>
    <author>
      <name>Maite Garolera</name>
    </author>
    <author>
      <name>Petia Radeva</name>
    </author>
    <author>
      <name>Xavier Giro-i-Nieto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MSc thesis at TelecomBCN, Universitat Politecnica de Catalunya 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.11949v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.11949v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.14571v1</id>
    <updated>2019-10-31T16:25:49Z</updated>
    <published>2019-10-31T16:25:49Z</published>
    <title>Fast Steganalysis Method for VoIP Streams</title>
    <summary>  In this letter, we present a novel and extremely fast steganalysis method of
Voice over IP (VoIP) streams, driven by the need for a quick and accurate
detection of possible steganography in VoIP streams. We firstly analyzed the
correlations in carriers. To better exploit the correlation in code-words, we
mapped vector quantization code-words into a semantic space. In order to
achieve high detection efficiency, only one hidden layer is utilized to extract
the correlations between these code-words. Finally, based on the extracted
correlation features, we used the softmax classifier to categorize the input
stream carriers. To boost the performance of this proposed model, we
incorporate a simple knowledge distillation framework into the training
process. Experimental results show that the proposed method achieves
state-of-the-art performance both in detection accuracy and efficiency. In
particular, the processing time of this method on average is only about 0.05\%
when sample length is as short as 0.1s, attaching strong practical value to
online serving of steganography monitor.
</summary>
    <author>
      <name>Hao Yang</name>
    </author>
    <author>
      <name>ZhongLiang Yang</name>
    </author>
    <author>
      <name>YongJian Bao</name>
    </author>
    <author>
      <name>YongFeng Huang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2019.2961610</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2019.2961610" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.14571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.14571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.00334v1</id>
    <updated>2019-11-01T12:32:40Z</updated>
    <published>2019-11-01T12:32:40Z</published>
    <title>Learning a Representation for Cover Song Identification Using
  Convolutional Neural Network</title>
    <summary>  Cover song identification represents a challenging task in the field of Music
Information Retrieval (MIR) due to complex musical variations between query
tracks and cover versions. Previous works typically utilize hand-crafted
features and alignment algorithms for the task. More recently, further
breakthroughs are achieved employing neural network approaches. In this paper,
we propose a novel Convolutional Neural Network (CNN) architecture based on the
characteristics of the cover song task. We first train the network through
classification strategies; the network is then used to extract music
representation for cover song identification. A scheme is designed to train
robust models against tempo changes. Experimental results show that our
approach outperforms state-of-the-art methods on all public datasets, improving
the performance especially on the large dataset.
</summary>
    <author>
      <name>Zhesong Yu</name>
    </author>
    <author>
      <name>Xiaoshuo Xu</name>
    </author>
    <author>
      <name>Xiaoou Chen</name>
    </author>
    <author>
      <name>Deshun Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">MIREX2020-Cover Song Identification</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.00334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.00334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.00682v2</id>
    <updated>2020-04-15T05:20:55Z</updated>
    <published>2019-11-02T08:51:46Z</published>
    <title>FCEM: A Novel Fast Correlation Extract Model For Real Time Steganalysis
  of VoIP Stream via Multi-head Attention</title>
    <summary>  Extracting correlation features between codes-words with high computational
efficiency is crucial to steganalysis of Voice over IP (VoIP) streams. In this
paper, we utilized attention mechanisms, which have recently attracted enormous
interests due to their highly parallelizable computation and flexibility in
modeling correlation in sequence, to tackle steganalysis problem of
Quantization Index Modulation (QIM) based steganography in compressed VoIP
stream. We design a light-weight neural network named Fast Correlation Extract
Model (FCEM) only based on a variant of attention called multi-head attention
to extract correlation features from VoIP frames. Despite its simple form, FCEM
outperforms complicated Recurrent Neural Networks (RNNs) and Convolutional
Neural Networks (CNNs) models on both prediction accuracy and time efficiency.
It significantly improves the best result in detecting both low embedded rates
and short samples recently. Besides, the proposed model accelerates the
detection speed as twice as before when the sample length is as short as 0.1s,
making it a excellent method for online services.
</summary>
    <author>
      <name>Hao Yang</name>
    </author>
    <author>
      <name>ZhongLiang Yang</name>
    </author>
    <author>
      <name>YongJian Bao</name>
    </author>
    <author>
      <name>Sheng Liu</name>
    </author>
    <author>
      <name>YongFeng Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures. accepted by ICASSP'2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.00682v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.00682v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.00753v1</id>
    <updated>2019-11-02T16:58:15Z</updated>
    <published>2019-11-02T16:58:15Z</published>
    <title>Hybrid blind robust image watermarking technique based on DFT-DCT and
  Arnold transform</title>
    <summary>  In this paper, a robust blind image watermarking method is proposed for
copyright protection of digital images. This hybrid method relies on combining
two well-known transforms that are the discrete Fourier transform (DFT) and the
discrete cosine transform (DCT). The motivation behind this combination is to
enhance the imperceptibility and the robustness. The imperceptibility
requirement is achieved by using magnitudes of DFT coefficients while the
robustness improvement is ensured by applying DCT to the DFT coefficients
magnitude. The watermark is embedded by modifying the coefficients of the
middle band of the DCT using a secret key. The security of the proposed method
is enhanced by applying Arnold transform (AT) to the watermark before
embedding. Experiments were conducted on natural and textured images. Results
show that, compared with state-of-the-art methods, the proposed method is
robust to a wide range of attacks while preserving high imperceptibility.
</summary>
    <author>
      <name>Mohamed Hamidi</name>
    </author>
    <author>
      <name>Mohamed El Haziti</name>
    </author>
    <author>
      <name>Hocine Cherifi</name>
    </author>
    <author>
      <name>Mohammed El Hassouni</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11042-018-5913-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11042-018-5913-9" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">34 page, 17 figures, published in Multimedia Tools and Applications
  Springer, 2018</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Multimedia Tools and Applications, 77(20), 27181-27214 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1911.00753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.00753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.00812v1</id>
    <updated>2019-11-03T02:56:24Z</updated>
    <published>2019-11-03T02:56:24Z</published>
    <title>Adaptive Rate Allocation for View-Aware Point-Cloud Streaming</title>
    <summary>  In the context of view-dependent point-cloud streaming in a scene, our rate
allocation is "adaptive" in the sense that it priorities the point-cloud models
depending on the camera view and the visibility of the objects and their
distance as described. The algorithm delivers higher bitrate to the point-cloud
models which are inside user's viewport, more likely for the user to look at,
or are closer to the view camera or, while delivers lower quality level to the
point-cloud models outside of a user's immediate viewport or farther away from
the camera. For that purpose, we hereby explain the rate allocation problem
within the context of multi-point-cloud streaming where multiple point-cloud
models are aimed to be streamed to the target device, and propose a rate
allocation heuristic algorithm to enable the adaptations within this context.
To the best of our knowledge, this is the first work to mathematically model,
and propose a rate allocation heuristic algorithm within the context of
point-cloud streaming.
</summary>
    <author>
      <name>Mohammad Hosseini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.23436.26244</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.23436.26244" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report, University of Illinois at Urbana-Champaign (UIUC),
  September 2017, 5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.00812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.00812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.01699v1</id>
    <updated>2019-11-05T10:19:07Z</updated>
    <published>2019-11-05T10:19:07Z</published>
    <title>Reversible Data Hiding in Encrypted Images based on Pixel Prediction and
  Bit-plane Compression</title>
    <summary>  Reversible data hiding in encrypted images (RDHEI) receives growing attention
because it protects the content of the original image while the embedded data
can be accurately extracted and the original image can be reconstructed
lossless. To make full use of the correlation of the adjacent pixels, this
paper proposes an RDHEI scheme based on pixel prediction and bit-plane
compression. Firstly, to vacate room for data embedding, the prediction error
of the original image is calculated and used for bit-plane rearrangement and
compression. Then, the image after vacating room is encrypted by a stream
cipher. Finally, the additional data is embedded in the vacated room by
multi-LSB substitution. Experimental results show that the embedding capacity
of the proposed method outperforms the state-of-the-art methods.
</summary>
    <author>
      <name>Zhaoxia Yin</name>
    </author>
    <author>
      <name>Yinyin Peng</name>
    </author>
    <author>
      <name>Youzhi Xiang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TDSC.2020.3019490</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TDSC.2020.3019490" rel="related"/>
    <link href="http://arxiv.org/abs/1911.01699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.01699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.03793v1</id>
    <updated>2019-11-09T22:48:35Z</updated>
    <published>2019-11-09T22:48:35Z</published>
    <title>A Robust Blind 3-D Mesh Watermarking based on Wavelet Transform for
  Copyright Protection</title>
    <summary>  Nowadays, three-dimensional meshes have been extensively used in several
applications such as, industrial, medical, computer-aided design (CAD) and
entertainment due to the processing capability improvement of computers and the
development of the network infrastructure. Unfortunately, like digital images
and videos, 3-D meshes can be easily modified, duplicated and redistributed by
unauthorized users. Digital watermarking came up while trying to solve this
problem. In this paper, we propose a blind robust watermarking scheme for
three-dimensional semiregular meshes for Copyright protection. The watermark is
embedded by modifying the norm of the wavelet coefficient vectors associated
with the lowest resolution level using the edge normal norms as synchronizing
primitives. The experimental results show that in comparison with alternative
3-D mesh watermarking approaches, the proposed method can resist to a wide
range of common attacks, such as similarity transformations including
translation, rotation, uniform scaling and their combination, noise addition,
Laplacian smoothing, quantization, while preserving high imperceptibility.
</summary>
    <author>
      <name>Mohamed Hamidi</name>
    </author>
    <author>
      <name>Mohamed El Haziti</name>
    </author>
    <author>
      <name>Hocine Cherifi</name>
    </author>
    <author>
      <name>Driss Aboutajdine</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ATSIP.2017.8075525</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ATSIP.2017.8075525" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, International Conference on Advanced Technologies
  for Signal and Image Processing (ATSIP'2017)</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.03793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.03793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.05609v1</id>
    <updated>2019-10-03T21:22:47Z</updated>
    <published>2019-10-03T21:22:47Z</published>
    <title>Affective Computing for Large-Scale Heterogeneous Multimedia Data: A
  Survey</title>
    <summary>  The wide popularity of digital photography and social networks has generated
a rapidly growing volume of multimedia data (i.e., image, music, and video),
resulting in a great demand for managing, retrieving, and understanding these
data. Affective computing (AC) of these data can help to understand human
behaviors and enable wide applications. In this article, we survey the
state-of-the-art AC technologies comprehensively for large-scale heterogeneous
multimedia data. We begin this survey by introducing the typical emotion
representation models from psychology that are widely employed in AC. We
briefly describe the available datasets for evaluating AC algorithms. We then
summarize and compare the representative methods on AC of different multimedia
types, i.e., images, music, videos, and multimodal data, with the focus on both
handcrafted features-based methods and deep learning methods. Finally, we
discuss some challenges and future directions for multimedia affective
computing.
</summary>
    <author>
      <name>Sicheng Zhao</name>
    </author>
    <author>
      <name>Shangfei Wang</name>
    </author>
    <author>
      <name>Mohammad Soleymani</name>
    </author>
    <author>
      <name>Dhiraj Joshi</name>
    </author>
    <author>
      <name>Qiang Ji</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3363560</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3363560" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACM TOMM</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.05609v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.05609v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.06981v3</id>
    <updated>2020-04-18T12:42:11Z</updated>
    <published>2019-11-16T07:24:20Z</published>
    <title>Parametric Graph-based Separable Transforms for Video Coding</title>
    <summary>  In many video coding systems, separable transforms (such as two-dimensional
DCT-2) have been used to code block residual signals obtained after prediction.
This paper proposes a parametric approach to build graph-based separable
transforms (GBSTs) for video coding. Specifically, a GBST is derived from a
pair of line graphs, whose weights are determined based on two non-negative
parameters. As certain choices of those parameters correspond to the discrete
sine and cosine transform types used in recent video coding standards
(including DCT-2, DST-7 and DCT-8), this paper further optimizes these graph
parameters to better capture residual block statistics and improve video coding
efficiency. The proposed GBSTs are tested on the Versatile Video Coding (VVC)
reference software, and the experimental results show that about 0.4% average
coding gain is achieved over the existing set of separable transforms
constructed based on DCT-2, DST-7 and DCT-8 in VVC.
</summary>
    <author>
      <name>Hilmi E. Egilmez</name>
    </author>
    <author>
      <name>Oguzhan Teke</name>
    </author>
    <author>
      <name>Amir Said</name>
    </author>
    <author>
      <name>Vadim Seregin</name>
    </author>
    <author>
      <name>Marta Karczewicz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, submitted to IEEE ICIP 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.06981v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.06981v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.07944v1</id>
    <updated>2019-11-18T20:45:42Z</updated>
    <published>2019-11-18T20:45:42Z</published>
    <title>A Knowledge-Driven Quality-of-Experience Model for Adaptive Streaming
  Videos</title>
    <summary>  The fundamental conflict between the enormous space of adaptive streaming
videos and the limited capacity for subjective experiment casts significant
challenges to objective Quality-of-Experience (QoE) prediction. Existing
objective QoE models exhibit complex functional form, failing to generalize
well in diverse streaming environments. In this study, we propose an objective
QoE model namely knowledge-driven streaming quality index (KSQI) to integrate
prior knowledge on the human visual system and human annotated data in a
principled way. By analyzing the subjective characteristics towards streaming
videos from a corpus of subjective studies, we show that a family of QoE
functions lies in a convex set. Using a variant of projected gradient descent,
we optimize the objective QoE model over a database of training videos. The
proposed KSQI demonstrates strong generalizability to diverse streaming
environments, evident by state-of-the-art performance on four publicly
available benchmark datasets.
</summary>
    <author>
      <name>Zhengfang Duanmu</name>
    </author>
    <author>
      <name>Wentao Liu</name>
    </author>
    <author>
      <name>Diqi Chen</name>
    </author>
    <author>
      <name>Zhuoran Li</name>
    </author>
    <author>
      <name>Zhou Wang</name>
    </author>
    <author>
      <name>Yizhou Wang</name>
    </author>
    <author>
      <name>Wen Gao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures, Journal paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.07944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.07944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.03878v2</id>
    <updated>2020-02-19T16:36:58Z</updated>
    <published>2019-12-09T07:14:05Z</published>
    <title>Universal Stego Post-processing for Enhancing Image Steganography</title>
    <summary>  It is well known that the designing or improving embedding cost becomes a key
issue for current steganographic methods. Unlike existing works, we propose a
novel framework to enhance the steganography security via post-processing on
the embedding units (i.e., pixel values and DCT coefficients) of stego
directly. In this paper, we firstly analyze the characteristics of STCs
(Syndrome-Trellis Codes), and then design the rule for post-processing to
ensure the correct extraction of hidden message. Since the steganography
artifacts are typically reflected on image residuals, we try to reduce the
residual distance between cover and the modified stego in order to enhance
steganography security. To this end, we model the post-processing as a
non-linear integer programming, and implement it via heuristic search. In
addition, we carefully determine several important issues in the proposed
post-processing, such as the candidate embedding units to be modified, the
direction and amplitude of post-modification, the adaptive filters for getting
residuals, and the distance measure of residuals. Extensive experimental
results evaluated on both hand-crafted steganalytic features and deep learning
based ones demonstrate that the proposed method can effectively enhance the
security of most modern steganographic methods both in spatial and JPEG
domains.
</summary>
    <author>
      <name>Bolin Chen</name>
    </author>
    <author>
      <name>Weiqi Luo</name>
    </author>
    <author>
      <name>Peijia Zheng</name>
    </author>
    <author>
      <name>Jiwu Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1912.03878v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.03878v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.07101v1</id>
    <updated>2019-12-15T20:09:16Z</updated>
    <published>2019-12-15T20:09:16Z</published>
    <title>Efficient Bitmap-based Indexing and Retrieval of Similarity Search Image
  Queries</title>
    <summary>  Finding similar images is a necessary operation in many multimedia
applications. Images are often represented and stored as a set of
high-dimensional features, which are extracted using localized feature
extraction algorithms. Locality Sensitive Hashing is one of the most popular
approximate processing techniques for finding similar points in
high-dimensional spaces. Locality Sensitive Hashing (LSH) and its variants are
designed to find similar points, but they are not designed to find objects
(such as images, which are made up of a collection of points) efficiently. In
this paper, we propose an index structure, Bitmap-Image LSH (bImageLSH), for
efficient processing of high-dimensional images. Using a real dataset, we
experimentally show the performance benefit of our novel design while keeping
the accuracy of the image results high.
</summary>
    <author>
      <name>Omid Jafari</name>
    </author>
    <author>
      <name>Parth Nagarkar</name>
    </author>
    <author>
      <name>Jonathan Monta√±o</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SSIAI49293.2020.9094616</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SSIAI49293.2020.9094616" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to 2020 IEEE Southwest Symposium on Image Analysis and
  Interpretation (SSIAI 2020)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2020 IEEE Southwest Symposium on Image Analysis and Interpretation
  (SSIAI), Albuquerque, NM, USA, 2020, pp. 58-61</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1912.07101v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.07101v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.2.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.07854v1</id>
    <updated>2019-12-17T07:35:26Z</updated>
    <published>2019-12-17T07:35:26Z</published>
    <title>Enhanced Spatially Interleaved Techniques for Multi-View Distributed
  Video Coding</title>
    <summary>  This paper presents a multi-view distributed video coding framework for
independent camera encoding and centralized decoding. Spatio-temporal-view
concealment methods are developed that exploit the interleaved nature of the
employed hybrid KEY/Wyner-Ziv frames for block-wise generation of the side
information (SI). We study a number of view concealment methods and develop a
joint approach that exploits all available correlation for forming the side
information. We apply a diversity technique for fusing multiple such
predictions thereby achieving more reliable results. We additionally introduce
systems enhancements for further improving the rate distortion performance
through selective feedback, inter-view bitplane projection and frame
subtraction. Results show a significant improvement in performance relative to
H.264 intra coding of up to 25% reduction in bitrate or equivalently 2.5 dB
increase in PSNR.
</summary>
    <author>
      <name>Nantheera Anantrasirichai</name>
    </author>
    <author>
      <name>Dimitris Agrafiotis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.07854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.07854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.09669v1</id>
    <updated>2019-12-20T07:20:05Z</updated>
    <published>2019-12-20T07:20:05Z</published>
    <title>An Efficient Coding Method for Spike Camera using Inter-Spike Intervals</title>
    <summary>  Recently, a novel bio-inspired spike camera has been proposed, which
continuously accumulates luminance intensity and fires spikes while the
dispatch threshold is reached. Compared to the conventional frame-based cameras
and the emerging dynamic vision sensors, the spike camera has shown great
advantages in capturing fast-moving scene in a frame-free manner with full
texture reconstruction capabilities. However, it is difficult to transmit or
store the large amount of spike data. To address this problem, we first
investigate the spatiotemporal distribution of inter-spike intervals and
propose an intensity-based measurement of spike train distance. Then, we design
an efficient spike coding method, which integrates the techniques of adaptive
temporal partitioning, intra-/inter-pixel prediction, quantization and entropy
coding into a unified lossy coding framework. Finally, we construct a PKU-Spike
dataset captured by the spike camera to evaluate the compression performance.
The experimental results on the dataset demonstrate that the proposed approach
is effective in compressing such spike data while maintaining the fidelity.
</summary>
    <author>
      <name>Siwei Dong</name>
    </author>
    <author>
      <name>Lin Zhu</name>
    </author>
    <author>
      <name>Daoyuan Xu</name>
    </author>
    <author>
      <name>Yonghong Tian</name>
    </author>
    <author>
      <name>Tiejun Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in DCC 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.09669v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.09669v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.10131v1</id>
    <updated>2019-12-20T22:55:40Z</updated>
    <published>2019-12-20T22:55:40Z</published>
    <title>Leveraging Topics and Audio Features with Multimodal Attention for Audio
  Visual Scene-Aware Dialog</title>
    <summary>  With the recent advancements in Artificial Intelligence (AI), Intelligent
Virtual Assistants (IVA) such as Alexa, Google Home, etc., have become a
ubiquitous part of many homes. Currently, such IVAs are mostly audio-based, but
going forward, we are witnessing a confluence of vision, speech and dialog
system technologies that are enabling the IVAs to learn audio-visual groundings
of utterances. This will enable agents to have conversations with users about
the objects, activities and events surrounding them. In this work, we present
three main architectural explorations for the Audio Visual Scene-Aware Dialog
(AVSD): 1) investigating `topics' of the dialog as an important contextual
feature for the conversation, 2) exploring several multimodal attention
mechanisms during response generation, 3) incorporating an end-to-end audio
classification ConvNet, AclNet, into our architecture. We discuss detailed
analysis of the experimental results and show that our model variations
outperform the baseline system presented for the AVSD task.
</summary>
    <author>
      <name>Shachi H Kumar</name>
    </author>
    <author>
      <name>Eda Okur</name>
    </author>
    <author>
      <name>Saurav Sahay</name>
    </author>
    <author>
      <name>Jonathan Huang</name>
    </author>
    <author>
      <name>Lama Nachman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 3rd Visually Grounded Interaction and Language
  (ViGIL) Workshop, NeurIPS 2019, Vancouver, Canada. arXiv admin note:
  substantial text overlap with arXiv:1812.08407, arXiv:1912.10132</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.10131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.10131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.10413v2</id>
    <updated>2020-01-01T18:51:59Z</updated>
    <published>2019-12-22T10:19:44Z</published>
    <title>Hiding Data in Images Using Cryptography and Deep Neural Network</title>
    <summary>  Steganography is an art of obscuring data inside another quotidian file of
similar or varying types. Hiding data has always been of significant importance
to digital forensics. Previously, steganography has been combined with
cryptography and neural networks separately. Whereas, this research combines
steganography, cryptography with the neural networks all together to hide an
image inside another container image of the larger or same size. Although the
cryptographic technique used is quite simple, but is effective when convoluted
with deep neural nets. Other steganography techniques involve hiding data
efficiently, but in a uniform pattern which makes it less secure. This method
targets both the challenges and make data hiding secure and non-uniform.
</summary>
    <author>
      <name>Kartik Sharma</name>
    </author>
    <author>
      <name>Ashutosh Aggarwal</name>
    </author>
    <author>
      <name>Tanay Singhania</name>
    </author>
    <author>
      <name>Deepak Gupta</name>
    </author>
    <author>
      <name>Ashish Khanna</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.33969/AIS.2019.11009</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.33969/AIS.2019.11009" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 9 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.10413v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.10413v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.10789v1</id>
    <updated>2019-11-01T22:55:26Z</updated>
    <published>2019-11-01T22:55:26Z</published>
    <title>JPEG Image Compression using the Discrete Cosine Transform: An Overview,
  Applications, and Hardware Implementation</title>
    <summary>  Digital images are becoming large in size containing more information day by
day to represent the as is state of the original one due to the availability of
high resolution digital cameras, smartphones, and medical tests images.
Therefore, we need to come up with some technique to convert these images into
smaller size without loosing much information from the actual. There are both
lossy and lossless image compression format available and JPEG is one of the
popular lossy compression among them. In this paper, we present the
architecture and implementation of JPEG compression using VHDL (VHSIC Hardware
Description Language) and compare the performance with some contemporary
implementation. JPEG compression takes place in five steps with color space
conversion, down sampling, discrete cosine transformation (DCT), quantization,
and entropy encoding. The five steps cover for the compression purpose only.
Additionally, we implement the reverse order in VHDL to get the original image
back. We use optimized matrix multiplication and quantization for DCT to
achieve better performance. Our experimental results show that significant
amount of compression ratio has been achieved with very little change in the
images, which is barely noticeable to human eye.
</summary>
    <author>
      <name>Ahmad Shawahna</name>
    </author>
    <author>
      <name>Md. Enamul Haque</name>
    </author>
    <author>
      <name>Alaaeldin Amin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.10789v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.10789v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.11318v1</id>
    <updated>2019-12-24T12:28:59Z</updated>
    <published>2019-12-24T12:28:59Z</published>
    <title>Quality of Experience for Streaming Services: Measurements, Challenges
  and Insights</title>
    <summary>  Over the last few years, the evolution of network and user handsets'
technologies, have challenged the telecom industry and the Internet ecosystem.
Especially, the unprecedented progress of multimedia streaming services like
YouTube, Vimeo and DailyMotion resulted in an impressive demand growth and a
significant need of Quality of Service (QoS) (e.g., high data rate, low
latency/jitter, etc.). Mainly, numerous difficulties are to be considered while
delivering a specific service, such as a strict QoS, human-centric features,
massive number of devices, heterogeneous devices and networks, and
uncontrollable environments. Thenceforth, the concept of Quality of Experience
(QoE) is gaining visibility, and tremendous research efforts have been spent on
improving and/or delivering reliable and addedvalue services, at a high user
experience. In this paper, we present the importance of QoE in wireless and
mobile networks (4G, 5G, and beyond), by providing standard definitions and the
most important measurement methods developed. Moreover, we exhibit notable
enhancements and controlling approaches proposed by researchers to meet the
user expectation in terms of service experience.
</summary>
    <author>
      <name>Khadija Bouraqia</name>
    </author>
    <author>
      <name>Essaid Sabir</name>
    </author>
    <author>
      <name>Mohamed Sadik</name>
    </author>
    <author>
      <name>Latif Ladid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 3 figures, Submitted to IEEE Access</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.11318v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.11318v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.12871v2</id>
    <updated>2022-02-18T10:06:26Z</updated>
    <published>2019-12-30T10:24:30Z</published>
    <title>Text Steganalysis with Attentional LSTM-CNN</title>
    <summary>  With the rapid development of Natural Language Processing (NLP) technologies,
text steganography methods have been significantly innovated recently, which
poses a great threat to cybersecurity. In this paper, we propose a novel
attentional LSTM-CNN model to tackle the text steganalysis problem. The
proposed method firstly maps words into semantic space for better exploitation
of the semantic feature in texts and then utilizes a combination of
Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM)
recurrent neural networks to capture both local and long-distance contextual
information in steganography texts. In addition, we apply attention mechanism
to recognize and attend to important clues within suspicious sentences. After
merge feature clues from Convolutional Neural Networks (CNNs) and Recurrent
Neural Networks (RNNs), we use a softmax layer to categorize the input text as
cover or stego. Experiments showed that our model can achieve the state-of-art
result in the text steganalysis task.
</summary>
    <author>
      <name>YongJian Bao</name>
    </author>
    <author>
      <name>Hao Yang</name>
    </author>
    <author>
      <name>Zhongliang Yang</name>
    </author>
    <author>
      <name>Sheng Liu</name>
    </author>
    <author>
      <name>Yongfeng Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">error content</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.12871v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.12871v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.01425v1</id>
    <updated>2020-02-04T17:47:36Z</updated>
    <published>2020-02-04T17:47:36Z</published>
    <title>Spatially Variant Laplacian Pyramids for Multi-Frame Exposure Fusion</title>
    <summary>  Laplacian Pyramid Blending is a commonly used method for several seamless
image blending tasks. While the method works well for images with comparable
intensity levels, it is often unable to produce artifact free images for
applications which handle images with large intensity variation such as
exposure fusion. This paper proposes a spatially varying Laplacian Pyramid
Blending to blend images with large intensity differences. The proposed method
dynamically alters the blending levels during the final stage of Pyramid
Reconstruction based on the amount of local intensity variation. The proposed
algorithm out performs state-of-the-art methods for image blending both
qualitatively as well as quantitatively on publicly available High Dynamic
Range (HDR) imaging dataset. Qualitative improvements are demonstrated in terms
of details, halos and dark halos. For quantitative comparison, the no-reference
perceptual metric MEF-SSIM was used.
</summary>
    <author>
      <name>Anmol Biswas</name>
    </author>
    <author>
      <name>Green Rosh K S</name>
    </author>
    <author>
      <name>Sachin Deepak Lomte</name>
    </author>
    <link href="http://arxiv.org/abs/2002.01425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.01425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.02370v1</id>
    <updated>2020-01-13T12:59:18Z</updated>
    <published>2020-01-13T12:59:18Z</published>
    <title>Data hiding in speech signal using steganography and encryption</title>
    <summary>  Data privacy and data security are always on highest priority in the world.
We need a reliable method to encrypt the data so that it reaches the
destination safely. Encryption is a simple yet effective way to protect our
data while transmitting it to a destination. The proposed method has state of
art technology of steganography and encryption. This paper puts forward a
different approach for data hiding in speech signals. A ten-digit number within
speech signal using audio steganography and encrypting it with a unique key for
better security. At the receiver end the same unique key is used to decrypt the
received signal and then hidden numbers are extracted. The proposed approach
performance can be evaluated by PSNR, MSE, SSIM and bit-error rate. The
simulation results give better performance compared to existing approach.
</summary>
    <author>
      <name>Hanisha Chowdary N</name>
    </author>
    <author>
      <name>Karan K</name>
    </author>
    <author>
      <name>Bharath K P</name>
    </author>
    <author>
      <name>Rajesh Kumar M</name>
    </author>
    <link href="http://arxiv.org/abs/2002.02370v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.02370v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.03156v1</id>
    <updated>2020-02-08T13:02:24Z</updated>
    <published>2020-02-08T13:02:24Z</published>
    <title>A Time-Frequency Perspective on Audio Watermarking</title>
    <summary>  Existing audio watermarking methods usually treat the host audio signals of a
function of time or frequency individually, while considering them in the joint
time-frequency (TF) domain has received less attention. This paper proposes an
audio watermarking framework from the perspective of TF analysis. The proposed
framework treats the host audio signal in the 2-dimensional (2D) TF plane, and
selects a series of patches within the 2D TF image. These patches correspond to
the TF clusters with minimum averaged energy, and are used to form the feature
vectors for watermark embedding. Classical spread spectrum embedding schemes
are incorporated in the framework. The feature patches that carry the
watermarks only occupy a few TF regions of the host audio signal, thus leading
to improved imperceptibility property. In addition, since the feature patches
contain a neighborhood area of TF representation of audio samples, the
correlations among the samples within a single patch could be exploited for
improved robustness against a series of processing attacks. Extensive
experiments are carried out to illustrate the effectiveness of the proposed
system, as compared to its counterpart systems. The aim of this work is to shed
some light on the notion of audio watermarking in TF feature domain, which may
potentially lead us to more robust watermarking solutions against malicious
attacks.
</summary>
    <author>
      <name>Haijian Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2002.03156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.03156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.09607v1</id>
    <updated>2020-02-22T02:53:39Z</updated>
    <published>2020-02-22T02:53:39Z</published>
    <title>Multi-Representation Knowledge Distillation For Audio Classification</title>
    <summary>  As an important component of multimedia analysis tasks, audio classification
aims to discriminate between different audio signal types and has received
intensive attention due to its wide applications. Generally speaking, the raw
signal can be transformed into various representations (such as Short Time
Fourier Transform and Mel Frequency Cepstral Coefficients), and information
implied in different representations can be complementary. Ensembling the
models trained on different representations can greatly boost the
classification performance, however, making inference using a large number of
models is cumbersome and computationally expensive. In this paper, we propose a
novel end-to-end collaborative learning framework for the audio classification
task. The framework takes multiple representations as the input to train the
models in parallel. The complementary information provided by different
representations is shared by knowledge distillation. Consequently, the
performance of each model can be significantly promoted without increasing the
computational overhead in the inference stage. Extensive experimental results
demonstrate that the proposed approach can improve the classification
performance and achieve state-of-the-art results on both acoustic scene
classification tasks and general audio tagging tasks.
</summary>
    <author>
      <name>Liang Gao</name>
    </author>
    <author>
      <name>Kele Xu</name>
    </author>
    <author>
      <name>Huaimin Wang</name>
    </author>
    <author>
      <name>Yuxing Peng</name>
    </author>
    <link href="http://arxiv.org/abs/2002.09607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.09607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.10651v1</id>
    <updated>2020-02-25T03:35:06Z</updated>
    <published>2020-02-25T03:35:06Z</published>
    <title>A Comparative Evaluation of Temporal Pooling Methods for Blind Video
  Quality Assessment</title>
    <summary>  Many objective video quality assessment (VQA) algorithms include a key step
of temporal pooling of frame-level quality scores. However, less attention has
been paid to studying the relative efficiencies of different pooling methods on
no-reference (blind) VQA. Here we conduct a large-scale comparative evaluation
to assess the capabilities and limitations of multiple temporal pooling
strategies on blind VQA of user-generated videos. The study yields insights and
general guidance regarding the application and selection of temporal pooling
models. In addition, we also propose an ensemble pooling model built on top of
high-performing temporal pooling models. Our experimental results demonstrate
the relative efficacies of the evaluated temporal pooling models, using several
popular VQA algorithms, and evaluated on two recent large-scale natural video
quality databases. In addition to the new ensemble model, we provide a general
recipe for applying temporal pooling of frame-based quality predictions.
</summary>
    <author>
      <name>Zhengzhong Tu</name>
    </author>
    <author>
      <name>Chia-Ju Chen</name>
    </author>
    <author>
      <name>Li-Heng Chen</name>
    </author>
    <author>
      <name>Neil Birkbeck</name>
    </author>
    <author>
      <name>Balu Adsumilli</name>
    </author>
    <author>
      <name>Alan C. Bovik</name>
    </author>
    <link href="http://arxiv.org/abs/2002.10651v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.10651v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.01958v2</id>
    <updated>2020-04-28T10:44:46Z</updated>
    <published>2020-03-04T08:57:59Z</published>
    <title>ASMD: an automatic framework for compiling multimodal datasets with
  audio and scores</title>
    <summary>  This paper describes an open-source Python framework for handling datasets
for music processing tasks, built with the aim of improving the reproducibility
of research projects in music computing and assessing the generalization
abilities of machine learning models. The framework enables the automatic
download and installation of several commonly used datasets for multimodal
music processing. Specifically, we provide a Python API to access the datasets
through Boolean set operations based on particular attributes, such as
intersections and unions of composers, instruments, and so on. The framework is
designed to ease the inclusion of new datasets and the respective ground-truth
annotations so that one can build, convert, and extend one's own collection as
well as distribute it by means of a compliant format to take advantage of the
API. All code and ground-truth are released under suitable open licenses.
</summary>
    <author>
      <name>Federico Simonetta</name>
    </author>
    <author>
      <name>Stavros Ntalampiras</name>
    </author>
    <author>
      <name>Federico Avanzini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.3898665</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.3898665" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the Sound and Music Computing Conference 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.01958v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.01958v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.5; I.2.m; H.3.7; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.02526v2</id>
    <updated>2020-07-16T13:43:21Z</updated>
    <published>2020-03-05T10:44:37Z</published>
    <title>Cloud Rendering-based Volumetric Video Streaming System for Mixed
  Reality Services</title>
    <summary>  Volumetric video is an emerging technology for immersive representation of 3D
spaces that captures objects from all directions using multiple cameras and
creates a dynamic 3D model of the scene. However, processing volumetric content
requires high amounts of processing power and is still a very demanding task
for today's mobile devices. To mitigate this, we propose a volumetric video
streaming system that offloads the rendering to a powerful cloud/edge server
and only sends the rendered 2D view to the client instead of the full
volumetric content. We use 6DoF head movement prediction techniques, WebRTC
protocol and hardware video encoding to ensure low-latency in different parts
of the processing chain. We demonstrate our system using both a browser-based
client and a Microsoft HoloLens client. Our application contains generic
interfaces that allow for easy deployment of various augmented/mixed reality
clients using the same server implementation.
</summary>
    <author>
      <name>Serhan G√ºl</name>
    </author>
    <author>
      <name>Dimitri Podborski</name>
    </author>
    <author>
      <name>Jangwoo Son</name>
    </author>
    <author>
      <name>Gurdeep Singh Bhullar</name>
    </author>
    <author>
      <name>Thomas Buchholz</name>
    </author>
    <author>
      <name>Thomas Schierl</name>
    </author>
    <author>
      <name>Cornelius Hellge</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3339825.3393583</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3339825.3393583" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">11th ACM Multimedia Systems Conference (MMSys) 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2003.02526v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.02526v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.08865v1</id>
    <updated>2020-03-19T15:28:08Z</updated>
    <published>2020-03-19T15:28:08Z</published>
    <title>DRST: Deep Residual Shearlet Transform for Densely Sampled Light Field
  Reconstruction</title>
    <summary>  The Image-Based Rendering (IBR) approach using Shearlet Transform (ST) is one
of the most effective methods for Densely-Sampled Light Field (DSLF)
reconstruction. The ST-based DSLF reconstruction typically relies on an
iterative thresholding algorithm for Epipolar-Plane Image (EPI) sparse
regularization in shearlet domain, involving dozens of transformations between
image domain and shearlet domain, which are in general time-consuming. To
overcome this limitation, a novel learning-based ST approach, referred to as
Deep Residual Shearlet Transform (DRST), is proposed in this paper.
Specifically, for an input sparsely-sampled EPI, DRST employs a deep fully
Convolutional Neural Network (CNN) to predict the residuals of the shearlet
coefficients in shearlet domain in order to reconstruct a densely-sampled EPI
in image domain. The DRST network is trained on synthetic Sparsely-Sampled
Light Field (SSLF) data only by leveraging elaborately-designed masks.
Experimental results on three challenging real-world light field evaluation
datasets with varying moderate disparity ranges (8 - 16 pixels) demonstrate the
superiority of the proposed learning-based DRST approach over the
non-learning-based ST method for DSLF reconstruction. Moreover, DRST provides a
2.4x speedup over ST, at least.
</summary>
    <author>
      <name>Yuan Gao</name>
    </author>
    <author>
      <name>Robert Bregovic</name>
    </author>
    <author>
      <name>Reinhard Koch</name>
    </author>
    <author>
      <name>Atanas Gotchev</name>
    </author>
    <link href="http://arxiv.org/abs/2003.08865v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.08865v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.09249v1</id>
    <updated>2020-03-20T13:06:55Z</updated>
    <published>2020-03-20T13:06:55Z</published>
    <title>Continuous QoE Prediction Based on WaveNet</title>
    <summary>  Continuous QoE prediction is crucial in the purpose of maximizing viewer
satisfaction, by which video service providers could improve the revenue.
Continuously predicting QoE is challenging since it requires QoE models that
are capable of capturing the complex dependencies among QoE influence factors.
The existing approaches that utilize Long-Short-Term-Memory (LSTM) network
successfully model such long-term dependencies, providing the superior QoE
prediction performance. However, the inherent drawback of sequential computing
of LSTM will result in high computational cost in training and prediction
tasks. Recently, WaveNet, a deep neural network for generating raw audio
waveform, has been introduced. Immediately, it gains a great attention since it
successfully leverages the characteristic of parallel computing of causal
convolution and dilated convolution to deal with time-series data (e.g., audio
signal). Being inspired by the success of WaveNet, in this paper, we propose
WaveNet-based QoE model for continuous QoE prediction in video streaming
services. The model is trained and tested upon on two publicly available
databases, namely, LFOVIA Video QoE and LIVE Mobile Stall Video II. The
experimental results demonstrate that the proposed model outperforms the
baselines models in terms of processing time, while maintaining sufficient
accuracy.
</summary>
    <author>
      <name>Phan Xuan Tan</name>
    </author>
    <author>
      <name>Tho Nguyen Duc</name>
    </author>
    <author>
      <name>Chanh Minh Tran</name>
    </author>
    <author>
      <name>Eiji Kamioka</name>
    </author>
    <link href="http://arxiv.org/abs/2003.09249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.09249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.10546v1</id>
    <updated>2020-03-09T05:47:05Z</updated>
    <published>2020-03-09T05:47:05Z</published>
    <title>Forensic Analysis of Residual Information in Adobe PDF Files</title>
    <summary>  In recent years, as electronic files include personal records and business
activities, these files can be used as important evidences in a digital
forensic investigation process. In general, the data that can be verified using
its own application programs is largely used in the investigation of document
files. However, in the case of the PDF file that has been largely used at the
present time, certain data, which include the data before some modifications,
exist in electronic document files unintentionally. Because such residual
information may present the writing process of a file, it can be usefully used
in a forensic viewpoint. This paper introduces why the residual information is
stored inside the PDF file and explains a way to extract the information. In
addition, we demonstrate the attributes of PDF files can be used to hide data.
</summary>
    <author>
      <name>Hyunji Chung</name>
    </author>
    <author>
      <name>Jungheum Park</name>
    </author>
    <author>
      <name>Sangjin Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.10546v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.10546v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.11100v1</id>
    <updated>2020-03-24T20:15:12Z</updated>
    <published>2020-03-24T20:15:12Z</published>
    <title>How deep is your encoder: an analysis of features descriptors for an
  autoencoder-based audio-visual quality metric</title>
    <summary>  The development of audio-visual quality assessment models poses a number of
challenges in order to obtain accurate predictions. One of these challenges is
the modelling of the complex interaction that audio and visual stimuli have and
how this interaction is interpreted by human users. The No-Reference
Audio-Visual Quality Metric Based on a Deep Autoencoder (NAViDAd) deals with
this problem from a machine learning perspective. The metric receives two sets
of audio and video features descriptors and produces a low-dimensional set of
features used to predict the audio-visual quality. A basic implementation of
NAViDAd was able to produce accurate predictions tested with a range of
different audio-visual databases. The current work performs an ablation study
on the base architecture of the metric. Several modules are removed or
re-trained using different configurations to have a better understanding of the
metric functionality. The results presented in this study provided important
feedback that allows us to understand the real capacity of the metric's
architecture and eventually develop a much better audio-visual quality metric.
</summary>
    <author>
      <name>Helard Martinez</name>
    </author>
    <author>
      <name>Andrew Hines</name>
    </author>
    <author>
      <name>Mylene C. Q. Farias</name>
    </author>
    <link href="http://arxiv.org/abs/2003.11100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.11100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.12428v1</id>
    <updated>2020-03-08T06:22:04Z</updated>
    <published>2020-03-08T06:22:04Z</published>
    <title>A General Approach for Using Deep Neural Network for Digital
  Watermarking</title>
    <summary>  Technologies of the Internet of Things (IoT) facilitate digital contents such
as images being acquired in a massive way. However, consideration from the
privacy or legislation perspective still demands the need for intellectual
content protection. In this paper, we propose a general deep neural network
(DNN) based watermarking method to fulfill this goal. Instead of training a
neural network for protecting a specific image, we train on an image set and
use the trained model to protect a distinct test image set in a bulk manner.
Respective evaluations both from the subjective and objective aspects confirm
the supremacy and practicability of our proposed method. To demonstrate the
robustness of this general neural watermarking mechanism, commonly used
manipulations are applied to the watermarked image to examine the corresponding
extracted watermark, which still retains sufficient recognizable traits. To the
best of our knowledge, we are the first to propose a general way to perform
watermarking using DNN. Considering its performance and economy, it is
concluded that subsequent studies that generalize our work on utilizing DNN for
intellectual content protection is a promising research trend.
</summary>
    <author>
      <name>Yurui Ming</name>
    </author>
    <author>
      <name>Weiping Ding</name>
    </author>
    <author>
      <name>Zehong Cao</name>
    </author>
    <author>
      <name>Chin-Teng Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2003.12428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.12428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.13217v1</id>
    <updated>2020-03-30T04:49:45Z</updated>
    <published>2020-03-30T04:49:45Z</published>
    <title>Deep Residual Neural Networks for Image in Speech Steganography</title>
    <summary>  Steganography is the art of hiding a secret message inside a publicly visible
carrier message. Ideally, it is done without modifying the carrier, and with
minimal loss of information in the secret message. Recently, various deep
learning based approaches to steganography have been applied to different
message types. We propose a deep learning based technique to hide a source RGB
image message inside finite length speech segments without perceptual loss. To
achieve this, we train three neural networks; an encoding network to hide the
message in the carrier, a decoding network to reconstruct the message from the
carrier and an additional image enhancer network to further improve the
reconstructed message. We also discuss future improvements to the algorithm
proposed.
</summary>
    <author>
      <name>Shivam Agarwal</name>
    </author>
    <author>
      <name>Siddarth Venkatraman</name>
    </author>
    <link href="http://arxiv.org/abs/2003.13217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.13217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.13684v1</id>
    <updated>2020-03-28T00:07:34Z</updated>
    <published>2020-03-28T00:07:34Z</published>
    <title>Social-Sensor Composition for Tapestry Scenes</title>
    <summary>  The extensive use of social media platforms and overwhelming amounts of
imagery data creates unique opportunities for sensing, gathering and sharing
information about events. One of its potential applications is to leverage
crowdsourced social media images to create a tapestry scene for scene analysis
of designated locations and time intervals. The existing attempts however
ignore the temporal-semantic relevance and spatio-temporal evolution of the
images and direction-oriented scene reconstruction. We propose a novel
social-sensor cloud (SocSen) service composition approach to form tapestry
scenes for scene analysis. The novelty lies in utilising images and image
meta-information to bypass expensive traditional image processing techniques to
reconstruct scenes. Metadata, such as geolocation, time and angle of view of an
image are modelled as non-functional attributes of a SocSen service. Our major
contribution lies on proposing a context and direction-aware spatio-temporal
clustering and recommendation approach for selecting a set of temporally and
semantically similar services to compose the best available SocSen services.
Analytical results based on real datasets are presented to demonstrate the
performance of the proposed approach.
</summary>
    <author>
      <name>Tooba Aamir</name>
    </author>
    <author>
      <name>Hai Dong</name>
    </author>
    <author>
      <name>Athman Bouguettaya</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSC.2020.2974741</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSC.2020.2974741" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages. IEEE Transactions on Services Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.13684v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.13684v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.8; H.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.02940v1</id>
    <updated>2020-04-06T19:02:31Z</updated>
    <published>2020-04-06T19:02:31Z</published>
    <title>Robust Wavelet-Based Watermarking Using Dynamic Strength Factor</title>
    <summary>  In unsecured network environments, ownership protection of digital contents,
such as images, is becoming a growing concern. Different watermarking methods
have been proposed to address the copyright protection of digital materials.
Watermarking methods are challenged with conflicting parameters of
imperceptibility and robustness. While embedding a watermark with a high
strength factor increases robustness, it also decreases imperceptibility of the
watermark. Thus embedding in visually less sensitive regions, i.e., complex
image blocks could satisfy both requirements. This paper presents a new
wavelet-based watermarking technique using an adaptive strength factor to
tradeoff between watermark transparency and robustness. We measure variations
of each image block to adaptively set a strength-factor for embedding the
watermark in that block. On the other hand, the decoder uses the selected
coefficients to safely extract the watermark through a voting algorithm. The
proposed method shows better results in terms of PSNR and BER in comparison to
recent methods for attacks, such as Median Filter, Gaussian Filter, and JPEG
compression.
</summary>
    <author>
      <name>Mahsa Kadkhodaei</name>
    </author>
    <author>
      <name>Shadrokh Samavi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">four pages, five figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.02940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.02940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.03413v2</id>
    <updated>2020-04-09T12:34:12Z</updated>
    <published>2020-04-07T14:05:30Z</published>
    <title>Direct Speech-to-image Translation</title>
    <summary>  Direct speech-to-image translation without text is an interesting and useful
topic due to the potential applications in human-computer interaction, art
creation, computer-aided design. etc. Not to mention that many languages have
no writing form. However, as far as we know, it has not been well-studied how
to translate the speech signals into images directly and how well they can be
translated. In this paper, we attempt to translate the speech signals into the
image signals without the transcription stage. Specifically, a speech encoder
is designed to represent the input speech signals as an embedding feature, and
it is trained with a pretrained image encoder using teacher-student learning to
obtain better generalization ability on new classes. Subsequently, a stacked
generative adversarial network is used to synthesize high-quality images
conditioned on the embedding feature. Experimental results on both synthesized
and real data show that our proposed method is effective to translate the raw
speech signals into images without the middle text representation. Ablation
study gives more insights about our method.
</summary>
    <author>
      <name>Jiguo Li</name>
    </author>
    <author>
      <name>Xinfeng Zhang</name>
    </author>
    <author>
      <name>Chuanmin Jia</name>
    </author>
    <author>
      <name>Jizheng Xu</name>
    </author>
    <author>
      <name>Li Zhang</name>
    </author>
    <author>
      <name>Yue Wang</name>
    </author>
    <author>
      <name>Siwei Ma</name>
    </author>
    <author>
      <name>Wen Gao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JSTSP.2020.2987417</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JSTSP.2020.2987417" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by JSTSP</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.03413v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.03413v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.04959v1</id>
    <updated>2020-04-10T09:18:12Z</updated>
    <published>2020-04-10T09:18:12Z</published>
    <title>Stacked Convolutional Deep Encoding Network for Video-Text Retrieval</title>
    <summary>  Existing dominant approaches for cross-modal video-text retrieval task are to
learn a joint embedding space to measure the cross-modal similarity. However,
these methods rarely explore long-range dependency inside video frames or
textual words leading to insufficient textual and visual details. In this
paper, we propose a stacked convolutional deep encoding network for video-text
retrieval task, which considers to simultaneously encode long-range and
short-range dependency in the videos and texts. Specifically, a multi-scale
dilated convolutional (MSDC) block within our approach is able to encode
short-range temporal cues between video frames or text words by adopting
different scales of kernel size and dilation size of convolutional layer. A
stacked structure is designed to expand the receptive fields by repeatedly
adopting the MSDC block, which further captures the long-range relations
between these cues. Moreover, to obtain more robust textual representations, we
fully utilize the powerful language model named Transformer in two stages:
pretraining phrase and fine-tuning phrase. Extensive experiments on two
different benchmark datasets (MSR-VTT, MSVD) show that our proposed method
outperforms other state-of-the-art approaches.
</summary>
    <author>
      <name>Rui Zhao</name>
    </author>
    <author>
      <name>Kecheng Zheng</name>
    </author>
    <author>
      <name>Zheng-jun Zha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.04959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.04959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.10345v1</id>
    <updated>2020-04-21T23:45:18Z</updated>
    <published>2020-04-21T23:45:18Z</published>
    <title>MIDI-Sheet Music Alignment Using Bootleg Score Synthesis</title>
    <summary>  MIDI-sheet music alignment is the task of finding correspondences between a
MIDI representation of a piece and its corresponding sheet music images. Rather
than using optical music recognition to bridge the gap between sheet music and
MIDI, we explore an alternative approach: projecting the MIDI data into pixel
space and performing alignment in the image domain. Our method converts the
MIDI data into a crude representation of the score that only contains
rectangular floating notehead blobs, a process we call bootleg score synthesis.
Furthermore, we project sheet music images into the same bootleg space by
applying a deep watershed notehead detector and filling in the bounding boxes
around each detected notehead. Finally, we align the bootleg representations
using a simple variant of dynamic time warping. On a dataset of 68 real scanned
piano scores from IMSLP and corresponding MIDI performances, our method
achieves a 97.3% accuracy at an error tolerance of one second, outperforming
several baseline systems that employ optical music recognition.
</summary>
    <author>
      <name>Thitaree Tanprasert</name>
    </author>
    <author>
      <name>Teerapat Jenrungrot</name>
    </author>
    <author>
      <name>Meinard Mueller</name>
    </author>
    <author>
      <name>T. J. Tsai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures, 1 table. Accepted paper at the International
  Society for Music Information Retrieval Conference (ISMIR) 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.10345v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.10345v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.11449v1</id>
    <updated>2020-04-23T20:29:26Z</updated>
    <published>2020-04-23T20:29:26Z</published>
    <title>Upgrading the Newsroom: An Automated Image Selection System for News
  Articles</title>
    <summary>  We propose an automated image selection system to assist photo editors in
selecting suitable images for news articles. The system fuses multiple textual
sources extracted from news articles and accepts multilingual inputs. It is
equipped with char-level word embeddings to help both modeling morphologically
rich languages, e.g. German, and transferring knowledge across nearby
languages. The text encoder adopts a hierarchical self-attention mechanism to
attend more to both keywords within a piece of text and informative components
of a news article. We extensively experiment with our system on a large-scale
text-image database containing multimodal multilingual news articles collected
from Swiss local news media websites. The system is compared with multiple
baselines with ablation studies and is shown to beat existing text-image
retrieval methods in a weakly-supervised learning setting. Besides, we also
offer insights on the advantage of using multiple textual sources and
multilingual data.
</summary>
    <author>
      <name>Fangyu Liu</name>
    </author>
    <author>
      <name>R√©mi Lebret</name>
    </author>
    <author>
      <name>Didier Orel</name>
    </author>
    <author>
      <name>Philippe Sordet</name>
    </author>
    <author>
      <name>Karl Aberer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACM Transactions on Multimedia Computing Communications
  and Applications (ACM TOMM)</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.11449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.11449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.11490v1</id>
    <updated>2020-04-23T23:44:58Z</updated>
    <published>2020-04-23T23:44:58Z</published>
    <title>Transformation of Mean Opinion Scores to Avoid Misleading of Ranked
  based Statistical Techniques</title>
    <summary>  The rank correlation coefficients and the ranked-based statistical tests (as
a subset of non-parametric techniques) might be misleading when they are
applied to subjectively collected opinion scores. Those techniques assume that
the data is measured at least at an ordinal level and define a sequence of
scores to represent a tied rank when they have precisely an equal numeric
value.
  In this paper, we show that the definition of tied rank, as mentioned above,
is not suitable for Mean Opinion Scores (MOS) and might be misleading
conclusions of rank-based statistical techniques. Furthermore, we introduce a
method to overcome this issue by transforming the MOS values considering their
$95\%$ Confidence Intervals. The rank correlation coefficients and ranked-based
statistical tests can then be safely applied to the transformed values. We also
provide open-source software packages in different programming languages to
utilize the application of our transformation method in the quality of
experience domain.
</summary>
    <author>
      <name>Babak Naderi</name>
    </author>
    <author>
      <name>Sebastian M√∂ller</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/QoMEX48832.2020.9123078</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/QoMEX48832.2020.9123078" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">his paper has been accepted for publication in the 2020 Twelfth
  International Conference on Quality of Multimedia Experience (QoMEX)</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.11490v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.11490v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.11511v1</id>
    <updated>2020-04-24T02:23:52Z</updated>
    <published>2020-04-24T02:23:52Z</published>
    <title>Reinforcing Short-Length Hashing</title>
    <summary>  Due to the compelling efficiency in retrieval and storage,
similarity-preserving hashing has been widely applied to approximate nearest
neighbor search in large-scale image retrieval. However, existing methods have
poor performance in retrieval using an extremely short-length hash code due to
weak ability of classification and poor distribution of hash bit. To address
this issue, in this study, we propose a novel reinforcing short-length hashing
(RSLH). In this proposed RSLH, mutual reconstruction between the hash
representation and semantic labels is performed to preserve the semantic
information. Furthermore, to enhance the accuracy of hash representation, a
pairwise similarity matrix is designed to make a balance between accuracy and
training expenditure on memory. In addition, a parameter boosting strategy is
integrated to reinforce the precision with hash bits fusion. Extensive
experiments on three large-scale image benchmarks demonstrate the superior
performance of RSLH under various short-length hashing scenarios.
</summary>
    <author>
      <name>Xingbo Liu</name>
    </author>
    <author>
      <name>Xiushan Nie</name>
    </author>
    <author>
      <name>Qi Dai</name>
    </author>
    <author>
      <name>Yupan Huang</name>
    </author>
    <author>
      <name>Yilong Yin</name>
    </author>
    <link href="http://arxiv.org/abs/2004.11511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.11511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.11977v1</id>
    <updated>2020-04-24T20:31:38Z</updated>
    <published>2020-04-24T20:31:38Z</published>
    <title>Steganography Based on Pixel Intensity Value Decomposition</title>
    <summary>  This paper focuses on steganography based on pixel intensity value
decomposition. A number of existing schemes such as binary, Fibonacci, Prime,
Natural, Lucas, and Catalan-Fibonacci (CF) are evaluated in terms of payload
capacity and stego quality. A new technique based on a specific representation
is used to decompose pixel intensity values into 16 (virtual) bit-planes
suitable for embedding purposes. The new decomposition scheme has a desirable
property whereby the sum of all bit-planes does not exceed the maximum pixel
intensity value, i.e. 255. Experimental results demonstrate that the new
decomposition scheme offers a better compromise between payload capacity and
stego quality than other existing decomposition schemes used for embedding
messages. However, embedding in the 6th bit-plane onwards, the proposed scheme
offers better stego quality. In general, the new decomposition technique has
less effect in terms of quality on pixel value when compared to most existing
pixel intensity value decomposition techniques when embedding messages in
higher bit-planes.
</summary>
    <author>
      <name>Alan Anwer Abdulla</name>
    </author>
    <author>
      <name>Harin Sellahewa</name>
    </author>
    <author>
      <name>Sabah A. Jassim</name>
    </author>
    <link href="http://arxiv.org/abs/2004.11977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.11977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.11984v1</id>
    <updated>2020-04-24T20:41:16Z</updated>
    <published>2020-04-24T20:41:16Z</published>
    <title>Efficient High Capacity Steganography Technique</title>
    <summary>  Performance indicators characterizing modern steganographic techniques
include capacity (i.e. the quantity of data that can be hidden in the cover
medium), stego quality (i.e. artifacts visibility), security (i.e.
undetectability), and strength or robustness (intended as the resistance
against active attacks aimed to destroy the secret message). Fibonacci based
embedding techniques have been researched and proposed in the literature to
achieve efficient steganography in terms of capacity with respect to stego
quality. In this paper, we investigated an innovative idea that extends
Fibonacci-like steganography by bit-plane(s) mapping instead of bit-plane(s)
replacement. Our proposed algorithm increases embedding capacity using
bit-plane mapping to embed two bits of the secret message in three bits of a
pixel of the cover, at the expense of a marginal loss in stego quality. While
existing Fibonacci embedding algorithms do not use certain intensities of the
cover for embedding due to the limitation imposed by the Zeckendorf theorem,
our proposal solve this problem and make all intensity values candidates for
embedding. Experimental results demonstrate that the proposed technique double
the embedding capacity when compared to existing Fibonacci methods, and it is
secure against statistical attacks such as RS, POV, and difference image
histogram (DIH).
</summary>
    <author>
      <name>Alan Anwer Abdulla</name>
    </author>
    <author>
      <name>Sabah A. Jassim</name>
    </author>
    <author>
      <name>Harin Sellahewa</name>
    </author>
    <link href="http://arxiv.org/abs/2004.11984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.11984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.11994v1</id>
    <updated>2020-04-24T21:22:20Z</updated>
    <published>2020-04-24T21:22:20Z</published>
    <title>Bharatanatyam Dance Transcription using Multimedia Ontology and Machine
  Learning</title>
    <summary>  Indian Classical Dance is an over 5000 years' old multi-modal language for
expressing emotions. Preservation of dance through multimedia technology is a
challenging task. In this paper, we develop a system to generate a parseable
representation of a dance performance. The system will help to preserve
intangible heritage, annotate performances for better tutoring, and synthesize
dance performances. We first attempt to capture the concepts of the basic steps
of an Indian Classical Dance form, named Bharatanatyam Adavus, in an
ontological model. Next, we build an event-based low-level model that relates
the ontology of Adavus to the ontology of multi-modal data streams (RGB-D of
Kinect in this case) for a computationally realizable framework. Finally, the
ontology is used for transcription into Labanotation. We also present a
transcription tool for encoding the performances of Bharatanatyam Adavus to
Labanotation and test it on our recorded data set. Our primary aim is to
document the complex movements of dance in terms of Labanotation using the
ontology.
</summary>
    <author>
      <name>Tanwi Mallick</name>
    </author>
    <author>
      <name>Patha Pratim Das</name>
    </author>
    <author>
      <name>Arun Kumar Majumdar</name>
    </author>
    <link href="http://arxiv.org/abs/2004.11994v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.11994v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.12038v1</id>
    <updated>2020-04-25T01:49:22Z</updated>
    <published>2020-04-25T01:49:22Z</published>
    <title>Fuzzy Logic Based Integration of Web Contextual Linguistic Structures
  for Enriching Conceptual Visual Representations</title>
    <summary>  Due to the difficulty of automatically mapping visual features with semantic
descriptors, state-of-the-art frameworks have exhibited poor performance in
terms of coverage and effectiveness for indexing the visual content. This
prompted us to investigate the use of both the Web as a large information
source from where to extract relevant contextual linguistic information and
bimodal visual-textual indexing as a technique to enrich the vocabulary of
index concepts. Our proposal is based on the Signal/Semantic approach for
multimedia indexing which generates multi-facetted conceptual representations
of the visual content. We propose to enrich these image representations with
concepts automatically extracted from the visual contextual information. We
specifically target the integration of semantic concepts which are more
specific than the initial index concepts since they represent the visual
content with greater accuracy and precision. Also, we aim to correct the faulty
indexes resulting from the automatic semantic tagging. Experimentally, the
details of the prototyping are given and the presented technique is tested in a
Web-scale evaluation on 30 queries representing elaborate image scenes.
</summary>
    <author>
      <name>M. Belkhatir</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TETCI.2018.2849417</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TETCI.2018.2849417" rel="related"/>
    <link href="http://arxiv.org/abs/2004.12038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.12038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.12467v1</id>
    <updated>2020-04-26T20:11:33Z</updated>
    <published>2020-04-26T20:11:33Z</published>
    <title>Stego Quality Enhancement by Message Size Reduction and Fibonacci
  Bit-Plane Mapping</title>
    <summary>  An efficient 2-step steganography technique is proposed to enhance stego
image quality and secret message un-detectability. The first step is a
preprocessing algorithm that reduces the size of secret images without losing
information. This results in improved stego image quality compared to other
existing image steganography methods. The proposed secret image size reduction
(SISR) algorithm is an efficient spatial domain technique. The second step is
an embedding mechanism that relies on Fibonacci representation of pixel
intensities to minimize the effect of embedding on the stego image quality. The
improvement is attained by using bit-plane(s) mapping instead of bit-plane(s)
replacement for embedding. The proposed embedding mechanism outperforms the
binary based LSB randomly embedding in two ways: reduced effect on stego
quality and increased robustness against statistical steganalysers.
Experimental results demonstrate the benefits of the proposed scheme in terms
of: 1) SISR ratio (indirectly results in increased capacity); 2) quality of the
stego; and 3) robustness against steganalysers such as RS, and WS. Furthermore,
experimental results show that the proposed SISR algorithm can be extended to
be applicable on DICOM standard medical images. Future security standardization
research is proposed that would focus on evaluating the security, performance,
and effectiveness of steganography algorithms.
</summary>
    <author>
      <name>Alan A. Abdulla</name>
    </author>
    <author>
      <name>Harin Sellahewa</name>
    </author>
    <author>
      <name>Sabah A. Jassim</name>
    </author>
    <link href="http://arxiv.org/abs/2004.12467v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.12467v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.12569v1</id>
    <updated>2020-04-27T03:35:32Z</updated>
    <published>2020-04-27T03:35:32Z</published>
    <title>DWT-GBT-SVD-based Robust Speech Steganography</title>
    <summary>  Steganography is a method that can improve network security and make
communications safer. In this method, a secret message is hidden in content
like audio signals that should not be perceptible by listening to the audio or
seeing the signal waves. Also, it should be robust against different common
attacks such as noise and compression. In this paper, we propose a new speech
steganography method based on a combination of Discrete Wavelet Transform,
Graph-based Transform, and Singular Value Decomposition (SVD). In this method,
we first find voiced frames based on energy and zero-crossing counts of the
frames and then embed a binary message into voiced frames. Experimental results
on the NOIZEUS database show that the proposed method is imperceptible and also
robust against Gaussian noise, re-sampling, re-quantization, high pass filter,
and low pass filter. Also, it is robust against MP3 compression and scaling for
watermarking applications.
</summary>
    <author>
      <name>Noshin Amiri</name>
    </author>
    <author>
      <name>Iman Naderi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.12569v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.12569v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.00836v1</id>
    <updated>2020-05-02T14:08:39Z</updated>
    <published>2020-05-02T14:08:39Z</published>
    <title>Towards Deep Learning Methods for Quality Assessment of
  Computer-Generated Imagery</title>
    <summary>  Video gaming streaming services are growing rapidly due to new services such
as passive video streaming, e.g. Twitch.tv, and cloud gaming, e.g. Nvidia
Geforce Now. In contrast to traditional video content, gaming content has
special characteristics such as extremely high motion for some games, special
motion patterns, synthetic content and repetitive content, which makes the
state-of-the-art video and image quality metrics perform weaker for this
special computer generated content. In this paper, we outline our plan to build
a deep learningbased quality metric for video gaming quality assessment. In
addition, we present initial results by training the network based on VMAF
values as a ground truth to give some insights on how to build a metric in
future. The paper describes the method that is used to choose an appropriate
Convolutional Neural Network architecture. Furthermore, we estimate the size of
the required subjective quality dataset which achieves a sufficiently high
performance. The results show that by taking around 5k images for training of
the last six modules of Xception, we can obtain a relatively high performance
metric to assess the quality of distorted video games.
</summary>
    <author>
      <name>Markus Utke</name>
    </author>
    <author>
      <name>Saman Zadtootaghaj</name>
    </author>
    <author>
      <name>Steven Schmidt</name>
    </author>
    <author>
      <name>Sebastian M√∂ller</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.00836v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.00836v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.02472v1</id>
    <updated>2020-05-05T20:21:53Z</updated>
    <published>2020-05-05T20:21:53Z</published>
    <title>Cross-media Structured Common Space for Multimedia Event Extraction</title>
    <summary>  We introduce a new task, MultiMedia Event Extraction (M2E2), which aims to
extract events and their arguments from multimedia documents. We develop the
first benchmark and collect a dataset of 245 multimedia news articles with
extensively annotated events and arguments. We propose a novel method, Weakly
Aligned Structured Embedding (WASE), that encodes structured representations of
semantic information from textual and visual data into a common embedding
space. The structures are aligned across modalities by employing a weakly
supervised training strategy, which enables exploiting available resources
without explicit cross-media annotation. Compared to uni-modal state-of-the-art
methods, our approach achieves 4.0% and 9.8% absolute F-score gains on text
event argument role labeling and visual event extraction. Compared to
state-of-the-art multimedia unstructured representations, we achieve 8.3% and
5.0% absolute F-score gains on multimedia event extraction and argument role
labeling, respectively. By utilizing images, we extract 21.4% more event
mentions than traditional text-only methods.
</summary>
    <author>
      <name>Manling Li</name>
    </author>
    <author>
      <name>Alireza Zareian</name>
    </author>
    <author>
      <name>Qi Zeng</name>
    </author>
    <author>
      <name>Spencer Whitehead</name>
    </author>
    <author>
      <name>Di Lu</name>
    </author>
    <author>
      <name>Heng Ji</name>
    </author>
    <author>
      <name>Shih-Fu Chang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as an oral paper at ACL 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.02472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.02472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.03286v1</id>
    <updated>2020-05-07T07:13:20Z</updated>
    <published>2020-05-07T07:13:20Z</published>
    <title>Multi-view data capture using edge-synchronised mobiles</title>
    <summary>  Multi-view data capture permits free-viewpoint video (FVV) content creation.
To this end, several users must capture video streams, calibrated in both time
and pose, framing the same object/scene, from different viewpoints.
New-generation network architectures (e.g. 5G) promise lower latency and larger
bandwidth connections supported by powerful edge computing, properties that
seem ideal for reliable FVV capture. We have explored this possibility, aiming
to remove the need for bespoke synchronisation hardware when capturing a scene
from multiple viewpoints, making it possible through off-the-shelf mobiles. We
propose a novel and scalable data capture architecture that exploits edge
resources to synchronise and harvest frame captures. We have designed an edge
computing unit that supervises the relaying of timing triggers to and from
multiple mobiles, in addition to synchronising frame harvesting. We empirically
show the benefits of our edge computing unit by analysing latencies and show
the quality of 3D reconstruction outputs against an alternative and popular
centralised solution based on Unity3D.
</summary>
    <author>
      <name>Matteo Bortolon</name>
    </author>
    <author>
      <name>Paul Chippendale</name>
    </author>
    <author>
      <name>Stefano Messelodi</name>
    </author>
    <author>
      <name>Fabio Poiesi</name>
    </author>
    <link href="http://arxiv.org/abs/2005.03286v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.03286v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.04400v1</id>
    <updated>2020-05-09T09:28:01Z</updated>
    <published>2020-05-09T09:28:01Z</published>
    <title>Comment on "No-Reference Video Quality Assessment Based on the Temporal
  Pooling of Deep Features"</title>
    <summary>  In Neural Processing Letters 50,3 (2019) a machine learning approach to blind
video quality assessment was proposed. It is based on temporal pooling of
features of video frames, taken from the last pooling layer of deep
convolutional neural networks. The method was validated on two established
benchmark datasets and gave results far better than the previous
state-of-the-art. In this letter we report the results from our careful
reimplementations. The performance results, claimed in the paper, cannot be
reached, and are even below the state-of-the-art by a large margin. We show
that the originally reported wrong performance results are a consequence of two
cases of data leakage. Information from outside the training dataset was used
in the fine-tuning stage and in the model evaluation.
</summary>
    <author>
      <name>Franz G√∂tz-Hahn</name>
    </author>
    <author>
      <name>Vlad Hosu</name>
    </author>
    <author>
      <name>Dietmar Saupe</name>
    </author>
    <link href="http://arxiv.org/abs/2005.04400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.04400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.04425v2</id>
    <updated>2020-05-12T14:07:55Z</updated>
    <published>2020-05-09T12:26:58Z</published>
    <title>Building a Manga Dataset "Manga109" with Annotations for Multimedia
  Applications</title>
    <summary>  Manga, or comics, which are a type of multimodal artwork, have been left
behind in the recent trend of deep learning applications because of the lack of
a proper dataset. Hence, we built Manga109, a dataset consisting of a variety
of 109 Japanese comic books (94 authors and 21,142 pages) and made it publicly
available by obtaining author permissions for academic use. We carefully
annotated the frames, speech texts, character faces, and character bodies; the
total number of annotations exceeds 500k. This dataset provides numerous manga
images and annotations, which will be beneficial for use in machine learning
algorithms and their evaluation. In addition to academic use, we obtained
further permission for a subset of the dataset for industrial use. In this
article, we describe the details of the dataset and present a few examples of
multimedia processing applications (detection, retrieval, and generation) that
apply existing deep learning methods and are made possible by the dataset.
</summary>
    <author>
      <name>Kiyoharu Aizawa</name>
    </author>
    <author>
      <name>Azuma Fujimoto</name>
    </author>
    <author>
      <name>Atsushi Otsubo</name>
    </author>
    <author>
      <name>Toru Ogawa</name>
    </author>
    <author>
      <name>Yusuke Matsui</name>
    </author>
    <author>
      <name>Koki Tsubota</name>
    </author>
    <author>
      <name>Hikaru Ikuta</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MMUL.2020.2987895</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MMUL.2020.2987895" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE MultiMedia 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2005.04425v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.04425v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.05319v1</id>
    <updated>2020-05-11T21:23:14Z</updated>
    <published>2020-05-11T21:23:14Z</published>
    <title>Hardware Implementation of Adaptive Watermarking Based on Local Spatial
  Disorder Analysis</title>
    <summary>  With the increasing use of the internet and the ease of exchange of
multimedia content, the protection of ownership rights has become a significant
concern. Watermarking is an efficient means for this purpose. In many
applications, real-time watermarking is required, which demands hardware
implementation of low complexity and robust algorithm. In this paper, an
adaptive watermarking is presented, which uses embedding in different
bit-planes to achieve transparency and robustness. Local disorder of pixels is
analyzed to control the strength of the watermark. A new low complexity method
for disorder analysis is proposed, and its hardware implantation is presented.
An embedding method is proposed, which causes lower degradation in the
watermarked image. Also, the performance of proposed watermarking architecture
is improved by a pipe-line structure and is tested on an FPGA device. Results
show that the algorithm produces transparent and robust watermarked images. The
synthesis report from FPGA implementation illustrates a low complexity hardware
structure.
</summary>
    <author>
      <name>Mohsen Hajabdolahi</name>
    </author>
    <author>
      <name>Nader Karimi</name>
    </author>
    <author>
      <name>Shahram Shirani</name>
    </author>
    <author>
      <name>Shadrokh Samavi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.05319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.05319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.06944v1</id>
    <updated>2020-04-19T13:50:30Z</updated>
    <published>2020-04-19T13:50:30Z</published>
    <title>The Hyper360 toolset for enriched 360$^\circ$ video</title>
    <summary>  360$^\circ$ video is a novel media format, rapidly becoming adopted in media
production and consumption as part of todays ongoing virtual reality
revolution. Due to its novelty, there is a lack of tools for producing highly
engaging 360$^\circ$ video for consumption on a multitude of platforms. In this
work, we describe the work done so far in the Hyper360 project on tools for
360$^\circ$ video. Furthermore, the first pilots which have been produced with
the Hyper360 tools are presented.
</summary>
    <author>
      <name>Hannes Fassold</name>
    </author>
    <author>
      <name>Antonis Karakottas</name>
    </author>
    <author>
      <name>Dorothea Tsatsou</name>
    </author>
    <author>
      <name>Dimitrios Zarpalas</name>
    </author>
    <author>
      <name>Barnabas Takacs</name>
    </author>
    <author>
      <name>Christian Fuhrhop</name>
    </author>
    <author>
      <name>Angelo Manfredi</name>
    </author>
    <author>
      <name>Nicolas Patz</name>
    </author>
    <author>
      <name>Simona Tonoli</name>
    </author>
    <author>
      <name>Iana Dulskaia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for Collaborative Research and Innovation Projects track at
  ICME 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.06944v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.06944v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.07925v1</id>
    <updated>2020-05-16T09:58:41Z</updated>
    <published>2020-05-16T09:58:41Z</published>
    <title>Spatiotemporal Adaptive Quantization for Video Compression Applications</title>
    <summary>  JCT-VC HEVC HM 16 includes a Coding Unit (CU) level adaptive Quantization
Parameter (QP) technique named AdaptiveQP. It is designed to perceptually
adjust the QP in Y, Cb and Cr Coding Blocks (CBs) based only on the variance of
samples in a luma CB. In this paper, we propose an adaptive quantisation
technique that consists of two contributions. The first contribution relates to
accounting for the variance of chroma samples, in addition to luma samples, in
a CU. The second contribution relates to accounting for CU temporal information
as well as CU spatial information. Moreover, we integrate into our method a
lambda refined QP technique to reduce complexity associated multiple QP
optimizations in the Rate Distortion Optimization process. We evaluate the
proposed technique on 4:4:4, 4:2:2, 4:2:0 and 4:0:0 YCbCr test sequences, for
which we quantify the results using the Bjontegaard Delta Rate (BD-Rate)
metric. Our method achieves a maximum BD-Rate reduction of 23.1% (Y), 26.7%
(Cr) and 25.2% (Cb). Furthermore, a maximum encoding time reduction of 4.4% is
achieved.
</summary>
    <author>
      <name>Lee Prangnell</name>
    </author>
    <link href="http://arxiv.org/abs/2005.07925v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.07925v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.07928v1</id>
    <updated>2020-05-16T10:02:01Z</updated>
    <published>2020-05-16T10:02:01Z</published>
    <title>Spatiotemporal Adaptive Quantization for the Perceptual Video Coding of
  RGB 4:4:4 Data</title>
    <summary>  Due to the spectral sensitivity phenomenon of the Human Visual System (HVS),
the color channels of raw RGB 4:4:4 sequences contain significant psychovisual
redundancies; these redundancies can be perceptually quantized. The default
quantization systems in the HEVC standard are known as Uniform Reconstruction
Quantization (URQ) and Rate Distortion Optimized Quantization (RDOQ); URQ and
RDOQ are not perceptually optimized for the coding of RGB 4:4:4 video data. In
this paper, we propose a novel spatiotemporal perceptual quantization technique
named SPAQ. With application for RGB 4:4:4 video data, SPAQ exploits HVS
spectral sensitivity-related color masking in addition to spatial masking and
temporal masking; SPAQ operates at the Coding Block (CB) level and the
Prediction Unit (PU) level. The proposed technique perceptually adjusts the
Quantization Step Size (QStep) at the CB level if high variance spatial data in
G, B and R CBs is detected and also if high motion vector magnitudes in PUs are
detected. Compared with anchor 1 (HEVC HM 16.17 RExt), SPAQ considerably
reduces bitrates with a maximum reduction of approximately 80%. The Mean
Opinion Score (MOS) in the subjective evaluations, in addition to the SSIM
scores, show that SPAQ successfully achieves perceptually lossless compression
compared with anchors.
</summary>
    <author>
      <name>Lee Prangnell</name>
    </author>
    <author>
      <name>Victor Sanchez</name>
    </author>
    <link href="http://arxiv.org/abs/2005.07928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.07928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.09639v1</id>
    <updated>2020-05-18T19:00:03Z</updated>
    <published>2020-05-18T19:00:03Z</published>
    <title>Webpage Segmentation for Extracting Images and Their Surrounding
  Contextual Information</title>
    <summary>  Web images come in hand with valuable contextual information. Although this
information has long been mined for various uses such as image annotation,
clustering of images, inference of image semantic content, etc., insufficient
attention has been given to address issues in mining this contextual
information. In this paper, we propose a webpage segmentation algorithm
targeting the extraction of web images and their contextual information based
on their characteristics as they appear on webpages. We conducted a user study
to obtain a human-labeled dataset to validate the effectiveness of our method
and experiments demonstrated that our method can achieve better results
compared to an existing segmentation algorithm.
</summary>
    <author>
      <name>F. Fauzi</name>
    </author>
    <author>
      <name>H. J. Long</name>
    </author>
    <author>
      <name>M. Belkhatir</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1631272.1631379</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1631272.1631379" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2005.02156</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.09639v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.09639v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.09984v1</id>
    <updated>2020-05-20T12:06:40Z</updated>
    <published>2020-05-20T12:06:40Z</published>
    <title>A Modified Fourier-Mellin Approach for Source Device Identification on
  Stabilized Videos</title>
    <summary>  To decide whether a digital video has been captured by a given device,
multimedia forensic tools usually exploit characteristic noise traces left by
the camera sensor on the acquired frames. This analysis requires that the noise
pattern characterizing the camera and the noise pattern extracted from video
frames under analysis are geometrically aligned. However, in many practical
scenarios this does not occur, thus a re-alignment or synchronization has to be
performed. Current solutions often require time consuming search of the
realignment transformation parameters. In this paper, we propose to overcome
this limitation by searching scaling and rotation parameters in the frequency
domain. The proposed algorithm tested on real videos from a well-known
state-of-the-art dataset shows promising results.
</summary>
    <author>
      <name>Sara Mandelli</name>
    </author>
    <author>
      <name>Fabrizio Argenti</name>
    </author>
    <author>
      <name>Paolo Bestagini</name>
    </author>
    <author>
      <name>Massimo Iuliani</name>
    </author>
    <author>
      <name>Alessandro Piva</name>
    </author>
    <author>
      <name>Stefano Tubaro</name>
    </author>
    <link href="http://arxiv.org/abs/2005.09984v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.09984v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.11735v2</id>
    <updated>2020-11-04T13:14:42Z</updated>
    <published>2020-05-24T12:51:25Z</published>
    <title>Robust Spatial-spread Deep Neural Image Watermarking</title>
    <summary>  Watermarking is an operation of embedding an information into an image in a
way that allows to identify ownership of the image despite applying some
distortions on it. In this paper, we presented a novel end-to-end solution for
embedding and recovering the watermark in the digital image using convolutional
neural networks. The method is based on spreading the message over the spatial
domain of the image, hence reducing the "local bits per pixel" capacity. To
obtain the model we used adversarial training and applied noiser layers between
the encoder and the decoder. Moreover, we broadened the spectrum of typically
considered attacks on the watermark and by grouping the attacks according to
their scope, we achieved high general robustness, most notably against JPEG
compression, Gaussian blurring, subsampling or resizing. To help us in the
models training we also proposed a precise differentiable approximation of
JPEG.
</summary>
    <author>
      <name>Marcin Plata</name>
    </author>
    <author>
      <name>Piotr Syga</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TrustCom50675.2020.00022</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TrustCom50675.2020.00022" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The article was accepted on TrustCom 2020: The 19th IEEE
  International Conference on Trust, Security and Privacy in Computing and
  Communications</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.11735v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.11735v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.12788v1</id>
    <updated>2020-05-26T15:12:08Z</updated>
    <published>2020-05-26T15:12:08Z</published>
    <title>Self-play Reinforcement Learning for Video Transmission</title>
    <summary>  Video transmission services adopt adaptive algorithms to ensure users'
demands. Existing techniques are often optimized and evaluated by a function
that linearly combines several weighted metrics. Nevertheless, we observe that
the given function fails to describe the requirement accurately. Thus, such
proposed methods might eventually violate the original needs. To eliminate this
concern, we propose \emph{Zwei}, a self-play reinforcement learning algorithm
for video transmission tasks. Zwei aims to update the policy by
straightforwardly utilizing the actual requirement. Technically, Zwei samples a
number of trajectories from the same starting point and instantly estimates the
win rate w.r.t the competition outcome. Here the competition result represents
which trajectory is closer to the assigned requirement. Subsequently, Zwei
optimizes the strategy by maximizing the win rate. To build Zwei, we develop
simulation environments, design adequate neural network models, and invent
training methods for dealing with different requirements on various video
transmission scenarios. Trace-driven analysis over two representative tasks
demonstrates that Zwei optimizes itself according to the assigned requirement
faithfully, outperforming the state-of-the-art methods under all considered
scenarios.
</summary>
    <author>
      <name>Tianchi Huang</name>
    </author>
    <author>
      <name>Rui-Xiao Zhang</name>
    </author>
    <author>
      <name>Lifeng Sun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3386290.3396930</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3386290.3396930" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in NOSSDAV'20</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.12788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.12788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.13331v2</id>
    <updated>2020-06-01T11:29:52Z</updated>
    <published>2020-05-27T12:52:07Z</published>
    <title>Memory Assessment of Versatile Video Coding</title>
    <summary>  This paper presents a memory assessment of the next-generation Versatile
Video Coding (VVC). The memory analyses are performed adopting as a baseline
the state-of-the-art High-Efficiency Video Coding (HEVC). The goal is to offer
insights and observations of how critical the memory requirements of VVC are
aggravated, compared to HEVC. The adopted methodology consists of two sets of
experiments: (1) an overall memory profiling and (2) an inter-prediction
specific memory analysis. The results obtained in the memory profiling show
that VVC access up to 13.4x more memory than HEVC. Moreover, the
inter-prediction module remains (as in HEVC) the most resource-intensive
operation in the encoder: 60%-90% of the memory requirements. The
inter-prediction specific analysis demonstrates that VVC requires up to 5.3x
more memory accesses than HEVC. Furthermore, our analysis indicates that up to
23% of such growth is due to VVC novel-CU sizes (larger than 64x64).
</summary>
    <author>
      <name>Arthur Cerveira</name>
    </author>
    <author>
      <name>Luciano Agostini</name>
    </author>
    <author>
      <name>Bruno Zatt</name>
    </author>
    <author>
      <name>Felipe Sampaio</name>
    </author>
    <link href="http://arxiv.org/abs/2005.13331v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.13331v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.13876v1</id>
    <updated>2020-05-28T09:46:53Z</updated>
    <published>2020-05-28T09:46:53Z</published>
    <title>Investigating Correlations of Automatically Extracted Multimodal
  Features and Lecture Video Quality</title>
    <summary>  Ranking and recommendation of multimedia content such as videos is usually
realized with respect to the relevance to a user query. However, for lecture
videos and MOOCs (Massive Open Online Courses) it is not only required to
retrieve relevant videos, but particularly to find lecture videos of high
quality that facilitate learning, for instance, independent of the video's or
speaker's popularity. Thus, metadata about a lecture video's quality are
crucial features for learning contexts, e.g., lecture video recommendation in
search as learning scenarios. In this paper, we investigate whether
automatically extracted features are correlated to quality aspects of a video.
A set of scholarly videos from a Mass Open Online Course (MOOC) is analyzed
regarding audio, linguistic, and visual features. Furthermore, a set of
cross-modal features is proposed which are derived by combining transcripts,
audio, video, and slide content. A user study is conducted to investigate the
correlations between the automatically collected features and human ratings of
quality aspects of a lecture video. Finally, the impact of our features on the
knowledge gain of the participants is discussed.
</summary>
    <author>
      <name>Jianwei Shi</name>
    </author>
    <author>
      <name>Christian Otto</name>
    </author>
    <author>
      <name>Anett Hoppe</name>
    </author>
    <author>
      <name>Peter Holtz</name>
    </author>
    <author>
      <name>Ralph Ewerth</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3347451.3356731</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3347451.3356731" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SALMM '19: Proceedings of the 1st International Workshop on Search
  as Learning with Multimedia Information, co-located with ACM Multimedia 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2005.13876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.13876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.03903v1</id>
    <updated>2020-06-06T16:08:24Z</updated>
    <published>2020-06-06T16:08:24Z</published>
    <title>Are Social Networks Watermarking Us or Are We (Unawarely) Watermarking
  Ourself?</title>
    <summary>  In the last decade, Social Networks (SNs) have deeply changed many aspects of
society, and one of the most widespread behaviours is the sharing of pictures.
However, malicious users often exploit shared pictures to create fake profiles
leading to the growth of cybercrime. Thus, keeping in mind this scenario,
authorship attribution and verification through image watermarking techniques
are becoming more and more important. In this paper, firstly, we investigate
how 13 most popular SNs treat the uploaded pictures, in order to identify a
possible implementation of image watermarking techniques by respective SNs.
Secondly, on these 13 SNs, we test the robustness of several image watermarking
algorithms. Finally, we verify whether a method based on the Photo-Response
Non-Uniformity (PRNU) technique can be successfully used as a watermarking
approach for authorship attribution and verification of pictures on SNs. The
proposed method is robust enough in spite of the fact that the pictures get
downgraded during the uploading process by SNs. The results of our analysis on
a real dataset of 8,400 pictures show that the proposed method is more
effective than other watermarking techniques and can help to address serious
questions about privacy and security on SNs.
</summary>
    <author>
      <name>Flavio Bertini</name>
    </author>
    <author>
      <name>Rajesh Sharma</name>
    </author>
    <author>
      <name>Danilo Montesi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/jimaging8050132</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/jimaging8050132" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">43 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.03903v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.03903v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.11424v2</id>
    <updated>2020-10-21T01:02:00Z</updated>
    <published>2020-06-19T22:16:52Z</published>
    <title>Capturing Video Frame Rate Variations via Entropic Differencing</title>
    <summary>  High frame rate videos are increasingly getting popular in recent years,
driven by the strong requirements of the entertainment and streaming industries
to provide high quality of experiences to consumers. To achieve the best
trade-offs between the bandwidth requirements and video quality in terms of
frame rate adaptation, it is imperative to understand the effects of frame rate
on video quality. In this direction, we devise a novel statistical entropic
differencing method based on a Generalized Gaussian Distribution model
expressed in the spatial and temporal band-pass domains, which measures the
difference in quality between reference and distorted videos. The proposed
design is highly generalizable and can be employed when the reference and
distorted sequences have different frame rates. Our proposed model correlates
very well with subjective scores in the recently proposed LIVE-YT-HFR database
and achieves state of the art performance when compared with existing
methodologies.
</summary>
    <author>
      <name>Pavan C. Madhusudana</name>
    </author>
    <author>
      <name>Neil Birkbeck</name>
    </author>
    <author>
      <name>Yilin Wang</name>
    </author>
    <author>
      <name>Balu Adsumilli</name>
    </author>
    <author>
      <name>Alan C. Bovik</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LSP.2020.3028687</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LSP.2020.3028687" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Signal Processing Letters. 27 (2020) 1809-1813</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2006.11424v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.11424v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.12697v1</id>
    <updated>2020-06-23T01:53:21Z</updated>
    <published>2020-06-23T01:53:21Z</published>
    <title>A Study on Impacts of Multiple Factors on Video Qualify of Experience</title>
    <summary>  HTTP Adaptive Streaming (HAS) has become a cost-effective means for
multimedia delivery nowadays. However, how the quality of experience (QoE) is
jointly affected by 1) varying perceptual quality and 2) interruptions is not
well-understood. In this paper, we present the first attempt to quantitatively
quantify the relative impacts of these factors on the QoE of streaming
sessions. To achieve this purpose, we first model the impacts of the factors
using histograms, which represent the frequency distributions of the individual
factors in a session. By using a large dataset, various insights into the
relative impacts of these factors are then provided, serving as suggestions to
improve the QoE of streaming sessions.
</summary>
    <author>
      <name>Huyen T. T. Tran</name>
    </author>
    <author>
      <name>Nam Pham Ngoc</name>
    </author>
    <author>
      <name>Truong Cong Thang</name>
    </author>
    <link href="http://arxiv.org/abs/2006.12697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.12697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.15984v6</id>
    <updated>2022-07-16T16:24:55Z</updated>
    <published>2020-06-29T12:32:16Z</published>
    <title>New Framework for Code-Mapping-based Reversible Data Hiding in JPEG
  Images</title>
    <summary>  Code mapping (CM) is an efficient technique for reversible data hiding (RDH)
in JPEG images, which embeds data by constructing a mapping relationship
between the used and unused codes in the JPEG bitstream. This study presents a
new framework for designing a CM-based RDH method. First, a new code mapping
strategy is proposed to suppress file size expansion and improve applicability.
Based on our proposed strategy, the mapped codes are redefined by creating a
new Huffman table rather than selecting them from the unused codes in the
original Huffman table. The critical issue of designing the CM-based RDH
method, that is, constructing code mapping, is converted into a combinatorial
optimization problem. This study proposes a novel CM-based RDH method that
utilizes a genetic algorithm (GA). The experimental results demonstrate that
the proposed method achieves a high embedding capacity with no signal
distortion while suppressing file size expansion.
</summary>
    <author>
      <name>Yang Du</name>
    </author>
    <author>
      <name>Zhaoxia Yin</name>
    </author>
    <link href="http://arxiv.org/abs/2006.15984v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.15984v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.03410v1</id>
    <updated>2020-07-07T13:26:25Z</updated>
    <published>2020-07-07T13:26:25Z</published>
    <title>Cost-Efficient Storage for On-Demand Video Streaming on Cloud</title>
    <summary>  Video stream is converted to several formats to support the user's device,
this conversion process is called video transcoding, which imposes high storage
and powerful resources. With emerging of cloud technology, video stream
companies adopted to process video on the cloud. Generally, many formats of the
same video are made (pre-transcoded) and streamed to the adequate user's
device. However, pre-transcoding demands huge storage space and incurs a
high-cost to the video stream companies. More importantly, the pre-transcoding
of video streams could be hierarchy carried out through different storage types
in the cloud. To minimize the storage cost, in this paper, we propose a method
to store video streams in the hierarchical storage of the cloud. Particularly,
we develop a method to decide which video stream should be pre-transcoded in
its suitable cloud storage to minimize the overall cost. Experimental
simulation and results show the effectiveness of our approach, specifically,
when the percentage of frequently accessed videos is high in repositories, the
proposed approach minimizes the overall cost by up to 40 percent.
</summary>
    <author>
      <name>Mahmoud Darwich</name>
    </author>
    <author>
      <name>Yasser Ismail</name>
    </author>
    <author>
      <name>Talal Darwich</name>
    </author>
    <author>
      <name>Magdy Bayoumi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/WF-IoT48130.2020.9221374</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/WF-IoT48130.2020.9221374" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International IEEE World Forum for Internet of Things</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.03410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.03410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.03922v3</id>
    <updated>2021-03-20T07:26:33Z</updated>
    <published>2020-07-08T06:59:00Z</published>
    <title>Reversible data hiding in encrypted images based on pixel prediction and
  multi-MSB planes rearrangement</title>
    <summary>  Great concern has arisen in the field of reversible data hiding in encrypted
images (RDHEI) due to the development of cloud storage and privacy protection.
RDHEI is an effective technology that can embed additional data after image
encryption, extract additional data error-free and reconstruct original images
losslessly. In this paper, a high-capacity and fully reversible RDHEI method is
proposed, which is based on pixel prediction and multi-MSB (most significant
bit) planes rearrangement. First, the median edge detector (MED) predictor is
used to calculate the predicted value. Next, unlike previous methods, in our
proposed method, signs of prediction errors (PEs) are represented by one bit
plane and absolute values of PEs are represented by other bit planes. Then, we
divide bit planes into uniform blocks and non-uniform blocks, and rearrange
these blocks. Finally, according to different pixel prediction schemes,
different numbers of additional data are embedded adaptively. The experimental
results prove that our method has higher embedding capacity compared with
state-of-the-art RDHEI methods.
</summary>
    <author>
      <name>Zhaoxia Yin</name>
    </author>
    <author>
      <name>Xiaomeng She</name>
    </author>
    <author>
      <name>Jin Tang</name>
    </author>
    <author>
      <name>Bin Luo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.sigpro.2021.108146</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.sigpro.2021.108146" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Signal Processing, 2021, 187: 1-11</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2007.03922v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.03922v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.04057v3</id>
    <updated>2021-09-24T09:34:56Z</updated>
    <published>2020-07-08T12:14:38Z</published>
    <title>Reversible Data Hiding in Encrypted Images Based on Bit-plane
  Compression of Prediction Error</title>
    <summary>  As a technology that can prevent the information from being disclosed, the
reversible data hiding in encrypted images (RDHEI) acts as an important role in
privacy protection and information security. To make use of the image
redundancy and further improve the embedding performance, a high-capacity RDHEI
method based on bit-plane compression of prediction error is proposed in this
paper. Firstly, the whole prediction error is calculated and divided into
blocks of the same size. Then, the content owner rearranges the bit-plane of
prediction error by block and compresses the bitstream with the joint encoding
algorithm to reserve room. Finally, the image is encrypted and the information
can be embedded into the reserved room. On the receiver side, the information
extraction and the image recovery are performed separably. Experimental results
show that the proposed method brings higher embedding capacity than
state-of-the-art RDHEI works.
</summary>
    <author>
      <name>Youqing Wu</name>
    </author>
    <author>
      <name>Wenjing Ma</name>
    </author>
    <author>
      <name>Yinyin Peng</name>
    </author>
    <author>
      <name>Ruiling Zhang</name>
    </author>
    <author>
      <name>Zhaoxia Yin</name>
    </author>
    <link href="http://arxiv.org/abs/2007.04057v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.04057v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.05025v1</id>
    <updated>2020-07-09T18:47:38Z</updated>
    <published>2020-07-09T18:47:38Z</published>
    <title>$\ell_1$SABMIS: $\ell_1$-minimization and sparse approximation based
  blind multi-image steganography scheme</title>
    <summary>  Steganography plays a vital role in achieving secret data security by
embedding it into cover media. The cover media and the secret data can be text
or multimedia, such as images, videos, etc. In this paper, we propose a novel
$\ell_1$-minimization and sparse approximation based blind multi-image
steganography scheme, termed $\ell_1$SABMIS. By using $\ell_1$SABMIS, multiple
secret images can be hidden in a single cover image. In $\ell_1$SABMIS, we
sampled cover image into four sub-images, sparsify each sub-image block-wise,
and then obtain linear measurements. Next, we obtain DCT (Discrete Cosine
Transform) coefficients of the secret images and then embed them into the cover
image\textquotesingle s linear measurements.
  We perform experiments on several standard gray-scale images, and evaluate
embedding capacity, PSNR (peak signal-to-noise ratio) value, mean SSIM
(structural similarity) index, NCC (normalized cross-correlation) coefficient,
NAE (normalized absolute error), and entropy. The value of these assessment
metrics indicates that $\ell_1$SABMIS outperforms similar existing
steganography schemes. That is, we successfully hide more than two secret
images in a single cover image without degrading the cover image significantly.
Also, the extracted secret images preserve good visual quality, and
$\ell_1$SABMIS is resistant to steganographic attack.
</summary>
    <author>
      <name>Rohit Agrawal</name>
    </author>
    <link href="http://arxiv.org/abs/2007.05025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.05025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.08301v2</id>
    <updated>2021-03-20T13:00:21Z</updated>
    <published>2020-07-16T12:41:15Z</published>
    <title>Robust adaptive steganography based on dither modulation and
  modification with re-compression</title>
    <summary>  Traditional adaptive steganography is a technique used for covert
communication with high security, but it is invalid in the case of stego images
are sent to legal receivers over networks which is lossy, such as JPEG
compression of channels. To deal with such problem, robust adaptive
steganography is proposed to enable the receiver to extract secret messages
from the damaged stego images. Previous works utilize reverse engineering and
compression-resistant domain constructing to implement robust adaptive
steganography. In this paper, we adopt modification with re-compression scheme
to improve the robustness of stego sequences in stego images. To balance
security and robustness, we move the embedding domain to the low frequency
region of DCT (Discrete Cosine Transform) coefficients to improve the security
of robust adaptive steganography. In addition, we add additional check codes to
further reduce the average extraction error rate based on the framework of
E-DMAS (Enhancing Dither Modulation based robust Adaptive Steganography).
Compared with GMAS (Generalized dither Modulation based robust Adaptive
Steganography) and E-DMAS, experiment results show that our scheme can achieve
strong robustness and improve the security of robust adaptive steganography
greatly when the channel quality factor is known.
</summary>
    <author>
      <name>Zhaoxia Yin</name>
    </author>
    <author>
      <name>Longfei Ke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TSIPN.2021.3081373</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TSIPN.2021.3081373" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Signal and Information Processing over
  Networks, 2021, 7: 336-345</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2007.08301v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.08301v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.00597v1</id>
    <updated>2020-12-01T16:06:12Z</updated>
    <published>2020-12-01T16:06:12Z</published>
    <title>Cost Efficient Repository Management for Cloud-Based On-Demand Video
  Streaming</title>
    <summary>  Video transcoding is the process of converting a video to the format
supported by the viewer's device. Video transcoding requires huge storage and
computational resources, thus, many video stream providers choose to carry it
out on the cloud. Video streaming providers generally need to prepare several
formats of the same video (termed pre-transcoding) and stream the appropriate
format to the viewer. However, pre-transcoding requires enormous storage space
and imposes a significant cost to the stream provider. More importantly,
pre-transcoding proven to be inefficient due to the long-tail access pattern to
video streams in a repository. To reduce the incurred cost, in this research,
we propose a method to partially pre-transcode video streams and re-transcode
the rest of it in an on-demand manner. We will develop a method to strike a
trade-off between pre-transcoding and on-demand transcoding of video streams to
reduce the overall cost. Experimental results show the efficiency of our
approach, particularly, when a high percentage of videos are accessed
frequently. In such repositories, the proposed approach reduces the incurred
cost by up to 70\%.
</summary>
    <author>
      <name>Mahmoud Darwich</name>
    </author>
    <author>
      <name>Ege Beyazit</name>
    </author>
    <author>
      <name>Mohsen Amini Salehiy</name>
    </author>
    <author>
      <name>Magdy Bayoumi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MobileCloud.2017.23</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MobileCloud.2017.23" rel="related"/>
    <link href="http://arxiv.org/abs/2012.00597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.00597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.01107v1</id>
    <updated>2020-12-02T11:47:20Z</updated>
    <published>2020-12-02T11:47:20Z</published>
    <title>Retracing the Flow of the Stream: Investigating Kodi Streaming Services</title>
    <summary>  Kodi is of one of the world's largest open-source streaming platforms for
viewing video content. Easily installed Kodi add-ons facilitate access to
online pirated videos and streaming content by facilitating the user to search
and view copyrighted videos with a basic level of technical knowledge. In some
countries, there have been paid child sexual abuse organizations
publishing/streaming child abuse material to an international paying clientele.
Open source software used for viewing videos from the Internet, such as Kodi,
is being exploited by criminals to conduct their activities. In this paper, we
describe a new method to quickly locate Kodi artifacts and gather information
for a successful prosecution. We also evaluate our approach on different
platforms; Windows, Android and Linux. Our experiments show the file location,
artifacts and a history of viewed content including their locations from the
Internet. Our approach will serve as a resource to forensic investigators to
examine Kodi or similar streaming platforms.
</summary>
    <author>
      <name>Samuel Todd Bromley</name>
    </author>
    <author>
      <name>John Sheppard</name>
    </author>
    <author>
      <name>Mark Scanlon</name>
    </author>
    <author>
      <name>Nhien-An Le-Khac</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Digital Forensics and Cyber Crime: 11th EAI International
  Conference on Digital Forensics and Cybercrime (ICDF2C), Boston, USA,
  September 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2012.01107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.01107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.04446v1</id>
    <updated>2020-12-08T14:32:05Z</updated>
    <published>2020-12-08T14:32:05Z</published>
    <title>LAMP: Label Augmented Multimodal Pretraining</title>
    <summary>  Multi-modal representation learning by pretraining has become an increasing
interest due to its easy-to-use and potential benefit for various
Visual-and-Language~(V-L) tasks. However its requirement of large volume and
high-quality vision-language pairs highly hinders its values in practice. In
this paper, we proposed a novel label-augmented V-L pretraining model, named
LAMP, to address this problem. Specifically, we leveraged auto-generated labels
of visual objects to enrich vision-language pairs with fine-grained alignment
and correspondingly designed a novel pretraining task. Besides, we also found
such label augmentation in second-stage pretraining would further universally
benefit various downstream tasks. To evaluate LAMP, we compared it with some
state-of-the-art models on four downstream tasks. The quantitative results and
analysis have well proven the value of labels in V-L pretraining and the
effectiveness of LAMP.
</summary>
    <author>
      <name>Jia Guo</name>
    </author>
    <author>
      <name>Chen Zhu</name>
    </author>
    <author>
      <name>Yilun Zhao</name>
    </author>
    <author>
      <name>Heda Wang</name>
    </author>
    <author>
      <name>Yao Hu</name>
    </author>
    <author>
      <name>Xiaofei He</name>
    </author>
    <author>
      <name>Deng Cai</name>
    </author>
    <link href="http://arxiv.org/abs/2012.04446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.04446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.04623v1</id>
    <updated>2020-12-08T18:46:09Z</updated>
    <published>2020-12-08T18:46:09Z</published>
    <title>Study on the Assessment of the Quality of Experience of Streaming Video</title>
    <summary>  Dynamic adaptive streaming over HTTP provides the work of most multimedia
services, however, the nature of this technology further complicates the
assessment of the QoE (Quality of Experience). In this paper, the influence of
various objective factors on the subjective estimation of the QoE of streaming
video is studied. The paper presents standard and handcrafted features, shows
their correlation and p-Value of significance. VQA (Video Quality Assessment)
models based on regression and gradient boosting with SRCC reaching up to
0.9647 on the validation subsample are proposed. The proposed regression models
are adapted for applied applications (both with and without a reference video);
the Gradient Boosting Regressor model is perspective for further improvement of
the quality estimation model. We take SQoE-III database, so far the largest and
most realistic of its kind. The VQA (video quality assessment) models are
available at https://github.com/AleksandrIvchenko/QoE-assesment
</summary>
    <author>
      <name>Aleksandr Ivchenko</name>
    </author>
    <author>
      <name>Pavel Kononyuk</name>
    </author>
    <author>
      <name>Alexander Dvorkovich</name>
    </author>
    <author>
      <name>Liubov Antiufrieva</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SYNCHROINFO49631.2020.9166092</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SYNCHROINFO49631.2020.9166092" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 16 figures, 7 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Systems of Signal Synchronization, Generating and Processing in
  Telecommunications (SYNCHROINFO). IEEE, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2012.04623v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.04623v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.05696v1</id>
    <updated>2020-12-10T14:25:55Z</updated>
    <published>2020-12-10T14:25:55Z</published>
    <title>A User-experience Driven SSIM-Aware Adaptation Approach for DASH Video
  Streaming</title>
    <summary>  Dynamic Adaptive Streaming over HTTP (DASH) is a video streaming technique
largely used. One key point is the adaptation mechanism which resides at the
client's side. This mechanism impacts greatly on the overall Quality of
Experience (QoE) of the video streaming. In this paper, we propose a new
adaptation algorithm for DASH, namely SSIM Based Adaptation (SBA). This
mechanism is user-experience driven: it uses the Structural Similarity Index
Measurement (SSIM) as main video perceptual quality indicator; moreover, the
adaptation is based on a joint consideration of SSIM indicator and the physical
resources (buffer occupancy, bandwidth) in order to minimize the buffer
starvation (rebuffering) and video quality instability, as well as to maximize
the overall video quality (through SSIM). To evaluate the performance of our
proposal, we carried out trace-driven emulation with real traffic traces
(captured in real mobile network). Comparisons with some representative
algorithms (BBA, FESTIVE, OSMF) through major QoE metrics show that our
adaptation algorithm SBA achieves an efficient adaptation minimizing both the
rebuffering and instability, whereas the displayed video is maintained at a
high level of bitrate.
</summary>
    <author>
      <name>Mustafa Othman</name>
    </author>
    <author>
      <name>Ken Chen</name>
    </author>
    <author>
      <name>Anissa Mokraoui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, (CENTRIC 2019) The Twelfth International Conference on
  Advances in Human-oriented and Personalized Mechanisms, Technologies, and
  Services</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.05696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.05696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.08034v1</id>
    <updated>2020-12-15T01:37:10Z</updated>
    <published>2020-12-15T01:37:10Z</published>
    <title>An Artistic Visualization of Music Modeling a Synesthetic Experience</title>
    <summary>  This project brings music to sight. Music can be a visual masterpiece. Some
people naturally experience a visualization of audio - a condition called
synesthesia. The type of synesthesia explored is when sounds create colors in
the 'mind's eye.' Project included interviews with people who experience
synesthesia, examination of prior art, and topic research to inform project
design. Audio input, digital signal processing (including Fast Fourier
Transforms (FFTs)) and data manipulation produce arguments required for our
visualization. Arguments are then applied to a physics particle simulator which
is re-purposed to model a synesthetic experience. The result of the project is
a simulator in MAX 8, which generates a visual performance using particles by
varying each particle's position, velocity, and color based on parameters
extracted via digital processing of input audio.
</summary>
    <author>
      <name>Matthew Joseph Adiletta</name>
    </author>
    <author>
      <name>Oliver Thomas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.08034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.08034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.08256v1</id>
    <updated>2020-12-15T12:47:17Z</updated>
    <published>2020-12-15T12:47:17Z</published>
    <title>A Deep Multi-Level Attentive network for Multimodal Sentiment Analysis</title>
    <summary>  Multimodal sentiment analysis has attracted increasing attention with broad
application prospects. The existing methods focuses on single modality, which
fails to capture the social media content for multiple modalities. Moreover, in
multi-modal learning, most of the works have focused on simply combining the
two modalities, without exploring the complicated correlations between them.
This resulted in dissatisfying performance for multimodal sentiment
classification. Motivated by the status quo, we propose a Deep Multi-Level
Attentive network, which exploits the correlation between image and text
modalities to improve multimodal learning. Specifically, we generate the
bi-attentive visual map along the spatial and channel dimensions to magnify
CNNs representation power. Then we model the correlation between the image
regions and semantics of the word by extracting the textual features related to
the bi-attentive visual features by applying semantic attention. Finally,
self-attention is employed to automatically fetch the sentiment-rich multimodal
features for the classification. We conduct extensive evaluations on four
real-world datasets, namely, MVSA-Single, MVSA-Multiple, Flickr, and Getty
Images, which verifies the superiority of our method.
</summary>
    <author>
      <name>Ashima Yadav</name>
    </author>
    <author>
      <name>Dinesh Kumar Vishwakarma</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3517139</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3517139" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Transactions on Multimedia Computing, Communications, and
  Applications, 2022</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2012.08256v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.08256v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.08865v2</id>
    <updated>2022-12-26T03:17:16Z</updated>
    <published>2020-12-16T11:08:09Z</published>
    <title>UAV-Assisted Image Acquisition: 3D UAV Trajectory Design and Camera
  Control</title>
    <summary>  In this paper, we consider a new unmanned aerial vehicle (UAV)-assisted
oblique image acquisition system where a UAV is dispatched to take images of
multiple ground targets (GTs). To study the three-dimensional (3D) UAV
trajectory design for image acquisition, we first propose a novel UAV-assisted
oblique photography model, which characterizes the image resolution with
respect to the UAV's 3D image-taking location. Then, we formulate a 3D UAV
trajectory optimization problem to minimize the UAV's traveling distance
subject to the image resolution constraints. The formulated problem is shown to
be equivalent to a modified 3D traveling salesman problem with neighbourhoods,
which is NP-hard in general. To tackle this difficult problem, we propose an
iterative algorithm to obtain a high-quality suboptimal solution efficiently,
by alternately optimizing the UAV's 3D image-taking waypoints and its visiting
order for the GTs. Numerical results show that the proposed algorithm
significantly reduces the UAV's traveling distance as compared to various
benchmark schemes, while meeting the image resolution requirement.
</summary>
    <author>
      <name>Xiao-Wei Tang</name>
    </author>
    <author>
      <name>Shuowen Zhang</name>
    </author>
    <author>
      <name>Changsheng You</name>
    </author>
    <author>
      <name>Xin-Lin Huang</name>
    </author>
    <author>
      <name>Rui Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted by IEEE VTC2022-Fall and will appear
  soon!</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.08865v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.08865v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.13491v1</id>
    <updated>2020-12-25T02:38:47Z</updated>
    <published>2020-12-25T02:38:47Z</published>
    <title>Study On Coding Tools Beyond Av1</title>
    <summary>  The Alliance for Open Media has recently initiated coding tool exploration
activities towards the next-generation video coding beyond AV1. With this
regard, this paper presents a package of coding tools that have been
investigated, implemented and tested on top of the codebase, known as libaom,
which is used for the exploration of next-generation video compression tools.
The proposed tools cover several technical areas based on a traditional hybrid
video coding structure, including block partitioning, prediction, transform and
loop filtering. The proposed coding tools are integrated as a package, and a
combined coding gain over AV1 is demonstrated in this paper. Furthermore, to
better understand the behavior of each tool, besides the combined coding gain,
the tool-on and tool-off tests are also simulated and reported for each
individual coding tool. Experimental results show that, compared to libaom, the
proposed methods achieve an average 8.0% (up to 22.0%) overall BD-rate
reduction for All Intra coding configuration a wide range of image and video
content.
</summary>
    <author>
      <name>Xin Zhao</name>
    </author>
    <author>
      <name>Liang Zhao</name>
    </author>
    <author>
      <name>Madhu Krishnan</name>
    </author>
    <author>
      <name>Yixin Du</name>
    </author>
    <author>
      <name>Shan Liu</name>
    </author>
    <author>
      <name>Debargha Mukherjee</name>
    </author>
    <author>
      <name>Yaowu Xu</name>
    </author>
    <author>
      <name>Adrian Grange</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures and 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.13491v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.13491v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.15003v1</id>
    <updated>2020-12-30T02:00:43Z</updated>
    <published>2020-12-30T02:00:43Z</published>
    <title>An Efficient QP Variable Convolutional Neural Network Based In-loop
  Filter for Intra Coding</title>
    <summary>  In this paper, a novel QP variable convolutional neural network based in-loop
filter is proposed for VVC intra coding. To avoid training and deploying
multiple networks, we develop an efficient QP attention module (QPAM) which can
capture compression noise levels for different QPs and emphasize meaningful
features along channel dimension. Then we embed QPAM into the residual block,
and based on it, we design a network architecture that is equipped with
controllability for different QPs. To make the proposed model focus more on
examples that have more compression artifacts or is hard to restore, a focal
mean square error (MSE) loss function is employed to fine tune the network.
Experimental results show that our approach achieves 4.03\% BD-Rate saving on
average for all intra configuration, which is even better than QP-separate CNN
models while having less model parameters.
</summary>
    <author>
      <name>Zhijie Huang</name>
    </author>
    <author>
      <name>Xiaopeng Guo</name>
    </author>
    <author>
      <name>Mingyu Shang</name>
    </author>
    <author>
      <name>Jie Gao</name>
    </author>
    <author>
      <name>Jun Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by DCC2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.15003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.15003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.15067v1</id>
    <updated>2020-12-30T07:42:53Z</updated>
    <published>2020-12-30T07:42:53Z</published>
    <title>Sub-sampled Cross-component Prediction for Emerging Video Coding
  Standards</title>
    <summary>  Cross-component linear model (CCLM) prediction has been repeatedly proven to
be effective in reducing the inter-channel redundancies in video compression.
Essentially speaking, the linear model is identically trained by employing
accessible luma and chroma reference samples at both encoder and decoder,
elevating the level of operational complexity due to the least square
regression or max-min based model parameter derivation. In this paper, we
investigate the capability of the linear model in the context of sub-sampled
based cross-component correlation mining, as a means of significantly releasing
the operation burden and facilitating the hardware and software design for both
encoder and decoder. In particular, the sub-sampling ratios and positions are
elaborately designed by exploiting the spatial correlation and the
inter-channel correlation. Extensive experiments verify that the proposed
method is characterized by its simplicity in operation and robustness in terms
of rate-distortion performance, leading to the adoption by Versatile Video
Coding (VVC) standard and the third generation of Audio Video Coding Standard
(AVS3).
</summary>
    <author>
      <name>Junru Li</name>
    </author>
    <author>
      <name>Meng Wang</name>
    </author>
    <author>
      <name>Li Zhang</name>
    </author>
    <author>
      <name>Shiqi Wang</name>
    </author>
    <author>
      <name>Kai Zhang</name>
    </author>
    <author>
      <name>Shanshe Wang</name>
    </author>
    <author>
      <name>Siwei Ma</name>
    </author>
    <author>
      <name>Wen Gao</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2021.3104191</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2021.3104191" rel="related"/>
    <link href="http://arxiv.org/abs/2012.15067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.15067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.15635v1</id>
    <updated>2020-12-31T14:50:42Z</updated>
    <published>2020-12-31T14:50:42Z</published>
    <title>Leveraging Audio Gestalt to Predict Media Memorability</title>
    <summary>  Memorability determines what evanesces into emptiness, and what worms its way
into the deepest furrows of our minds. It is the key to curating more
meaningful media content as we wade through daily digital torrents. The
Predicting Media Memorability task in MediaEval 2020 aims to address the
question of media memorability by setting the task of automatically predicting
video memorability. Our approach is a multimodal deep learning-based late
fusion that combines visual, semantic, and auditory features. We used audio
gestalt to estimate the influence of the audio modality on overall video
memorability, and accordingly inform which combination of features would best
predict a given video's memorability scores.
</summary>
    <author>
      <name>Lorin Sweeney</name>
    </author>
    <author>
      <name>Graham Healy</name>
    </author>
    <author>
      <name>Alan F. Smeaton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 Figure, 2 Tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">MediaEval Multimedia Benchmark Workshop Working Notes, 14-15
  December 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2012.15635v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.15635v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.15650v1</id>
    <updated>2020-12-31T15:12:52Z</updated>
    <published>2020-12-31T15:12:52Z</published>
    <title>Overview of MediaEval 2020 Predicting Media Memorability Task: What
  Makes a Video Memorable?</title>
    <summary>  This paper describes the MediaEval 2020 \textit{Predicting Media
Memorability} task. After first being proposed at MediaEval 2018, the
Predicting Media Memorability task is in its 3rd edition this year, as the
prediction of short-term and long-term video memorability (VM) remains a
challenging task. In 2020, the format remained the same as in previous
editions. This year the videos are a subset of the TRECVid 2019 Video-to-Text
dataset, containing more action rich video content as compared with the 2019
task. In this paper a description of some aspects of this task is provided,
including its main characteristics, a description of the collection, the ground
truth dataset, evaluation metrics and the requirements for participants' run
submissions.
</summary>
    <author>
      <name>Alba Garc√≠a Seco De Herrera</name>
    </author>
    <author>
      <name>Rukiye Savran Kiziltepe</name>
    </author>
    <author>
      <name>Jon Chamberlain</name>
    </author>
    <author>
      <name>Mihai Gabriel Constantin</name>
    </author>
    <author>
      <name>Claire-H√©l√®ne Demarty</name>
    </author>
    <author>
      <name>Faiyaz Doctor</name>
    </author>
    <author>
      <name>Bogdan Ionescu</name>
    </author>
    <author>
      <name>Alan F. Smeaton</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 Figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">MediaEval Multimedia Benchmark Workshop Working Notes, 14-15
  December 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2012.15650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.15650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.00611v1</id>
    <updated>2021-01-03T11:57:04Z</updated>
    <published>2021-01-03T11:57:04Z</published>
    <title>Duration-Squeezing-Aware Communication and Computing for Proactive VR</title>
    <summary>  Proactive tile-based virtual reality video streaming computes and delivers
the predicted tiles to be requested before playback. All existing works
overlook the important fact that computing and communication (CC) tasks for a
segment may squeeze the time for the tasks for the next segment, which will
cause less and less available time for the latter segments. In this paper, we
jointly optimize the durations for CC tasks to maximize the completion rate of
CC tasks under the task duration-squeezing-aware constraint. To ensure the
latter segments remain enough time for the tasks, the CC tasks for a segment
are not allowed to squeeze the time for computing and delivering the subsequent
segment. We find the closed-form optimal solution, from which we find a
minimum-resource-limited, an unconditional and a conditional resource-tradeoff
regions, which are determined by the total time for proactive CC tasks and the
playback duration of a segment. Owing to the duration-squeezing-prohibited
constraints, the increase of the configured resources may not be always useful
for improving the completion rate of CC tasks. Numerical results validate the
impact of the duration-squeezing-prohibited constraints and illustrate the
three regions.
</summary>
    <author>
      <name>Xing Wei</name>
    </author>
    <author>
      <name>Chenyang Yang</name>
    </author>
    <author>
      <name>Shengqian Han</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, submit to IEEE for possible publication</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.00611v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.00611v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.03413v1</id>
    <updated>2021-01-09T19:23:45Z</updated>
    <published>2021-01-09T19:23:45Z</published>
    <title>Facial Biometric System for Recognition using Extended LGHP Algorithm on
  Raspberry Pi</title>
    <summary>  In todays world, where the need for security is paramount and biometric
access control systems are gaining mass acceptance due to their increased
reliability, research in this area is quite relevant. Also with the advent of
IOT devices and increased community support for cheap and small computers like
Raspberry Pi its convenient than ever to design a complete standalone system
for any purpose. This paper proposes a Facial Biometric System built on the
client-server paradigm using Raspberry Pi 3 model B running a novel local
descriptor based parallel algorithm. This paper also proposes an extended
version of Local Gradient Hexa Pattern with improved accuracy. The proposed
extended version of LGHP improved performance as shown in performance analysis.
Extended LGHP shows improvement over other state-of-the-art descriptors namely
LDP, LTrP, MLBP and LVP on the most challenging benchmark facial image
databases, i.e. Cropped Extended Yale-B, CMU-PIE, color-FERET, LFW, and
Ghallager database. Proposed system is also compared with various patents
having similar system design and intent to emphasize the difference and novelty
of the system proposed.
</summary>
    <author>
      <name>Soumendu Chakraborty</name>
    </author>
    <author>
      <name>Satish Kumar Singh</name>
    </author>
    <author>
      <name>Kush Kumar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/JSEN.2020.2979907</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/JSEN.2020.2979907" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Sensors Journal, vol-20, no-14, pp. 8117-8127, (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2101.03413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.03413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.04475v1</id>
    <updated>2021-01-12T13:45:18Z</updated>
    <published>2021-01-12T13:45:18Z</published>
    <title>Network-Distributed Video Coding</title>
    <summary>  Nowadays, an enormous amount of videos are streamed every day to countless
users, all using different devices and networks. These videos must be adapted
in order to provide users with the most suitable video representation based on
their device properties and current network conditions. However, the two most
common techniques for video adaptation, simulcast and transcoding, represent
two extremes. The former offers excellent scalability, but requires a large
amount of storage, while the latter has a small storage cost, but is not
scalable to many users due to the additional computing cost per requested
representation. As a third, in-between approach, network-distributed video
coding (NDVC) was proposed within the Moving Picture Experts Group (MPEG). The
aim of NDVC is to reduce the storage cost compared to simulcast, while
retaining a smaller computing cost compared to transcoding. By exploring the
proposed techniques for NDVC, we show the workings of this third option for
video providers to deliver their contents to their clients.
</summary>
    <author>
      <name>Johan De Praeter</name>
    </author>
    <author>
      <name>Christopher Hollmann</name>
    </author>
    <author>
      <name>Rickard Sjoberg</name>
    </author>
    <author>
      <name>Glenn Van Wallendael</name>
    </author>
    <author>
      <name>Peter Lambert</name>
    </author>
    <link href="http://arxiv.org/abs/2101.04475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.04475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.06383v1</id>
    <updated>2021-01-16T06:37:00Z</updated>
    <published>2021-01-16T06:37:00Z</published>
    <title>A Novel Local Binary Pattern Based Blind Feature Image Steganography</title>
    <summary>  Steganography methods in general terms tend to embed more and more secret
bits in the cover images. Most of these methods are designed to embed secret
information in such a way that the change in the visual quality of the
resulting stego image is not detectable. There exists some methods which
preserve the global structure of the cover after embedding. However, the
embedding capacity of these methods is very less. In this paper a novel feature
based blind image steganography technique is proposed, which preserves the LBP
(Local binary pattern) feature of the cover with comparable embedding rates.
Local binary pattern is a well known image descriptor used for image
representation. The proposed scheme computes the local binary pattern to hide
the bits of the secret image in such a way that the local relationship that
exists in the cover are preserved in the resulting stego image. The performance
of the proposed steganography method has been tested on several images of
different types to show the robustness. State of the art LSB based
steganography methods are compared with the proposed method to show the
effectiveness of feature based image steganography
</summary>
    <author>
      <name>Soumendu Chakraborty</name>
    </author>
    <author>
      <name>Anand Singh Jalal</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11042-020-08828-3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11042-020-08828-3" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Multimedia Tools and Applications, vol-79, no-27-28, pp.
  19561-19574, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2101.06383v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.06383v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.07144v1</id>
    <updated>2021-01-18T16:16:58Z</updated>
    <published>2021-01-18T16:16:58Z</published>
    <title>Designing a mobile game to generate player data -- lessons learned</title>
    <summary>  User friendly tools have lowered the requirements of high-quality game design
to the point where researchers without development experience can release their
own games. However, there is no established best-practice as few games have
been produced for research purposes. Having developed a mobile game without the
guidance of similar projects, we realised the need to share our experience so
future researchers have a path to follow. Research into game balancing and
system simulation required an experimental case study, which inspired the
creation of "RPGLite", a multiplayer mobile game. In creating RPGLitewith no
development expertise we learned a series of lessons about effective amateur
game development for research purposes. In this paper we reflect on the entire
development process and present these lessons.
</summary>
    <author>
      <name>William Wallis</name>
    </author>
    <author>
      <name>William Kavanagh</name>
    </author>
    <author>
      <name>Alice Miller</name>
    </author>
    <author>
      <name>Tim Storer</name>
    </author>
    <link href="http://arxiv.org/abs/2101.07144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.07144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.07451v1</id>
    <updated>2021-01-19T03:55:26Z</updated>
    <published>2021-01-19T03:55:26Z</published>
    <title>Wide Color Gamut Image Content Characterization: Method, Evaluation, and
  Applications</title>
    <summary>  In this paper, we propose a novel framework to characterize a wide color
gamut image content based on perceived quality due to the processes that change
color gamut, and demonstrate two practical use cases where the framework can be
applied. We first introduce the main framework and implementation details.
Then, we provide analysis for understanding of existing wide color gamut
datasets with quantitative characterization criteria on their characteristics,
where four criteria, i.e., coverage, total coverage, uniformity, and total
uniformity, are proposed. Finally, the framework is applied to content
selection in a gamut mapping evaluation scenario in order to enhance
reliability and robustness of the evaluation results. As a result, the
framework fulfils content characterization for studies where quality of
experience of wide color gamut stimuli is involved.
</summary>
    <author>
      <name>Junghyuk Lee</name>
    </author>
    <author>
      <name>Toinon Vigier</name>
    </author>
    <author>
      <name>Patrick Le Callet</name>
    </author>
    <author>
      <name>Jong-Seok Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2020.3032026</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2020.3032026" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Multimedia (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2101.07451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.07451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.10039v1</id>
    <updated>2021-01-25T12:13:32Z</updated>
    <published>2021-01-25T12:13:32Z</published>
    <title>Latent Factor Modeling of Users Subjective Perception for Stereoscopic
  3D Video Recommendation</title>
    <summary>  Numerous stereoscopic 3D movies are released every year to theaters and
created large revenues. Despite the improvement in stereo capturing and 3D
video post-production technology, stereoscopic artifacts which cause viewer
discomfort continue to appear even in high-budget films. Existing automatic 3D
video quality measurement tools can detect distortions in stereoscopic images
or videos, but they fail to consider the viewer's subjective perception of
those artifacts, and how these distortions affect their choices. In this paper,
we introduce a novel recommendation system for stereoscopic 3D movies based on
a latent factor model that meticulously analyse the viewer's subjective ratings
and influence of 3D video distortions on their preferences. To the best of our
knowledge, this is a first-of-its-kind model that recommends 3D movies based on
stereo-film quality ratings accounting correlation between the viewer's visual
discomfort and stereoscopic-artifact perception. The proposed model is trained
and tested on benchmark Nama3ds1-cospad1 and LFOVIAS3DPh2 S3D video quality
assessment datasets. The experiments revealed that resulting
matrix-factorization based recommendation system is able to generalize
considerably better for the viewer's subjective ratings.
</summary>
    <author>
      <name>Balasubramanyam Appina</name>
    </author>
    <author>
      <name>Mansi Sharma</name>
    </author>
    <author>
      <name>Santosh Kumar</name>
    </author>
    <link href="http://arxiv.org/abs/2101.10039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.10039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.02189v1</id>
    <updated>2021-03-03T05:34:06Z</updated>
    <published>2021-03-03T05:34:06Z</published>
    <title>User Generated HDR Gaming Video Streaming: Dataset, Codec Comparison and
  Challenges</title>
    <summary>  Gaming video streaming services have grown tremendously in the past few
years, with higher resolutions, higher frame rates and HDR gaming videos
getting increasingly adopted among the gaming community. Since gaming content
as such is different from non-gaming content, it is imperative to evaluate the
performance of the existing encoders to help understand the bandwidth
requirements of such services, as well as further improve the compression
efficiency of such encoders. Towards this end, we present in this paper
GamingHDRVideoSET, a dataset consisting of eighteen 10-bit UHD-HDR gaming
videos and encoded video sequences using four different codecs, together with
their objective evaluation results. The dataset is available online at [to be
added after paper acceptance]. Additionally, the paper discusses the codec
compression efficiency of most widely used practical encoders, i.e., x264
(H.264/AVC), x265 (H.265/HEVC) and libvpx (VP9), as well the recently proposed
encoder libaom (AV1), on 10-bit, UHD-HDR content gaming content. Our results
show that the latest compression standard AV1 results in the best compression
efficiency, followed by HEVC, H.264, and VP9.
</summary>
    <author>
      <name>Nabajeet Barman</name>
    </author>
    <author>
      <name>Maria G Martini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 8 figures, submitted to IEEE journal</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.02189v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.02189v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.02777v1</id>
    <updated>2021-03-04T00:58:53Z</updated>
    <published>2021-03-04T00:58:53Z</published>
    <title>Application of Reversible Data Hiding for Printing with Special Color
  Inks to Preserve Compatibility with Normal Printing</title>
    <summary>  We propose an efficient framework with compatibility between normal printing
and printing with special color inks in this paper. Special color inks can be
used for printing to represent some particular colors and specific optical
properties, which are difficult to express using only CMYK inks. Special color
layers are required in addition to the general color layer for printing with
special color inks. We introduce a reversible data hiding (RDH) method to embed
the special color layers into the general color layer without visible
artifacts. The proposed method can realize both normal printing and printing
with special color inks by using a single layer. Our experimental results show
that the quality of the marked image is virtually identical to that of the
original image, i.e., the general color layer.
</summary>
    <author>
      <name>Kotoko Hiraoka</name>
    </author>
    <author>
      <name>Kensuke Fukumoto</name>
    </author>
    <author>
      <name>Takashi Yamazoe</name>
    </author>
    <author>
      <name>Norimichi Tsumura</name>
    </author>
    <author>
      <name>Satoshi Kaneko</name>
    </author>
    <author>
      <name>Wataru Arai</name>
    </author>
    <author>
      <name>Shoko Imaizumi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1541/ieejeiss.141.155</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1541/ieejeiss.141.155" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEJ Trans. Electr. Inf. &amp; Syst., vol.141, no.2, pp.155-162,
  February 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2103.02777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.02777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.07883v2</id>
    <updated>2021-03-20T06:08:59Z</updated>
    <published>2021-03-14T10:26:50Z</published>
    <title>Multi-view data capture for dynamic object reconstruction using handheld
  augmented reality mobiles</title>
    <summary>  We propose a system to capture nearly-synchronous frame streams from multiple
and moving handheld mobiles that is suitable for dynamic object 3D
reconstruction. Each mobile executes Simultaneous Localisation and Mapping
on-board to estimate its pose, and uses a wireless communication channel to
send or receive synchronisation triggers. Our system can harvest frames and
mobile poses in real time using a decentralised triggering strategy and a
data-relay architecture that can be deployed either at the Edge or in the
Cloud. We show the effectiveness of our system by employing it for 3D skeleton
and volumetric reconstructions. Our triggering strategy achieves equal
performance to that of an NTP-based synchronisation approach, but offers higher
flexibility, as it can be adjusted online based on application needs. We
created a challenging new dataset, namely 4DM, that involves six handheld
augmented reality mobiles recording an actor performing sports actions
outdoors. We validate our system on 4DM, analyse its strengths and limitations,
and compare its modules with alternative ones.
</summary>
    <author>
      <name>M. Bortolon</name>
    </author>
    <author>
      <name>L. Bazzanella</name>
    </author>
    <author>
      <name>F. Poiesi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in Journal of Real-Time Image Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.07883v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.07883v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.10572v2</id>
    <updated>2021-03-22T06:55:13Z</updated>
    <published>2021-03-18T23:59:43Z</published>
    <title>Quantum-inspired Multimodal Fusion for Video Sentiment Analysis</title>
    <summary>  We tackle the crucial challenge of fusing different modalities of features
for multimodal sentiment analysis. Mainly based on neural networks, existing
approaches largely model multimodal interactions in an implicit and
hard-to-understand manner. We address this limitation with inspirations from
quantum theory, which contains principled methods for modeling complicated
interactions and correlations. In our quantum-inspired framework, the word
interaction within a single modality and the interaction across modalities are
formulated with superposition and entanglement respectively at different
stages. The complex-valued neural network implementation of the framework
achieves comparable results to state-of-the-art systems on two benchmarking
video sentiment analysis datasets. In the meantime, we produce the unimodal and
bimodal sentiment directly from the model to interpret the entangled decision.
</summary>
    <author>
      <name>Qiuchi Li</name>
    </author>
    <author>
      <name>Dimitris Gkoumas</name>
    </author>
    <author>
      <name>Christina Lioma</name>
    </author>
    <author>
      <name>Massimo Melucci</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Post-print accepted by Information Fusion</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.10572v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.10572v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.12516v1</id>
    <updated>2021-01-14T13:34:34Z</updated>
    <published>2021-01-14T13:34:34Z</published>
    <title>Edge-Cloud Collaboration Enabled Video Service Enhancement: A Hybrid
  Human-Artificial Intelligence Scheme</title>
    <summary>  In this paper, a video service enhancement strategy is investigated under an
edge-cloud collaboration framework, where video caching and delivery decisions
are made in the cloud and edge respectively. We aim to guarantee the user
fairness in terms of video coding rate under statistical delay constraint and
edge caching capacity constraint. A hybrid human-artificial intelligence
approach is developed to improve the user hit rate for video caching.
Specifically, individual user interest is first characterized by merging
factorization machine (FM) model and multi-layer perceptron (MLP) model, where
both low-order and high-order features can be well learned simultaneously.
Thereafter, a social aware similarity model is constructed to transferred
individual user interest to group interest, based on which, videos can be
selected to cache. Furthermore, a double bisection exploration scheme is
proposed to optimize wireless resource allocation and video coding rate. The
effectiveness of the proposed video caching scheme and video delivery scheme is
finally validated by extensive experiments with a real-world data set.
</summary>
    <author>
      <name>Dapeng Wu</name>
    </author>
    <author>
      <name>Ruili Bao</name>
    </author>
    <author>
      <name>Zhidu Li</name>
    </author>
    <author>
      <name>Honggang Wang</name>
    </author>
    <author>
      <name>Ruyan Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been submitted to IEEE Transactions on Multimedia for
  review</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.12516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.12516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.13477v2</id>
    <updated>2021-03-26T02:49:43Z</updated>
    <published>2021-03-24T20:52:23Z</published>
    <title>A Survey of Multimedia Technologies and Robust Algorithms</title>
    <summary>  Multimedia technologies are now more practical and deployable in real life,
and the algorithms are widely used in various researching areas such as deep
learning, signal processing, haptics, computer vision, robotics, and medical
multimedia processing. This survey provides an overview of multimedia
technologies and robust algorithms in multimedia data processing, medical
multimedia processing, human facial expression tracking and pose recognition,
and multimedia in education and training. This survey will also analyze and
propose a future research direction based on the overview of current robust
algorithms and multimedia technologies. We want to thank the research and
previous work done by the Multimedia Research Centre (MRC), the University of
Alberta, which is the inspiration and starting point for future research.
</summary>
    <author>
      <name>Zijian Kuang</name>
    </author>
    <author>
      <name>Xinran Tie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2010.12968</arxiv:comment>
    <link href="http://arxiv.org/abs/2103.13477v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.13477v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.15602v1</id>
    <updated>2021-03-29T13:27:30Z</updated>
    <published>2021-03-29T13:27:30Z</published>
    <title>Product semantics translation from brain activity via adversarial
  learning</title>
    <summary>  A small change of design semantics may affect a user's satisfaction with a
product. To modify a design semantic of a given product from personalised brain
activity via adversarial learning, in this work, we propose a deep generative
transformation model to modify product semantics from the brain signal. We
attempt to accomplish such synthesis: 1) synthesising the product image with
new features corresponding to EEG signal; 2) maintaining the other image
features that irrelevant to EEG signal. We leverage the idea of StarGAN and the
model is designed to synthesise products with preferred design semantics
(colour &amp; shape) via adversarial learning from brain activity, and is applied
with a case study to generate shoes with different design semantics from
recorded EEG signals. To verify our proposed cognitive transformation model, a
case study has been presented. The results work as a proof-of-concept that our
framework has the potential to synthesis product semantic from brain activity.
</summary>
    <author>
      <name>Pan Wang</name>
    </author>
    <author>
      <name>Zhifeng Gong</name>
    </author>
    <author>
      <name>Shuo Wang</name>
    </author>
    <author>
      <name>Hao Dong</name>
    </author>
    <author>
      <name>Jialu Fan</name>
    </author>
    <author>
      <name>Ling Li</name>
    </author>
    <author>
      <name>Peter Childs</name>
    </author>
    <author>
      <name>Yike Guo</name>
    </author>
    <link href="http://arxiv.org/abs/2103.15602v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.15602v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.00955v1</id>
    <updated>2021-04-02T09:30:47Z</updated>
    <published>2021-04-02T09:30:47Z</published>
    <title>An attention-based unsupervised adversarial model for movie review spam
  detection</title>
    <summary>  With the prevalence of the Internet, online reviews have become a valuable
information resource for people. However, the authenticity of online reviews
remains a concern, and deceptive reviews have become one of the most urgent
network security problems to be solved. Review spams will mislead users into
making suboptimal choices and inflict their trust in online reviews. Most
existing research manually extracted features and labeled training samples,
which are usually complicated and time-consuming. This paper focuses primarily
on a neglected emerging domain - movie review, and develops a novel
unsupervised spam detection model with an attention mechanism. By extracting
the statistical features of reviews, it is revealed that users will express
their sentiments on different aspects of movies in reviews. An attention
mechanism is introduced in the review embedding, and the conditional generative
adversarial network is exploited to learn users' review style for different
genres of movies. The proposed model is evaluated on movie reviews crawled from
Douban, a Chinese online community where people could express their feelings
about movies. The experimental results demonstrate the superior performance of
the proposed approach.
</summary>
    <author>
      <name>Yuan Gao</name>
    </author>
    <author>
      <name>Maoguo Gong</name>
    </author>
    <author>
      <name>Yu Xie</name>
    </author>
    <author>
      <name>A. K. Qin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2020.2990085</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2020.2990085" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Multimedia, vol. 23, pp. 784-796, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.00955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.00955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04371v1</id>
    <updated>2021-04-09T14:04:06Z</updated>
    <published>2021-04-09T14:04:06Z</published>
    <title>Speech Quality Assessment in Crowdsourcing: Comparison Category Rating
  Method</title>
    <summary>  Traditionally, Quality of Experience (QoE) for a communication system is
evaluated through a subjective test. The most common test method for speech QoE
is the Absolute Category Rating (ACR), in which participants listen to a set of
stimuli, processed by the underlying test conditions, and rate their perceived
quality for each stimulus on a specific scale. The Comparison Category Rating
(CCR) is another standard approach in which participants listen to both
reference and processed stimuli and rate their quality compared to the other
one. The CCR method is particularly suitable for systems that improve the
quality of input speech. This paper evaluates an adaptation of the CCR test
procedure for assessing speech quality in the crowdsourcing set-up. The CCR
method was introduced in the ITU-T Rec. P.800 for laboratory-based experiments.
We adapted the test for the crowdsourcing approach following the guidelines
from ITU-T Rec. P.800 and P.808. We show that the results of the CCR procedure
via crowdsourcing are highly reproducible. We also compared the CCR test
results with widely used ACR test procedures obtained in the laboratory and
crowdsourcing. Our results show that the CCR procedure in crowdsourcing is a
reliable and valid test method.
</summary>
    <author>
      <name>Babak Naderi</name>
    </author>
    <author>
      <name>Sebastian M√∂ller</name>
    </author>
    <author>
      <name>Ross Cutler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for QoMEX2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.04371v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04371v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.06183v1</id>
    <updated>2021-04-13T13:34:09Z</updated>
    <published>2021-04-13T13:34:09Z</published>
    <title>Optimal Transmission of Multi-Quality Tiled 360 VR Video in MIMO-OFDMA
  Systems</title>
    <summary>  In this paper, we study the optimal transmission of a multi-quality tiled 360
virtual reality (VR) video from a multi-antenna server (e.g., access point or
base station) to multiple single-antenna users in a multiple-input
multiple-output (MIMO)-orthogonal frequency division multiple access (OFDMA)
system. We minimize the total transmission power with respect to the subcarrier
allocation constraints, rate allocation constraints, and successful
transmission constraints, by optimizing the beamforming vector and subcarrier,
transmission power and rate allocation. The formulated resource allocation
problem is a challenging mixed discrete-continuous optimization problem. We
obtain an asymptotically optimal solution in the case of a large antenna array,
and a suboptimal solution in the general case. As far as we know, this is the
first work providing optimization-based design for 360 VR video transmission in
MIMO-OFDMA systems. Finally, by numerical results, we show that the proposed
solutions achieve significant improvement in performance compared to the
existing solutions.
</summary>
    <author>
      <name>Chengjun Guo</name>
    </author>
    <author>
      <name>Ying Cui</name>
    </author>
    <author>
      <name>Zhi Liu</name>
    </author>
    <author>
      <name>Derrick Wing Kwan Ng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, to appear in IEEE ICC 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.06183v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.06183v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.07569v1</id>
    <updated>2021-04-15T16:24:56Z</updated>
    <published>2021-04-15T16:24:56Z</published>
    <title>AffectiveNet: Affective-Motion Feature Learningfor Micro Expression
  Recognition</title>
    <summary>  Micro-expressions are hard to spot due to fleeting and involuntary moments of
facial muscles. Interpretation of micro emotions from video clips is a
challenging task. In this paper we propose an affective-motion imaging that
cumulates rapid and short-lived variational information of micro expressions
into a single response. Moreover, we have proposed an
AffectiveNet:affective-motion feature learning network that can perceive subtle
changes and learns the most discriminative dynamic features to describe the
emotion classes. The AffectiveNet holds two blocks: MICRoFeat and MFL block.
MICRoFeat block conserves the scale-invariant features, which allows network to
capture both coarse and tiny edge variations. While MFL block learns
micro-level dynamic variations from two different intermediate convolutional
layers. Effectiveness of the proposed network is tested over four datasets by
using two experimental setups: person independent (PI) and cross dataset (CD)
validation. The experimental results of the proposed network outperforms the
state-of-the-art approaches with significant margin for MER approaches.
</summary>
    <author>
      <name>Monu Verma</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Student, Member, IEEE</arxiv:affiliation>
    </author>
    <author>
      <name>Santosh Kumar Vipparthi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Member, IEEE</arxiv:affiliation>
    </author>
    <author>
      <name>Girdhari Singh</name>
    </author>
    <link href="http://arxiv.org/abs/2104.07569v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.07569v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.08510v1</id>
    <updated>2021-04-17T10:51:55Z</updated>
    <published>2021-04-17T10:51:55Z</published>
    <title>Exploring Deep Learning for Joint Audio-Visual Lip Biometrics</title>
    <summary>  Audio-visual (AV) lip biometrics is a promising authentication technique that
leverages the benefits of both the audio and visual modalities in speech
communication. Previous works have demonstrated the usefulness of AV lip
biometrics. However, the lack of a sizeable AV database hinders the exploration
of deep-learning-based audio-visual lip biometrics. To address this problem, we
compile a moderate-size database using existing public databases. Meanwhile, we
establish the DeepLip AV lip biometrics system realized with a convolutional
neural network (CNN) based video module, a time-delay neural network (TDNN)
based audio module, and a multimodal fusion module. Our experiments show that
DeepLip outperforms traditional speaker recognition models in context modeling
and achieves over 50% relative improvements compared with our best single
modality baseline, with an equal error rate of 0.75% and 1.11% on the test
datasets, respectively.
</summary>
    <author>
      <name>Meng Liu</name>
    </author>
    <author>
      <name>Longbiao Wang</name>
    </author>
    <author>
      <name>Kong Aik Lee</name>
    </author>
    <author>
      <name>Hanyi Zhang</name>
    </author>
    <author>
      <name>Chang Zeng</name>
    </author>
    <author>
      <name>Jianwu Dang</name>
    </author>
    <link href="http://arxiv.org/abs/2104.08510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.08510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.08592v1</id>
    <updated>2021-04-17T16:33:43Z</updated>
    <published>2021-04-17T16:33:43Z</published>
    <title>New Technology, New Rules for Journalism and a New World of Engagement</title>
    <summary>  The ways in which people learn, communicate and engage in discussion have
changed profoundly during the past decade. As Jenkins related in her book, The
Convergence Crisis: An Impending Paradigm Shift in Advertising, Millenials do
not want to be told the whole story. Rather, they want someone to begin a
conversation that will engage others to become participants in the development
of that story (2015). Technology now allows that to happen, sometimes with
unintended and/or ill consequences, but technology also generates a dynamic
potential to create international and interactive discourse aimed at addressing
shared global challenges.
</summary>
    <author>
      <name>Loup M. Langton</name>
    </author>
    <author>
      <name>Mercedes L. de Uriarte</name>
    </author>
    <author>
      <name>Kim Grinfeder</name>
    </author>
    <author>
      <name>Paulo Nuno Vicente</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings WJEC 2019, Paris</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.08592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.08592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.10116v1</id>
    <updated>2021-04-20T16:54:44Z</updated>
    <published>2021-04-20T16:54:44Z</published>
    <title>Detection of Audio-Video Synchronization Errors Via Event Detection</title>
    <summary>  We present a new method and a large-scale database to detect audio-video
synchronization(A/V sync) errors in tennis videos. A deep network is trained to
detect the visual signature of the tennis ball being hit by the racquet in the
video stream. Another deep network is trained to detect the auditory signature
of the same event in the audio stream. During evaluation, the audio stream is
searched by the audio network for the audio event of the ball being hit. If the
event is found in audio, the neighboring interval in video is searched for the
corresponding visual signature. If the event is not found in the video stream
but is found in the audio stream, A/V sync error is flagged. We developed a
large-scaled database of 504,300 frames from 6 hours of videos of tennis
events, simulated A/V sync errors, and found our method achieves high accuracy
on the task.
</summary>
    <author>
      <name>Joshua P. Ebenezer</name>
    </author>
    <author>
      <name>Yongjun Wu</name>
    </author>
    <author>
      <name>Hai Wei</name>
    </author>
    <author>
      <name>Sriram Sethuraman</name>
    </author>
    <author>
      <name>Zongyi Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in ICASSP 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.10116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.10116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.11317v1</id>
    <updated>2021-04-22T20:56:54Z</updated>
    <published>2021-04-22T20:56:54Z</published>
    <title>Improving Hierarchy Storage for Video Streaming in Cloud</title>
    <summary>  Frequently accessed video streams are pre-transcoded into several formats to
satisfy the characteristics of all display devices. Storing several video
stream formats imposes a high cost on video stream providers using the old
classical way. Alternatively, cloud providers offer a high flexibility of using
their services and at a low cost relatively. Therefore, video stream companies
adopted cloud technology to store their video streams. Generally, having all
video streams stored in one type of cloud storage, the cost rises gradually.
More importantly, the variation of the access pattern to frequently accessed
video streams impacts negatively the storage cost and increases it
significantly. To optimize storage usage and lower its cost, we propose a
method that manages the cloud hierarchy storage. Particularly, we develop an
algorithm that operates on parts of different videos that are frequently
accessed and stores them in their suitable storage type cloud. Experiments came
up with promising results on reducing the cost of using cloud storage by 18.75
%.
</summary>
    <author>
      <name>Mahmoud Darwich</name>
    </author>
    <author>
      <name>Yasser Ismail</name>
    </author>
    <author>
      <name>Talal Darwich</name>
    </author>
    <author>
      <name>Magdy Bayoumi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2021 IEEE 7th World Forum on Internet of Things (WF-IoT)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.11317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.11317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.11568v1</id>
    <updated>2021-04-23T12:53:33Z</updated>
    <published>2021-04-23T12:53:33Z</published>
    <title>The Influence of Audio on Video Memorability with an Audio Gestalt
  Regulated Video Memorability System</title>
    <summary>  Memories are the tethering threads that tie us to the world, and memorability
is the measure of their tensile strength. The threads of memory are spun from
fibres of many modalities, obscuring the contribution of a single fibre to a
thread's overall tensile strength. Unfurling these fibres is the key to
understanding the nature of their interaction, and how we can ultimately create
more meaningful media content. In this paper, we examine the influence of audio
on video recognition memorability, finding evidence to suggest that it can
facilitate overall video recognition memorability rich in high-level (gestalt)
audio features. We introduce a novel multimodal deep learning-based late-fusion
system that uses audio gestalt to estimate the influence of a given video's
audio on its overall short-term recognition memorability, and selectively
leverages audio features to make a prediction accordingly. We benchmark our
audio gestalt based system on the Memento10k short-term video memorability
dataset, achieving top-2 state-of-the-art results.
</summary>
    <author>
      <name>Lorin Sweeney</name>
    </author>
    <author>
      <name>Graham Healy</name>
    </author>
    <author>
      <name>Alan F. Smeaton</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CBMI50038.2021.9461903</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CBMI50038.2021.9461903" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, 4 tables, paper accepted in CBMI 2021 for
  publication and oral presentation</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.11568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.11568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12734v1</id>
    <updated>2021-04-26T17:24:57Z</updated>
    <published>2021-04-26T17:24:57Z</published>
    <title>DVMark: A Deep Multiscale Framework for Video Watermarking</title>
    <summary>  Video watermarking embeds a message into a cover video in an imperceptible
manner, which can be retrieved even if the video undergoes certain
modifications or distortions. Traditional watermarking methods are often
manually designed for particular types of distortions and thus cannot
simultaneously handle a broad spectrum of distortions. To this end, we propose
a robust deep learning-based solution for video watermarking that is end-to-end
trainable. Our model consists of a novel multiscale design where the watermarks
are distributed across multiple spatial-temporal scales. It gains robustness
against various distortions through a differentiable distortion layer, whereas
non-differentiable distortions, such as popular video compression standards,
are modeled by a differentiable proxy. Extensive evaluations on a wide variety
of distortions show that our method outperforms traditional video watermarking
methods as well as deep image watermarking models by a large margin. We further
demonstrate the practicality of our method on a realistic video-editing
application.
</summary>
    <author>
      <name>Xiyang Luo</name>
    </author>
    <author>
      <name>Yinxiao Li</name>
    </author>
    <author>
      <name>Huiwen Chang</name>
    </author>
    <author>
      <name>Ce Liu</name>
    </author>
    <author>
      <name>Peyman Milanfar</name>
    </author>
    <author>
      <name>Feng Yang</name>
    </author>
    <link href="http://arxiv.org/abs/2104.12734v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.12734v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.12770v1</id>
    <updated>2021-04-26T17:16:18Z</updated>
    <published>2021-04-26T17:16:18Z</published>
    <title>Adaptive Encoding for Constrained Video Delivery in HEVC, VP9, AV1 and
  VVC Compression Standards and Adaptation to Video Content</title>
    <summary>  The dissertation proposes the use of a multi-objective optimization framework
for designing and selecting among enhanced GOP configurations in video
compression standards. The proposed methods achieve fine optimization over a
set of general modes that include: (i) maximum video quality, (ii) minimum
bitrate, (iii) maximum encoding rate (previously minimum encoding time mode)
and (iv) can be shown to improve upon the YouTube/Netflix default encoder mode
settings over a set of opposing constraints to guarantee satisfactory
performance. The dissertation describes the implementation of a codec-agnostic
approach using different video coding standards (x265, VP9, AV1) on a wide
range of videos derived from different video datasets. The results demonstrate
that the optimal encoding parameters obtained from the Pareto front space can
provide significant bandwidth savings without sacrificing video quality. This
is achieved by the use of effective regression models that allow for the
selection of video encoding settings that are jointly optimal in the encoding
time, bitrate, and video quality space. The dissertation applies the proposed
methods to x265, VP9, AV1 and using new GOP configurations in x265, delivering
over 40% of the optimal encodings in two standard reference videos.
</summary>
    <author>
      <name>Gangadharan Esakki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Video codecs, Pareto front, Regression models, Video encoding, Video
  quality assessment</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.12770v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.12770v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.14115v1</id>
    <updated>2021-04-29T05:24:58Z</updated>
    <published>2021-04-29T05:24:58Z</published>
    <title>LIQA: Lifelong Blind Image Quality Assessment</title>
    <summary>  Existing blind image quality assessment (BIQA) methods are mostly designed in
a disposable way and cannot evolve with unseen distortions adaptively, which
greatly limits the deployment and application of BIQA models in real-world
scenarios. To address this problem, we propose a novel Lifelong blind Image
Quality Assessment (LIQA) approach, targeting to achieve the lifelong learning
of BIQA. Without accessing to previous training data, our proposed LIQA can not
only learn new distortions, but also mitigate the catastrophic forgetting of
seen distortions. Specifically, we adopt the Split-and-Merge distillation
strategy to train a single-head network that makes task-agnostic predictions.
In the split stage, we first employ a distortion-specific generator to obtain
the pseudo features of each seen distortion. Then, we use an auxiliary
multi-head regression network to generate the predicted quality of each seen
distortion. In the merge stage, we replay the pseudo features paired with
pseudo labels to distill the knowledge of multiple heads, which can build the
final regressed single head. Experimental results demonstrate that the proposed
LIQA method can handle the continuous shifts of different distortion types and
even datasets. More importantly, our LIQA model can achieve stable performance
even if the task sequence is long.
</summary>
    <author>
      <name>Jianzhao Liu</name>
    </author>
    <author>
      <name>Wei Zhou</name>
    </author>
    <author>
      <name>Jiahua Xu</name>
    </author>
    <author>
      <name>Xin Li</name>
    </author>
    <author>
      <name>Shukun An</name>
    </author>
    <author>
      <name>Zhibo Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2104.14115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.14115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.14522v3</id>
    <updated>2021-09-13T10:23:13Z</updated>
    <published>2021-04-29T17:40:50Z</published>
    <title>Automatic Generation of H.264 Parameter Sets to Recover Video File
  Fragments</title>
    <summary>  We address the problem of decoding video file fragments when the necessary
encoding parameters are missing. With this objective, we propose a method that
automatically generates H.264 video headers containing these parameters and
extracts coded pictures in the partially available compressed video data. To
accomplish this, we examined a very large corpus of videos to learn patterns of
encoding settings commonly used by encoders and created a parameter dictionary.
Further, to facilitate a more efficient search our method identifies
characteristics of a coded bitstream to discriminate the entropy coding mode.
It also utilizes the application logs created by the decoder to identify
correct parameter values. Evaluation of the effectiveness of the proposed
method on more than 55K videos with diverse provenance shows that it can
generate valid headers on average in 11.3 decoding trials per video. This
result represents an improvement by more than a factor of 10 over the
conventional approach of video header stitching to recover video file
fragments.
</summary>
    <author>
      <name>Enes Altinisik</name>
    </author>
    <author>
      <name>H√ºsrev Taha Sencar</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIFS.2021.3118876</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIFS.2021.3118876" rel="related"/>
    <link href="http://arxiv.org/abs/2104.14522v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.14522v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.14799v1</id>
    <updated>2021-04-30T07:35:55Z</updated>
    <published>2021-04-30T07:35:55Z</published>
    <title>Cross-Modal Music-Video Recommendation: A Study of Design Choices</title>
    <summary>  In this work, we study music/video cross-modal recommendation, i.e.
recommending a music track for a video or vice versa. We rely on a
self-supervised learning paradigm to learn from a large amount of unlabelled
data. We rely on a self-supervised learning paradigm to learn from a large
amount of unlabelled data. More precisely, we jointly learn audio and video
embeddings by using their co-occurrence in music-video clips. In this work, we
build upon a recent video-music retrieval system (the VM-NET), which originally
relies on an audio representation obtained by a set of statistics computed over
handcrafted features. We demonstrate here that using audio representation
learning such as the audio embeddings provided by the pre-trained MuSimNet,
OpenL3, MusicCNN or by AudioSet, largely improves recommendations. We also
validate the use of the cross-modal triplet loss originally proposed in the
VM-NET compared to the binary cross-entropy loss commonly used in
self-supervised learning. We perform all our experiments using the Music Video
Dataset (MVD).
</summary>
    <author>
      <name>Laure Pretet</name>
    </author>
    <author>
      <name>Gael Richard</name>
    </author>
    <author>
      <name>Geoffroy Peeters</name>
    </author>
    <link href="http://arxiv.org/abs/2104.14799v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.14799v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.14802v1</id>
    <updated>2021-04-30T07:36:49Z</updated>
    <published>2021-04-30T07:36:49Z</published>
    <title>Dance Generation with Style Embedding: Learning and Transferring Latent
  Representations of Dance Styles</title>
    <summary>  Choreography refers to creation of dance steps and motions for dances
according to the latent knowledge in human mind, where the created dance
motions are in general style-specific and consistent. So far, such latent
style-specific knowledge about dance styles cannot be represented explicitly in
human language and has not yet been learned in previous works on music-to-dance
generation tasks. In this paper, we propose a novel music-to-dance synthesis
framework with controllable style embeddings. These embeddings are learned
representations of style-consistent kinematic abstraction of reference dance
clips, which act as controllable factors to impose style constraints on dance
generation in a latent manner. Thus, the dance styles can be transferred to
dance motions by merely modifying the style embeddings. To support this study,
we build a large music-to-dance dataset. The qualitative and quantitative
evaluations demonstrate the advantage of our proposed framework, as well as the
ability of synthesizing diverse styles of dances from identical music via style
embeddings.
</summary>
    <author>
      <name>Xinjian Zhang</name>
    </author>
    <author>
      <name>Yi Xu</name>
    </author>
    <author>
      <name>Su Yang</name>
    </author>
    <author>
      <name>Longwen Gao</name>
    </author>
    <author>
      <name>Huyang Sun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submit to IJCAI-21</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.14802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.14802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.00136v1</id>
    <updated>2021-05-01T00:49:26Z</updated>
    <published>2021-05-01T00:49:26Z</published>
    <title>Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical
  Visual Question Answering</title>
    <summary>  Due to the severe lack of labeled data, existing methods of medical visual
question answering usually rely on transfer learning to obtain effective image
feature representation and use cross-modal fusion of visual and linguistic
features to achieve question-related answer prediction. These two phases are
performed independently and without considering the compatibility and
applicability of the pre-trained features for cross-modal fusion. Thus, we
reformulate image feature pre-training as a multi-task learning paradigm and
witness its extraordinary superiority, forcing it to take into account the
applicability of features for the specific image comprehension task.
Furthermore, we introduce a cross-modal self-attention~(CMSA) module to
selectively capture the long-range contextual relevance for more effective
fusion of visual and linguistic features. Experimental results demonstrate that
the proposed method outperforms existing state-of-the-art methods. Our code and
models are available at https://github.com/haifangong/CMSA-MTPT-4-MedicalVQA.
</summary>
    <author>
      <name>Haifan Gong</name>
    </author>
    <author>
      <name>Guanqi Chen</name>
    </author>
    <author>
      <name>Sishuo Liu</name>
    </author>
    <author>
      <name>Yizhou Yu</name>
    </author>
    <author>
      <name>Guanbin Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICMR '21: ACM International Conference on Multimedia Retrieval,
  Taipei, Taiwan, August 21-24, 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.00136v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.00136v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.00567v1</id>
    <updated>2021-05-02T22:40:21Z</updated>
    <published>2021-05-02T22:40:21Z</published>
    <title>Multi-feature 360 Video Quality Estimation</title>
    <summary>  We propose a new method for the visual quality assessment of 360-degree
(omnidirectional) videos. The proposed method is based on computing multiple
spatio-temporal objective quality features on viewports extracted from
360-degree videos. A new model is learnt to properly combine these features
into a metric that closely matches subjective quality scores. The main
motivations for the proposed approach are that: 1) quality metrics computed on
viewports better captures the user experience than metrics computed on the
projection domain; 2) the use of viewports easily supports different projection
methods being used in current 360-degree video systems; and 3) no individual
objective image quality metric always performs the best for all types of visual
distortions, while a learned combination of them is able to adapt to different
conditions. Experimental results, based on both the largest available
360-degree videos quality dataset and a cross-dataset validation, demonstrate
that the proposed metric outperforms state-of-the-art 360-degree and 2D video
quality metrics.
</summary>
    <author>
      <name>Roberto G. de A. Azevedo</name>
    </author>
    <author>
      <name>Neil Birkbeck</name>
    </author>
    <author>
      <name>Ivan Janatra</name>
    </author>
    <author>
      <name>Balu Adsumilli</name>
    </author>
    <author>
      <name>Pascal Frossard</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/OJCAS.2021.3073891</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/OJCAS.2021.3073891" rel="related"/>
    <link href="http://arxiv.org/abs/2105.00567v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.00567v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.01701v2</id>
    <updated>2021-05-13T12:16:44Z</updated>
    <published>2021-05-04T18:47:30Z</published>
    <title>Viewport-Aware Dynamic 360¬∞ Video Segment Categorization</title>
    <summary>  Unlike conventional videos, 360{\deg} videos give freedom to users to turn
their heads, watch and interact with the content owing to its immersive
spherical environment. Although these movements are arbitrary, similarities can
be observed between viewport patterns of different users and different videos.
Identifying such patterns can assist both content and network providers to
enhance the 360{\deg} video streaming process, eventually increasing the
end-user Quality of Experience (QoE). But a study on how viewport patterns
display similarities across different video content, and their potential
applications has not yet been done. In this paper, we present a comprehensive
analysis of a dataset of 88 360{\deg} videos and propose a novel video
categorization algorithm that is based on similarities of viewports. First, we
propose a novel viewport clustering algorithm that outperforms the existing
algorithms in terms of clustering viewports with similar positioning and speed.
Next, we develop a novel and unique dynamic video segment categorization
algorithm that shows notable improvement in similarity for viewport
distributions within the clusters when compared to that of existing static
video categorizations.
</summary>
    <author>
      <name>Amaya Dharmasiri</name>
    </author>
    <author>
      <name>Chamara Kattadige</name>
    </author>
    <author>
      <name>Vincent Zhang</name>
    </author>
    <author>
      <name>Kanchana Thilakarathna</name>
    </author>
    <link href="http://arxiv.org/abs/2105.01701v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.01701v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.02409v1</id>
    <updated>2021-05-06T03:01:21Z</updated>
    <published>2021-05-06T03:01:21Z</published>
    <title>Multimedia Edge Computing</title>
    <summary>  In this paper, we investigate the recent studies on multimedia edge
computing, from sensing not only traditional visual/audio data but also
individuals' geographical preference and mobility behaviors, to performing
distributed machine learning over such data using the joint edge and cloud
infrastructure and using evolutional strategies like reinforcement learning and
online learning at edge devices to optimize the quality of experience for
multimedia services at the last mile proactively. We provide both a
retrospective view of recent rapid migration (resp. merge) of cloud multimedia
to (resp. and) edge-aware multimedia and insights on the fundamental guidelines
for designing multimedia edge computing strategies that target satisfying the
changing demand of quality of experience. By showing the recent research
studies and industrial solutions, we also provide future directions towards
high-quality multimedia services over edge computing.
</summary>
    <author>
      <name>Zhi Wang</name>
    </author>
    <author>
      <name>Wenwu Zhu</name>
    </author>
    <author>
      <name>Lifeng Sun</name>
    </author>
    <author>
      <name>Han Hu</name>
    </author>
    <author>
      <name>Ge Ma</name>
    </author>
    <author>
      <name>Ming Ma</name>
    </author>
    <author>
      <name>Haitian Pang</name>
    </author>
    <author>
      <name>Jiahui Ye</name>
    </author>
    <author>
      <name>Hongshan Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 9 figures. arXiv admin note: text overlap with
  arXiv:1702.07627</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.02409v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.02409v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.03611v1</id>
    <updated>2021-05-08T06:36:01Z</updated>
    <published>2021-05-08T06:36:01Z</published>
    <title>360NorVic: 360-Degree Video Classification from Mobile Encrypted Video
  Traffic</title>
    <summary>  Streaming 360{\deg} video demands high bandwidth and low latency, and poses
significant challenges to Internet Service Providers (ISPs) and Mobile Network
Operators (MNOs). The identification of 360{\deg} video traffic can therefore
benefits fixed and mobile carriers to optimize their network and provide better
Quality of Experience (QoE) to the user. However, end-to-end encryption of
network traffic has obstructed identifying those 360{\deg} videos from regular
videos. As a solution this paper presents 360NorVic, a near-realtime and
offline Machine Learning (ML) classification engine to distinguish 360{\deg}
videos from regular videos when streamed from mobile devices. We collect packet
and flow level data for over 800 video traces from YouTube &amp; Facebook
accounting for 200 unique videos under varying streaming conditions. Our
results show that for near-realtime and offline classification at packet level,
average accuracy exceeds 95%, and that for flow level, 360NorVic achieves more
than 92% average accuracy. Finally, we pilot our solution in the commercial
network of a large MNO showing the feasibility and effectiveness of 360NorVic
in production settings.
</summary>
    <author>
      <name>Chamara Kattadige</name>
    </author>
    <author>
      <name>Aravindh Raman</name>
    </author>
    <author>
      <name>Kanchana Thilakarathna</name>
    </author>
    <author>
      <name>Andra Lutu</name>
    </author>
    <author>
      <name>Diego Perino</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3458306.3460998</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3458306.3460998" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 15 figures, accepted in Workshop on Network and
  OperatingSystem Support for Digital Audio and Video (NOSSDAV 21)</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.03611v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.03611v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.06361v2</id>
    <updated>2022-01-22T05:33:37Z</updated>
    <published>2021-05-13T15:40:39Z</published>
    <title>Forensic Analysis of Video Files Using Metadata</title>
    <summary>  The unprecedented ease and ability to manipulate video content has led to a
rapid spread of manipulated media. The availability of video editing tools
greatly increased in recent years, allowing one to easily generate
photo-realistic alterations. Such manipulations can leave traces in the
metadata embedded in video files. This metadata information can be used to
determine video manipulations, brand of video recording device, the type of
video editing tool, and other important evidence. In this paper, we focus on
the metadata contained in the popular MP4 video wrapper/container. We describe
our method for metadata extractor that uses the MP4's tree structure. Our
approach for analyzing the video metadata produces a more compact
representation. We will describe how we construct features from the metadata
and then use dimensionality reduction and nearest neighbor classification for
forensic analysis of a video file. Our approach allows one to visually inspect
the distribution of metadata features and make decisions. The experimental
results confirm that the performance of our approach surpasses other methods.
</summary>
    <author>
      <name>Ziyue Xiang</name>
    </author>
    <author>
      <name>J√°nos Horv√°th</name>
    </author>
    <author>
      <name>Sriram Baireddy</name>
    </author>
    <author>
      <name>Paolo Bestagini</name>
    </author>
    <author>
      <name>Stefano Tubaro</name>
    </author>
    <author>
      <name>Edward J. Delp</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CVPRW53098.2021.00115</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CVPRW53098.2021.00115" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">v2: fixed a typo in Section 3.4; added page number; added IEEE
  copyright notice</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.06361v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.06361v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.09280v1</id>
    <updated>2021-04-03T08:35:11Z</updated>
    <published>2021-04-03T08:35:11Z</published>
    <title>A Deep Learning Scheme for Efficient Multimedia IoT Data Compression</title>
    <summary>  Given the voluminous nature of the multimedia sensed data, the Multimedia
Internet of Things (MIoT) devices and networks will present several limitations
in terms of power and communication overhead. One traditional solution to cope
with the large-size data challenge is to use lossy compression. However,
current lossy compression schemes require low compression rate to guarantee
acceptable perceived image quality, which results in a low reduction of the
communicated data size and consequently a low reduction in the energy and
bandwidth consumption. Thus, an efficient compression solution is required for
striking a good balance between data size (and consequently communication
overhead) and visual degradation. In this paper, a Deep-Learning (DL)
super-resolution model is applied to recuperate high quality images (at the
application server side) given as input degraded images with a high compression
ratio (at the sender side). The experimental analysis shows the effectiveness
of the proposed solution in enhancing the visual quality of the compressed and
down-scaled images. Consequently, the proposed solution reduces the overall
communication overhead and power consumption of limited MIoT devices.
</summary>
    <author>
      <name>Hassan N. Noura</name>
    </author>
    <author>
      <name>Ola Salman</name>
    </author>
    <author>
      <name>Rapha√´l Couturier</name>
    </author>
    <link href="http://arxiv.org/abs/2105.09280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.09280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.09281v1</id>
    <updated>2021-04-06T18:11:31Z</updated>
    <published>2021-04-06T18:11:31Z</published>
    <title>A Decade of Research for Image Compression In Multimedia Laboratory</title>
    <summary>  With the advancement of technology, we have supercomputers with high
processing power and affordable prices. In addition, using multimedia expanded
all around the world. This caused a vast use of images and videos in different
fields. As this kind of data consists of a large amount of information, there
is a need to use compression methods to store, manage or transfer them better
and faster. One effective technique, which was introduced is variable
resolution. This technique stimulates human vision and divides regions in
pictures into two different parts, including the area of interest that needs
more detail and periphery parts with less detail. This results in better
compression. The variable resolution was used for image, video, and 3D motion
data compression. This paper investigates the mentioned technique and some
other research in this regard.
</summary>
    <author>
      <name>Shahrokh Paravarzar</name>
    </author>
    <author>
      <name>Javaneh Alavi</name>
    </author>
    <link href="http://arxiv.org/abs/2105.09281v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.09281v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.09284v1</id>
    <updated>2021-04-25T05:00:53Z</updated>
    <published>2021-04-25T05:00:53Z</published>
    <title>SemEval-2021 Task 6: Detection of Persuasion Techniques in Texts and
  Images</title>
    <summary>  We describe SemEval-2021 task 6 on Detection of Persuasion Techniques in
Texts and Images: the data, the annotation guidelines, the evaluation setup,
the results, and the participating systems. The task focused on memes and had
three subtasks: (i) detecting the techniques in the text, (ii) detecting the
text spans where the techniques are used, and (iii) detecting techniques in the
entire meme, i.e., both in the text and in the image. It was a popular task,
attracting 71 registrations, and 22 teams that eventually made an official
submission on the test set. The evaluation results for the third subtask
confirmed the importance of both modalities, the text and the image. Moreover,
some teams reported benefits when not just combining the two modalities, e.g.,
by using early or late fusion, but rather modeling the interaction between them
in a joint model.
</summary>
    <author>
      <name>Dimitar Dimitrov</name>
    </author>
    <author>
      <name>Bishr Bin Ali</name>
    </author>
    <author>
      <name>Shaden Shaar</name>
    </author>
    <author>
      <name>Firoj Alam</name>
    </author>
    <author>
      <name>Fabrizio Silvestri</name>
    </author>
    <author>
      <name>Hamed Firooz</name>
    </author>
    <author>
      <name>Preslav Nakov</name>
    </author>
    <author>
      <name>Giovanni Da San Martino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">propaganda, disinformation, misinformation, fake news, memes,
  multimodality</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SemEval-2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2105.09284v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.09284v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.11563v1</id>
    <updated>2021-05-24T22:48:35Z</updated>
    <published>2021-05-24T22:48:35Z</published>
    <title>VAD360: Viewport Aware Dynamic 360-Degree Video Frame Tiling</title>
    <summary>  360{\deg} videos a.k.a. spherical videos are getting popular among users
nevertheless, omnidirectional view of these videos demands high bandwidth and
processing power at the end devices. Recently proposed viewport aware streaming
mechanisms can reduce the amount of data transmitted by streaming a limited
portion of the frame covering the current user viewport (VP). However, they
still suffer from sending a high amount of redundant data, as the fixed tile
mechanisms can not provide finer granularity to the user VP. Though making the
tiles smaller can provide a finer granularity for user viewport, high encoding
overhead incurred. To overcome this trade-off, in this paper, we present a
computational geometric approach based adaptive tiling mechanism named VAD360,
which takes visual attention information on the 360{\deg} video frame as the
input and provide a suitable non-overlapping variable size tile cover on the
frame. Experimental results shows that VAD360 can save up to 31.1% of pixel
redundancy before compression and 35.4% of bandwidth saving compared to
recently proposed fixed tile configurations, providing tile schemes within
0.98($\pm$0.11)s time frame.
</summary>
    <author>
      <name>Chamara Kattadige</name>
    </author>
    <author>
      <name>Kanchana Thilakarathna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.11563v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.11563v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.09814v1</id>
    <updated>2021-06-17T20:55:44Z</updated>
    <published>2021-06-17T20:55:44Z</published>
    <title>PixInWav: Residual Steganography for Hiding Pixels in Audio</title>
    <summary>  Steganography comprises the mechanics of hiding data in a host media that may
be publicly available. While previous works focused on unimodal setups (e.g.,
hiding images in images, or hiding audio in audio), PixInWav targets the
multimodal case of hiding images in audio. To this end, we propose a novel
residual architecture operating on top of short-time discrete cosine transform
(STDCT) audio spectrograms. Among our results, we find that the residual audio
steganography setup we propose allows independent encoding of the hidden image
from the host audio without compromising quality. Accordingly, while previous
works require both host and hidden signals to hide a signal, PixInWav can
encode images offline -- which can be later hidden, in a residual fashion, into
any audio signal. Finally, we test our scheme in a lab setting to transmit
images over airwaves from a loudspeaker to a microphone verifying our
theoretical insights and obtaining promising results.
</summary>
    <author>
      <name>Margarita Geleta</name>
    </author>
    <author>
      <name>Cristina Punti</name>
    </author>
    <author>
      <name>Kevin McGuinness</name>
    </author>
    <author>
      <name>Jordi Pons</name>
    </author>
    <author>
      <name>Cristian Canton</name>
    </author>
    <author>
      <name>Xavier Giro-i-Nieto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract presented in CVPR 2021 Women in Computer Vision
  Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.09814v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.09814v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.10134v1</id>
    <updated>2021-06-18T13:55:51Z</updated>
    <published>2021-06-18T13:55:51Z</published>
    <title>An Audio-Driven System For Real-Time Music Visualisation</title>
    <summary>  Computer-generated visualisations can accompany recorded or live music to
create novel audiovisual experiences for audiences. We present a system to
streamline the creation of audio-driven visualisations based on audio feature
extraction and mapping interfaces. Its architecture is based on three modular
software components: backend (audio plugin), frontend (3D game-like
environment), and middleware (visual mapping interface). We conducted a user
evaluation comprising two stages. Results from the first stage (34
participants) indicate that music visualisations generated with the system were
significantly better at complementing the music than a baseline visualisation.
Nine participants took part in the second stage involving interactive tasks.
Overall, the system yielded a Creativity Support Index above average (68.1) and
a System Usability Scale index (58.6) suggesting that ease of use can be
improved. Thematic analysis revealed that participants enjoyed the system's
synchronicity and expressive capabilities, but found technical problems and
difficulties understanding the audio feature terminology.
</summary>
    <author>
      <name>Max Graf</name>
    </author>
    <author>
      <name>Harold Chijioke Opara</name>
    </author>
    <author>
      <name>Mathieu Barthet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AES 150 Convention, Paper 10498, May 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2106.10134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.10134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.13574v1</id>
    <updated>2021-06-25T11:53:48Z</updated>
    <published>2021-06-25T11:53:48Z</published>
    <title>Multiview Video Compression Using Advanced HEVC Screen Content Coding</title>
    <summary>  The paper presents a new approach to multiview video coding using Screen
Content Coding. It is assumed that for a time instant the frames corresponding
to all views are packed into a single frame, i.e. the frame-compatible approach
to multiview coding is applied. For such coding scenario, the paper
demonstrates that Screen Content Coding can be efficiently used for multiview
video coding. Two approaches are considered: the first using standard HEVC
Screen Content Coding, and the second using Advanced Screen Content Coding. The
latter is the original proposal of the authors that exploits quarter-pel motion
vectors and other nonstandard extensions of HEVC Screen Content Coding. The
experimental results demonstrate that multiview video coding even using
standard HEVC Screen Content Coding is much more efficient than simulcast HEVC
coding. The proposed Advanced Screen Content Coding provides virtually the same
coding efficiency as MV-HEVC, which is the state-of-the-art multiview video
compression technique. The authors suggest that Advanced Screen Content Coding
can be efficiently used within the new Versatile Video Coding (VVC) technology.
Nevertheless a reference multiview extension of VVC does not exist yet,
therefore, for VVC-based coding, the experimental comparisons are left for
future work.
</summary>
    <author>
      <name>Jaros≈Çaw Samelak</name>
    </author>
    <author>
      <name>Marek Doma≈Ñski</name>
    </author>
    <link href="http://arxiv.org/abs/2106.13574v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.13574v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.00054v1</id>
    <updated>2021-07-30T19:33:47Z</updated>
    <published>2021-07-30T19:33:47Z</published>
    <title>A Point-to-Distribution Joint Geometry and Color Metric for Point Cloud
  Quality Assessment</title>
    <summary>  Point clouds (PCs) are a powerful 3D visual representation paradigm for many
emerging application domains, especially virtual and augmented reality, and
autonomous vehicles. However, the large amount of PC data required for highly
immersive and realistic experiences requires the availability of efficient,
lossy PC coding solutions are critical. Recently, two MPEG PC coding standards
have been developed to address the relevant application requirements and
further developments are expected in the future. In this context, the
assessment of PC quality, notably for decoded PCs, is critical and asks for the
design of efficient objective PC quality metrics. In this paper, a novel
point-to-distribution metric is proposed for PC quality assessment considering
both the geometry and texture. This new quality metric exploits the
scale-invariance property of the Mahalanobis distance to assess first the
geometry and color point-to-distribution distortions, which are after fused to
obtain a joint geometry and color quality metric. The proposed quality metric
significantly outperforms the best PC quality assessment metrics in the
literature.
</summary>
    <author>
      <name>Alireza Javaheri</name>
    </author>
    <author>
      <name>Catarina Brites</name>
    </author>
    <author>
      <name>Fernando Pereira</name>
    </author>
    <author>
      <name>Jo√£o Ascenso</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MMSP53017.2021.9733670</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MMSP53017.2021.9733670" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted for publication in IEEE Workshop on
  Multimedia Signal Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.00054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.00054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.00970v1</id>
    <updated>2021-08-02T15:14:43Z</updated>
    <published>2021-08-02T15:14:43Z</published>
    <title>Is there a "language of music-video clips" ? A qualitative and
  quantitative study</title>
    <summary>  Recommending automatically a video given a music or a music given a video has
become an important asset for the audiovisual industry - with user-generated or
professional content. While both music and video have specific temporal
organizations, most current works do not consider those and only focus on
globally recommending a media. As a first step toward the improvement of these
recommendation systems, we study in this paper the relationship between music
and video temporal organization. We do this for the case of official music
videos, with a quantitative and a qualitative approach. Our assumption is that
the movement in the music are correlated to the ones in the video. To validate
this, we first interview a set of internationally recognized music video
experts. We then perform a large-scale analysis of official music-video clips
(which we manually annotated into video genres) using MIR description tools
(downbeats and functional segments estimation) and Computer Vision tools (shot
detection). Our study confirms that a "language of music-video clips" exists;
i.e. editors favor the co-occurrence of music and video events using strategies
such as anticipation. It also highlights that the amount of co-occurrence
depends on the music and video genres.
</summary>
    <author>
      <name>Laure Pr√©tet</name>
    </author>
    <author>
      <name>Ga√´l Richard</name>
    </author>
    <author>
      <name>Geoffroy Peeters</name>
    </author>
    <link href="http://arxiv.org/abs/2108.00970v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.00970v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.04187v1</id>
    <updated>2021-08-09T17:17:29Z</updated>
    <published>2021-08-09T17:17:29Z</published>
    <title>Scaling New Peaks: A Viewership-centric Approach to Automated Content
  Curation</title>
    <summary>  Summarizing video content is important for video streaming services to engage
the user in a limited time span. To this end, current methods involve manual
curation or using passive interest cues to annotate potential high-interest
segments to form the basis of summarized videos, and are costly and unreliable.
We propose a viewership-driven, automated method that accommodates a range of
segment identification goals. Using satellite television viewership data as a
source of ground truth for viewer interest, we apply statistical anomaly
detection on a timeline of viewership metrics to identify 'seed' segments of
high viewer interest. These segments are post-processed using empirical rules
and several sources of content metadata, e.g. shot boundaries, adding in
personalization aspects to produce the final highlights video.
  To demonstrate the flexibility of our approach, we present two case studies,
on the United States Democratic Presidential Debate on 19th December 2019, and
Wimbledon Women's Final 2019. We perform qualitative comparisons with their
publicly available highlights, as well as early vs. late viewership comparisons
for insights into possible media and social influence on viewing behavior.
</summary>
    <author>
      <name>Subhabrata Majumdar</name>
    </author>
    <author>
      <name>Deirdre Paul</name>
    </author>
    <author>
      <name>Eric Zavesky</name>
    </author>
    <link href="http://arxiv.org/abs/2108.04187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.04187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.08083v1</id>
    <updated>2021-08-18T10:29:53Z</updated>
    <published>2021-08-18T10:29:53Z</published>
    <title>Promoting Mental Well-Being for Audiences in a Live-Streaming Game by
  Highlight-Based Bullet Comments</title>
    <summary>  This paper proposes a method for generating bullet comments for
live-streaming games based on highlights (i.e., the exciting parts of video
clips) extracted from the game content and evaluate the effect of mental health
promotion. Game live streaming is becoming a popular theme for academic
research. Compared to traditional online video sharing platforms, such as
Youtube and Vimeo, video live streaming platform has the benefits of
communicating with other viewers in real-time. In sports broadcasting, the
commentator plays an essential role as mood maker by making matches more
exciting. The enjoyment emerged while watching game live streaming also
benefits the audience's mental health. However, many e-sports live streaming
channels do not have a commentator for entertaining viewers. Therefore, this
paper presents a design of an AI commentator that can be embedded in live
streaming games. To generate bullet comments for real-time game live streaming,
the system employs highlight evaluation to detect the highlights, and generate
the bullet comments. An experiment is conducted and the effectiveness of
generated bullet comments in a live-streaming fighting game channel is
evaluated.
</summary>
    <author>
      <name>Junjie H. Xu</name>
    </author>
    <author>
      <name>Yulin Cai</name>
    </author>
    <author>
      <name>Zhou Fang</name>
    </author>
    <author>
      <name>Pujana Paliyawan</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2021 IEEE 10th Global Conference on Consumer Electronics (GCCE
  2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2108.08083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.08083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.08112v1</id>
    <updated>2021-08-18T12:26:21Z</updated>
    <published>2021-08-18T12:26:21Z</published>
    <title>Fighting Game Commentator with Pitch and Loudness Adjustment Utilizing
  Highlight Cues</title>
    <summary>  This paper presents a commentator for providing real-time game commentary in
a fighting game. The commentary takes into account highlight cues, obtained by
analyzing scenes during gameplay, as input to adjust the pitch and loudness of
commentary to be spoken by using a Text-to-Speech (TTS) technology. We
investigate different designs for pitch and loudness adjustment. The proposed
AI consists of two parts: a dynamic adjuster for controlling pitch and loudness
of the TTS and a real-time game commentary generator. We conduct a pilot study
on a fighting game, and our result shows that by adjusting the loudness
significantly according to the level of game highlight, the entertainment of
the gameplay can be enhanced.
</summary>
    <author>
      <name>Junjie H. Xu</name>
    </author>
    <author>
      <name>Zhou Fang</name>
    </author>
    <author>
      <name>Qihang Chen</name>
    </author>
    <author>
      <name>Satoru Ohno</name>
    </author>
    <author>
      <name>Pujana Paliyawan</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2021 IEEE 10th Global Conference on Consumer Electronics (GCCE
  2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2108.08112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.08112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.08985v1</id>
    <updated>2021-08-20T03:36:46Z</updated>
    <published>2021-08-20T03:36:46Z</published>
    <title>Metaverse for Social Good: A University Campus Prototype</title>
    <summary>  In recent years, the metaverse has attracted enormous attention from around
the world with the development of related technologies. The expected metaverse
should be a realistic society with more direct and physical interactions, while
the concepts of race, gender, and even physical disability would be weakened,
which would be highly beneficial for society. However, the development of
metaverse is still in its infancy, with great potential for improvement.
Regarding metaverse's huge potential, industry has already come forward with
advance preparation, accompanied by feverish investment, but there are few
discussions about metaverse in academia to scientifically guide its
development. In this paper, we highlight the representative applications for
social good. Then we propose a three-layer metaverse architecture from a macro
perspective, containing infrastructure, interaction, and ecosystem. Moreover,
we journey toward both a historical and novel metaverse with a detailed
timeline and table of specific attributes. Lastly, we illustrate our
implemented blockchain-driven metaverse prototype of a university campus and
discuss the prototype design and insights.
</summary>
    <author>
      <name>Haihan Duan</name>
    </author>
    <author>
      <name>Jiaye Li</name>
    </author>
    <author>
      <name>Sizheng Fan</name>
    </author>
    <author>
      <name>Zhonghao Lin</name>
    </author>
    <author>
      <name>Xiao Wu</name>
    </author>
    <author>
      <name>Wei Cai</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3474085.3479238</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3474085.3479238" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 29th ACM International Conference on Multimedia
  (MM '21), October 20--24, 2021, Virtual Event, China</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2108.08985v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.08985v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.09479v1</id>
    <updated>2021-08-21T09:57:21Z</updated>
    <published>2021-08-21T09:57:21Z</published>
    <title>Grid-VLP: Revisiting Grid Features for Vision-Language Pre-training</title>
    <summary>  Existing approaches to vision-language pre-training (VLP) heavily rely on an
object detector based on bounding boxes (regions), where salient objects are
first detected from images and then a Transformer-based model is used for
cross-modal fusion. Despite their superior performance, these approaches are
bounded by the capability of the object detector in terms of both effectiveness
and efficiency. Besides, the presence of object detection imposes unnecessary
constraints on model designs and makes it difficult to support end-to-end
training. In this paper, we revisit grid-based convolutional features for
vision-language pre-training, skipping the expensive region-related steps. We
propose a simple yet effective grid-based VLP method that works surprisingly
well with the grid features. By pre-training only with in-domain datasets, the
proposed Grid-VLP method can outperform most competitive region-based VLP
methods on three examined vision-language understanding tasks. We hope that our
findings help to further advance the state of the art of vision-language
pre-training, and provide a new direction towards effective and efficient VLP.
</summary>
    <author>
      <name>Ming Yan</name>
    </author>
    <author>
      <name>Haiyang Xu</name>
    </author>
    <author>
      <name>Chenliang Li</name>
    </author>
    <author>
      <name>Bin Bi</name>
    </author>
    <author>
      <name>Junfeng Tian</name>
    </author>
    <author>
      <name>Min Gui</name>
    </author>
    <author>
      <name>Wei Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2108.09479v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.09479v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.01774v2</id>
    <updated>2021-09-07T01:51:11Z</updated>
    <published>2021-09-04T03:09:38Z</published>
    <title>What Matters for Ad-hoc Video Search? A Large-scale Evaluation on
  TRECVID</title>
    <summary>  For quantifying progress in Ad-hoc Video Search (AVS), the annual TRECVID AVS
task is an important international evaluation. Solutions submitted by the task
participants vary in terms of their choices of cross-modal matching models,
visual features and training data. As such, what one may conclude from the
evaluation is at a high level that is insufficient to reveal the influence of
the individual components. In order to bridge the gap between the current
solution-level comparison and the desired component-wise comparison, we propose
in this paper a large-scale and systematic evaluation on TRECVID. By selected
combinations of state-of-the-art matching models, visual features and
(pre-)training data, we construct a set of 25 different solutions and evaluate
them on the TRECVID AVS tasks 2016--2020. The presented evaluation helps answer
the key question of what matters for AVS. The resultant observations and
learned lessons are also instructive for developing novel AVS solutions.
</summary>
    <author>
      <name>Aozhu Chen</name>
    </author>
    <author>
      <name>Fan Hu</name>
    </author>
    <author>
      <name>Zihan Wang</name>
    </author>
    <author>
      <name>Fangming Zhou</name>
    </author>
    <author>
      <name>Xirong Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ViRal'21@ICCV 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.01774v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.01774v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.04440v1</id>
    <updated>2021-09-09T17:38:42Z</updated>
    <published>2021-09-09T17:38:42Z</published>
    <title>'1e0a': A Computational Approach to Rhythm Training</title>
    <summary>  We present a computational assessment system that promotes the learning of
basic rhythmic patterns. The system is capable of generating multiple rhythmic
patterns with increasing complexity within various cycle lengths. For a
generated rhythm pattern the performance assessment of the learner is carried
out through the statistical deviations calculated from the onset detection and
temporal assessment of a learner's performance. This is compared with the
generated pattern, and their performance accuracy forms the feedback to the
learner. The system proceeds to generate a new pattern of increased complexity
when performance assessment results are within certain error bounds. The system
thus mimics a learner-teacher relationship as the learner progresses in their
feedback-based learning. The choice of progression within a cycle for each
pattern is determined by a predefined complexity metric. This metric is based
on a coded element model for the perceptual processing of sequential stimuli.
The model earlier proposed for a sequence of tones and non-tones, is now used
for onsets and silences. This system is developed into a web-based application
and provides accessibility for learning purposes. Analysis of the performance
assessments shows that the complexity metric is indicative of the perceptual
processing of rhythm patterns and can be used for rhythm learning.
</summary>
    <author>
      <name>Noel Alben</name>
    </author>
    <author>
      <name>Ranjani H. G</name>
    </author>
    <link href="http://arxiv.org/abs/2109.04440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.04440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.07149v1</id>
    <updated>2021-09-15T08:21:01Z</updated>
    <published>2021-09-15T08:21:01Z</published>
    <title>Fusion with Hierarchical Graphs for Mulitmodal Emotion Recognition</title>
    <summary>  Automatic emotion recognition (AER) based on enriched multimodal inputs,
including text, speech, and visual clues, is crucial in the development of
emotionally intelligent machines. Although complex modality relationships have
been proven effective for AER, they are still largely underexplored because
previous works predominantly relied on various fusion mechanisms with simply
concatenated features to learn multimodal representations for emotion
classification. This paper proposes a novel hierarchical fusion graph
convolutional network (HFGCN) model that learns more informative multimodal
representations by considering the modality dependencies during the feature
fusion procedure. Specifically, the proposed model fuses multimodality inputs
using a two-stage graph construction approach and encodes the modality
dependencies into the conversation representation. We verified the
interpretable capabilities of the proposed method by projecting the emotional
states to a 2D valence-arousal (VA) subspace. Extensive experiments showed the
effectiveness of our proposed model for more accurate AER, which yielded
state-of-the-art results on two public datasets, IEMOCAP and MELD.
</summary>
    <author>
      <name>Shuyun Tang</name>
    </author>
    <author>
      <name>Zhaojie Luo</name>
    </author>
    <author>
      <name>Guoshun Nan</name>
    </author>
    <author>
      <name>Yuichiro Yoshikawa</name>
    </author>
    <author>
      <name>Ishiguro Hiroshi</name>
    </author>
    <link href="http://arxiv.org/abs/2109.07149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.07149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.08007v1</id>
    <updated>2021-09-16T14:31:32Z</updated>
    <published>2021-09-16T14:31:32Z</published>
    <title>Graph Fourier Transform based Audio Zero-watermarking</title>
    <summary>  The frequent exchange of multimedia information in the present era projects
an increasing demand for copyright protection. In this work, we propose a novel
audio zero-watermarking technology based on graph Fourier transform for
enhancing the robustness with respect to copyright protection. In this
approach, the combined shift operator is used to construct the graph signal,
upon which the graph Fourier analysis is performed. The selected maximum
absolute graph Fourier coefficients representing the characteristics of the
audio segment are then encoded into a feature binary sequence using K-means
algorithm. Finally, the resultant feature binary sequence is XOR-ed with the
watermark binary sequence to realize the embedding of the zero-watermarking.
The experimental studies show that the proposed approach performs more
effectively in resisting common or synchronization attacks than the existing
state-of-the-art methods.
</summary>
    <author>
      <name>Longting Xu</name>
    </author>
    <author>
      <name>Daiyu Huang</name>
    </author>
    <author>
      <name>Syed Faham Ali Zaidi</name>
    </author>
    <author>
      <name>Abdul Rauf</name>
    </author>
    <author>
      <name>Rohan Kumar Das</name>
    </author>
    <link href="http://arxiv.org/abs/2109.08007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.08007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.08275v2</id>
    <updated>2023-01-28T02:56:58Z</updated>
    <published>2021-09-17T01:34:15Z</published>
    <title>Multi-Level Visual Similarity Based Personalized Tourist Attraction
  Recommendation Using Geo-Tagged Photos</title>
    <summary>  Geo-tagged photo based tourist attraction recommendation can discover users'
travel preferences from their taken photos, so as to recommend suitable tourist
attractions to them. However, existing visual content based methods cannot
fully exploit the user and tourist attraction information of photos to extract
visual features, and do not differentiate the significances of different
photos. In this paper, we propose multi-level visual similarity based
personalized tourist attraction recommendation using geo-tagged photos (MEAL).
MEAL utilizes the visual contents of photos and interaction behavior data to
obtain the final embeddings of users and tourist attractions, which are then
used to predict the visit probabilities. Specifically, by crossing the user and
tourist attraction information of photos, we define four visual similarity
levels and introduce a corresponding quintuplet loss to embed the visual
contents of photos. In addition, to capture the significances of different
photos, we exploit the self-attention mechanism to obtain the visual
representations of users and tourist attractions. We conducted experiments on a
dataset crawled from Flickr, and the experimental results proved the advantage
of this method.
</summary>
    <author>
      <name>Ling Chen</name>
    </author>
    <author>
      <name>Dandan Lyu</name>
    </author>
    <author>
      <name>Shanshan Yu</name>
    </author>
    <author>
      <name>Gencai Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by TKDD</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.08275v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.08275v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.10016v1</id>
    <updated>2021-09-21T08:07:27Z</updated>
    <published>2021-09-21T08:07:27Z</published>
    <title>CONQUER: Contextual Query-aware Ranking for Video Corpus Moment
  Retrieval</title>
    <summary>  This paper tackles a recently proposed Video Corpus Moment Retrieval task.
This task is essential because advanced video retrieval applications should
enable users to retrieve a precise moment from a large video corpus. We propose
a novel CONtextual QUery-awarE Ranking~(CONQUER) model for effective moment
localization and ranking. CONQUER explores query context for multi-modal fusion
and representation learning in two different steps. The first step derives
fusion weights for the adaptive combination of multi-modal video content. The
second step performs bi-directional attention to tightly couple video and query
as a single joint representation for moment localization. As query context is
fully engaged in video representation learning, from feature fusion to
transformation, the resulting feature is user-centered and has a larger
capacity in capturing multi-modal signals specific to query. We conduct studies
on two datasets, TVR for closed-world TV episodes and DiDeMo for open-world
user-generated videos, to investigate the potential advantages of fusing video
and query online as a joint representation for moment retrieval.
</summary>
    <author>
      <name>Zhijian Hou</name>
    </author>
    <author>
      <name>Chong-Wah Ngo</name>
    </author>
    <author>
      <name>Wing Kwong Chan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3474085.3475281</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3474085.3475281" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures, 2021 MultiMedia, code:
  https://github.com/houzhijian/CONQUER</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.10016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.10016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.10572v1</id>
    <updated>2021-09-22T08:06:43Z</updated>
    <published>2021-09-22T08:06:43Z</published>
    <title>Realism of Simulation Models in Serious Gaming: Two case studies from
  Urban Water Management Higher Education</title>
    <summary>  For games used in educational contexts, realism, i.e., the degree of
congruence between the simulation models used in the games and the real-world
systems represented, is an important characteristic for achieving learning
goals well. However, in the past, the realism of especially entertainment games
has often been identified as insufficient. Thus, this study is investigating
the degree of realism provided by current games. To this purpose, two games in
the domain urban water management, a subdomain of environmental engineering
(EE), are examined. One is ANAWAK, a web-based serious game on water management
and climate change. For ANAWAK, an analysis of the simulation model is
conducted. Second, the simulation model of the entertainment game Cities:
Skylines (CS) is analyzed. In addition, a survey among CS players (N=61) is
conducted. Thereby, different degrees of realism in various EE subdomains are
revealed. All in all, there are still considerable deficits regarding the
degree of realism in the CS simulation model. However, modding as a means of
achieving more realistic simulation models is more widely supported than in the
past.
</summary>
    <author>
      <name>Darwin Droll</name>
    </author>
    <author>
      <name>Heinrich S√∂bke</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-92182-8_29</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-92182-8_29" rel="related"/>
    <link href="http://arxiv.org/abs/2109.10572v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.10572v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.4; K.3.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.11735v3</id>
    <updated>2022-01-22T14:42:44Z</updated>
    <published>2021-09-24T04:05:22Z</published>
    <title>On the Robustness of "Robust reversible data hiding scheme based on
  two-layer embedding strategy"</title>
    <summary>  In the paper "Robust reversible data hiding scheme based on two-layer
embedding strategy" published in INS recently, Kumar et al. proposed a robust
reversible data hiding (RRDH) scheme based on two-layer embedding. Secret data
was embedded into the most significant bit (MSB) planes to increase robustness,
and a sorting strategy based on local complexity was adopted to reduce
distortion. However, Kumar et al.'s reversible data hiding (RDH) scheme is not
as robust against joint photographic experts group (JPEG) compression as stated
and can not be called RRDH. This comment first gives a brief description of
their RDH scheme, then analyses their scheme's robustness from the perspective
of JPEG compression principles. JPEG compression will change pixel values,
thereby destroying auxiliary information and pixel value ordering required to
extract secret data correctly, making their scheme not robust. Next, the
changes in both bit plane and pixel value ordering after JPEG compression are
shown and analysed by different robustness-testing experiments. Finally, some
suggestions are given to improve the robustness.
</summary>
    <author>
      <name>Wen Yin</name>
    </author>
    <author>
      <name>Longfei Ke</name>
    </author>
    <author>
      <name>Zhaoxia Yin</name>
    </author>
    <author>
      <name>Jin Tang</name>
    </author>
    <author>
      <name>Bin Luo</name>
    </author>
    <link href="http://arxiv.org/abs/2109.11735v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.11735v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.11913v1</id>
    <updated>2021-09-24T12:07:49Z</updated>
    <published>2021-09-24T12:07:49Z</published>
    <title>Spatial Information Refinement for Chroma Intra Prediction in Video
  Coding</title>
    <summary>  Video compression benefits from advanced chroma intra prediction methods,
such as the Cross-Component Linear Model (CCLM) which uses linear models to
approximate the relationship between the luma and chroma components. Recently
it has been proven that advanced cross-component prediction methods based on
Neural Networks (NN) can bring additional coding gains. In this paper, spatial
information refinement is proposed for improving NN-based chroma intra
prediction. Specifically, the performance of chroma intra prediction can be
improved by refined down-sampling or by incorporating location information.
Experimental results show that the two proposed methods obtain 0.31%, 2.64%,
2.02% and 0.33%, 3.00%, 2.12% BD-rate reduction on Y, Cb and Cr components,
respectively, under All-Intra configuration, when implemented in Versatile
Video Coding (H.266/VVC) test model. Index Terms-Chroma intra prediction,
convolutional neural networks, spatial information refinement.
</summary>
    <author>
      <name>Chengyi Zou</name>
    </author>
    <author>
      <name>Shuai Wan</name>
    </author>
    <author>
      <name>Tiannan Ji</name>
    </author>
    <author>
      <name>Marta Mrak</name>
    </author>
    <author>
      <name>Marc Gorriz Blanch</name>
    </author>
    <author>
      <name>Luis Herranz</name>
    </author>
    <link href="http://arxiv.org/abs/2109.11913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.11913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.12294v3</id>
    <updated>2022-05-13T10:52:59Z</updated>
    <published>2021-09-25T07:00:00Z</published>
    <title>Revisiting Pre-analysis Information Based Rate Control in x265</title>
    <summary>  Due to the excellent compression and high real-time performance, x265 is
widely used in practical applications. Combined with CU-tree based
pre-analysis, x265 rate control can obtain high rate-distortion (R-D)
performance. However, the pre-analysis information is not fully utilized, and
the accuracy of rate control is not satisfactory in x265 because of an
empirical linear model. In this paper, we propose an improved cost-guided rate
control scheme for x265. Firstly, the pre-analysis information is further used
to refine the bit allocation. Secondly, CU-tree is combined with the
lambda-domain model for more accurate rate control and higher R-D performance.
Experimental results show that compared with the original x265, our method can
achieve 10.3\% BD-rate gain with only 0.22\textperthousand bitrate error.
</summary>
    <author>
      <name>Hewei Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2109.12294v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.12294v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.13354v1</id>
    <updated>2021-09-27T21:25:31Z</updated>
    <published>2021-09-27T21:25:31Z</published>
    <title>Audio-to-Image Cross-Modal Generation</title>
    <summary>  Cross-modal representation learning allows to integrate information from
different modalities into one representation. At the same time, research on
generative models tends to focus on the visual domain with less emphasis on
other domains, such as audio or text, potentially missing the benefits of
shared representations. Studies successfully linking more than one modality in
the generative setting are rare. In this context, we verify the possibility to
train variational autoencoders (VAEs) to reconstruct image archetypes from
audio data. Specifically, we consider VAEs in an adversarial training framework
in order to ensure more variability in the generated data and find that there
is a trade-off between the consistency and diversity of the generated images -
this trade-off can be governed by scaling the reconstruction loss up or down,
respectively. Our results further suggest that even in the case when the
generated images are relatively inconsistent (diverse), features that are
critical for proper image classification are preserved.
</summary>
    <author>
      <name>Maciej ≈ªelaszczyk</name>
    </author>
    <author>
      <name>Jacek Ma≈Ñdziuk</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Joint Conference on Neural Networks, IJCNN 2022,
  Padua, Italy, 1-8</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2109.13354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.13354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.00138v1</id>
    <updated>2021-10-01T00:32:07Z</updated>
    <published>2021-10-01T00:32:07Z</published>
    <title>Deep Connection: Making Virtual Reality Artworks with Medical Scan Data</title>
    <summary>  Deep Connection is an installation and virtual reality artwork made using
full body 3D and 4D magnetic resonance scan datasets. When the user enters Deep
Connection, they see a scanned body lying prone in mid-air. The user can walk
around the body and inspect it. The user can dive inside and see its inner
workings, its lungs, spine, brain. The user can take hold of the figure's
outstretched hand: holding the hand triggers the 4D dataset, making the heart
beat and the lungs breathe. When the user lets go of the hand, the heart stops
beating and the lungs stop breathing. Deep Connection creates a scenario where
an embodied human becomes the companion for a virtual body. This paper maps the
conceptual and theoretical framework for Deep Connection such as virtual
intimacy and digital mediated companionship. It also reflects on working with
scanned bodies more generally in virtual reality by discussing transparency,
the cyberbody versus the data body, as well as data privacy and data ethics.
The paper also explains the technical and procedural aspects of the Deep
Connection project with respect to acquiring scan data for the creation of
virtual reality artworks.
</summary>
    <author>
      <name>Marilene Oliver</name>
    </author>
    <author>
      <name>Gary James Joynes</name>
    </author>
    <author>
      <name>Kumar Punithakumar</name>
    </author>
    <author>
      <name>Peter Seres</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 15 figures, submitted to VISAP 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.00138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.00138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.00416v1</id>
    <updated>2021-08-09T06:33:29Z</updated>
    <published>2021-08-09T06:33:29Z</published>
    <title>FiLMing Multimodal Sarcasm Detection with Attention</title>
    <summary>  Sarcasm detection identifies natural language expressions whose intended
meaning is different from what is implied by its surface meaning. It finds
applications in many NLP tasks such as opinion mining, sentiment analysis, etc.
Today, social media has given rise to an abundant amount of multimodal data
where users express their opinions through text and images. Our paper aims to
leverage multimodal data to improve the performance of the existing systems for
sarcasm detection. So far, various approaches have been proposed that uses text
and image modality and a fusion of both. We propose a novel architecture that
uses the RoBERTa model with a co-attention layer on top to incorporate context
incongruity between input text and image attributes. Further, we integrate
feature-wise affine transformation by conditioning the input image through
FiLMed ResNet blocks with the textual features using the GRU network to capture
the multimodal information. The output from both the models and the CLS token
from RoBERTa is concatenated and used for the final prediction. Our results
demonstrate that our proposed model outperforms the existing state-of-the-art
method by 6.14% F1 score on the public Twitter multimodal sarcasm detection
dataset.
</summary>
    <author>
      <name>Sundesh Gupta</name>
    </author>
    <author>
      <name>Aditya Shah</name>
    </author>
    <author>
      <name>Miten Shah</name>
    </author>
    <author>
      <name>Laribok Syiemlieh</name>
    </author>
    <author>
      <name>Chandresh Maurya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.00416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.00416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.00957v3</id>
    <updated>2022-02-15T08:14:43Z</updated>
    <published>2021-10-03T09:09:08Z</published>
    <title>Graph Representation Learning for Spatial Image Steganalysis</title>
    <summary>  In this paper, we introduce a graph representation learning architecture for
spatial image steganalysis, which is motivated by the assumption that
steganographic modifications unavoidably distort the statistical
characteristics of the hidden graph features derived from cover images. In the
detailed architecture, we translate each image to a graph, where nodes
represent the patches of the image and edges indicate the local relationships
between the patches. Each node is associated with a feature vector determined
from the corresponding patch by a shallow convolutional neural network (CNN)
structure. By feeding the graph to an attention network, the discriminative
features can be learned for efficient steganalysis. Experiments indicate that
the reported architecture achieves a competitive performance compared to the
benchmark CNN model, which has shown the potential of graph learning for
steganalysis.
</summary>
    <author>
      <name>Qiyun Liu</name>
    </author>
    <author>
      <name>Hanzhou Wu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">https://scholar.google.com/citations?user=IdiF7M0AAAAJ&amp;hl=en</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Workshop on Multimedia Signal Processing (2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2110.00957v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.00957v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.01001v1</id>
    <updated>2021-10-03T13:24:00Z</updated>
    <published>2021-10-03T13:24:00Z</published>
    <title>Multimodal Fusion Based Attentive Networks for Sequential Music
  Recommendation</title>
    <summary>  Music has the power to evoke intense emotional experiences and regulate the
mood of an individual. With the advent of online streaming services, research
in music recommendation services has seen tremendous progress. Modern methods
leveraging the listening histories of users for session-based song
recommendations have overlooked the significance of features extracted from
lyrics and acoustic content. We address the task of song prediction through
multiple modalities, including tags, lyrics, and acoustic content. In this
paper, we propose a novel deep learning approach by refining Attentive Neural
Networks using representations derived via a Transformer model for lyrics and
Variational Autoencoder for acoustic features. Our model achieves significant
improvement in performance over existing state-of-the-art models using lyrical
and acoustic features alone. Furthermore, we conduct a study to investigate the
impact of users' psychological health on our model's performance.
</summary>
    <author>
      <name>Kunal Vaswani</name>
    </author>
    <author>
      <name>Yudhik Agrawal</name>
    </author>
    <author>
      <name>Vinoo Alluri</name>
    </author>
    <link href="http://arxiv.org/abs/2110.01001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.01001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.01010v2</id>
    <updated>2022-12-17T02:08:00Z</updated>
    <published>2021-10-03T14:21:46Z</published>
    <title>High Capacity Reversible Data Hiding in Encrypted 3D Mesh Models Based
  on Multi-MSB Prediction</title>
    <summary>  As a new generation of digital media for covert transmission, three-dimension
(3D) mesh models are frequently used and distributed on the network. Facing the
huge massive of network data, it is urgent to study a method to protect and
store this large amounts of data. In this paper, we proposed a high capacity
reversible data hiding in encrypted 3D mesh models. This method divides the
vertices of all 3D mesh into "embedded sets" and "prediction sets" based on the
parity of the index. In addition, the multiple most significant bit (Multi-MSB)
prediction reserved space is used to adaptively embed secret message, and the
auxiliary information is compressed by arithmetic coding to further free up
redundant space of the 3D mesh models. We use the majority voting system(MSV)
principle to restore the original mesh model with high quality. The
experimental results show that our method achieves a higher embedding capacity
compared with state-of-the-art RDH-ED methods on 3D mesh models and can restore
the original 3D mesh models with high quality.
</summary>
    <author>
      <name>Wanli Lv</name>
    </author>
    <author>
      <name>Lulu Cheng</name>
    </author>
    <author>
      <name>Zhaoxia Yin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.sigpro.2022.108686</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.sigpro.2022.108686" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in Signal Processing</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Volume 201, December 2022, 108686</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2110.01010v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.01010v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.02436v1</id>
    <updated>2021-10-06T00:46:43Z</updated>
    <published>2021-10-06T00:46:43Z</published>
    <title>A Deep Learning-based Audio-in-Image Watermarking Scheme</title>
    <summary>  This paper presents a deep learning-based audio-in-image watermarking scheme.
Audio-in-image watermarking is the process of covertly embedding and extracting
audio watermarks on a cover-image. Using audio watermarks can open up
possibilities for different downstream applications. For the purpose of
implementing an audio-in-image watermarking that adapts to the demands of
increasingly diverse situations, a neural network architecture is designed to
automatically learn the watermarking process in an unsupervised manner. In
addition, a similarity network is developed to recognize the audio watermarks
under distortions, therefore providing robustness to the proposed method.
Experimental results have shown high fidelity and robustness of the proposed
blind audio-in-image watermarking scheme.
</summary>
    <author>
      <name>Arjon Das</name>
    </author>
    <author>
      <name>Xin Zhong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been accepted for publication by the 2021 IEEE Visual
  Communications and Image Processing. The copyright is with the IEEE</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.02436v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.02436v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.06520v1</id>
    <updated>2021-10-13T06:17:01Z</updated>
    <published>2021-10-13T06:17:01Z</published>
    <title>Impacts of Device Caching of Content Fractions on Expected Content
  Quality</title>
    <summary>  This paper explores caching of fractions of a video content, not caching of
an entire content, to increase the expected video quality. We first show that
the highest-quality content is better to be cached and propose the caching
policy of video chunks having different qualities. Our caching policy utilizes
the characteristics of video contents that video files can be encoded into
multiple versions with different qualities, each file consists of many chunks,
and chunks can have different qualities. Extensive performance evaluations are
conducted to show that caching of content fractions, rather than an entire
content, can improve the expected video quality especially when the channel
conditions is sufficiently good to cooperate with nearby BS or helpers.
</summary>
    <author>
      <name>Dongjae Kim</name>
    </author>
    <author>
      <name>Minseok Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures, (will be) submitted to IEEE Wireless
  Communications Letter</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.06520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.06520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.06529v1</id>
    <updated>2021-10-13T06:47:32Z</updated>
    <published>2021-10-13T06:47:32Z</published>
    <title>Power Consumption of Video-Decoders on Various Android Devices</title>
    <summary>  The critical constraint of mobile devices is a limited battery life that is
significantly reduced during video playback. The power efficiency of video
playback mainly depends on the used compression standard, video-decoder, and
device model. We propose a software-based method to estimate the power
consumption of video-decoders on various Android devices. Experiments on two
devices of the same model show a small variation of the power playback
consumption and a lack of dependence between the power consumption and the
battery level. We have implemented an automatic system that includes the VEQE
Android application to measure the power consumption of decoders and a server
to collect the power metrics. Our system has collected power-consumption and
decoding-speed dataset for video-decoders of six standards (AV1, HEVC, VP9,
H.264, VP8, and MPEG-4) operating on 285 devices, representing 147 models. We
demonstrate some slices of the created dataset: the top 30 models and
video-decoders in terms of power efficiency for playback and for decoding only,
as well as video-decoder ratings by power consumption and decoding speed for a
given device model.
</summary>
    <author>
      <name>Roman Kazantsev</name>
    </author>
    <author>
      <name>Dmitriy Vatolin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/PCS50896.2021.9477481</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/PCS50896.2021.9477481" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">For more details about mobile video-decoders ranking, see
  https://videoprocessing.ai/benchmarks/mobile-video-codec-benchmark.html</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.06529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.06529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.06805v1</id>
    <updated>2021-10-13T15:41:45Z</updated>
    <published>2021-10-13T15:41:45Z</published>
    <title>Assisting News Media Editors with Cohesive Visual Storylines</title>
    <summary>  Creating a cohesive, high-quality, relevant, media story is a challenge that
news media editors face on a daily basis. This challenge is aggravated by the
flood of highly relevant information that is constantly pouring onto the
newsroom. To assist news media editors in this daunting task, this paper
proposes a framework to organize news content into cohesive, high-quality,
relevant visual storylines. First, we formalize, in a nonsubjective manner, the
concept of visual story transition. Leveraging it, we propose four graph-based
methods of storyline creation, aiming for global story cohesiveness. These were
created and implemented to take full advantage of existing graph algorithms,
ensuring their correctness and good computational performance. They leverage a
strong ensemble-based estimator which was trained to predict story transition
quality based on both the semantic and visual features present in the pair of
images under scrutiny. A user study covered a total of 28 curated stories about
sports and cultural events. Experiments showed that (i) visual transitions in
storylines can be learned with a quality above 90%, and (ii) the proposed graph
methods can produce cohesive storylines with quality in the range of 88% to
96%.
</summary>
    <author>
      <name>Gon√ßalo Marcelino</name>
    </author>
    <author>
      <name>David Semedo</name>
    </author>
    <author>
      <name>Andr√© Mour√£o</name>
    </author>
    <author>
      <name>Saverio Blasi</name>
    </author>
    <author>
      <name>Marta Mrak</name>
    </author>
    <author>
      <name>Jo√£o Magalh√£es</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ACM Multimedia 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.06805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.06805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.06827v1</id>
    <updated>2021-10-13T16:12:18Z</updated>
    <published>2021-10-13T16:12:18Z</published>
    <title>NoisyActions2M: A Multimedia Dataset for Video Understanding from Noisy
  Labels</title>
    <summary>  Deep learning has shown remarkable progress in a wide range of problems.
However, efficient training of such models requires large-scale datasets, and
getting annotations for such datasets can be challenging and costly. In this
work, we explore the use of user-generated freely available labels from web
videos for video understanding. We create a benchmark dataset consisting of
around 2 million videos with associated user-generated annotations and other
meta information. We utilize the collected dataset for action classification
and demonstrate its usefulness with existing small-scale annotated datasets,
UCF101 and HMDB51. We study different loss functions and two pretraining
strategies, simple and self-supervised learning. We also show how a network
pretrained on the proposed dataset can help against video corruption and label
noise in downstream datasets. We present this as a benchmark dataset in noisy
learning for video understanding. The dataset, code, and trained models will be
publicly available for future research.
</summary>
    <author>
      <name>Mohit Sharma</name>
    </author>
    <author>
      <name>Raj Patra</name>
    </author>
    <author>
      <name>Harshal Desai</name>
    </author>
    <author>
      <name>Shruti Vyas</name>
    </author>
    <author>
      <name>Yogesh Rawat</name>
    </author>
    <author>
      <name>Rajiv Ratn Shah</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ACM Multimedia Asia 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2110.06827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.06827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.08146v2</id>
    <updated>2021-10-30T18:50:09Z</updated>
    <published>2021-10-15T15:20:49Z</published>
    <title>ACOA -- Chronological Analysis of the Exhibition of Artistic Works</title>
    <summary>  We are currently prioritizing home activities, avoiding human contact, and
carrying out external activities mostly by necessity. Therefore, and due to the
loss of adhesion to cultural events on the part of the population, the cultural
digital transformation process has been boosted, aiming to reach interested
communities through digital media. The ACOA platform supports the organization
of multiple sources of information related to creative processes behind complex
artworks and their trajectories over time. This information is of great
interest to conservators and curators, as well as to the general public, as it
allows to document changes in the artwork, from the moment it was conceived by
the artist, until its most recent exhibition. This platform houses a
chronological evolution of the work, through the contextual dissemination of
associated multimedia content. Works by the Portuguese artist Ana Vieira
(1940-2016) were chosen as case studies for the implementation of the platform.
</summary>
    <author>
      <name>Daniela Prado</name>
    </author>
    <author>
      <name>Armanda Rodrigues</name>
    </author>
    <author>
      <name>Nuno Correia</name>
    </author>
    <author>
      <name>Rita Macedo</name>
    </author>
    <author>
      <name>Sofia Gomes</name>
    </author>
    <link href="http://arxiv.org/abs/2110.08146v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.08146v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.11616v1</id>
    <updated>2021-10-22T06:43:55Z</updated>
    <published>2021-10-22T06:43:55Z</published>
    <title>Compressed Geometric Arrays for Point Cloud Processing</title>
    <summary>  The ever-increasing demand for 3D modeling in the emerging immersive
applications has made point clouds an essential class of data for 3D image and
video processing. Tree based structures are commonly used for representing
point clouds where pointers are used to realize the connection between nodes.
Tree-based structures significantly suffer from irregular access patterns for
large point clouds. Memory access indirection in such structures is disruptive
to bandwidth efficiency and performance. In this paper, we propose a point
cloud representation format based on compressed geometric arrays (CGA). Then,
we examine new methods for point cloud processing based on CGA. The proposed
format enables a higher bandwidth efficiency via eliminating memory access
indirections (i.e., pointer chasing at the nodes of tree) thereby improving the
efficiency of point cloud processing. Our experimental results show that using
CGA for point cloud operations achieves 1328x speed up, 1321x better bandwidth
utilization, and 54% reduction in the volume of transferred data as compared to
the state-of-the-art tree-based format from point cloud library (PCL).
</summary>
    <author>
      <name>Hoda Roodaki</name>
    </author>
    <author>
      <name>Mahdi Nazm Bojnordi</name>
    </author>
    <link href="http://arxiv.org/abs/2110.11616v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.11616v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.00417v1</id>
    <updated>2021-10-31T07:13:34Z</updated>
    <published>2021-10-31T07:13:34Z</published>
    <title>Hierarchical Deep Residual Reasoning for Temporal Moment Localization</title>
    <summary>  Temporal Moment Localization (TML) in untrimmed videos is a challenging task
in the field of multimedia, which aims at localizing the start and end points
of the activity in the video, described by a sentence query. Existing methods
mainly focus on mining the correlation between video and sentence
representations or investigating the fusion manner of the two modalities. These
works mainly understand the video and sentence coarsely, ignoring the fact that
a sentence can be understood from various semantics, and the dominant words
affecting the moment localization in the semantics are the action and object
reference. Toward this end, we propose a Hierarchical Deep Residual Reasoning
(HDRR) model, which decomposes the video and sentence into multi-level
representations with different semantics to achieve a finer-grained
localization. Furthermore, considering that videos with different resolution
and sentences with different length have different difficulty in understanding,
we design the simple yet effective Res-BiGRUs for feature fusion, which is able
to grasp the useful information in a self-adapting manner. Extensive
experiments conducted on Charades-STA and ActivityNet-Captions datasets
demonstrate the superiority of our HDRR model compared with other
state-of-the-art methods.
</summary>
    <author>
      <name>Ziyang Ma</name>
    </author>
    <author>
      <name>Xianjing Han</name>
    </author>
    <author>
      <name>Xuemeng Song</name>
    </author>
    <author>
      <name>Yiran Cui</name>
    </author>
    <author>
      <name>Liqiang Nie</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3469877.3490595</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3469877.3490595" rel="related"/>
    <link href="http://arxiv.org/abs/2111.00417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.00417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.06324v1</id>
    <updated>2021-11-11T17:31:10Z</updated>
    <published>2021-11-11T17:31:10Z</published>
    <title>How player and opponent personalities influence cooperative gameplay</title>
    <summary>  Research has shown that digital game players often feel engagement and
rapport with a game hero or character when they can channel their own ambitions
and goals through the hero's journey in the game world; in essence, they feel a
sense of accomplishment and fulfilment whenever they put the game mechanics to
use to help the hero reach a positive ending to the game quests. In the case of
cooperative gameplay, rapport also has to do with their perception of their
peers' skills, gameplay style and behaviour within the game. In this paper, we
describe an experiment to identify whether matching players with different
personalities, as characterized by the OCEAN or Big-5 personality model, can
influence their player experience with a custom-made, cooperative game.
</summary>
    <author>
      <name>Konstantina Ntretska</name>
    </author>
    <author>
      <name>Nikos Avrantinis</name>
    </author>
    <author>
      <name>George Tsatiris</name>
    </author>
    <author>
      <name>Kostas Karpouzis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper presented at the 3rd International Conference Digital Culture &amp;
  AudioVisual Challenges, Interdisciplinary Creativity in Arts and Technology</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.06324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.06324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.08380v1</id>
    <updated>2021-11-16T11:28:31Z</updated>
    <published>2021-11-16T11:28:31Z</published>
    <title>Video Background Music Generation with Controllable Music Transformer</title>
    <summary>  In this work, we address the task of video background music generation. Some
previous works achieve effective music generation but are unable to generate
melodious music tailored to a particular video, and none of them considers the
video-music rhythmic consistency. To generate the background music that matches
the given video, we first establish the rhythmic relations between video and
background music. In particular, we connect timing, motion speed, and motion
saliency from video with beat, simu-note density, and simu-note strength from
music, respectively. We then propose CMT, a Controllable Music Transformer that
enables local control of the aforementioned rhythmic features and global
control of the music genre and instruments. Objective and subjective
evaluations show that the generated background music has achieved satisfactory
compatibility with the input videos, and at the same time, impressive music
quality. Code and models are available at
https://github.com/wzk1015/video-bgm-generation.
</summary>
    <author>
      <name>Shangzhe Di</name>
    </author>
    <author>
      <name>Zeren Jiang</name>
    </author>
    <author>
      <name>Si Liu</name>
    </author>
    <author>
      <name>Zhaokai Wang</name>
    </author>
    <author>
      <name>Leyan Zhu</name>
    </author>
    <author>
      <name>Zexin He</name>
    </author>
    <author>
      <name>Hongming Liu</name>
    </author>
    <author>
      <name>Shuicheng Yan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3474085.3475195</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3474085.3475195" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACM Multimedia 2021. Project website at
  https://wzk1015.github.io/cmt/</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.08380v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.08380v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.09531v1</id>
    <updated>2021-11-18T05:45:23Z</updated>
    <published>2021-11-18T05:45:23Z</published>
    <title>Triple Attention Network architecture for MovieQA</title>
    <summary>  Movie question answering, or MovieQA is a multimedia related task wherein one
is provided with a video, the subtitle information, a question and candidate
answers for it. The task is to predict the correct answer for the question
using the components of the multimedia - namely video/images, audio and text.
Traditionally, MovieQA is done using the image and text component of the
multimedia. In this paper, we propose a novel network with triple-attention
architecture for the inclusion of audio in the Movie QA task. This architecture
is fashioned after a traditional dual attention network focused only on video
and text. Experiments show that the inclusion of audio using the
triple-attention network results provides complementary information for Movie
QA task which is not captured by visual or textual component in the data.
Experiments with a wide range of audio features show that using such a network
can indeed improve MovieQA performance by about 7% relative to just using only
visual features.
</summary>
    <author>
      <name>Ankit Shah</name>
    </author>
    <author>
      <name>Tzu-Hsiang Lin</name>
    </author>
    <author>
      <name>Shijie Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2111.09531v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.09531v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.09771v3</id>
    <updated>2022-04-06T07:22:37Z</updated>
    <published>2021-11-18T16:09:39Z</published>
    <title>Transformer-S2A: Robust and Efficient Speech-to-Animation</title>
    <summary>  We propose a novel robust and efficient Speech-to-Animation (S2A) approach
for synchronized facial animation generation in human-computer interaction.
Compared with conventional approaches, the proposed approach utilizes phonetic
posteriorgrams (PPGs) of spoken phonemes as input to ensure the cross-language
and cross-speaker ability, and introduces corresponding prosody features (i.e.
pitch and energy) to further enhance the expression of generated animation.
Mixture-of-experts (MOE)-based Transformer is employed to better model
contextual information while provide significant optimization on computation
efficiency. Experiments demonstrate the effectiveness of the proposed approach
on both objective and subjective evaluation with 17x inference speedup compared
with the state-of-the-art approach.
</summary>
    <author>
      <name>Liyang Chen</name>
    </author>
    <author>
      <name>Zhiyong Wu</name>
    </author>
    <author>
      <name>Jun Ling</name>
    </author>
    <author>
      <name>Runnan Li</name>
    </author>
    <author>
      <name>Xu Tan</name>
    </author>
    <author>
      <name>Sheng Zhao</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICASSP 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.09771v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.09771v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.00331v1</id>
    <updated>2021-12-01T07:53:38Z</updated>
    <published>2021-12-01T07:53:38Z</published>
    <title>Mutltimodal AI Companion for Interactive Fairytale Co-creation</title>
    <summary>  AI fairy tale companions play an important role in early childhood education
as an augmentation for parents' efforts to close the participation gap and
boost kids' mental and language development. Existing systems are generally
designed to provide vivid materials as unidirectional entertaining reading
environments, e.g, visualizing inputting texts. However, due to the limited
vocabulary of kids, these systems failed to afford effective interaction to
motivate kids to write their own fairy tales. In this work, we propose AI.R
Taletorium, an illustrative, immersive, and inclusive multimodal AI companion,
for interactive fairy tale co-creation that actively involves kids to create
fairy tales with both the AI agent and their normal peers. AI.R Taletorium
consists a neural story generator and a doodler-based fairy tale visualizer. We
design a character-centric bidirectional connection mechanism between the story
generator and visualizer equipped with Contrastive Language Image Pretraining
(CLIP), thus enabling kids to participant in the story generation process by
simple sketching. Extensive experiments and user studies show that our system
was able to generate and visualize meaningful and vivid fairy tales with
limited training data and complete the full interaction cycle under various
inputs (text, doodler) through the bidirectional connection.
</summary>
    <author>
      <name>Ruiyang Liu</name>
    </author>
    <author>
      <name>Predrag K. Nikolic</name>
    </author>
    <link href="http://arxiv.org/abs/2112.00331v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.00331v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01131v1</id>
    <updated>2021-12-02T11:12:09Z</updated>
    <published>2021-12-02T11:12:09Z</published>
    <title>FNR: A Similarity and Transformer-Based Approach to Detect Multi-Modal
  Fake News in Social Media</title>
    <summary>  The availability and interactive nature of social media have made them the
primary source of news around the globe. The popularity of social media tempts
criminals to pursue their immoral intentions by producing and disseminating
fake news using seductive text and misleading images. Therefore, verifying
social media news and spotting fakes is crucial. This work aims to analyze
multi-modal features from texts and images in social media for detecting fake
news. We propose a Fake News Revealer (FNR) method that utilizes transform
learning to extract contextual and semantic features and contrastive loss to
determine the similarity between image and text. We applied FNR on two real
social media datasets. The results show the proposed method achieves higher
accuracies in detecting fake news compared to the previous works.
</summary>
    <author>
      <name>Faeze Ghorbanpour</name>
    </author>
    <author>
      <name>Maryam Ramezani</name>
    </author>
    <author>
      <name>Mohammad A. Fazli</name>
    </author>
    <author>
      <name>Hamid R. Rabiee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 11 figures, 4 tables and 20 references</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.01131v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.01131v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.01832v3</id>
    <updated>2022-07-27T02:50:30Z</updated>
    <published>2021-12-03T10:41:12Z</published>
    <title>Lightweight Attentional Feature Fusion: A New Baseline for Text-to-Video
  Retrieval</title>
    <summary>  In this paper we revisit feature fusion, an old-fashioned topic, in the new
context of text-to-video retrieval. Different from previous research that
considers feature fusion only at one end, let it be video or text, we aim for
feature fusion for both ends within a unified framework. We hypothesize that
optimizing the convex combination of the features is preferred to modeling
their correlations by computationally heavy multi-head self attention. We
propose Lightweight Attentional Feature Fusion (LAFF). LAFF performs feature
fusion at both early and late stages and at both video and text ends, making it
a powerful method for exploiting diverse (off-the-shelf) features. The
interpretability of LAFF can be used for feature selection. Extensive
experiments on five public benchmark sets (MSR-VTT, MSVD, TGIF, VATEX and
TRECVID AVS 2016-2020) justify LAFF as a new baseline for text-to-video
retrieval.
</summary>
    <author>
      <name>Fan Hu</name>
    </author>
    <author>
      <name>Aozhu Chen</name>
    </author>
    <author>
      <name>Ziyue Wang</name>
    </author>
    <author>
      <name>Fangming Zhou</name>
    </author>
    <author>
      <name>Jianfeng Dong</name>
    </author>
    <author>
      <name>Xirong Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ECCV2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.01832v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.01832v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.02070v1</id>
    <updated>2021-12-03T18:34:54Z</updated>
    <published>2021-12-03T18:34:54Z</published>
    <title>Malakai: Music That Adapts to the Shape of Emotions</title>
    <summary>  The advent of ML music models such as Google Magenta's MusicVAE now allow us
to extract and replicate compositional features from otherwise complex
datasets. These models allow computational composers to parameterize abstract
variables such as style and mood. By leveraging these models and combining them
with procedural algorithms from the last few decades, it is possible to create
a dynamic song that composes music in real-time to accompany interactive
experiences. Malakai is a tool that helps users of varying skill levels create,
listen to, remix and share such dynamic songs. Using Malakai, a Composer can
create a dynamic song that can be interacted with by a Listener
</summary>
    <author>
      <name>Zack Harris</name>
    </author>
    <author>
      <name>Liam Atticus Clarke</name>
    </author>
    <author>
      <name>Pietro Gagliano</name>
    </author>
    <author>
      <name>Dante Camarena</name>
    </author>
    <author>
      <name>Manal Siddiqui</name>
    </author>
    <author>
      <name>Pablo S. Castro</name>
    </author>
    <link href="http://arxiv.org/abs/2112.02070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.02070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.05428v1</id>
    <updated>2021-12-10T10:13:42Z</updated>
    <published>2021-12-10T10:13:42Z</published>
    <title>Protecting Your NLG Models with Semantic and Robust Watermarks</title>
    <summary>  Natural language generation (NLG) applications have gained great popularity
due to the powerful deep learning techniques and large training corpus. The
deployed NLG models may be stolen or used without authorization, while
watermarking has become a useful tool to protect Intellectual Property (IP) of
deep models. However, existing watermarking technologies using backdoors are
easily detected or harmful for NLG applications. In this paper, we propose a
semantic and robust watermarking scheme for NLG models that utilize unharmful
phrase pairs as watermarks for IP protection. The watermarks give NLG models
personal preference for some special phrase combinations. Specifically, we
generate watermarks by following a semantic combination pattern and
systematically augment the watermark corpus to enhance the robustness. Then, we
embed these watermarks into an NLG model without misleading its original
attention mechanism. We conduct extensive experiments and the results
demonstrate the effectiveness, robustness, and undetectability of the proposed
scheme.
</summary>
    <author>
      <name>Tao Xiang</name>
    </author>
    <author>
      <name>Chunlong Xie</name>
    </author>
    <author>
      <name>Shangwei Guo</name>
    </author>
    <author>
      <name>Jiwei Li</name>
    </author>
    <author>
      <name>Tianwei Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2112.05428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.05428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.08432v1</id>
    <updated>2021-12-15T19:20:04Z</updated>
    <published>2021-12-15T19:20:04Z</published>
    <title>Expert and Crowd-Guided Affect Annotation and Prediction</title>
    <summary>  We employ crowdsourcing to acquire time-continuous affective annotations for
movie clips, and refine noisy models trained from these crowd annotations
incorporating expert information within a Multi-task Learning (MTL) framework.
We propose a novel \textbf{e}xpert \textbf{g}uided MTL (EG-MTL) algorithm,
which minimizes the loss with respect to both crowd and expert labels to learn
a set of weights corresponding to each movie clip for which crowd annotations
are acquired. We employ EG-MTL to solve two problems, namely,
\textbf{\texttt{P1}}: where dynamic annotations acquired from both experts and
crowdworkers for the \textbf{Validation} set are used to train a regression
model with audio-visual clip descriptors as features, and predict dynamic
arousal and valence levels on 5--15 second snippets derived from the clips; and
\textbf{\texttt{P2}}: where a classification model trained on the
\textbf{Validation} set using dynamic crowd and expert annotations (as
features) and static affective clip labels is used for binary emotion
recognition on the \textbf{Evaluation} set for which only dynamic crowd
annotations are available. Observed experimental results confirm the
effectiveness of the EG-MTL algorithm, which is reflected via improved arousal
and valence estimation for \textbf{\texttt{P1}}, and higher recognition
accuracy for \textbf{\texttt{P2}}.
</summary>
    <author>
      <name>Ramanathan Subramanian</name>
    </author>
    <author>
      <name>Yan Yan</name>
    </author>
    <author>
      <name>Nicu Sebe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Manuscript submitted for review to IEEE Transactions on Affective
  Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.08432v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.08432v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.10381v1</id>
    <updated>2021-12-20T08:12:52Z</updated>
    <published>2021-12-20T08:12:52Z</published>
    <title>Automated Vision-Based Wellness Analysis for Elderly Care Centers</title>
    <summary>  The growth in the aging population requires caregivers to improve both
efficiency and quality of healthcare. In this study, we develop an automatic,
vision-based system for monitoring and analyzing the physical and mental
well-being of senior citizens. Through collaboration with Haven of Hope
Christian Service, we collect video recording data in the care center with
surveillance cameras. We then process and extract personalized facial,
activity, and interaction features from the video data using deep neural
networks. This integrated health information systems can assist caregivers to
gain better insights into the seniors they are taking care of. These insights,
including wellness metrics and long-term health patterns of senior citizens,
can help caregivers update their caregiving strategies. We report the findings
of our analysis and evaluate the system quantitatively. We also summarize
technical challenges and additional functionalities and technologies needed for
offering a comprehensive system.
</summary>
    <author>
      <name>Xijie Huang</name>
    </author>
    <author>
      <name>Jeffry Wicaksana</name>
    </author>
    <author>
      <name>Shichao Li</name>
    </author>
    <author>
      <name>Kwang-Ting Cheng</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be appeared at AAAI22 health intelligence workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.10381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.10381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.10603v2</id>
    <updated>2021-12-22T06:43:47Z</updated>
    <published>2021-12-20T15:17:57Z</published>
    <title>A Multi-user Oriented Live Free-viewpoint Video Streaming System Based
  On View Interpolation</title>
    <summary>  As an important application form of immersive multimedia services,
free-viewpoint video(FVV) enables users with great immersive experience by
strong interaction. However, the computational complexity of virtual view
synthesis algorithms poses a significant challenge to the real-time performance
of an FVV system. Furthermore, the individuality of user interaction makes it
difficult to serve multiple users simultaneously for a system with conventional
architecture. In this paper, we novelly introduce a CNN-based view
interpolation algorithm to synthesis dense virtual views in real time. Based on
this, we also build an end-to-end live free-viewpoint system with a multi-user
oriented streaming strategy. Our system can utilize a single edge server to
serve multiple users at the same time without having to bring a large view
synthesis load on the client side. We analyze the whole system and show that
our approaches give the user a pleasant immersive experience, in terms of both
visual quality and latency.
</summary>
    <author>
      <name>Jingchuan Hu</name>
    </author>
    <author>
      <name>Shuai Guo</name>
    </author>
    <author>
      <name>Kai Zhou</name>
    </author>
    <author>
      <name>Yu Dong</name>
    </author>
    <author>
      <name>Jun Xu</name>
    </author>
    <author>
      <name>Li Song</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.10603v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.10603v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.12273v1</id>
    <updated>2021-12-22T23:36:59Z</updated>
    <published>2021-12-22T23:36:59Z</published>
    <title>Perceptual Evaluation of 360 Audiovisual Quality and Machine Learning
  Predictions</title>
    <summary>  In an earlier study, we gathered perceptual evaluations of the audio, video,
and audiovisual quality for 360 audiovisual content. This paper investigates
perceived audiovisual quality prediction based on objective quality metrics and
subjective scores of 360 video and spatial audio content. Thirteen objective
video quality metrics and three objective audio quality metrics were evaluated
for five stimuli for each coding parameter. Four regression-based machine
learning models were trained and tested here, i.e., multiple linear regression,
decision tree, random forest, and support vector machine. Each model was
constructed using a combination of audio and video quality metrics and two
cross-validation methods (k-Fold and Leave-One-Out) were investigated and
produced 312 predictive models. The results indicate that the model based on
the evaluation of VMAF and AMBIQUAL is better than other combinations of
audio-video quality metric. In this study, support vector machine provides
higher performance using k-Fold (PCC = 0.909, SROCC = 0.914, and RMSE = 0.416).
These results can provide insights for the design of multimedia quality metrics
and the development of predictive models for audiovisual omnidirectional media.
</summary>
    <author>
      <name>Randy Frans Fela</name>
    </author>
    <author>
      <name>Nick Zacharov</name>
    </author>
    <author>
      <name>S√∏ren Forchhammer</name>
    </author>
    <link href="http://arxiv.org/abs/2112.12273v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.12273v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.12907v1</id>
    <updated>2021-12-24T01:56:09Z</updated>
    <published>2021-12-24T01:56:09Z</published>
    <title>3D Point Cloud Reconstruction and SLAM as an Input</title>
    <summary>  To handle the different types of surface reconstruction tasks, we have
replicated as well as modified a few of reconstruction methods and have made
comparisons between the traditional method and data-driven method for
reconstruction the surface of an object with dense point cloud as input. On top
of that, we proposed a system using tightly-coupled SLAM as an input to
generate deskewed point cloud and odometry and a Truncated Signed Distance
Function based Surface Reconstruction Library. To get higher accuracy,
IMU(Inertial Measurement Unit) pre-integration and pose graph optimization are
conduct in the SLAM part. With the help of the Robot Operating System, we could
build a system containing those two parts, which can conduct a real-time
outdoor surface reconstruction.
</summary>
    <author>
      <name>Ziyu Li</name>
    </author>
    <author>
      <name>Fangyang Ye</name>
    </author>
    <author>
      <name>Xinran Guan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.12907v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.12907v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.01977v1</id>
    <updated>2022-03-03T19:37:12Z</updated>
    <published>2022-03-03T19:37:12Z</published>
    <title>Audio-Visual Object Classification for Human-Robot Collaboration</title>
    <summary>  Human-robot collaboration requires the contactless estimation of the physical
properties of containers manipulated by a person, for example while pouring
content in a cup or moving a food box. Acoustic and visual signals can be used
to estimate the physical properties of such objects, which may vary
substantially in shape, material and size, and also be occluded by the hands of
the person. To facilitate comparisons and stimulate progress in solving this
problem, we present the CORSMAL challenge and a dataset to assess the
performance of the algorithms through a set of well-defined performance scores.
The tasks of the challenge are the estimation of the mass, capacity, and
dimensions of the object (container), and the classification of the type and
amount of its content. A novel feature of the challenge is our
real-to-simulation framework for visualising and assessing the impact of
estimation errors in human-to-robot handovers.
</summary>
    <author>
      <name>A. Xompero</name>
    </author>
    <author>
      <name>Y. L. Pang</name>
    </author>
    <author>
      <name>T. Patten</name>
    </author>
    <author>
      <name>A. Prabhakar</name>
    </author>
    <author>
      <name>B. Calli</name>
    </author>
    <author>
      <name>A. Cavallaro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, 1 table; accepted at ICASSP 2022; Challenge
  webpage, see https://corsmal.eecs.qmul.ac.uk/challenge.html</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.01977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.01977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.03091v2</id>
    <updated>2022-04-08T09:11:18Z</updated>
    <published>2022-03-07T01:51:44Z</published>
    <title>Modeling Field-level Factor Interactions for Fashion Recommendation</title>
    <summary>  Personalized fashion recommendation aims to explore patterns from historical
interactions between users and fashion items and thereby predict the future
ones. It is challenging due to the sparsity of the interaction data and the
diversity of user preference in fashion. To tackle the challenge, this paper
investigates multiple factor fields in fashion domain, such as colour, style,
brand, and tries to specify the implicit user-item interaction into field
level. Specifically, an attentional factor field interaction graph (AFFIG)
approach is proposed which models both the user-factor interactions and
cross-field factors interactions for predicting the recommendation probability
at specific field. In addition, an attention mechanism is equipped to aggregate
the cross-field factor interactions for each field. Extensive experiments have
been conducted on three E-Commerce fashion datasets and the results demonstrate
the effectiveness of the proposed method for fashion recommendation. The
influence of various factor fields on recommendation in fashion domain is also
discussed through experiments.
</summary>
    <author>
      <name>Yujuan Ding</name>
    </author>
    <author>
      <name>P. Y. Mok</name>
    </author>
    <author>
      <name>Xun Yang</name>
    </author>
    <author>
      <name>Yanghong Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Key information missing. We will improve the work and publish a
  new-version later</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.03091v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.03091v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.05349v1</id>
    <updated>2022-03-10T12:56:10Z</updated>
    <published>2022-03-10T12:56:10Z</published>
    <title>Two-stream Hierarchical Similarity Reasoning for Image-text Matching</title>
    <summary>  Reasoning-based approaches have demonstrated their powerful ability for the
task of image-text matching. In this work, two issues are addressed for
image-text matching. First, for reasoning processing, conventional approaches
have no ability to find and use multi-level hierarchical similarity
information. To solve this problem, a hierarchical similarity reasoning module
is proposed to automatically extract context information, which is then
co-exploited with local interaction information for efficient reasoning.
Second, previous approaches only consider learning single-stream similarity
alignment (i.e., image-to-text level or text-to-image level), which is
inadequate to fully use similarity information for image-text matching. To
address this issue, a two-stream architecture is developed to decompose
image-text matching into image-to-text level and text-to-image level similarity
computation. These two issues are investigated by a unifying framework that is
trained in an end-to-end manner, namely two-stream hierarchical similarity
reasoning network. The extensive experiments performed on the two benchmark
datasets of MSCOCO and Flickr30K show the superiority of the proposed approach
as compared to existing state-of-the-art methods.
</summary>
    <author>
      <name>Ran Chen</name>
    </author>
    <author>
      <name>Hanli Wang</name>
    </author>
    <author>
      <name>Lei Wang</name>
    </author>
    <author>
      <name>Sam Kwong</name>
    </author>
    <link href="http://arxiv.org/abs/2203.05349v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.05349v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.12692v2</id>
    <updated>2022-03-31T05:20:40Z</updated>
    <published>2022-03-23T19:28:20Z</published>
    <title>Affective Feedback Synthesis Towards Multimodal Text and Image Data</title>
    <summary>  In this paper, we have defined a novel task of affective feedback synthesis
that deals with generating feedback for input text &amp; corresponding image in a
similar way as humans respond towards the multimodal data. A feedback synthesis
system has been proposed and trained using ground-truth human comments along
with image-text input. We have also constructed a large-scale dataset
consisting of image, text, Twitter user comments, and the number of likes for
the comments by crawling the news articles through Twitter feeds. The proposed
system extracts textual features using a transformer-based textual encoder
while the visual features have been extracted using a Faster region-based
convolutional neural networks model. The textual and visual features have been
concatenated to construct the multimodal features using which the decoder
synthesizes the feedback. We have compared the results of the proposed system
with the baseline models using quantitative and qualitative measures. The
generated feedbacks have been analyzed using automatic and human evaluation.
They have been found to be semantically similar to the ground-truth comments
and relevant to the given text-image input.
</summary>
    <author>
      <name>Puneet Kumar</name>
    </author>
    <author>
      <name>Gaurav Bhat</name>
    </author>
    <author>
      <name>Omkar Ingle</name>
    </author>
    <author>
      <name>Daksh Goyal</name>
    </author>
    <author>
      <name>Balasubramanian Raman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ACM Transactions on Multimedia Computing,
  Communications, and Applications</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.12692v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.12692v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.12843v1</id>
    <updated>2022-03-24T04:44:51Z</updated>
    <published>2022-03-24T04:44:51Z</published>
    <title>Steganalysis of Image with Adaptively Parametric Activation</title>
    <summary>  Steganalysis as a method to detect whether image contains se-cret message, is
a crucial study avoiding the imperils from abus-ing steganography. The point of
steganalysis is to detect the weak embedding signals which is hardly learned by
convolution-al layer and easily suppressed. In this paper, to enhance
embed-ding signals, we study the insufficiencies of activation function,
filters and loss function from the aspects of reduce embedding signal loss and
enhance embedding signal capture ability. Adap-tive Parametric Activation
Module is designed to reserve nega-tive embedding signal. For embedding signal
capture ability enhancement, we add constraints on the high-pass filters to
im-prove residual diversity which enables the filters extracts rich embedding
signals. Besides, a loss function based on contrastive learning is applied to
overcome the limitations of cross-entropy loss by maximum inter-class distance.
It helps the network make a distinction between embedding signals and semantic
edges. We use images from BOSSbase 1.01 and make stegos by WOW and S-UNIWARD
for experiments. Compared to state-of-the-art methods, our method has a
competitive performance.
</summary>
    <author>
      <name>Hai Su</name>
    </author>
    <author>
      <name>Meiyin Han</name>
    </author>
    <author>
      <name>Junle Liang</name>
    </author>
    <author>
      <name>Songsen Yu</name>
    </author>
    <link href="http://arxiv.org/abs/2203.12843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.12843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.13031v2</id>
    <updated>2022-03-30T09:31:29Z</updated>
    <published>2022-03-24T12:18:06Z</published>
    <title>Continuous Emotion Recognition using Visual-audio-linguistic
  information: A Technical Report for ABAW3</title>
    <summary>  We propose a cross-modal co-attention model for continuous emotion
recognition using visual-audio-linguistic information. The model consists of
four blocks. The visual, audio, and linguistic blocks are used to learn the
spatial-temporal features of the multi-modal input. A co-attention block is
designed to fuse the learned features with the multi-head co-attention
mechanism. The visual encoding from the visual block is concatenated with the
attention feature to emphasize the visual information. To make full use of the
data and alleviate over-fitting, cross-validation is carried out on the
training and validation set. The concordance correlation coefficient (CCC)
centering is used to merge the results from each fold. The achieved CCC on the
test set is $0.520$ for valence and $0.602$ for arousal, which significantly
outperforms the baseline method with the corresponding CCC of 0.180 and 0.170
for valence and arousal, respectively. The code is available at
https://github.com/sucv/ABAW3.
</summary>
    <author>
      <name>Su Zhang</name>
    </author>
    <author>
      <name>Ruyi An</name>
    </author>
    <author>
      <name>Yi Ding</name>
    </author>
    <author>
      <name>Cuntai Guan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure. arXiv admin note: substantial text overlap with
  arXiv:2107.01175</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.13031v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.13031v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.13535v1</id>
    <updated>2022-03-25T09:42:11Z</updated>
    <published>2022-03-25T09:42:11Z</published>
    <title>SeCo: Separating Unknown Musical Visual Sounds with Consistency Guidance</title>
    <summary>  Recent years have witnessed the success of deep learning on the visual sound
separation task. However, existing works follow similar settings where the
training and testing datasets share the same musical instrument categories,
which to some extent limits the versatility of this task. In this work, we
focus on a more general and challenging scenario, namely the separation of
unknown musical instruments, where the categories in training and testing
phases have no overlap with each other. To tackle this new setting, we propose
the Separation-with-Consistency (SeCo) framework, which can accomplish the
separation on unknown categories by exploiting the consistency constraints.
Furthermore, to capture richer characteristics of the novel melodies, we devise
an online matching strategy, which can bring stable enhancements with no cost
of extra parameters. Experiments demonstrate that our SeCo framework exhibits
strong adaptation ability on the novel musical categories and outperforms the
baseline methods by a significant margin.
</summary>
    <author>
      <name>Xinchi Zhou</name>
    </author>
    <author>
      <name>Dongzhan Zhou</name>
    </author>
    <author>
      <name>Wanli Ouyang</name>
    </author>
    <author>
      <name>Hang Zhou</name>
    </author>
    <author>
      <name>Ziwei Liu</name>
    </author>
    <author>
      <name>Di Hu</name>
    </author>
    <link href="http://arxiv.org/abs/2203.13535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.13535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.13932v1</id>
    <updated>2022-03-25T22:40:53Z</updated>
    <published>2022-03-25T22:40:53Z</published>
    <title>A Cross-Domain Approach for Continuous Impression Recognition from
  Dyadic Audio-Visual-Physio Signals</title>
    <summary>  The impression we make on others depends not only on what we say, but also,
to a large extent, on how we say it. As a sub-branch of affective computing and
social signal processing, impression recognition has proven critical in both
human-human conversations and spoken dialogue systems. However, most research
has studied impressions only from the signals expressed by the emitter,
ignoring the response from the receiver. In this paper, we perform impression
recognition using a proposed cross-domain architecture on the dyadic IMPRESSION
dataset. This improved architecture makes use of cross-domain attention and
regularization. The cross-domain attention consists of intra- and
inter-attention mechanisms, which capture intra- and inter-domain relatedness,
respectively. The cross-domain regularization includes knowledge distillation
and similarity enhancement losses, which strengthen the feature connections
between the emitter and receiver. The experimental evaluation verified the
effectiveness of our approach. Our approach achieved a concordance correlation
coefficient of 0.770 in competence dimension and 0.748 in warmth dimension.
</summary>
    <author>
      <name>Yuanchao Li</name>
    </author>
    <author>
      <name>Catherine Lai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures, submitted to INTERSPEECH 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.13932v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.13932v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.15961v1</id>
    <updated>2022-03-30T00:37:19Z</updated>
    <published>2022-03-30T00:37:19Z</published>
    <title>Using Active Speaker Faces for Diarization in TV shows</title>
    <summary>  Speaker diarization is one of the critical components of computational media
intelligence as it enables a character-level analysis of story portrayals and
media content understanding. Automated audio-based speaker diarization of
entertainment media poses challenges due to the diverse acoustic conditions
present in media content, be it background music, overlapping speakers, or
sound effects. At the same time, speaking faces in the visual modality provide
complementary information and not prone to the errors seen in the audio
modality. In this paper, we address the problem of speaker diarization in TV
shows using the active speaker faces. We perform face clustering on the active
speaker faces and show superior speaker diarization performance compared to the
state-of-the-art audio-based diarization methods. We additionally report a
systematic analysis of the impact of active speaker face detection quality on
the diarization performance. We also observe that a moderately well-performing
active speaker system could outperform the audio-based diarization systems.
</summary>
    <author>
      <name>Rahul Sharma</name>
    </author>
    <author>
      <name>Shrikanth Narayanan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Interspeech 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.15961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.15961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.04272v1</id>
    <updated>2022-04-08T19:54:43Z</updated>
    <published>2022-04-08T19:54:43Z</published>
    <title>Matrix Syncer -- A Multi-chain Data Aggregator For Supporting
  Blockchain-based Metaverses</title>
    <summary>  Due to the rising complexity of the metaverse's business logic and the
low-latency nature of the metaverse, developers typically encounter the
challenge of effectively reading, writing, and retrieving historical on-chain
data in order to facilitate their functional implementations at scale. While it
is true that accessing blockchain states is simple, more advanced real-world
operations such as search, aggregation, and conditional filtering are not
available when interacting directly with blockchain networks, particularly when
dealing with requirements for on-chain event reflection. We offer Matrix
Syncer, the ultimate middleware that bridges the data access gap between
blockchains and end-user applications. Matrix Syncer is designed to facilitate
the consolidation of on-chain information into a distributed data warehouse
while also enabling customized on-chain state transformation for a scalable
storage, access, and retrieval. It offers a unified layer for both on- and
off-chain state, as well as a fast and flexible atomic query. Matrix Syncer is
easily incorporated into any infrastructure to aggregate data from various
blockchains concurrently, such as Ethereum and Flow. The system has been
deployed to support several metaverse projects with a total value of more than
$15 million USD.
</summary>
    <author>
      <name>Xinyao Sun</name>
    </author>
    <author>
      <name>Yi Lu</name>
    </author>
    <author>
      <name>Jinghan Sun</name>
    </author>
    <author>
      <name>Bohao Tang</name>
    </author>
    <author>
      <name>Kyle D. Rehak</name>
    </author>
    <author>
      <name>Shuyi Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2204.04272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.04272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.06134v1</id>
    <updated>2022-04-13T01:51:04Z</updated>
    <published>2022-04-13T01:51:04Z</published>
    <title>CWcollab: A Context-Aware Web-Based Collaborative Multimedia System</title>
    <summary>  Remote collaboration tools for conferencing and presentation are gaining
significant popularity during the COVID-19 pandemic period. Most prior work has
issues, such as a) limited support for media types, b) lack of interactivity,
for example, an efficient replay mechanism, c) large bandwidth consumption for
screen sharing tools. In this paper, we propose a general-purpose multimedia
collaboration platform-CWcollab. It supports collaboration on general
multimedia by using simple messages to represent media controls with an
object-prioritized synchronization approach. Thus, CWcollab can not only
support fine-grained accurate collaboration, but also rich functionalities such
as replay of these collaboration events. The evaluation shows hundreds of
kilobytes can be enough to store the events in a collaboration session for
accurate replays, compared with hundreds of megabytes of Google Hangouts.
</summary>
    <author>
      <name>Chunxu Tang</name>
    </author>
    <author>
      <name>Beinan Wang</name>
    </author>
    <author>
      <name>C. Y. Roger Chen</name>
    </author>
    <author>
      <name>Huijun Wu</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICC42927.2021.9500377</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICC42927.2021.9500377" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In ICC 2021-IEEE International Conference on Communications (pp.
  1-6). IEEE (2021, June)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2204.06134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.06134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.07062v1</id>
    <updated>2022-03-22T10:42:19Z</updated>
    <published>2022-03-22T10:42:19Z</published>
    <title>Network state Estimation using Raw Video Analysis: vQoS-GAN based
  non-intrusive Deep Learning Approach</title>
    <summary>  Content based providers transmits real time complex signal such as video data
from one region to another. During this transmission process, the signals
usually end up distorted or degraded where the actual information present in
the video is lost. This normally happens in the streaming video services
applications. Hence there is a need to know the level of degradation that
happened in the receiver side. This video degradation can be estimated by
network state parameters like data rate and packet loss values. Our proposed
solution vQoS GAN (video Quality of Service Generative Adversarial Network) can
estimate the network state parameters from the degraded received video data
using a deep learning approach of semi supervised generative adversarial
network algorithm. A robust and unique design of deep learning network model
has been trained with the video data along with data rate and packet loss class
labels and achieves over 95 percent of training accuracy. The proposed semi
supervised generative adversarial network can additionally reconstruct the
degraded video data to its original form for a better end user experience.
</summary>
    <author>
      <name>Renith G</name>
    </author>
    <author>
      <name>Harikrishna Warrier</name>
    </author>
    <author>
      <name>Yogesh Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.07062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.07062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.09423v1</id>
    <updated>2022-04-20T12:31:56Z</updated>
    <published>2022-04-20T12:31:56Z</published>
    <title>Cost Minimization of Cloud Services for On-Demand Video Streaming</title>
    <summary>  Cloud Technology is adopted to process video streams because of the great
features provided to video stream providers such as the high flexibility of
using virtual machines and storage servers at low rates. Video stream providers
prepare several formats of the same video to satisfy all users' devices'
specifications. Video streams in the cloud are either transcoded or stored.
However, storing all formats of videos is still costly. In this research, we
develop an approach that optimizes cloud storage. Particularly, we propose a
method that decides which video in which cloud storage should be stored to
minimize the overall cost of cloud services. The results of the proposed
approach are promising, it shows effectiveness when the number of frequently
accessed video grow in a repository, and when the views of videos increases.
The proposed method decreases the cost of using cloud services by up to 22%.
</summary>
    <author>
      <name>Mahmoud Darwich</name>
    </author>
    <author>
      <name>Yasser Ismail</name>
    </author>
    <author>
      <name>Talal Darwich</name>
    </author>
    <author>
      <name>Magdy Bayoumi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s42979-022-01140-x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s42979-022-01140-x" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SN COMPUT. SCI. 3, 226 (2022)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2204.09423v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.09423v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.12366v1</id>
    <updated>2022-04-26T15:07:44Z</updated>
    <published>2022-04-26T15:07:44Z</published>
    <title>Robust Audio-Visual Instance Discrimination via Active Contrastive Set
  Mining</title>
    <summary>  The recent success of audio-visual representation learning can be largely
attributed to their pervasive property of audio-visual synchronization, which
can be used as self-annotated supervision. As a state-of-the-art solution,
Audio-Visual Instance Discrimination (AVID) extends instance discrimination to
the audio-visual realm. Existing AVID methods construct the contrastive set by
random sampling based on the assumption that the audio and visual clips from
all other videos are not semantically related. We argue that this assumption is
rough, since the resulting contrastive sets have a large number of faulty
negatives. In this paper, we overcome this limitation by proposing a novel
Active Contrastive Set Mining (ACSM) that aims to mine the contrastive sets
with informative and diverse negatives for robust AVID. Moreover, we also
integrate a semantically-aware hard-sample mining strategy into our ACSM. The
proposed ACSM is implemented into two most recent state-of-the-art AVID methods
and significantly improves their performance. Extensive experiments conducted
on both action and sound recognition on multiple datasets show the remarkably
improved performance of our method.
</summary>
    <author>
      <name>Hanyu Xuan</name>
    </author>
    <author>
      <name>Yihong Xu</name>
    </author>
    <author>
      <name>Shuo Chen</name>
    </author>
    <author>
      <name>Zhiliang Wu</name>
    </author>
    <author>
      <name>Jian Yang</name>
    </author>
    <author>
      <name>Yan Yan</name>
    </author>
    <author>
      <name>Xavier Alameda-Pineda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures, accepted at IJCAI 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.12366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.12366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.12587v1</id>
    <updated>2022-03-25T17:53:39Z</updated>
    <published>2022-03-25T17:53:39Z</published>
    <title>hate-alert@DravidianLangTech-ACL2022: Ensembling Multi-Modalities for
  Tamil TrollMeme Classification</title>
    <summary>  Social media platforms often act as breeding grounds for various forms of
trolling or malicious content targeting users or communities. One way of
trolling users is by creating memes, which in most cases unites an image with a
short piece of text embedded on top of it. The situation is more complex for
multilingual(e.g., Tamil) memes due to the lack of benchmark datasets and
models. We explore several models to detect Troll memes in Tamil based on the
shared task, "Troll Meme Classification in DravidianLangTech2022" at ACL-2022.
We observe while the text-based model MURIL performs better for Non-troll meme
classification, the image-based model VGG16 performs better for Troll-meme
classification. Further fusing these two modalities help us achieve stable
outcomes in both classes. Our fusion model achieved a 0.561 weighted average F1
score and ranked second in this task.
</summary>
    <author>
      <name>Mithun Das</name>
    </author>
    <author>
      <name>Somnath Banerjee</name>
    </author>
    <author>
      <name>Animesh Mukherjee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at ACL 2022 DravidianLangTech Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.12587v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.12587v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.13988v3</id>
    <updated>2023-06-14T10:42:24Z</updated>
    <published>2022-04-20T06:15:50Z</published>
    <title>A Taxonomy of Prompt Modifiers for Text-To-Image Generation</title>
    <summary>  Text-to-image generation has seen an explosion of interest since 2021. Today,
beautiful and intriguing digital images and artworks can be synthesized from
textual inputs ("prompts") with deep generative models. Online communities
around text-to-image generation and AI generated art have quickly emerged. This
paper identifies six types of prompt modifiers used by practitioners in the
online community based on a 3-month ethnographic study. The novel taxonomy of
prompt modifiers provides researchers a conceptual starting point for
investigating the practice of text-to-image generation, but may also help
practitioners of AI generated art improve their images. We further outline how
prompt modifiers are applied in the practice of "prompt engineering." We
discuss research opportunities of this novel creative practice in the field of
Human-Computer Interaction (HCI). The paper concludes with a discussion of
broader implications of prompt engineering from the perspective of Human-AI
Interaction (HAI) in future applications beyond the use case of text-to-image
generation and AI generated art.
</summary>
    <author>
      <name>Jonas Oppenlaender</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/0144929X.2023.2286532</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/0144929X.2023.2286532" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.13988v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.13988v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5; H.m; J.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.1925v1</id>
    <updated>2007-05-14T12:23:43Z</updated>
    <published>2007-05-14T12:23:43Z</published>
    <title>Double Sided Watermark Embedding and Detection with Perceptual Analysis</title>
    <summary>  In our previous work, we introduced a double-sided technique that utilizes
but not reject the host interference. Due to its nice property of utilizing but
not rejecting the host interference, it has a big advantage over the host
interference schemes in that the perceptual analysis can be easily implemented
for our scheme to achieve the locally bounded maximum embedding strength. Thus,
in this work, we detail how to implement the perceptual analysis in our
double-sided schemes since the perceptual analysis is very important for
improving the fidelity of watermarked contents. Through the extensive
performance comparisons, we can further validate the performance advantage of
our double-sided schemes.
</summary>
    <author>
      <name>Jidong Zhong</name>
    </author>
    <author>
      <name>Shangteng Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is a supplement to a paper to be published in IEEE
  Transactions on Information Forensics and Security</arxiv:comment>
    <link href="http://arxiv.org/abs/0705.1925v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.1925v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.0405v1</id>
    <updated>2008-03-04T10:27:59Z</updated>
    <published>2008-03-04T10:27:59Z</published>
    <title>Multi-dimensional sparse time series: feature extraction</title>
    <summary>  We show an analysis of multi-dimensional time series via entropy and
statistical linguistic techniques. We define three markers encoding the
behavior of the series, after it has been translated into a multi-dimensional
symbolic sequence. The leading component and the trend of the series with
respect to a mobile window analysis result from the entropy analysis and label
the dynamical evolution of the series. The diversification formalizes the
differentiation in the use of recurrent patterns, from a Zipf law point of
view. These markers are the starting point of further analysis such as
classification or clustering of large database of multi-dimensional time
series, prediction of future behavior and attribution of new data. We also
present an application to economic data. We deal with measurements of money
investments of some business companies in advertising market for different
media sources.
</summary>
    <author>
      <name>Marco Franciosi</name>
    </author>
    <author>
      <name>Giulia Menconi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keywords: multimedia mining, trend, entropy, Zipf law</arxiv:comment>
    <link href="http://arxiv.org/abs/0803.0405v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.0405v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.0699v1</id>
    <updated>2011-05-03T21:57:18Z</updated>
    <published>2011-05-03T21:57:18Z</published>
    <title>Robust Sign Language Recognition System Using ToF Depth Cameras</title>
    <summary>  Sign language recognition is a difficult task, yet required for many
applications in real-time speed. Using RGB cameras for recognition of sign
languages is not very successful in practical situations and accurate 3D
imaging requires expensive and complex instruments. With introduction of
Time-of-Flight (ToF) depth cameras in recent years, it has become easier to
scan the environment for accurate, yet fast depth images of the objects without
the need of any extra calibrating object. In this paper, a robust system for
sign language recognition using ToF depth cameras is presented for converting
the recorded signs to a standard and portable XML sign language named SiGML for
easy transferring and converting to real-time 3D virtual characters animations.
Feature extraction using moments and classification using nearest neighbor
classifier are used to track hand gestures and significant result of 100% is
achieved for the proposed approach.
</summary>
    <author>
      <name>Morteza Zahedi</name>
    </author>
    <author>
      <name>Ali Reza Manashty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 Pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">World of Computer Science and Information Technology Journal
  (WCSIT), Vol. 1, No. 3, 50-55, 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1105.0699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.0699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.0826v1</id>
    <updated>2011-05-04T13:46:31Z</updated>
    <published>2011-05-04T13:46:31Z</published>
    <title>Streaming Multimedia Information Using the Features of the DVB-S Card</title>
    <summary>  This paper presents a study of audio-video streaming using the additional
possibilities of a DVB-S card. The board used for experiments (Technisat
SkyStar 2) is one of the most frequently used cards for this purpose. Using the
main blocks of the board's software support it is possible the implement a
really useful and full functional system for audio-video streaming. The
streaming is possible to be implemented either for decoded MPEG stream or for
transport stream. In this last case it is possible to view not only a program,
but any program from the same multiplex. This allows us to implement
</summary>
    <author>
      <name>Radu Arsinte</name>
    </author>
    <author>
      <name>Eugen Lupu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scientific Bulletin of the "Politehnica" University Timi\c{s}oara,
  Transaction on Electronics and Telecomunications, Tom 51(65), Fascicola 1-2,
  pag. 181-184, 2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1105.0826v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.0826v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.1561v2</id>
    <updated>2011-08-03T18:57:17Z</updated>
    <published>2011-05-09T00:07:56Z</published>
    <title>Efficient Image Transmission Through Analog Error Correction</title>
    <summary>  This paper presents a new paradigm for image transmission through analog
error correction codes. Conventional schemes rely on digitizing images through
quantization (which inevitably causes significant bandwidth expansion) and
transmitting binary bit-streams through digital error correction codes (which
do not automatically differentiate the different levels of significance among
the bits). To strike a better overall performance in terms of transmission
efficiency and quality, we propose to use a single analog error correction code
in lieu of digital quantization, digital code and digital modulation. The key
is to get analog coding right. We show that this can be achieved by cleverly
exploiting an elegant "butterfly" property of chaotic systems. Specifically, we
demonstrate a tail-biting triple-branch baker's map code and its
maximum-likelihood decoding algorithm. Simulations show that the proposed
analog code can actually outperform digital turbo code, one of the best codes
known to date. The results and findings discussed in this paper speak volume
for the promising potential of analog codes, in spite of their rather short
history.
</summary>
    <author>
      <name>Yang Liu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tiffany</arxiv:affiliation>
    </author>
    <author>
      <name> Jing</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Tiffany</arxiv:affiliation>
    </author>
    <author>
      <name> Li</name>
    </author>
    <author>
      <name>Kai Xie</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.1561v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.1561v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.1948v1</id>
    <updated>2011-05-10T13:55:31Z</updated>
    <published>2011-05-10T13:55:31Z</published>
    <title>Analytical Classification of Multimedia Index Structures by Using a
  Partitioning Method-Based Framework</title>
    <summary>  Due to the advances in hardware technology and increase in production of
multimedia data in many applications, during the last decades, multimedia
databases have become increasingly important. Contentbased multimedia retrieval
is one of an important research area in the field of multimedia databases. Lots
of research on this field has led to proposition of different kinds of index
structures to support fast and efficient similarity search to retrieve
multimedia data from these databases. Due to variety and plenty of proposed
index structures, we suggest a systematic framework based on partitioning
method used in these structures to classify multimedia index structures, and
then we evaluated these structures based on important functional measures. We
hope this proposed framework will lead to empirical and technical comparison of
multimedia index structures and development of more efficient structures at
future.
</summary>
    <author>
      <name>Mohammadreza keyvanpour</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer Engineering, Alzahra University, Tehran, Iran</arxiv:affiliation>
    </author>
    <author>
      <name>Najva Izadpanah</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Department of Computer Engineering, Islamic Azad University, Qazvin Branch, Qazvin, Iran</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The International Journal of Multimedia &amp; Its Applications (IJMA)
  Vol.3, No.1, February 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1105.1948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.1948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.2344v1</id>
    <updated>2011-05-12T00:43:46Z</updated>
    <published>2011-05-12T00:43:46Z</published>
    <title>Learning content similarity for music recommendation</title>
    <summary>  Many tasks in music information retrieval, such as recommendation, and
playlist generation for online radio, fall naturally into the query-by-example
setting, wherein a user queries the system by providing a song, and the system
responds with a list of relevant or similar song recommendations. Such
applications ultimately depend on the notion of similarity between items to
produce high-quality results. Current state-of-the-art systems employ
collaborative filter methods to represent musical items, effectively comparing
items in terms of their constituent users. While collaborative filter
techniques perform well when historical data is available for each item, their
reliance on historical data impedes performance on novel or unpopular items. To
combat this problem, practitioners rely on content-based similarity, which
naturally extends to novel items, but is typically out-performed by
collaborative filter methods.
  In this article, we propose a method for optimizing contentbased similarity
by learning from a sample of collaborative filter data. The optimized
content-based similarity metric can then be applied to answer queries on novel
and unpopular items, while still maintaining high recommendation accuracy. The
proposed system yields accurate and efficient representations of audio content,
and experimental results show significant improvements in accuracy over
competing content-based recommendation techniques.
</summary>
    <author>
      <name>Brian McFee</name>
    </author>
    <author>
      <name>Luke Barrington</name>
    </author>
    <author>
      <name>Gert Lanckriet</name>
    </author>
    <link href="http://arxiv.org/abs/1105.2344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.2344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.5675v1</id>
    <updated>2011-05-28T00:44:54Z</updated>
    <published>2011-05-28T00:44:54Z</published>
    <title>Scale-Invariant Local Descriptor for Event Recognition in 1D Sensor
  Signals</title>
    <summary>  In this paper, we introduce a shape-based, time-scale invariant feature
descriptor for 1-D sensor signals. The time-scale invariance of the feature
allows us to use feature from one training event to describe events of the same
semantic class which may take place over varying time scales such as walking
slow and walking fast. Therefore it requires less training set. The descriptor
takes advantage of the invariant location detection in the scale space theory
and employs a high level shape encoding scheme to capture invariant local
features of events. Based on this descriptor, a scale-invariant classifier with
"R" metric (SIC-R) is designed to recognize multi-scale events of human
activities. The R metric combines the number of matches of keypoint in scale
space with the Dynamic Time Warping score. SICR is tested on various types of
1-D sensors data from passive infrared, accelerometer and seismic sensors with
more than 90% classification accuracy.
</summary>
    <author>
      <name>Jierui Xie</name>
    </author>
    <author>
      <name>Mandis S. Beigi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Multimedia &amp;
  Expo(ICME),Page(s):1226 - 1229, 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1105.5675v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.5675v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.2027v1</id>
    <updated>2011-12-09T07:05:49Z</updated>
    <published>2011-12-09T07:05:49Z</published>
    <title>Automatic Classification of X-rated Videos using Obscene Sound Analysis
  based on a Repeated Curve-like Spectrum Feature</title>
    <summary>  This paper addresses the automatic classification of X-rated videos by
analyzing its obscene sounds. In this paper, obscene sounds refer to audio
signals generated from sexual moans and screams during sexual scenes. By
analyzing various sound samples, we determined the distinguishable
characteristics of obscene sounds and propose a repeated curve-like spectrum
feature that represents the characteristics of such sounds. We constructed
6,269 audio clips to evaluate the proposed feature, and separately constructed
1,200 X-rated and general videos for classification. The proposed feature has
an F1-score, precision, and recall rate of 96.6%, 98.2%, and 95.2%,
respectively, for the original dataset, and 92.6%, 97.6%, and 88.0% for a noisy
dataset of 5dB SNR. And, in classifying videos, the feature has more than a 90%
F1-score, 97% precision, and an 84% recall rate. From the measured performance,
X-rated videos can be classified with only the audio features and the repeated
curve-like spectrum feature is suitable to detect obscene sounds.
</summary>
    <author>
      <name>JaeDeok Lim</name>
    </author>
    <author>
      <name>ByeongCheol Choi</name>
    </author>
    <author>
      <name>SeungWan Han</name>
    </author>
    <author>
      <name>ChoelHoon Lee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 5 figures, 11 tables, IJMA(The International Journal of
  Multimedia &amp; Its Applications)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The International Journal of Multimedia &amp; Its Applications (IJMA)
  Vol.3, No.4, November 2011, pp.1-17</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1112.2027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.2027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.2261v1</id>
    <updated>2011-12-10T08:34:51Z</updated>
    <published>2011-12-10T08:34:51Z</published>
    <title>Lossless Digital Image Compression Method for Bitmap Images</title>
    <summary>  In this research paper, the authors propose a new approach to digital image
compression using crack coding This method starts with the original image and
develop crack codes in a recursive manner, marking the pixels visited earlier
and expanding the entropy in four directions. The proposed method is
experimented with sample bitmap images and results are tabulated. The method is
implemented in uni-processor machine using C language source code.
</summary>
    <author>
      <name>Dr. T. Meyyappan</name>
    </author>
    <author>
      <name>SM. Thamarai</name>
    </author>
    <author>
      <name>N. M. Jeya Nachiaban</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijma.2011.3407</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijma.2011.3407" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 7 figures, presented in First International Conference on
  Digital Image Processing and Pattern Recognition (DPPR-2011)In conjunction
  with (CCSEIT-2011), Manonmaniam Sundharanar University, September 23~25,
  2011, Tirunelveli, Tamil Nadu, India</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The International Journal of Multimedia &amp; Its Applications (IJMA)
  Vol.3, No.4, November 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1112.2261v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.2261v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="65D18" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1112.2809v1</id>
    <updated>2011-12-13T06:56:25Z</updated>
    <published>2011-12-13T06:56:25Z</published>
    <title>Steganography Algorithm to Hide Secret Message inside an Image</title>
    <summary>  In this paper, the authors propose a new algorithm to hide data inside image
using steganography technique. The proposed algorithm uses binary codes and
pixels inside an image. The zipped file is used before it is converted to
binary codes to maximize the storage of data inside the image. By applying the
proposed algorithm, a system called Steganography Imaging System (SIS) is
developed. The system is then tested to see the viability of the proposed
algorithm. Various sizes of data are stored inside the images and the PSNR
(Peak signal-to-noise ratio) is also captured for each of the images tested.
Based on the PSNR value of each images, the stego image has a higher PSNR
value. Hence this new steganography algorithm is very efficient to hide the
data inside the image.
</summary>
    <author>
      <name>Rosziati Ibrahim</name>
    </author>
    <author>
      <name>Teoh Suk Kuan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computer Technology and Application 2 (2011) 102-108</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1112.2809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1112.2809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.8165v1</id>
    <updated>2012-10-30T21:05:16Z</updated>
    <published>2012-10-30T21:05:16Z</published>
    <title>Non-uniform Quantization of Detail Components in Wavelet Transformed
  Image for Lossy JPEG2000 Compression</title>
    <summary>  The paper introduces the idea of non-uniform quantization in the detail
components of wavelet transformed image. It argues that most of the
coefficients of horizontal, vertical and diagonal components lie near to zeros
and the coefficients representing large differences are few at the extreme ends
of histogram. Therefore, this paper advocates need for variable step size
quantization scheme which preserves the edge information at the edge of
histogram and removes redundancy with the minimal number of quantized values.
To support the idea, preliminary results are provided using a non-uniform
quantization algorithm. We believe that successful implementation of
non-uniform quantization in detail components in JPEG-2000 still image standard
will improve image quality and compression efficiency with lesser number of
quantized values.
</summary>
    <author>
      <name>Madhur Srivastava</name>
    </author>
    <author>
      <name>Prasanta K. Panigrahi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5220/0004333706040607</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5220/0004333706040607" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures, conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Pattern Recognition Applications and
  Methods (ICPRAM 2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.8165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.8165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.0221v1</id>
    <updated>2013-06-02T15:53:26Z</updated>
    <published>2013-06-02T15:53:26Z</published>
    <title>Survey on QoE\QoS Correlation Models For Multimedia Services</title>
    <summary>  This paper presents a brief review of some existing correlation models which
attempt to map Quality of Service (QoS) to Quality of Experience (QoE) for
multimedia services. The term QoS refers to deterministic network behaviour, so
that data can be transported with a minimum of packet loss, delay and maximum
bandwidth. QoE is a subjective measure that involves human dimensions; it ties
together user perception, expectations, and experience of the application and
network performance. The Holy Grail of subjective measurement is to predict it
from the objective measurements; in other words predict QoE from a given set of
QoS parameters or vice versa. Whilst there are many quality models for
multimedia, most of them are only partial solutions to predicting QoE from a
given QoS. This contribution analyses a number of previous attempts and
optimisation techniquesthat can reliably compute the weighting coefficients for
the QoS/QoE mapping.
</summary>
    <author>
      <name>Mohammed Alreshoodi</name>
    </author>
    <author>
      <name>John Woods</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, International Journal of Distributed and Parallel Systems
  (IJDPS)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Distributed and Parallel Systems (IJDPS)
  Vol.4, No.3, May 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1306.0221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.0221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.6920v1</id>
    <updated>2013-06-03T10:23:38Z</updated>
    <published>2013-06-03T10:23:38Z</published>
    <title>Enhanced Tiny Encryption Algorithm with Embedding (ETEA)</title>
    <summary>  As computer systems become more pervasive and complex, security is
increasingly important. Secure Transmission refers to the transfer of data such
as confidential or proprietary information over a secure channel. Many secure
transmission methods require a type of encryption. Secure transmissions are put
in place to prevent attacks such as ARP spoofing and general data loss. Hence,
in order to provide a better security mechanism, in this paper we propose
Enhanced Tiny Encryption Algorithm with Embedding (ETEA), a data hiding
technique called steganography along with the technique of encryption
(Cryptography). The advantage of ETEA is that it incorporates cryptography and
steganography. The advantage proposed algorithm is that it hides the messages.
</summary>
    <author>
      <name>Deepali Virmani</name>
    </author>
    <author>
      <name>Nidhi Beniwal</name>
    </author>
    <author>
      <name>Gargi Mandal</name>
    </author>
    <author>
      <name>Saloni Talwar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures, International Journal of Computers &amp; Technology,
  Vol 7, No 1, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.6920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.6920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00083v1</id>
    <updated>2015-02-28T06:59:42Z</updated>
    <published>2015-02-28T06:59:42Z</published>
    <title>A Computation Control Motion Estimation Method for Complexity-Scalable
  Video Coding</title>
    <summary>  In this paper, a new Computation-Control Motion Estimation (CCME) method is
proposed which can perform Motion Estimation (ME) adaptively under different
computation or power budgets while keeping high coding performance. We first
propose a new class-based method to measure the Macroblock (MB) importance
where MBs are classified into different classes and their importance is
measured by combining their class information as well as their initial matching
cost information. Based on the new MB importance measure, a complete CCME
framework is then proposed to allocate computation for ME. The proposed method
performs ME in a one-pass flow. Experimental results demonstrate that the
proposed method can allocate computation more accurately than previous methods
and thus has better performance under the same computation budget.
</summary>
    <author>
      <name>Weiyao Lin</name>
    </author>
    <author>
      <name>Krit Panusopone</name>
    </author>
    <author>
      <name>David M. Baylon</name>
    </author>
    <author>
      <name>Ming-Ting Sun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TCSVT.2010.2077773</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TCSVT.2010.2077773" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This manuscript is the accepted version for TCSVT (IEEE Transactions
  on Circuits and Systems for Video Technology)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Trans. Circuits and Systems for Video Technology, vol. 20,
  no. 11, pp. 1533-1543, 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.00083v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00083v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00087v1</id>
    <updated>2015-02-28T07:14:01Z</updated>
    <published>2015-02-28T07:14:01Z</published>
    <title>Macroblock Classification Method for Video Applications Involving
  Motions</title>
    <summary>  In this paper, a macroblock classification method is proposed for various
video processing applications involving motions. Based on the analysis of the
Motion Vector field in the compressed video, we propose to classify Macroblocks
of each video frame into different classes and use this class information to
describe the frame content. We demonstrate that this low-computation-complexity
method can efficiently catch the characteristics of the frame. Based on the
proposed macroblock classification, we further propose algorithms for different
video processing applications, including shot change detection, motion
discontinuity detection, and outlier rejection for global motion estimation.
Experimental results demonstrate that the methods based on the proposed
approach can work effectively on these applications.
</summary>
    <author>
      <name>Weiyao Lin</name>
    </author>
    <author>
      <name>Ming-Ting Sun</name>
    </author>
    <author>
      <name>Hongxiang Li</name>
    </author>
    <author>
      <name>Zhenzhong Chen</name>
    </author>
    <author>
      <name>Wei Li</name>
    </author>
    <author>
      <name>Bing Zhou</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TBC.2011.2170611</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TBC.2011.2170611" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This manuscript is the accepted version for TB (IEEE Transactions on
  Broadcasting)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Trans. Broadcasting, vol. 58, no. 1, pp. 34-46, 2012</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.00087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00118v1</id>
    <updated>2015-02-28T11:42:42Z</updated>
    <published>2015-02-28T11:42:42Z</published>
    <title>An Efficient Coding Method for Coding Region-of-Interest Locations in
  AVS2</title>
    <summary>  Region-of-Interest (ROI) location information in videos has many practical
usages in video coding field, such as video content analysis and user
experience improvement. Although ROI-based coding has been studied widely by
many researchers to improve coding efficiency for video contents, the ROI
location information itself is seldom coded in video bitstream. In this paper,
we will introduce our proposed ROI location coding tool which has been adopted
in surveillance profile of AVS2 video coding standard (surveillance profile).
Our tool includes three schemes: direct-coding scheme, differential- coding
scheme, and reconstructed-coding scheme. We will illustrate the details of
these schemes, and perform analysis of their advantages and disadvantages,
respectively.
</summary>
    <author>
      <name>Mingliang Chen</name>
    </author>
    <author>
      <name>Weiyao Lin</name>
    </author>
    <author>
      <name>Xiaozhen Zheng</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICMEW.2014.6890688</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICMEW.2014.6890688" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This manuscript is the accepted version for ICMEW (IEEE Intl. Conf.
  Multimedia &amp; Expo Workshop), IEEE Intl. Conf. Multimedia &amp; Expo Workshop
  (ICME), 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.00118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00388v1</id>
    <updated>2015-03-02T01:41:39Z</updated>
    <published>2015-03-02T01:41:39Z</published>
    <title>A Novel Image Steganographic Approach for Hiding Text in Color Images
  using HSI Color Model</title>
    <summary>  Image Steganography is the process of embedding text in images such that its
existence cannot be detected by Human Visual System (HVS) and is known only to
sender and receiver. This paper presents a novel approach for image
steganography using Hue-Saturation-Intensity (HSI) color space based on Least
Significant Bit (LSB). The proposed method transforms the image from RGB color
space to Hue-Saturation-Intensity (HSI) color space and then embeds secret data
inside the Intensity Plane (I-Plane) and transforms it back to RGB color model
after embedding. The said technique is evaluated by both subjective and
Objective Analysis. Experimentally it is found that the proposed method have
larger Peak Signal-to Noise Ratio (PSNR) values, good imperceptibility and
multiple security levels which shows its superiority as compared to several
existing methods
</summary>
    <author>
      <name>Khan Muhammad</name>
    </author>
    <author>
      <name>Jamil Ahmad</name>
    </author>
    <author>
      <name>Haleem Farman</name>
    </author>
    <author>
      <name>Muhammad Zubair</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5829/idosi.mejsr.2014.22.05.21946</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5829/idosi.mejsr.2014.22.05.21946" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An easy to follow paper of 11 pages. arXiv admin note: text overlap
  with arXiv:1502.07808</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Middle-East Journal of Scientific Research 22.5 (2014): 647-654</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.00388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00843v1</id>
    <updated>2015-03-03T07:17:46Z</updated>
    <published>2015-03-03T07:17:46Z</published>
    <title>A Survey On Video Forgery Detection</title>
    <summary>  The Digital Forgeries though not visibly identifiable to human perception it
may alter or meddle with underlying natural statistics of digital content.
Tampering involves fiddling with video content in order to cause damage or make
unauthorized alteration/modification. Tampering detection in video is
cumbersome compared to image when considering the properties of the video.
Tampering impacts need to be studied and the applied technique/method is used
to establish the factual information for legal course in judiciary. In this
paper we give an overview of the prior literature and challenges involved in
video forgery detection where passive approach is found.
</summary>
    <author>
      <name>Sowmya K. N.</name>
    </author>
    <author>
      <name>H. R. Chennamma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures, International Journal of Computer Engineering
  and Applications, Volume IX, Issue II, February 2015</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Engineering and Applications,
  Volume IX, Issue II, pp. 17-27, February 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.00843v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00843v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.01620v2</id>
    <updated>2015-07-13T08:12:27Z</updated>
    <published>2015-03-05T12:31:36Z</published>
    <title>Gaussian Mixture Model Based Contrast Enhancement</title>
    <summary>  In this paper, a method for enhancing low contrast images is proposed. This
method, called Gaussian Mixture Model based Contrast Enhancement (GMMCE),
brings into play the Gaussian mixture modeling of histograms to model the
content of the images. Based on the fact that each homogeneous area in natural
images has a Gaussian-shaped histogram, it decomposes the narrow histogram of
low contrast images into a set of scaled and shifted Gaussians. The individual
histograms are then stretched by increasing their variance parameters, and are
diffused on the entire histogram by scattering their mean parameters, to build
a broad version of the histogram. The number of Gaussians as well as their
parameters are optimized to set up a GMM with lowest approximation error and
highest similarity to the original histogram. Compared to the existing
histogram-based methods, the experimental results show that the quality of
GMMCE enhanced pictures are mostly consistent and outperform other benchmark
methods. Additionally, the computational complexity analysis show that GMMCE is
a low complexity method.
</summary>
    <author>
      <name>Mohsen Abdoli</name>
    </author>
    <author>
      <name>Hossein Sarikhani</name>
    </author>
    <author>
      <name>Mohammad Ghanbari</name>
    </author>
    <author>
      <name>Patrice Brault</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1049/iet-ipr.2014.0583</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1049/iet-ipr.2014.0583" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Image Processing, IET, Vol. 9, No. 7, pp. 569-577, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.01620v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.01620v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.01817v2</id>
    <updated>2016-04-25T20:10:14Z</updated>
    <published>2015-03-05T23:43:42Z</published>
    <title>YFCC100M: The New Data in Multimedia Research</title>
    <summary>  We present the Yahoo Flickr Creative Commons 100 Million Dataset (YFCC100M),
the largest public multimedia collection that has ever been released. The
dataset contains a total of 100 million media objects, of which approximately
99.2 million are photos and 0.8 million are videos, all of which carry a
Creative Commons license. Each media object in the dataset is represented by
several pieces of metadata, e.g. Flickr identifier, owner name, camera, title,
tags, geo, media source. The collection provides a comprehensive snapshot of
how photos and videos were taken, described, and shared over the years, from
the inception of Flickr in 2004 until early 2014. In this article we explain
the rationale behind its creation, as well as the implications the dataset has
for science, research, engineering, and development. We further present several
new challenges in multimedia research that can now be expanded upon with our
dataset.
</summary>
    <author>
      <name>Bart Thomee</name>
    </author>
    <author>
      <name>David A. Shamma</name>
    </author>
    <author>
      <name>Gerald Friedland</name>
    </author>
    <author>
      <name>Benjamin Elizalde</name>
    </author>
    <author>
      <name>Karl Ni</name>
    </author>
    <author>
      <name>Douglas Poland</name>
    </author>
    <author>
      <name>Damian Borth</name>
    </author>
    <author>
      <name>Li-Jia Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2812802</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2812802" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Communications of the ACM, 59(2), pp. 64-73, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.01817v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.01817v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.01934v1</id>
    <updated>2015-03-06T12:42:32Z</updated>
    <published>2015-03-06T12:42:32Z</published>
    <title>Reliable SVD based Semi-blind and Invisible Watermarking Schemes</title>
    <summary>  A semi-blind watermarking scheme is presented based on Singular Value
Decomposition (SVD), which makes essential use of the fact that, the SVD
subspace preserves significant amount of information of an image and is a one
way decomposition. The principal components are used, along with the
corresponding singular vectors of the watermark image to watermark the target
image. For further security, the semi-blind scheme is extended to an invisible
hash based watermarking scheme. The hash based scheme commits a watermark with
a key such that, it is incoherent with the actual watermark, and can only be
extracted using the key. Its security is analyzed in the random oracle model
and shown to be unforgeable, invisible and satisfying the property of
non-repudiation.
</summary>
    <author>
      <name>Subhayan Roy Moulick</name>
    </author>
    <author>
      <name>Siddharth Arora</name>
    </author>
    <author>
      <name>Chirag Jain</name>
    </author>
    <author>
      <name>Prasanta K. Panigrahi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 Pages, 1 Figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.01934v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.01934v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.03674v1</id>
    <updated>2015-03-12T11:16:18Z</updated>
    <published>2015-03-12T11:16:18Z</published>
    <title>A novel hash based least significant bit (2-3-3) image steganography in
  spatial domain</title>
    <summary>  This paper presents a novel 2-3-3 LSB insertion method. The image
steganography takes the advantage of human eye limitation. It uses color image
as cover media for embedding secret message.The important quality of a
steganographic system is to be less distortive while increasing the size of the
secret message. In this paper a method is proposed to embed a color secret
image into a color cover image. A 2-3-3 LSB insertion method has been used for
image steganography. Experimental results show an improvement in the Mean
squared error (MSE) and Peak Signal to Noise Ratio (PSNR) values of the
proposed technique over the base technique of hash based 3-3-2 LSB insertion.
</summary>
    <author>
      <name>G. R. Manjula</name>
    </author>
    <author>
      <name>Ajit Danti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijsptm.2015.4102</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijsptm.2015.4102" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages International journal of security privacy and trust
  management Feb 2015 issue</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.03674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.03674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.04263v1</id>
    <updated>2015-03-14T03:56:25Z</updated>
    <published>2015-03-14T03:56:25Z</published>
    <title>User Centric Content Management System for Open IPTV Over SNS (ICTC2012)</title>
    <summary>  Coupled schemes between service-oriented architecture (SOA) and Web 2.0 have
recently been researched. Web-based content providers and telecommunications
company (Telecom) based Internet protocol television (IPTV) providers have
struggled against each other to accommodate more three-screen service
subscribers. Since the advent of Web 2.0, more abundant reproduced content can
be circulated. However, because according to increasing device's resolution and
content formats IPTV providers transcode content in advance, network bandwidth,
storage and operation costs for content management systems (CMSs) are wasted.
In this paper, we present a user centric CMS for open IPTV, which integrates
SOA and Web 2.0. Considering content popularity based on a Zipf-like
distribution to solve these problems, we analyze the performance between the
user centric CMS and the conventional Web syndication system for normalized
costs. Based on the user centric CMS, we implement a social Web TV with
device-aware function, which can aggregate, transcode, and deploy content over
social networking service (SNS) independently.
</summary>
    <author>
      <name>Seung Hyun Jeon</name>
    </author>
    <author>
      <name>Sanghong An</name>
    </author>
    <author>
      <name>Changwoo Yoon</name>
    </author>
    <author>
      <name>Hyun-woo Lee</name>
    </author>
    <author>
      <name>Junkyun Choi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 17 figures, An earlier version of this paper was awarded as
  best paper at the IEEE International Conference on ICT Convergence (ICTC),
  Jeju, Korea, October 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.04263v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.04263v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01775v1</id>
    <updated>2017-11-06T08:16:07Z</updated>
    <published>2017-11-06T08:16:07Z</published>
    <title>Multimodal Signal Processing and Learning Aspects of Human-Robot
  Interaction for an Assistive Bathing Robot</title>
    <summary>  We explore new aspects of assistive living on smart human-robot interaction
(HRI) that involve automatic recognition and online validation of speech and
gestures in a natural interface, providing social features for HRI. We
introduce a whole framework and resources of a real-life scenario for elderly
subjects supported by an assistive bathing robot, addressing health and hygiene
care issues. We contribute a new dataset and a suite of tools used for data
acquisition and a state-of-the-art pipeline for multimodal learning within the
framework of the I-Support bathing robot, with emphasis on audio and RGB-D
visual streams. We consider privacy issues by evaluating the depth visual
stream along with the RGB, using Kinect sensors. The audio-gestural recognition
task on this new dataset yields up to 84.5%, while the online validation of the
I-Support system on elderly users accomplishes up to 84% when the two
modalities are fused together. The results are promising enough to support
further research in the area of multimodal recognition for assistive social
HRI, considering the difficulties of the specific task. Upon acceptance of the
paper part of the data will be publicly available.
</summary>
    <author>
      <name>A. Zlatintsi</name>
    </author>
    <author>
      <name>I. Rodomagoulakis</name>
    </author>
    <author>
      <name>P. Koutras</name>
    </author>
    <author>
      <name>A. C. Dometios</name>
    </author>
    <author>
      <name>V. Pitsikalis</name>
    </author>
    <author>
      <name>C. S. Tzafestas</name>
    </author>
    <author>
      <name>P. Maragos</name>
    </author>
    <link href="http://arxiv.org/abs/1711.01775v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01775v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.02150v1</id>
    <updated>2017-11-06T20:13:58Z</updated>
    <published>2017-11-06T20:13:58Z</published>
    <title>ADS: Adaptive and Dynamic Scaling Mechanism for Multimedia Conferencing
  Services in the Cloud</title>
    <summary>  Multimedia conferencing is used extensively in a wide range of applications,
such as online games and distance learning. These applications need to
efficiently scale the conference size as the number of participants fluctuates.
Cloud is a technology that addresses the scalability issue. However, the
proposed cloud-based solutions have several shortcomings in considering the
future demand of applications while meeting both Quality of Service (QoS)
requirements and efficiency in resource usage. In this paper, we propose an
Adaptive and Dynamic Scaling mechanism (ADS) for multimedia conferencing
services in the cloud. This mechanism enables scalable and elastic resource
allocation with respect to the number of participants. ADS produces a
cost-efficient scaling schedule while considering the QoS requirements and the
future demand of the conferencing service. We formulate the problem using
Integer Linear Programming (ILP) and design a heuristic for it. Simulation
results show that ADS mechanism elastically scales conferencing services.
Moreover, the ADS heuristic is shown to outperform a greedy algorithm from a
resource-efficiency perspective.
</summary>
    <author>
      <name>Abbas Soltanian</name>
    </author>
    <author>
      <name>Diala Naboulsi</name>
    </author>
    <author>
      <name>Mohammad A. Salahuddin</name>
    </author>
    <author>
      <name>Roch Glitho</name>
    </author>
    <author>
      <name>Halima Elbiaze</name>
    </author>
    <author>
      <name>Constant Wette</name>
    </author>
    <link href="http://arxiv.org/abs/1711.02150v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.02150v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.02386v1</id>
    <updated>2017-11-07T10:50:13Z</updated>
    <published>2017-11-07T10:50:13Z</published>
    <title>Viewport-aware adaptive 360¬∞ video streaming using tiles for
  virtual reality</title>
    <summary>  360{\deg} video is attracting an increasing amount of attention in the
context of Virtual Reality (VR). Owing to its very high-resolution
requirements, existing professional streaming services for 360{\deg} video
suffer from severe drawbacks. This paper introduces a novel end-to-end
streaming system from encoding to displaying, to transmit 8K resolution
360{\deg} video and to provide an enhanced VR experience using Head Mounted
Displays (HMDs). The main contributions of the proposed system are about
tiling, integration of the MPEG-Dynamic Adaptive Streaming over HTTP (DASH)
standard, and viewport-aware bitrate level selection. Tiling and adaptive
streaming enable the proposed system to deliver very high-resolution 360{\deg}
video at good visual quality. Further, the proposed viewport-aware bitrate
assignment selects an optimum DASH representation for each tile in a
viewport-aware manner. The quality performance of the proposed system is
verified in simulations with varying network bandwidth using realistic view
trajectories recorded from user experiments. Our results show that the proposed
streaming system compares favorably compared to existing methods in terms of
PSNR and SSIM inside the viewport.
</summary>
    <author>
      <name>Cagri Ozcinar</name>
    </author>
    <author>
      <name>Ana De Abreu</name>
    </author>
    <author>
      <name>Aljosa Smolic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Image Processing (ICIP) 2017</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2017 IEEE International Conference on Image Processing (ICIP),
  Beijing, China, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1711.02386v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.02386v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07201v1</id>
    <updated>2017-11-20T08:49:32Z</updated>
    <published>2017-11-20T08:49:32Z</published>
    <title>End-to-end Trained CNN Encode-Decoder Networks for Image Steganography</title>
    <summary>  All the existing image steganography methods use manually crafted features to
hide binary payloads into cover images. This leads to small payload capacity
and image distortion. Here we propose a convolutional neural network based
encoder-decoder architecture for embedding of images as payload. To this end,
we make following three major contributions: (i) we propose a deep learning
based generic encoder-decoder architecture for image steganography; (ii) we
introduce a new loss function that ensures joint end-to-end training of
encoder-decoder networks; (iii) we perform extensive empirical evaluation of
proposed architecture on a range of challenging publicly available datasets
(MNIST, CIFAR10, PASCAL-VOC12, ImageNet, LFW) and report state-of-the-art
payload capacity at high PSNR and SSIM values.
</summary>
    <author>
      <name>Atique ur Rehman</name>
    </author>
    <author>
      <name>Rafia Rahim</name>
    </author>
    <author>
      <name>M Shahroz Nadeem</name>
    </author>
    <author>
      <name>Sibt ul Hussain</name>
    </author>
    <link href="http://arxiv.org/abs/1711.07201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.08118v1</id>
    <updated>2017-11-22T03:02:28Z</updated>
    <published>2017-11-22T03:02:28Z</published>
    <title>Channel Transition Invariant Fast Broadcasting Scheme</title>
    <summary>  Fast broadcasting (FB) is a popular near video-on-demand system where a video
is divided into equal size segments those are repeatedly transmitted over a
number of channels following a pattern. For user satisfaction, it is required
to reduce the initial user waiting time and client side buffer requirement at
streaming. Use of additional channels can achieve the objective. However, some
augmentation is required to the basic FB scheme as it lacks any mechanism to
realise a well defined relationship among the segment sizes at channel
transition. Lack of correspondence between the segments causes intermediate
waiting for the clients while watching videos. Use of additional channel
requires additional bandwidth. In this paper, we propose a modified FB scheme
that achieves zero initial clients waiting time and provides a mechanism to
control client side buffer requirement at streaming without requiring
additional channels. We present several results to demonstrate the
effectiveness of the proposed FB scheme over the existing ones.
</summary>
    <author>
      <name>Mohammad Saidur Rahman</name>
    </author>
    <author>
      <name>Ashfaqur Rahman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2014 9th International Forum on Strategic Technology (IFOST)</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.08118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.08118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.08571v2</id>
    <updated>2018-08-30T01:51:31Z</updated>
    <published>2017-11-23T04:05:56Z</published>
    <title>Calibrated Audio Steganalysis</title>
    <summary>  Calibration is a common practice in image steganalysis for extracting
prominent features. Based on the idea of reembedding, a new set of calibrated
features for audio steganalysis applications are proposed. These features are
extracted from a model that has maximum deviation from human auditory system
and had been specifically designed for audio steganalysis. Ability of the
proposed system is tested extensively. Simulations demonstrate that the
proposed method can accurately detect the presence of hidden messages even in
very low embedding rates. Proposed method achieves an accuracy of 99.3%
(StegHide@0.76% BPB) which is 9.5% higher than the previous R-MFCC based
steganalysis method.
</summary>
    <author>
      <name>Hamzeh Ghasemzadeh</name>
    </author>
    <author>
      <name>Mohammad H. Kayvanrad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures, 3 tables, 2016 1st International Conference on
  New Research Achievements in Electrical and Computer Engineering,
  https://en.civilica.com/Paper-CBCONF01-CBCONF01_0107=Calibrated-Audio-Steganalysis.html</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.08571v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.08571v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.11115v1</id>
    <updated>2017-11-29T21:35:56Z</updated>
    <published>2017-11-29T21:35:56Z</published>
    <title>Real-Time System for Human Activity Analysis</title>
    <summary>  We propose a real-time human activity analysis system, where a user's
activity can be quantiatively evaluated with respect to a ground truth
recording. We use two Kinects to solve the ptorblem of self-occlusion through
extraction optimal joint positions using Singular Value Decomposition (SVD) and
Sequential Quadratic Programming (SQP). Incremental Dynamic Time Warping (IDTW)
is used to compare the user and expert (ground truth) to quantiatively score
the user's performance. Furthermore, the user's performance is displayed
through a visual feedback system, where colors on the skeleton represent the
user's score. Our experiements use a motion capture suit as ground truth to
compare our dual Kinect setup to a single Kinect. We also show that with out
visual feedback method, users gain statistically significant boost to learning
as opposed to watching a simple video.
</summary>
    <author>
      <name>Randy Tan</name>
    </author>
    <author>
      <name>Naimul Khan</name>
    </author>
    <author>
      <name>Ling Guan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at IEEE Symposium on Multimedia (ISM) 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.11115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.11115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00619v1</id>
    <updated>2018-05-02T04:24:27Z</updated>
    <published>2018-05-02T04:24:27Z</published>
    <title>Delay-Constrained Rate Control for Real-Time Video Streaming with
  Bounded Neural Network</title>
    <summary>  Rate control is widely adopted during video streaming to provide both high
video qualities and low latency under various network conditions. However,
despite that many work have been proposed, they fail to tackle one major
problem: previous methods determine a future transmission rate as a single for
value which will be used in an entire time-slot, while real-world network
conditions, unlike lab setup, often suffer from rapid and stochastic changes,
resulting in the failures of predictions.
  In this paper, we propose a delay-constrained rate control approach based on
end-to-end deep learning. The proposed model predicts future bit rate not as a
single value, but as possible bit rate ranges using target delay gradient, with
which the transmission delay is guaranteed. We collect a large scale of
real-world live streaming data to train our model, and as a result, it
automatically learns the correlation between throughput and target delay
gradient. We build a testbed to evaluate our approach. Compared with the
state-of-the-art methods, our approach demonstrates a better performance in
bandwidth utilization. In all considered scenarios, a range based rate control
approach outperforms the one without range by 19% to 35% in average QoE
improvement.
</summary>
    <author>
      <name>Tianchi Huang</name>
    </author>
    <author>
      <name>Rui-Xiao Zhang</name>
    </author>
    <author>
      <name>Chao Zhou</name>
    </author>
    <author>
      <name>Lifeng Sun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3210445.3210446</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3210445.3210446" rel="related"/>
    <link href="http://arxiv.org/abs/1805.00619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.02482v3</id>
    <updated>2018-10-27T09:40:11Z</updated>
    <published>2018-05-07T12:50:10Z</published>
    <title>QARC: Video Quality Aware Rate Control for Real-Time Video Streaming via
  Deep Reinforcement Learning</title>
    <summary>  Due to the fluctuation of throughput under various network conditions, how to
choose a proper bitrate adaptively for real-time video streaming has become an
upcoming and interesting issue. Recent work focuses on providing high video
bitrates instead of video qualities. Nevertheless, we notice that there exists
a trade-off between sending bitrate and video quality, which motivates us to
focus on how to get a balance between them. In this paper, we propose QARC
(video Quality Awareness Rate Control), a rate control algorithm that aims to
have a higher perceptual video quality with possibly lower sending rate and
transmission latency. Starting from scratch, QARC uses deep reinforcement
learning(DRL) algorithm to train a neural network to select future bitrates
based on previously observed network status and past video frames, and we
design a neural network to predict future perceptual video quality as a vector
for taking the place of the raw picture in the DRL's inputs. We evaluate QARC
over a trace-driven emulation. As excepted, QARC betters existing approaches.
</summary>
    <author>
      <name>Tianchi Huang</name>
    </author>
    <author>
      <name>Rui-Xiao Zhang</name>
    </author>
    <author>
      <name>Chao Zhou</name>
    </author>
    <author>
      <name>Lifeng Sun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3240508.3240545</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3240508.3240545" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ACM Multimedia 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.02482v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.02482v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03060v1</id>
    <updated>2018-05-08T14:38:58Z</updated>
    <published>2018-05-08T14:38:58Z</published>
    <title>CloudAR: A Cloud-based Framework for Mobile Augmented Reality</title>
    <summary>  Computation capabilities of recent mobile devices enable natural feature
processing for Augmented Reality (AR). However, mobile AR applications are
still faced with scalability and performance challenges. In this paper, we
propose CloudAR, a mobile AR framework utilizing the advantages of cloud and
edge computing through recognition task offloading. We explore the design space
of cloud-based AR exhaustively and optimize the offloading pipeline to minimize
the time and energy consumption. We design an innovative tracking system for
mobile devices which provides lightweight tracking in 6 degree of freedom
(6DoF) and hides the offloading latency from users' perception. We also design
a multi-object image retrieval pipeline that executes fast and accurate image
recognition tasks on servers. In our evaluations, the mobile AR application
built with the CloudAR framework runs at 30 frames per second (FPS) on average
with precise tracking of only 1~2 pixel errors and image recognition of at
least 97% accuracy. Our results also show that CloudAR outperforms one of the
leading commercial AR framework in several performance metrics.
</summary>
    <author>
      <name>Wenxiao Zhang</name>
    </author>
    <author>
      <name>Sikun Lin</name>
    </author>
    <author>
      <name>Farshid Hassani Bijarbooneh</name>
    </author>
    <author>
      <name>Hao Fei Cheng</name>
    </author>
    <author>
      <name>And Pan Hui</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03105v1</id>
    <updated>2018-05-08T15:28:23Z</updated>
    <published>2018-05-08T15:28:23Z</published>
    <title>Optimization of Occlusion-Inducing Depth Pixels in 3-D Video Coding</title>
    <summary>  The optimization of occlusion-inducing depth pixels in depth map coding has
received little attention in the literature, since their associated texture
pixels are occluded in the synthesized view and their effect on the synthesized
view is considered negligible. However, the occlusion-inducing depth pixels
still need to consume the bits to be transmitted, and will induce geometry
distortion that inherently exists in the synthesized view. In this paper, we
propose an efficient depth map coding scheme specifically for the
occlusion-inducing depth pixels by using allowable depth distortions. Firstly,
we formulate a problem of minimizing the overall geometry distortion in the
occlusion subject to the bit rate constraint, for which the depth distortion is
properly adjusted within the set of allowable depth distortions that introduce
the same disparity error as the initial depth distortion. Then, we propose a
dynamic programming solution to find the optimal depth distortion vector for
the occlusion. The proposed algorithm can improve the coding efficiency without
alteration of the occlusion order. Simulation results confirm the performance
improvement compared to other existing algorithms.
</summary>
    <author>
      <name>Pan Gao</name>
    </author>
    <author>
      <name>Cagri Ozcinar</name>
    </author>
    <author>
      <name>Aljosa Smolic</name>
    </author>
    <link href="http://arxiv.org/abs/1805.03105v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03105v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.03894v2</id>
    <updated>2018-05-31T12:58:32Z</updated>
    <published>2018-05-10T08:56:30Z</published>
    <title>Enhancing HEVC Compressed Videos with a Partition-masked Convolutional
  Neural Network</title>
    <summary>  In this paper, we propose a partition-masked Convolution Neural Network (CNN)
to achieve compressed-video enhancement for the state-of-the-art coding
standard, High Efficiency Video Coding (HECV). More precisely, our method
utilizes the partition information produced by the encoder to guide the quality
enhancement process. In contrast to existing CNN-based approaches, which only
take the decoded frame as the input to the CNN, the proposed approach considers
the coding unit (CU) size information and combines it with the distorted
decoded frame such that the degradation introduced by HEVC is reduced more
efficiently. Experimental results show that our approach leads to over 9.76%
BD-rate saving on benchmark sequences, which achieves the state-of-the-art
performance.
</summary>
    <author>
      <name>Xiaoyi He</name>
    </author>
    <author>
      <name>Qiang Hu</name>
    </author>
    <author>
      <name>Xintong Han</name>
    </author>
    <author>
      <name>Xiaoyun Zhang</name>
    </author>
    <author>
      <name>Chongyang Zhang</name>
    </author>
    <author>
      <name>Weiyao Lin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICIP.2018.8451086</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICIP.2018.8451086" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by ICIP 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.03894v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.03894v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.05575v1</id>
    <updated>2018-05-15T05:53:31Z</updated>
    <published>2018-05-15T05:53:31Z</published>
    <title>Visual Comfort Assessment for Stereoscopic Image Retargeting</title>
    <summary>  In recent years, visual comfort assessment (VCA) for 3D/stereoscopic content
has aroused extensive attention. However, much less work has been done on the
perceptual evaluation of stereoscopic image retargeting. In this paper, we
first build a Stereoscopic Image Retargeting Database (SIRD), which contains
source images and retargeted images produced by four typical stereoscopic
retargeting methods. Then, the subjective experiment is conducted to assess
four aspects of visual distortion, i.e. visual comfort, image quality, depth
quality and the overall quality. Furthermore, we propose a Visual Comfort
Assessment metric for Stereoscopic Image Retargeting (VCA-SIR). Based on the
characteristics of stereoscopic retargeted images, the proposed model
introduces novel features like disparity range, boundary disparity as well as
disparity intensity distribution into the assessment model. Experimental
results demonstrate that VCA-SIR can achieve high consistency with subjective
perception.
</summary>
    <author>
      <name>Ya Zhou</name>
    </author>
    <author>
      <name>Wei Zhou</name>
    </author>
    <author>
      <name>Ping An</name>
    </author>
    <author>
      <name>Zhibo Chen</name>
    </author>
    <link href="http://arxiv.org/abs/1805.05575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06021v1</id>
    <updated>2018-05-15T20:19:46Z</updated>
    <published>2018-05-15T20:19:46Z</published>
    <title>Topological Eulerian Synthesis of Slow Motion Periodic Videos</title>
    <summary>  We consider the problem of taking a video that is comprised of multiple
periods of repetitive motion, and reordering the frames of the video into a
single period, producing a detailed, single cycle video of motion. This problem
is challenging, as such videos often contain noise, drift due to camera motion
and from cycle to cycle, and irrelevant background motion/occlusions, and these
factors can confound the relevant periodic motion we seek in the video. To
address these issues in a simple and efficient manner, we introduce a tracking
free Eulerian approach for synthesizing a single cycle of motion. Our approach
is geometric: we treat each frame as a point in high-dimensional Euclidean
space, and analyze the sliding window embedding formed by this sequence of
points, which yields samples along a topological loop regardless of the type of
periodic motion. We combine tools from topological data analysis and spectral
geometric analysis to estimate the phase of each window, and we exploit the
sliding window structure to robustly reorder frames. We show quantitative
results that highlight the robustness of our technique to camera shake, noise,
and occlusions, and qualitative results of single-cycle motion synthesis across
a variety of scenarios.
</summary>
    <author>
      <name>Christopher Tralie</name>
    </author>
    <author>
      <name>Matthew Berger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 Figures. IEEE International Conference on Image
  Processing, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.06021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.5.1; I.3.3; I.4.m; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06782v1</id>
    <updated>2018-05-16T08:18:53Z</updated>
    <published>2018-05-16T08:18:53Z</published>
    <title>Extraction and Analysis of Dynamic Conversational Networks from TV
  Series</title>
    <summary>  Identifying and characterizing the dynamics of modern tv series subplots is
an open problem. One way is to study the underlying social network of
interactions between the characters. Standard dynamic network extraction
methods rely on temporal integration, either over the whole considered period,
or as a sequence of several time-slices. However, they turn out to be
inappropriate in the case of tv series, because the scenes shown onscreen
alternatively focus on parallel storylines, and do not necessarily respect a
traditional chronology. In this article, we introduce Narrative Smoothing, a
novel network extraction method taking advantage of the plot properties to
solve some of their limitations. We apply our method to a corpus of 3 popular
series, and compare it to both standard approaches. Narrative smoothing leads
to more relevant observations when it comes to the characterization of the
protagonists and their relationships, confirming its appropriateness to model
the intertwined storylines constituting the plots.
</summary>
    <author>
      <name>Xavier Bost</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIA</arxiv:affiliation>
    </author>
    <author>
      <name>Vincent Labatut</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIA</arxiv:affiliation>
    </author>
    <author>
      <name>Serigne Gueye</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIA</arxiv:affiliation>
    </author>
    <author>
      <name>Georges Linar√®s</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIA</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-78196-9_3</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-78196-9_3" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1602.07811</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In: Social Network Based Big Data Analysis and Applications,
  p.55-84, Springer, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1805.06782v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06782v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11254v2</id>
    <updated>2018-08-15T07:42:15Z</updated>
    <published>2018-05-29T05:48:14Z</published>
    <title>Hierarchical One Permutation Hashing: Efficient Multimedia Near
  Duplicate Detection</title>
    <summary>  With advances in multimedia technologies and the proliferation of smart
phone, digital cameras, storage devices, there are a rapidly growing massive
amount of multimedia data collected in many applications such as multimedia
retrieval and management system, in which the data element is composed of text,
image, video and audio. Consequently, the study of multimedia near duplicate
detection has attracted significant concern from research organizations and
commercial communities. Traditional solution minwish hashing (\minwise) faces
two challenges: expensive preprocessing time and lower comparison speed. Thus,
this work first introduce a hashing method called one permutation hashing
(\oph) to shun the costly preprocessing time. Based on \oph, a more efficient
strategy group based one permutation hashing (\goph) is developed to deal with
the high comparison time. Based on the fact that the similarity of most
multimedia data is not very high, this work design an new hashing method namely
hierarchical one permutation hashing (\hoph) to further improve the
performance. Comprehensive experiments on real multimedia datasets clearly show
that with similar accuracy \hoph is five to seven times faster than
</summary>
    <author>
      <name>Chengyuan Zhang</name>
    </author>
    <author>
      <name>Yunwu Lin</name>
    </author>
    <author>
      <name>Lei Zhu</name>
    </author>
    <author>
      <name>XinPan Yuan</name>
    </author>
    <author>
      <name>Jun Long</name>
    </author>
    <author>
      <name>Fang Huang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11042-018-6178-z</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11042-018-6178-z" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to appear at Multimedia Tools and Applications</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.11254v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11254v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.03867v1</id>
    <updated>2018-09-08T04:12:14Z</updated>
    <published>2018-09-08T04:12:14Z</published>
    <title>Efficient Multimedia Similarity Measurement Using Similar Elements</title>
    <summary>  Online social networking techniques and large-scale multimedia systems are
developing rapidly, which not only has brought great convenience to our daily
life, but generated, collected, and stored large-scale multimedia data. This
trend has put forward higher requirements and greater challenges on massive
multimedia data retrieval. In this paper, we investigate the problem of image
similarity measurement which is used to lots of applications. At first we
propose the definition of similarity measurement of images and the related
notions. Based on it we present a novel basic method of similarity measurement
named SMIN. To improve the performance of calculation, we propose a novel
indexing structure called SMI Temp Index (SMII for short). Besides, we
establish an index of potential similar visual words off-line to solve to
problem that the index cannot be reused. Experimental evaluations on two real
image datasets demonstrate that our solution outperforms state-of-the-art
method.
</summary>
    <author>
      <name>Chengyuan Zhang</name>
    </author>
    <author>
      <name>Yunwu Lin</name>
    </author>
    <author>
      <name>Lei Zhu</name>
    </author>
    <author>
      <name>Zuping Zhang</name>
    </author>
    <author>
      <name>Xinpan Yuan</name>
    </author>
    <author>
      <name>Fang Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages. arXiv admin note: text overlap with arXiv:1808.09610</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.03867v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.03867v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.04094v2</id>
    <updated>2019-03-24T08:59:05Z</updated>
    <published>2018-09-11T18:09:44Z</published>
    <title>FIVR: Fine-grained Incident Video Retrieval</title>
    <summary>  This paper introduces the problem of Fine-grained Incident Video Retrieval
(FIVR). Given a query video, the objective is to retrieve all associated
videos, considering several types of associations that range from duplicate
videos to videos from the same incident. FIVR offers a single framework that
contains several retrieval tasks as special cases. To address the benchmarking
needs of all such tasks, we construct and present a large-scale annotated video
dataset, which we call FIVR-200K, and it comprises 225,960 videos. To create
the dataset, we devise a process for the collection of YouTube videos based on
major news events from recent years crawled from Wikipedia and deploy a
retrieval pipeline for the automatic selection of query videos based on their
estimated suitability as benchmarks. We also devise a protocol for the
annotation of the dataset with respect to the four types of video associations
defined by FIVR. Finally, we report the results of an experimental study on the
dataset comparing five state-of-the-art methods developed based on a variety of
visual descriptors, highlighting the challenges of the current problem.
</summary>
    <author>
      <name>Giorgos Kordopatis-Zilos</name>
    </author>
    <author>
      <name>Symeon Papadopoulos</name>
    </author>
    <author>
      <name>Ioannis Patras</name>
    </author>
    <author>
      <name>Ioannis Kompatsiaris</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2019.2905741</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2019.2905741" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Multimedia 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1809.04094v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.04094v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.05220v1</id>
    <updated>2018-09-14T02:07:30Z</updated>
    <published>2018-09-14T02:07:30Z</published>
    <title>On Evaluating Perceptual Quality of Online User-Generated Videos</title>
    <summary>  This paper deals with the issue of the perceptual quality evaluation of
user-generated videos shared online, which is an important step toward
designing video-sharing services that maximize users' satisfaction in terms of
quality. We first analyze viewers' quality perception patterns by applying
graph analysis techniques to subjective rating data. We then examine the
performance of existing state-of-the-art objective metrics for the quality
estimation of user-generated videos. In addition, we investigate the
feasibility of metadata accompanied with videos in online video-sharing
services for quality estimation. Finally, various issues in the quality
assessment of online user-generated videos are discussed, including
difficulties and opportunities.
</summary>
    <author>
      <name>Soobeom Jang</name>
    </author>
    <author>
      <name>Jong-Seok Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2016.2581582</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2016.2581582" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in IEEE Transactions on Multimedia</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">S. Jang and J. S. Lee, "On evaluating perceptual quality of online
  user-generated videos,"IEEE Transactions on Multimedia, vol. 18, no. 9, pp.
  1808-1818, Sep. 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1809.05220v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.05220v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.06196v1</id>
    <updated>2018-09-17T13:38:41Z</updated>
    <published>2018-09-17T13:38:41Z</published>
    <title>Intermediate Deep Feature Compression: the Next Battlefield of
  Intelligent Sensing</title>
    <summary>  The recent advances of hardware technology have made the intelligent analysis
equipped at the front-end with deep learning more prevailing and practical. To
better enable the intelligent sensing at the front-end, instead of compressing
and transmitting visual signals or the ultimately utilized top-layer deep
learning features, we propose to compactly represent and convey the
intermediate-layer deep learning features of high generalization capability, to
facilitate the collaborating approach between front and cloud ends. This
strategy enables a good balance among the computational load, transmission load
and the generalization ability for cloud servers when deploying the deep neural
networks for large scale cloud based visual analysis. Moreover, the presented
strategy also makes the standardization of deep feature coding more feasible
and promising, as a series of tasks can simultaneously benefit from the
transmitted intermediate layers. We also present the results for evaluation of
lossless deep feature compression with four benchmark data compression methods,
which provides meaningful investigations and baselines for future research and
standardization activities.
</summary>
    <author>
      <name>Zhuo Chen</name>
    </author>
    <author>
      <name>Weisi Lin</name>
    </author>
    <author>
      <name>Shiqi Wang</name>
    </author>
    <author>
      <name>Lingyu Duan</name>
    </author>
    <author>
      <name>Alex C. Kot</name>
    </author>
    <link href="http://arxiv.org/abs/1809.06196v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.06196v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.07792v1</id>
    <updated>2018-08-29T03:19:26Z</updated>
    <published>2018-08-29T03:19:26Z</published>
    <title>Binocular Rivalry - Psychovisual Challenge in Stereoscopic Video Error
  Concealment</title>
    <summary>  During Stereoscopic 3D (S3D) video transmission, one or both views can be
affected by bit errors and packet losses caused by adverse channel conditions,
delay or jitter. Typically, the Human Visual System (HVS) is incapable of
aligning and fusing stereoscopic content if one view is affected by artefacts
caused by compression, transmission and rendering with distorted patterns being
perceived as alterations of the original which presents a shimmering effect
known as binocular rivalry and is detrimental to a user's Quality of Experience
(QoE). This study attempts to quantify the effects of binocular rivalry for
stereoscopic videos. Existing approaches, in which one or more frames are lost
in one or both views undergo error concealment, are implemented. Then,
subjective testing is carried out on the error concealed 3D video sequences.
The evaluations provided by these subjects were then combined and analysed
using a standard Student t-test thus quantifying the impact of binocular
rivalry and allowing the impact to be compared with that of monocular viewing.
The main focus is implementing error-resilient video communication, avoiding
the detrimental effects of binocular rivalry and improving the overall QoE of
viewers.
</summary>
    <author>
      <name>Md Mehedi Hasan</name>
    </author>
    <author>
      <name>John F. Arnold</name>
    </author>
    <author>
      <name>Michael R. Frater</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 9 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.07792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.07792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.08593v1</id>
    <updated>2018-09-23T13:13:23Z</updated>
    <published>2018-09-23T13:13:23Z</published>
    <title>Understanding the Gist of Images - Ranking of Concepts for Multimedia
  Indexing</title>
    <summary>  Nowadays, where multimedia data is continuously generated, stored, and
distributed, multimedia indexing, with its purpose of group- ing similar data,
becomes more important than ever. Understanding the gist (=message) of
multimedia instances is framed in related work as a ranking of concepts from a
knowledge base, i.e., Wikipedia. We cast the task of multimedia indexing as a
gist understanding problem. Our pipeline benefits from external knowledge and
two subsequent learning- to-rank (l2r) settings. The first l2r produces a
ranking of concepts rep- resenting the respective multimedia instance. The
second l2r produces a mapping between the concept representation of an instance
and the targeted class topic(s) for the multimedia indexing task. The
evaluation on an established big size corpus (MIRFlickr25k, with 25,000
images), shows that multimedia indexing benefits from understanding the gist.
Finally, with a MAP of 61.42, it can be shown that the multimedia in- dexing
task benefits from understanding the gist. Thus, the presented end-to-end
setting outperforms DBM and competes with Hashing-based methods.
</summary>
    <author>
      <name>Lydia Weiland</name>
    </author>
    <author>
      <name>Simone Paolo Ponzetto</name>
    </author>
    <author>
      <name>Wolfgang Effelsberg</name>
    </author>
    <author>
      <name>Laura Dietz</name>
    </author>
    <link href="http://arxiv.org/abs/1809.08593v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.08593v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.08753v1</id>
    <updated>2018-09-24T04:36:17Z</updated>
    <published>2018-09-24T04:36:17Z</published>
    <title>An Iterative Refinement Approach for Social Media Headline Prediction</title>
    <summary>  In this study, we propose a novel iterative refinement approach to predict
the popularity score of the social media meta-data effectively. With the rapid
growth of the social media on the Internet, how to adequately forecast the view
count or popularity becomes more important. Conventionally, the ensemble
approach such as random forest regression achieves high and stable performance
on various prediction tasks. However, most of the regression methods may not
precisely predict the extreme high or low values. To address this issue, we
first predict the initial popularity score and retrieve their residues. In
order to correctly compensate those extreme values, we adopt an ensemble
regressor to compensate the residues to further improve the prediction
performance. Comprehensive experiments are conducted to demonstrate the
proposed iterative refinement approach outperforms the state-of-the-art
regression approach.
</summary>
    <author>
      <name>Chih-Chung Hsu</name>
    </author>
    <author>
      <name>Chia-Yen Lee</name>
    </author>
    <author>
      <name>Ting-Xuan Liao</name>
    </author>
    <author>
      <name>Jun-Yi Lee</name>
    </author>
    <author>
      <name>Tsai-Yne Hou</name>
    </author>
    <author>
      <name>Ying-Chu Kuo</name>
    </author>
    <author>
      <name>Jing-Wen Lin</name>
    </author>
    <author>
      <name>Ching-Yi Hsueh</name>
    </author>
    <author>
      <name>Zhong-Xuan Zhan</name>
    </author>
    <author>
      <name>Hsiang-Chin Chien</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, ACM Multimedia Conference 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.08753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.08753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.08754v3</id>
    <updated>2018-10-18T14:55:42Z</updated>
    <published>2018-09-24T04:45:24Z</published>
    <title>Learning to Detect Fake Face Images in the Wild</title>
    <summary>  Although Generative Adversarial Network (GAN) can be used to generate the
realistic image, improper use of these technologies brings hidden concerns. For
example, GAN can be used to generate a tampered video for specific people and
inappropriate events, creating images that are detrimental to a particular
person, and may even affect that personal safety. In this paper, we will
develop a deep forgery discriminator (DeepFD) to efficiently and effectively
detect the computer-generated images. Directly learning a binary classifier is
relatively tricky since it is hard to find the common discriminative features
for judging the fake images generated from different GANs. To address this
shortcoming, we adopt contrastive loss in seeking the typical features of the
synthesized images generated by different GANs and follow by concatenating a
classifier to detect such computer-generated images. Experimental results
demonstrate that the proposed DeepFD successfully detected 94.7% fake images
generated by several state-of-the-art GANs.
</summary>
    <author>
      <name>Chih-Chung Hsu</name>
    </author>
    <author>
      <name>Chia-Yen Lee</name>
    </author>
    <author>
      <name>Yi-Xiu Zhuang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages to appear in IEEE IS3C Conference (IEEE International
  Symposium on Computer, Consumer and Control Conference), Dec. 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.08754v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.08754v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.10260v1</id>
    <updated>2018-09-26T22:55:10Z</updated>
    <published>2018-09-26T22:55:10Z</published>
    <title>A Coarse-To-Fine Framework For Video Object Segmentation</title>
    <summary>  In this study, we develop an unsupervised coarse-to-fine video analysis
framework and prototype system to extract a salient object in a video sequence.
This framework starts from tracking grid-sampled points along temporal frames,
typically using KLT tracking method. The tracking points could be divided into
several groups due to their inconsistent movements. At the same time, the SLIC
algorithm is extended into 3D space to generate supervoxels. Coarse
segmentation is achieved by combining the categorized tracking points and
supervoxels of the corresponding frame in the video sequence. Finally, a
graph-based fine segmentation algorithm is used to extract the moving object in
the scene. Experimental results reveal that this method outperforms the
previous approaches in terms of accuracy and robustness.
</summary>
    <author>
      <name>Chi Zhang</name>
    </author>
    <author>
      <name>Alexander Loui</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 11 figures, to appear in Electronic Imaging, Visual
  Information Processing and Communication VIII, pp. 32-37(6)</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.10260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.10260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.00551v1</id>
    <updated>2018-12-03T04:47:18Z</updated>
    <published>2018-12-03T04:47:18Z</published>
    <title>Music Popularity: Metrics, Characteristics, and Audio-Based Prediction</title>
    <summary>  Understanding music popularity is important not only for the artists who
create and perform music but also for the music-related industry. It has not
been studied well how music popularity can be defined, what its characteristics
are, and whether it can be predicted, which are addressed in this paper. We
first define eight popularity metrics to cover multiple aspects of popularity.
Then, the analysis of each popularity metric is conducted with long-term
real-world chart data to deeply understand the characteristics of music
popularity in the real world. We also build classification models for
predicting popularity metrics using acoustic data. In particular, we focus on
evaluating features describing music complexity together with other
conventional acoustic features including MPEG-7 and Mel-frequency cepstral
coefficient (MFCC) features. The results show that, although room still exists
for improvement, it is feasible to predict the popularity metrics of a song
significantly better than random chance based on its audio signal, particularly
using both the complexity and MFCC features.
</summary>
    <author>
      <name>Junghyuk Lee</name>
    </author>
    <author>
      <name>Jong-Seok Lee</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2018.2820903</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2018.2820903" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Multimedia, vol. 20, no. 11, pp. 3173-3182,
  Nov. 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.00551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.00551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.00816v1</id>
    <updated>2018-11-30T18:04:56Z</updated>
    <published>2018-11-30T18:04:56Z</published>
    <title>A Robust Algorithm for Tile-based 360-degree Video Streaming with
  Uncertain FoV Estimation</title>
    <summary>  We propose a robust scheme for streaming 360-degree immersive videos to
maximize the quality of experience (QoE). Our streaming approach introduces a
holistic analytical framework built upon the formal method of stochastic
optimization. We propose a robust algorithm which provides a streaming rate
such that the video quality degrades below that rate with very low probability
even in presence of uncertain head movement, and bandwidth. It assumes the
knowledge of the viewing probability of different portions (tiles) of a
panoramic scene. Such probabilities can be easily derived from crowdsourced
measurements performed by 360 video content providers. We then propose
efficient methods to solve the problem at runtime while achieving a bounded
optimality gap (in terms of the QoE). We implemented our proposed approaches
using emulation. Using real users' head movement traces and real cellular
bandwidth traces, we show that our algorithms significantly outperform the
baseline algorithms by at least in $30\%$ in the QoE metric. Our algorithm
gives a streaming rate which is $50\%$ higher compared to the baseline
algorithms when the prediction error is high.
</summary>
    <author>
      <name>Arnob Ghosh</name>
    </author>
    <author>
      <name>Vaneet Aggarwal</name>
    </author>
    <author>
      <name>Feng Qian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1704.08215</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.00816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.00816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.00828v1</id>
    <updated>2018-12-03T15:20:59Z</updated>
    <published>2018-12-03T15:20:59Z</published>
    <title>Novel Quality Metric for Duration Variability Compensation in Speaker
  Verification using i-Vectors</title>
    <summary>  Automatic speaker verification (ASV) is the process to recognize persons
using voice as biometric. The ASV systems show considerable recognition
performance with sufficient amount of speech from matched condition. One of the
crucial challenges of ASV technology is to improve recognition performance with
speech segments of short duration. In short duration condition, the model
parameters are not properly estimated due to inadequate speech information, and
this results poor recognition accuracy even with the state-of-the-art i-vector
based ASV system. We hypothesize that considering the estimation quality during
recognition process would help to improve the ASV performance. This can be
incorporated as a quality measure during fusion of ASV systems. This paper
investigates a new quality measure for i-vector representation of speech
utterances computed directly from Baum-Welch statistics. The proposed metric is
subsequently used as quality measure during fusion of ASV systems. In
experiments with the NIST SRE 2008 corpus, We have shown that inclusion of
proposed quality metric exhibits considerable improvement in speaker
verification performance. The results also indicate the potentiality of the
proposed method in real-world scenario with short test utterances.
</summary>
    <author>
      <name>Arnab Poddar</name>
    </author>
    <author>
      <name>Md Sahidullah</name>
    </author>
    <author>
      <name>Goutam Saha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted and presented in ICAPR 2017, Bangalore, India</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.00828v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.00828v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.07200v1</id>
    <updated>2018-12-18T07:11:19Z</updated>
    <published>2018-12-18T07:11:19Z</published>
    <title>D{√©}tection de locuteurs dans les s{√©}ries TV</title>
    <summary>  Speaker diarization of audio streams turns out to be particularly challenging
when applied to fictional films, where many characters talk in various acoustic
conditions (background music, sound effects, variations in intonation...).
Despite this acoustic variability, such movies exhibit specific visual
patterns, particularly within dialogue scenes. In this paper, we introduce a
two-step method to achieve speaker diarization in TV series: speaker
diarization is first performed locally within scenes visually identified as
dialogues; then, the hypothesized local speakers are compared to each other
during a second clustering process in order to detect recurring speakers: this
second stage of clustering is subject to the constraint that the different
speakers involved in the same dialogue have to be assigned to different
clusters. The performances of our approach are compared to those obtained by
standard speaker diarization tools applied to the same data.
</summary>
    <author>
      <name>Xavier Bost</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIA</arxiv:affiliation>
    </author>
    <author>
      <name>Georges Linares</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIA</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in French</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Coria 2015, Mar 2015, Paris, France</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.07200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.07200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.07205v2</id>
    <updated>2018-12-29T14:59:28Z</updated>
    <published>2018-12-18T07:21:36Z</published>
    <title>Audiovisual speaker diarization of TV series</title>
    <summary>  Speaker diarization may be difficult to achieve when applied to narrative
films, where speakers usually talk in adverse acoustic conditions: background
music, sound effects, wide variations in intonation may hide the inter-speaker
variability and make audio-based speaker diarization approaches error prone. On
the other hand, such fictional movies exhibit strong regularities at the image
level, particularly within dialogue scenes. In this paper, we propose to
perform speaker diarization within dialogue scenes of TV series by combining
the audio and video modalities: speaker diarization is first performed by using
each modality, the two resulting partitions of the instance set are then
optimally matched, before the remaining instances, corresponding to cases of
disagreement between both modalities, are finally processed. The results
obtained by applying such a multi-modal approach to fictional films turn out to
outperform those obtained by relying on a single modality.
</summary>
    <author>
      <name>Xavier Bost</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIA</arxiv:affiliation>
    </author>
    <author>
      <name>Georges Linar√®s</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIA</arxiv:affiliation>
    </author>
    <author>
      <name>Serigne Gueye</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIA</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP.2015.7178882</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP.2015.7178882" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2015 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), Apr 2015, Brisbane, Australia. IEEE, pp.4799-4803, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.07205v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.07205v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.07209v2</id>
    <updated>2018-12-29T15:04:31Z</updated>
    <published>2018-12-18T07:29:27Z</published>
    <title>Constrained speaker diarization of TV series based on visual patterns</title>
    <summary>  Speaker diarization, usually denoted as the ''who spoke when'' task, turns
out to be particularly challenging when applied to fictional films, where many
characters talk in various acoustic conditions (background music, sound
effects...). Despite this acoustic variability , such movies exhibit specific
visual patterns in the dialogue scenes. In this paper, we introduce a two-step
method to achieve speaker diarization in TV series: a speaker diarization is
first performed locally in the scenes detected as dialogues; then, the
hypothesized local speakers are merged in a second agglomerative clustering
process, with the constraint that speakers locally hypothesized to be distinct
must not be assigned to the same cluster. The performances of our approach are
compared to those obtained by standard speaker diarization tools applied to the
same data.
</summary>
    <author>
      <name>Xavier Bost</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIA</arxiv:affiliation>
    </author>
    <author>
      <name>Georges Linares</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIA</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SLT.2014.7078606</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SLT.2014.7078606" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2014 IEEE Spoken Language Technology Workshop (SLT), Dec 2014,
  South Lake Tahoe, United States. IEEE, pp.390-395, 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.07209v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.07209v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.09127v1</id>
    <updated>2018-12-03T07:57:14Z</updated>
    <published>2018-12-03T07:57:14Z</published>
    <title>A Smart Security System with Face Recognition</title>
    <summary>  Web-based technology has improved drastically in the past decade. As a
result, security technology has become a major help to protect our daily life.
In this paper, we propose a robust security based on face recognition system
(SoF). In particular, we develop this system to giving access into a home for
authenticated users. The classifier is trained by using a new adaptive learning
method. The training data are initially collected from social networks. The
accuracy of the classifier is incrementally improved as the user starts using
the system. A novel method has been introduced to improve the classifier model
by human interaction and social media. By using a deep learning framework -
TensorFlow, it will be easy to reuse the framework to adopt with many devices
and applications.
</summary>
    <author>
      <name>Trung Nguyen</name>
    </author>
    <author>
      <name>Barth Lakshmanan</name>
    </author>
    <author>
      <name>Weihua Sheng</name>
    </author>
    <link href="http://arxiv.org/abs/1812.09127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.09127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.09443v2</id>
    <updated>2019-04-20T14:54:01Z</updated>
    <published>2018-12-22T03:25:27Z</published>
    <title>Learned Scalable Image Compression with Bidirectional Context
  Disentanglement Network</title>
    <summary>  In this paper, we propose a learned scalable/progressive image compression
scheme based on deep neural networks (DNN), named Bidirectional Context
Disentanglement Network (BCD-Net). For learning hierarchical representations,
we first adopt bit-plane decomposition to decompose the information coarsely
before the deep-learning-based transformation. However, the information carried
by different bit-planes is not only unequal in entropy but also of different
importance for reconstruction. We thus take the hidden features corresponding
to different bit-planes as the context and design a network topology with
bidirectional flows to disentangle the contextual information for more
effective compressed representations. Our proposed scheme enables us to obtain
the compressed codes with scalable rates via a one-pass encoding-decoding.
Experiment results demonstrate that our proposed model outperforms the
state-of-the-art DNN-based scalable image compression methods in both PSNR and
MS-SSIM metrics. In addition, our proposed model achieves higher performance in
MS-SSIM metric than conventional scalable image codecs. Effectiveness of our
technical components is also verified through sufficient ablation experiments.
</summary>
    <author>
      <name>Zhizheng Zhang</name>
    </author>
    <author>
      <name>Zhibo Chen</name>
    </author>
    <author>
      <name>Jianxin Lin</name>
    </author>
    <author>
      <name>Weiping Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Multimedia and Expo (ICME2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.09443v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.09443v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.09499v1</id>
    <updated>2018-12-22T10:57:48Z</updated>
    <published>2018-12-22T10:57:48Z</published>
    <title>Reversible Data Hiding in Encrypted Images based on MSB Prediction and
  Huffman Coding</title>
    <summary>  With the development of cloud storage and privacy protection, reversible data
hiding in encrypted images (RDHEI) has attracted increasing attention as a
technology that can embed additional data in the encryption domain. In general,
an RDHEI method embeds secret data in an encrypted image while ensuring that
the embedded data can be extracted error-free and the original image can be
restored lossless. In this paper, A high-capacity RDHEI algorithm is proposed.
At first, the Most Significant Bits (MSB) of each pixel was predicted
adaptively and marked by Huffman coding in the original image. Then, the image
was encrypted by a stream cipher method. At last, the vacated space can be used
to embed additional data. Experimental results show that our method achieved
higher embedding capacity while comparing with the state-of-the-art methods.
</summary>
    <author>
      <name>Youzhi Xiang</name>
    </author>
    <author>
      <name>Zhaoxia Yin</name>
    </author>
    <author>
      <name>Xinpeng Zhang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TMM.2019.2936314</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TMM.2019.2936314" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 8 figures, to submit to IEEE Trans. Multimedia</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">[J]. IEEE Transactions on Multimedia, 2019, 22(4): 874-884</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1812.09499v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.09499v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.09681v2</id>
    <updated>2019-08-21T16:42:04Z</updated>
    <published>2018-12-23T09:59:49Z</published>
    <title>Scene Graph Reasoning with Prior Visual Relationship for Visual Question
  Answering</title>
    <summary>  One of the key issues of Visual Question Answering (VQA) is to reason with
semantic clues in the visual content under the guidance of the question, how to
model relational semantics still remains as a great challenge. To fully capture
visual semantics, we propose to reason over a structured visual representation
- scene graph, with embedded objects and inter-object relationships. This shows
great benefit over vanilla vector representations and implicit visual
relationship learning. Based on existing visual relationship models, we propose
a visual relationship encoder that projects visual relationships into a learned
deep semantic space constrained by visual context and language priors. Upon the
constructed graph, we propose a Scene Graph Convolutional Network (SceneGCN) to
jointly reason the object properties and relational semantics for the correct
answer. We demonstrate the model's effectiveness and interpretability on the
challenging GQA dataset and the classical VQA 2.0 dataset, remarkably achieving
state-of-the-art 54.56% accuracy on GQA compared to the existing best model.
</summary>
    <author>
      <name>Zhuoqian Yang</name>
    </author>
    <author>
      <name>Zengchang Qin</name>
    </author>
    <author>
      <name>Jing Yu</name>
    </author>
    <author>
      <name>Yue Hu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.09681v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.09681v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.10067v2</id>
    <updated>2019-03-07T11:54:39Z</updated>
    <published>2018-12-25T08:42:55Z</published>
    <title>Learning based Facial Image Compression with Semantic Fidelity Metric</title>
    <summary>  Surveillance and security scenarios usually require high efficient facial
image compression scheme for face recognition and identification. While either
traditional general image codecs or special facial image compression schemes
only heuristically refine codec separately according to face verification
accuracy metric. We propose a Learning based Facial Image Compression (LFIC)
framework with a novel Regionally Adaptive Pooling (RAP) module whose
parameters can be automatically optimized according to gradient feedback from
an integrated hybrid semantic fidelity metric, including a successfully
exploration to apply Generative Adversarial Network (GAN) as metric directly in
image compression scheme. The experimental results verify the framework's
efficiency by demonstrating performance improvement of 71.41%, 48.28% and
52.67% bitrate saving separately over JPEG2000, WebP and neural network-based
codecs under the same face verification accuracy distortion metric. We also
evaluate LFIC's superior performance gain compared with latest specific facial
image codecs. Visual experiments also show some interesting insight on how LFIC
can automatically capture the information in critical areas based on semantic
distortion metrics for optimized compression, which is quite different from the
heuristic way of optimization in traditional image compression algorithms.
</summary>
    <author>
      <name>Zhibo Chen</name>
    </author>
    <author>
      <name>Tianyu He</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neucom.2019.01.086</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neucom.2019.01.086" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by Neurocomputing</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.10067v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.10067v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.03175v1</id>
    <updated>2019-05-21T07:08:22Z</updated>
    <published>2019-05-21T07:08:22Z</published>
    <title>Image Encryption Algorithm Based on Facebook Social Network</title>
    <summary>  Facebook is the online social networks (OSNs) platform with the largest
number of users in the world today, information protection based on Facebook
social network platform have important practical significance. Since the
information users share on social networks is often based on images, this paper
proposes a more secure image encryption algorithm based on Facebook social
network platform to ensure the loss of information as much as possible. When
the sender encrypts the image for uploading, it can first resist the third
party's attack on the encrypted image and prevent the image data from leaking,
simultaneously processed by some unknown processing such as compression and
filtering of the image on the Facebook platform, the receiver can still decrypt
the corresponding image data.
</summary>
    <author>
      <name>Xiaoqing Liu</name>
    </author>
    <author>
      <name>Yinyin Peng</name>
    </author>
    <author>
      <name>Jie Wang</name>
    </author>
    <author>
      <name>Zhaoxia Yin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Chinese</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.03175v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.03175v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.03395v1</id>
    <updated>2019-06-08T06:03:09Z</updated>
    <published>2019-06-08T06:03:09Z</published>
    <title>Frequency-Dependent Perceptual Quantisation for Visually Lossless
  Compression Applications</title>
    <summary>  The default quantisation algorithms in the state-of-the-art High Efficiency
Video Coding (HEVC) standard, namely Uniform Reconstruction Quantisation (URQ)
and Rate-Distortion Optimised Quantisation (RDOQ), do not take into account the
perceptual relevance of individual transform coefficients. In this paper, a
Frequency-Dependent Perceptual Quantisation (FDPQ) technique for HEVC is
proposed. FDPQ exploits the well-established Modulation Transfer Function (MTF)
characteristics of the linear transformation basis functions by taking into
account the Euclidean distance of an AC transform coefficient from the DC
coefficient. As such, in luma and chroma Cb and Cr Transform Blocks (TBs), FDPQ
quantises more coarsely the least perceptually relevant transform coefficients
(i.e., the high frequency AC coefficients). Conversely, FDPQ preserves the
integrity of the DC coefficient and the very low frequency AC coefficients.
Compared with RDOQ, which is the most widely used transform coefficient-level
quantisation technique in video coding, FDPQ successfully achieves bitrate
reductions of up to 41%. Furthermore, the subjective evaluations confirm that
the FDPQ-coded video data is perceptually indistinguishable (i.e., visually
lossless) from the raw video data for a given Quantisation Parameter (QP).
</summary>
    <author>
      <name>Lee Prangnell</name>
    </author>
    <link href="http://arxiv.org/abs/1906.03395v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.03395v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.03940v1</id>
    <updated>2019-05-21T00:38:33Z</updated>
    <published>2019-05-21T00:38:33Z</published>
    <title>Predicting TED Talk Ratings from Language and Prosody</title>
    <summary>  We use the largest open repository of public speaking---TED Talks---to
predict the ratings of the online viewers. Our dataset contains over 2200 TED
Talk transcripts (includes over 200 thousand sentences), audio features and the
associated meta information including about 5.5 Million ratings from
spontaneous visitors of the website. We propose three neural network
architectures and compare with statistical machine learning. Our experiments
reveal that it is possible to predict all the 14 different ratings with an
average AUC of 0.83 using the transcripts and prosody features only. The
dataset and the complete source code is available for further analysis.
</summary>
    <author>
      <name>Md Iftekhar Tanveer</name>
    </author>
    <author>
      <name>Md Kamrul Hassan</name>
    </author>
    <author>
      <name>Daniel Gildea</name>
    </author>
    <author>
      <name>M. Ehsan Hoque</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1905.08392</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.03940v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.03940v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.06147v2</id>
    <updated>2019-07-28T20:09:00Z</updated>
    <published>2019-06-13T02:12:01Z</published>
    <title>Grounding Object Detections With Transcriptions</title>
    <summary>  A vast amount of audio-visual data is available on the Internet thanks to
video streaming services, to which users upload their content. However, there
are difficulties in exploiting available data for supervised statistical models
due to the lack of labels. Unfortunately, generating labels for such amount of
data through human annotation can be expensive, time-consuming and prone to
annotation errors. In this paper, we propose a method to automatically extract
entity-video frame pairs from a collection of instruction videos by using
speech transcriptions and videos. We conduct experiments on image recognition
and visual grounding tasks on the automatically constructed entity-video frame
dataset of How2. The models will be evaluated on new manually annotated portion
of How2 dev5 and val set and on the Flickr30k dataset. This work constitutes a
first step towards meta-algorithms capable of automatically construct
task-specific training sets.
</summary>
    <author>
      <name>Yasufumi Moriya</name>
    </author>
    <author>
      <name>Ramon Sanabria</name>
    </author>
    <author>
      <name>Florian Metze</name>
    </author>
    <author>
      <name>Gareth J. F. Jones</name>
    </author>
    <link href="http://arxiv.org/abs/1906.06147v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.06147v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.01285v1</id>
    <updated>2019-09-03T16:28:40Z</updated>
    <published>2019-09-03T16:28:40Z</published>
    <title>Robust Invisible Video Watermarking with Attention</title>
    <summary>  The goal of video watermarking is to embed a message within a video file in a
way such that it minimally impacts the viewing experience but can be recovered
even if the video is redistributed and modified, allowing media producers to
assert ownership over their content. This paper presents RivaGAN, a novel
architecture for robust video watermarking which features a custom
attention-based mechanism for embedding arbitrary data as well as two
independent adversarial networks which critique the video quality and optimize
for robustness. Using this technique, we are able to achieve state-of-the-art
results in deep learning-based video watermarking and produce watermarked
videos which have minimal visual distortion and are robust against common video
processing operations.
</summary>
    <author>
      <name>Kevin Alex Zhang</name>
    </author>
    <author>
      <name>Lei Xu</name>
    </author>
    <author>
      <name>Alfredo Cuesta-Infante</name>
    </author>
    <author>
      <name>Kalyan Veeramachaneni</name>
    </author>
    <link href="http://arxiv.org/abs/1909.01285v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.01285v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.02772v3</id>
    <updated>2020-02-25T06:02:36Z</updated>
    <published>2019-09-06T08:49:25Z</published>
    <title>Cumulative Quality Modeling for HTTP Adaptive Streaming</title>
    <summary>  Thanks to the abundance of Web platforms and broadband connections, HTTP
Adaptive Streaming has become the de facto choice for multimedia delivery
nowadays. However, the visual quality of adaptive video streaming may fluctuate
strongly during a session due to bandwidth fluctuations. So, it is important to
evaluate the quality of a streaming session over time. In this paper, we
propose a model to estimate the cumulative quality for HTTP Adaptive Streaming.
In the model, a sliding window of video segments is employed as the basic
building block. Through statistical analysis using a subjective dataset, we
identify three important components of the cumulative quality model, namely the
minimum window quality, the last window quality, and the average window
quality. Experiment results show that the proposed model achieves high
prediction performance and outperforms related quality models. In addition,
another advantage of the proposed model is its simplicity and effectiveness for
deployment in real-time estimation. The source code of the proposed model has
been made available to the public at https://github.com/TranHuyen1191/CQM.
</summary>
    <author>
      <name>Huyen T. T. Tran</name>
    </author>
    <author>
      <name>Nam Pham Ngoc</name>
    </author>
    <author>
      <name>Tobias Ho√üfeld</name>
    </author>
    <author>
      <name>Michael Seufert</name>
    </author>
    <author>
      <name>Truong Cong Thang</name>
    </author>
    <link href="http://arxiv.org/abs/1909.02772v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.02772v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.03766v1</id>
    <updated>2019-09-09T11:19:24Z</updated>
    <published>2019-09-09T11:19:24Z</published>
    <title>Hit Ratio Driven Mobile Edge Caching Scheme for Video on Demand Services</title>
    <summary>  More and more scholars focus on mobile edge computing (MEC) technology,
because the strong storage and computing capabilities of MEC servers can reduce
the long transmission delay, bandwidth waste, energy consumption, and privacy
leaks in the data transmission process. In this paper, we study the cache
placement problem to determine how to cache videos and which videos to be
cached in a mobile edge computing system. First, we derive the video request
probability by taking into account video popularity, user preference and the
characteristic of video representations. Second, based on the acquired request
probability, we formulate a cache placement problem with the objective to
maximize the cache hit ratio subject to the storage capacity constraints.
Finally, in order to solve the formulated problem, we transform it into a
grouping knapsack problem and develop a dynamic programming algorithm to obtain
the optimal caching strategy. Simulation results show that the proposed
algorithm can greatly improve the cache hit ratio.
</summary>
    <author>
      <name>Xing Chen</name>
    </author>
    <author>
      <name>Lijun He</name>
    </author>
    <author>
      <name>Shang Xu</name>
    </author>
    <author>
      <name>Shibo Hu</name>
    </author>
    <author>
      <name>Qingzhou Li</name>
    </author>
    <author>
      <name>Guizhong Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1909.03766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.03766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.04573v1</id>
    <updated>2019-09-10T15:36:07Z</updated>
    <published>2019-09-10T15:36:07Z</published>
    <title>Camera Fingerprint Extraction via Spatial Domain Averaged Frames</title>
    <summary>  Photo Response Non-Uniformity (PRNU) based camera attribution is an effective
method to determine the source camera of visual media (an image or a video). To
apply this method, images or videos need to be obtained from a camera to create
a "camera fingerprint" which then can be compared against the PRNU of the query
media whose origin is under question. The fingerprint extraction process can be
time-consuming when a large number of video frames or images have to be
denoised. This may need to be done when the individual images have been
subjected to high compression or other geometric processing such as video
stabilization. This paper investigates a simple, yet effective and efficient
technique to create a camera fingerprint when so many still images need to be
denoised. The technique utilizes Spatial Domain Averaged (SDA) frames. An
SDA-frame is the arithmetic mean of multiple still images. When it is used for
fingerprint extraction, the number of denoising operations can be significantly
decreased with little or no performance loss. Experimental results show that
the proposed method can work more than 50 times faster than conventional
methods while providing similar matching results.
</summary>
    <author>
      <name>Samet Taspinar</name>
    </author>
    <author>
      <name>Manoranjan Mohanty</name>
    </author>
    <author>
      <name>Nasir Memon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 9 figures, submitted to IEEE Transactions on Information
  Forensics and Security journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.04573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.04573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.04685v1</id>
    <updated>2019-09-10T18:00:53Z</updated>
    <published>2019-09-10T18:00:53Z</published>
    <title>Image Steganography: Protection of Digital Properties against
  Eavesdropping</title>
    <summary>  Steganography is the art of hiding the fact that communication is taking
place, by hiding information in other information. Different types of carrier
file formats can be used, but digital images are the most popular ones because
of their frequency on the internet. For hiding secret information in images,
there exists a large variety of steganography techniques. Some are more complex
than others and all of them have respective strong and weak points. Many
applications may require absolute invisibility of the secret information. This
paper intends to give an overview of image steganography, it's usage and
techniques, basically, to store the confidential information within images such
as details of working strategy, secret missions, criminal and confidential
information in various organizations that work for the national security such
as army, police, FBI, secret service etc. We develop a desktop application that
incorporates Advanced Encryption Standard for encryption of the original
message, and Spatially Desynchronized Steganography Algorithm for hiding the
text file inside the image.
</summary>
    <author>
      <name>Ramita Maharjan</name>
    </author>
    <author>
      <name>Ajay Kumar Shrestha</name>
    </author>
    <author>
      <name>Rejina Basnet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 11 figures, 9TH International conference on software,
  knowledge, information management and applications (SKIMA 2015)</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.04685v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.04685v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.07556v3</id>
    <updated>2019-10-02T01:31:03Z</updated>
    <published>2019-09-17T02:34:21Z</published>
    <title>Enhancing JPEG Steganography using Iterative Adversarial Examples</title>
    <summary>  Convolutional Neural Networks (CNN) based methods have significantly improved
the performance of image steganalysis compared with conventional ones based on
hand-crafted features. However, many existing literatures on computer vision
have pointed out that those effective CNN-based methods can be easily fooled by
adversarial examples. In this paper, we propose a novel steganography framework
based on adversarial example in an iterative manner. The proposed framework
first starts from an existing embedding cost, such as J-UNIWARD in this work,
and then updates the cost iteratively based on adversarial examples derived
from a series of steganalytic networks until achieving satisfactory results. We
carefully analyze two important factors that would affect the security
performance of the proposed framework, i.e. the percentage of selected
gradients with larger amplitude and the adversarial intensity to modify
embedding cost. The experimental results evaluated on three modern steganalytic
models, including GFR, SCA-GFR and SRNet, show that the proposed framework is
very promising to enhance the security performances of JPEG steganography.
</summary>
    <author>
      <name>Huaxiao Mo</name>
    </author>
    <author>
      <name>Tingting Song</name>
    </author>
    <author>
      <name>Bolin Chen</name>
    </author>
    <author>
      <name>Weiqi Luo</name>
    </author>
    <author>
      <name>Jiwu Huang</name>
    </author>
    <link href="http://arxiv.org/abs/1909.07556v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.07556v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.07730v1</id>
    <updated>2019-09-17T11:43:57Z</updated>
    <published>2019-09-17T11:43:57Z</published>
    <title>Multi-Task Music Representation Learning from Multi-Label Embeddings</title>
    <summary>  This paper presents a novel approach to music representation learning.
Triplet loss based networks have become popular for representation learning in
various multimedia retrieval domains. Yet, one of the most crucial parts of
this approach is the appropriate selection of triplets, which is indispensable,
considering that the number of possible triplets grows cubically. We present an
approach to harness multi-tag annotations for triplet selection, by using
Latent Semantic Indexing to project the tags onto a high-dimensional space.
From this we estimate tag-relatedness to select hard triplets. The approach is
evaluated in a multi-task scenario for which we introduce four large multi-tag
annotations for the Million Song Dataset for the music properties genres,
styles, moods, and themes.
</summary>
    <author>
      <name>Alexander Schindler</name>
    </author>
    <author>
      <name>Peter Knees</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Best Student Paper award</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on Content-Based
  Multimedia Indexing (CBMI2019)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1909.07730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.07730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.12526v1</id>
    <updated>2019-09-27T07:27:04Z</updated>
    <published>2019-09-27T07:27:04Z</published>
    <title>Query by Semantic Sketch</title>
    <summary>  Sketch-based query formulation is very common in image and video retrieval as
these techniques often complement textual retrieval methods that are based on
either manual or machine generated annotations. In this paper, we present a
retrieval approach that allows to query visual media collections by sketching
concept maps, thereby merging sketch-based retrieval with the search for
semantic labels. Users can draw a spatial distribution of different concept
labels, such as "sky", "sea" or "person" and then use these sketches to find
images or video scenes that exhibit a similar distribution of these concepts.
Hence, this approach does not only take the semantic concepts themselves into
account, but also their semantic relations as well as their spatial context.
The efficient vector representation enables efficient retrieval even in large
multimedia collections. We have integrated the semantic sketch query mode into
our retrieval engine vitrivr and demonstrated its effectiveness.
</summary>
    <author>
      <name>Luca Rossetto</name>
    </author>
    <author>
      <name>Ralph Gasser</name>
    </author>
    <author>
      <name>Heiko Schuldt</name>
    </author>
    <link href="http://arxiv.org/abs/1909.12526v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.12526v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.13287v1</id>
    <updated>2019-09-29T14:01:36Z</updated>
    <published>2019-09-29T14:01:36Z</published>
    <title>MG-VAE: Deep Chinese Folk Songs Generation with Specific Regional Style</title>
    <summary>  Regional style in Chinese folk songs is a rich treasure that can be used for
ethnic music creation and folk culture research. In this paper, we propose
MG-VAE, a music generative model based on VAE (Variational Auto-Encoder) that
is capable of capturing specific music style and generating novel tunes for
Chinese folk songs (Min Ge) in a manipulatable way. Specifically, we
disentangle the latent space of VAE into four parts in an adversarial training
way to control the information of pitch and rhythm sequence, as well as of
music style and content. In detail, two classifiers are used to separate style
and content latent space, and temporal supervision is utilized to disentangle
the pitch and rhythm sequence. The experimental results show that the
disentanglement is successful and our model is able to create novel folk songs
with controllable regional styles. To our best knowledge, this is the first
study on applying deep generative model and adversarial training for Chinese
music generation.
</summary>
    <author>
      <name>Jing Luo</name>
    </author>
    <author>
      <name>Xinyu Yang</name>
    </author>
    <author>
      <name>Shulei Ji</name>
    </author>
    <author>
      <name>Juan Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted by the 7th Conference on Sound and Music Technology, 2019,
  Harbin, China</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.13287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.13287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.13689v1</id>
    <updated>2019-09-30T13:34:58Z</updated>
    <published>2019-09-30T13:34:58Z</published>
    <title>Diachronic Cross-modal Embeddings</title>
    <summary>  Understanding the semantic shifts of multimodal information is only possible
with models that capture cross-modal interactions over time. Under this
paradigm, a new embedding is needed that structures visual-textual interactions
according to the temporal dimension, thus, preserving data's original temporal
organisation. This paper introduces a novel diachronic cross-modal embedding
(DCM), where cross-modal correlations are represented in embedding space,
throughout the temporal dimension, preserving semantic similarity at each
instant t. To achieve this, we trained a neural cross-modal architecture, under
a novel ranking loss strategy, that for each multimodal instance, enforces
neighbour instances' temporal alignment, through subspace structuring
constraints based on a temporal alignment window. Experimental results show
that our DCM embedding successfully organises instances over time. Quantitative
experiments, confirm that DCM is able to preserve semantic cross-modal
correlations at each instant t while also providing better alignment
capabilities. Qualitative experiments unveil new ways to browse multimodal
content and hint that multimodal understanding tasks can benefit from this new
embedding.
</summary>
    <author>
      <name>David Semedo</name>
    </author>
    <author>
      <name>Jo√£o Magalh√£es</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3343031.3351036</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3343031.3351036" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ACM MM 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.13689v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.13689v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.13733v1</id>
    <updated>2019-09-30T14:20:06Z</updated>
    <published>2019-09-30T14:20:06Z</published>
    <title>Cross-Modal Subspace Learning with Scheduled Adaptive Margin Constraints</title>
    <summary>  Cross-modal embeddings, between textual and visual modalities, aim to
organise multimodal instances by their semantic correlations. State-of-the-art
approaches use maximum-margin methods, based on the hinge-loss, to enforce a
constant margin m, to separate projections of multimodal instances from
different categories. In this paper, we propose a novel scheduled adaptive
maximum-margin (SAM) formulation that infers triplet-specific constraints
during training, therefore organising instances by adaptively enforcing
inter-category and inter-modality correlations. This is supported by a
scheduled adaptive margin function, that is smoothly activated, replacing a
static margin by an adaptively inferred one reflecting triplet-specific
semantic correlations while accounting for the incremental learning behaviour
of neural networks to enforce category cluster formation and enforcement.
Experiments on widely used datasets show that our model improved upon
state-of-the-art approaches, by achieving a relative improvement of up to
~12.5% over the second best method, thus confirming the effectiveness of our
scheduled adaptive margin formulation.
</summary>
    <author>
      <name>David Semedo</name>
    </author>
    <author>
      <name>Jo√£o Magalh√£es</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3343031.3351030</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3343031.3351030" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in ACM MM 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.13733v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.13733v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.01403v2</id>
    <updated>2020-02-20T03:23:42Z</updated>
    <published>2020-01-06T05:20:40Z</published>
    <title>Joint Communication and Computational Resource Allocation for QoE-driven
  Point Cloud Video Streaming</title>
    <summary>  Point cloud video is the most popular representation of hologram, which is
the medium to precedent natural content in VR/AR/MR and is expected to be the
next generation video. Point cloud video system provides users immersive
viewing experience with six degrees of freedom and has wide applications in
many fields such as online education, entertainment. To further enhance these
applications, point cloud video streaming is in critical demand. The inherent
challenges lie in the large size by the necessity of recording the
three-dimensional coordinates besides color information, and the associated
high computation complexity of encoding. To this end, this paper proposes a
communication and computation resource allocation scheme for QoE-driven point
cloud video streaming. In particular, we maximize system resource utilization
by selecting different quantities, transmission forms and quality level tiles
to maximize the quality of experience. Extensive simulations are conducted and
the simulation results show the superior performance over the existing schemes
</summary>
    <author>
      <name>Jie Li</name>
    </author>
    <author>
      <name>Cong Zhang</name>
    </author>
    <author>
      <name>Zhi Liu</name>
    </author>
    <author>
      <name>Wei Sun</name>
    </author>
    <author>
      <name>Qiyue Li</name>
    </author>
    <link href="http://arxiv.org/abs/2001.01403v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01403v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.03536v1</id>
    <updated>2020-01-10T16:06:24Z</updated>
    <published>2020-01-10T16:06:24Z</published>
    <title>QoE-driven Coupled Uplink and Downlink Rate Adaptation for 360-degree
  Video Live Streaming</title>
    <summary>  360-degree video provides an immersive 360-degree viewing experience and has
been widely used in many areas. The 360-degree video live streaming systems
involve capturing, compression, uplink (camera to video server) and downlink
(video server to user) transmissions. However, few studies have jointly
investigated such complex systems, especially the rate adaptation for the
coupled uplink and downlink in the 360-degree video streaming under limited
bandwidth constraints. In this letter, we propose a quality of experience
(QoE)-driven 360-degree video live streaming system, in which a video server
performs rate adaptation based on the uplink and downlink bandwidths and
information concerning each user's real-time field-of-view (FOV). We formulate
it as a nonlinear integer programming problem and propose an algorithm, which
combines the Karush-Kuhn-Tucker (KKT) condition and branch and bound method, to
solve it. The numerical results show that the proposed optimization model can
improve users' QoE significantly in comparison with other baseline schemes.
</summary>
    <author>
      <name>Jie Li</name>
    </author>
    <author>
      <name>Ransheng Feng</name>
    </author>
    <author>
      <name>Zhi Liu</name>
    </author>
    <author>
      <name>Wei Sun</name>
    </author>
    <author>
      <name>Qiyue Li</name>
    </author>
    <link href="http://arxiv.org/abs/2001.03536v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.03536v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.03542v1</id>
    <updated>2020-01-10T16:15:06Z</updated>
    <published>2020-01-10T16:15:06Z</published>
    <title>Exploratory Study on User's Dynamic Visual Acuity and Quality Perception
  of Impaired Images</title>
    <summary>  In this paper we assess the impact of head movement on user's visual acuity
and their quality perception of impaired images. There are physical limitations
on the amount of visual information a person can perceive and physical
limitations regarding the speed at which our body, and as a consequence our
head, can explore a scene. In these limitations lie fundamental solutions for
the communication of multimedia systems. As such, subjects were asked to
evaluate the perceptual quality of static images presented on a TV screen while
their head was in a dynamic (moving) state. The idea is potentially applicable
to virtual reality applications and therefore, we also measured the image
quality perception of each subject on a head mounted display. Experiments show
the significant decrease in visual acuity and quality perception when the
user's head is not static, and give an indication on how much the quality can
be reduced without the user noticing any impairments.
</summary>
    <author>
      <name>Jolien De Letter</name>
    </author>
    <author>
      <name>Anissa All</name>
    </author>
    <author>
      <name>Lieven De Marez</name>
    </author>
    <author>
      <name>Vasileios Avramelos</name>
    </author>
    <author>
      <name>Peter Lambert</name>
    </author>
    <author>
      <name>Glenn Van Wallendael</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.03542v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.03542v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.04580v1</id>
    <updated>2020-01-14T01:04:59Z</updated>
    <published>2020-01-14T01:04:59Z</published>
    <title>Distortion Agnostic Deep Watermarking</title>
    <summary>  Watermarking is the process of embedding information into an image that can
survive under distortions, while requiring the encoded image to have little or
no perceptual difference from the original image. Recently, deep learning-based
methods achieved impressive results in both visual quality and message payload
under a wide variety of image distortions. However, these methods all require
differentiable models for the image distortions at training time, and may
generalize poorly to unknown distortions. This is undesirable since the types
of distortions applied to watermarked images are usually unknown and
non-differentiable. In this paper, we propose a new framework for
distortion-agnostic watermarking, where the image distortion is not explicitly
modeled during training. Instead, the robustness of our system comes from two
sources: adversarial training and channel coding. Compared to training on a
fixed set of distortions and noise levels, our method achieves comparable or
better results on distortions available during training, and better performance
on unknown distortions.
</summary>
    <author>
      <name>Xiyang Luo</name>
    </author>
    <author>
      <name>Ruohan Zhan</name>
    </author>
    <author>
      <name>Huiwen Chang</name>
    </author>
    <author>
      <name>Feng Yang</name>
    </author>
    <author>
      <name>Peyman Milanfar</name>
    </author>
    <link href="http://arxiv.org/abs/2001.04580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.04580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.06466v2</id>
    <updated>2020-07-16T13:45:30Z</updated>
    <published>2020-01-17T18:35:31Z</published>
    <title>Low-latency Cloud-based Volumetric Video Streaming Using Head Motion
  Prediction</title>
    <summary>  Volumetric video is an emerging key technology for immersive representation
of 3D spaces and objects. Rendering volumetric video requires lots of
computational power which is challenging especially for mobile devices. To
mitigate this, we developed a streaming system that renders a 2D view from the
volumetric video at a cloud server and streams a 2D video stream to the client.
However, such network-based processing increases the motion-to-photon (M2P)
latency due to the additional network and processing delays. In order to
compensate the added latency, prediction of the future user pose is necessary.
We developed a head motion prediction model and investigated its potential to
reduce the M2P latency for different look-ahead times. Our results show that
the presented model reduces the rendering errors caused by the M2P latency
compared to a baseline system in which no prediction is performed.
</summary>
    <author>
      <name>Serhan G√ºl</name>
    </author>
    <author>
      <name>Dimitri Podborski</name>
    </author>
    <author>
      <name>Thomas Buchholz</name>
    </author>
    <author>
      <name>Thomas Schierl</name>
    </author>
    <author>
      <name>Cornelius Hellge</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3386290.3396933</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3386290.3396933" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">30th ACM Workshop on Network and Operating Systems Support for
  Digital Audio and Video (NOSSDAV) 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2001.06466v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.06466v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.07494v1</id>
    <updated>2020-01-21T13:04:28Z</updated>
    <published>2020-01-21T13:04:28Z</published>
    <title>Evaluation of a course mediatised with Xerte</title>
    <summary>  Interactive multimedia educational content has recently been of interest to
attract attention on the learner and increase understanding by the latter. In
parallel several open source authoring tools offer a quick and easy production
of this type of content. As such, our contribution is to mediatize a course
i.e. 'English' with the authoring system 'Xerte' which is intended both for
simple users and developers in ActionScript. An experiment of course is
conducted on a sample of a private school's students. At the end of this
experience, we administered a questionnaire to evaluate the device, the results
obtained, evidenced by the favorable reception of interactive multimedia
integration in educational content.
</summary>
    <author>
      <name>Ghalia Merzougui</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TECHN√â - EA 6316</arxiv:affiliation>
    </author>
    <author>
      <name>Roumaissa Dehkal</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TECHN√â - EA 6316</arxiv:affiliation>
    </author>
    <author>
      <name>Maheiddine Djoudi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TECHN√â - EA 6316</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference of Computing for Engineering and Sciences
  (ICCES'2015), Jul 2015, Istanbul,, Turkey</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2001.07494v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.07494v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.04511v1</id>
    <updated>2020-08-11T04:56:53Z</updated>
    <published>2020-08-11T04:56:53Z</published>
    <title>Content Format and Quality of Experience in Virtual Reality</title>
    <summary>  In this paper, we investigate three forms of virtual reality content
production and consumption. Namely, 360 stereoscopic video, the combination of
a 3D environment with a video billboard for dynamic elements, and a full 3D
rendered scene. On one hand, video based techniques facilitate the acquisition
of content, but they can limit the experience of the user since the content is
captured from a fixed point of view. On the other hand, 3D content allows for
point of view translation, but real-time photorealistic rendering is not
trivial and comes at high production and processing costs. We also compare the
two extremes with an approach that combines dynamic video elements with a 3D
virtual environment. We discuss the advantages and disadvantages of these
systems, and present the result of a user study with 24 participants. In the
study, we evaluated the quality of experience, including presence, simulation
sickness and participants' assessment of content quality, of three versions of
a cinematic segment with two actors. We found that, in this context, mixing
video and 3D content produced the best experience.
</summary>
    <author>
      <name>Henrique Galvan Debarba</name>
    </author>
    <author>
      <name>Mario Montagud</name>
    </author>
    <author>
      <name>Sylvain Chagu√©</name>
    </author>
    <author>
      <name>Javier Lajara</name>
    </author>
    <author>
      <name>Ignacio Lacosta</name>
    </author>
    <author>
      <name>Sergi Fernandez Langa</name>
    </author>
    <author>
      <name>Caecilia Charbonnier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.04511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.04511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.05672v2</id>
    <updated>2020-08-28T01:03:32Z</updated>
    <published>2020-08-13T03:43:35Z</published>
    <title>JQF: Optimal JPEG Quantization Table Fusion by Simulated Annealing on
  Texture Images and Predicting Textures</title>
    <summary>  JPEG has been a widely used lossy image compression codec for nearly three
decades. The JPEG standard allows to use customized quantization table;
however, it's still a challenging problem to find an optimal quantization table
within acceptable computational cost. This work tries to solve the dilemma of
balancing between computational cost and image specific optimality by
introducing a new concept of texture mosaic images. Instead of optimizing a
single image or a collection of representative images, the simulated annealing
technique is applied to texture mosaic images to search for an optimal
quantization table for each texture category. We use pre-trained VGG-16 CNN
model to learn those texture features and predict the new image's texture
distribution, then fuse optimal texture tables to come out with an image
specific optimal quantization table. On the Kodak dataset with the quality
setting $Q=95$, our experiment shows a size reduction of 23.5% over the JPEG
standard table with a slightly 0.35% FSIM decrease, which is visually
unperceivable. The proposed JQF method achieves per image optimality for JPEG
encoding with less than one second additional timing cost. The online demo is
available at https://matthorn.s3.amazonaws.com/JQF/qtbl_vis.html
</summary>
    <author>
      <name>Chen-Hsiu Huang</name>
    </author>
    <author>
      <name>Ja-Ling Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2008.05672v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.05672v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.09559v1</id>
    <updated>2020-08-21T15:55:32Z</updated>
    <published>2020-08-21T15:55:32Z</published>
    <title>NANCY: Neural Adaptive Network Coding methodologY for video distribution
  over wireless networks</title>
    <summary>  This paper presents NANCY, a system that generates adaptive bit rates (ABR)
for video and adaptive network coding rates (ANCR) using reinforcement learning
(RL) for video distribution over wireless networks. NANCY trains a neural
network model with rewards formulated as quality of experience (QoE) metrics.
It performs joint optimization in order to select: (i) adaptive bit rates for
future video chunks to counter variations in available bandwidth and (ii)
adaptive network coding rates to encode the video chunk slices to counter
packet losses in wireless networks. We present the design and implementation of
NANCY, and evaluate its performance compared to state-of-the-art video rate
adaptation algorithms including Pensieve and robustMPC. Our results show that
NANCY provides 29.91% and 60.34% higher average QoE than Pensieve and
robustMPC, respectively.
</summary>
    <author>
      <name>Paresh Saxena</name>
    </author>
    <author>
      <name>Mandan Naresh</name>
    </author>
    <author>
      <name>Manik Gupta</name>
    </author>
    <author>
      <name>Anirudh Achanta</name>
    </author>
    <author>
      <name>Sastri Kota</name>
    </author>
    <author>
      <name>Smrati Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in Globecom, 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.09559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.09559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.11042v1</id>
    <updated>2020-08-25T14:10:42Z</updated>
    <published>2020-08-25T14:10:42Z</published>
    <title>ByeGlassesGAN: Identity Preserving Eyeglasses Removal for Face Images</title>
    <summary>  In this paper, we propose a novel image-to-image GAN framework for eyeglasses
removal, called ByeGlassesGAN, which is used to automatically detect the
position of eyeglasses and then remove them from face images. Our ByeGlassesGAN
consists of an encoder, a face decoder, and a segmentation decoder. The encoder
is responsible for extracting information from the source face image, and the
face decoder utilizes this information to generate glasses-removed images. The
segmentation decoder is included to predict the segmentation mask of eyeglasses
and completed face region. The feature vectors generated by the segmentation
decoder are shared with the face decoder, which facilitates better
reconstruction results. Our experiments show that ByeGlassesGAN can provide
visually appealing results in the eyeglasses-removed face images even for
semi-transparent color eyeglasses or glasses with glare. Furthermore, we
demonstrate significant improvement in face recognition accuracy for face
images with glasses by applying our method as a pre-processing step in our face
recognition experiment.
</summary>
    <author>
      <name>Yu-Hui Lee</name>
    </author>
    <author>
      <name>Shang-Hong Lai</name>
    </author>
    <link href="http://arxiv.org/abs/2008.11042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.11420v1</id>
    <updated>2020-08-26T07:21:54Z</updated>
    <published>2020-08-26T07:21:54Z</published>
    <title>Low Complexity Trellis-Coded Quantization in Versatile Video Coding</title>
    <summary>  The forthcoming Versatile Video Coding (VVC) standard adopts the
trellis-coded quantization, which leverages the delicate trellis graph to map
the quantization candidates within one block into the optimal path. Despite the
high compression efficiency, the complex trellis search with soft decision
quantization may hinder the applications due to high complexity and low
throughput capacity. To reduce the complexity, in this paper, we propose a low
complexity trellis-coded quantization scheme in a scientifically sound way with
theoretical modeling of the rate and distortion. As such, the trellis departure
point can be adaptively adjusted, and unnecessarily visited branches are
accordingly pruned, leading to the shrink of total trellis stages and
simplification of transition branches. Extensive experimental results on the
VVC test model show that the proposed scheme is effective in reducing the
encoding complexity by 11% and 5% with all intra and random access
configurations, respectively, at the cost of only 0.11% and 0.05% BD-Rate
increase. Meanwhile, on average 24% and 27% quantization time savings can be
achieved under all intra and random access configurations. Due to the excellent
performance, the VVC test model has adopted one implementation of the proposed
scheme.
</summary>
    <author>
      <name>Meng Wang</name>
    </author>
    <author>
      <name>Shiqi Wang</name>
    </author>
    <author>
      <name>Junru Li</name>
    </author>
    <author>
      <name>Li Zhang</name>
    </author>
    <author>
      <name>Yue Wang</name>
    </author>
    <author>
      <name>Siwei Ma</name>
    </author>
    <author>
      <name>Sam Kwong</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TIP.2021.3051460</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TIP.2021.3051460" rel="related"/>
    <link href="http://arxiv.org/abs/2008.11420v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.11420v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.12017v1</id>
    <updated>2020-08-27T09:38:46Z</updated>
    <published>2020-08-27T09:38:46Z</published>
    <title>Quality of Service (QoS): Measurements of Video Streaming</title>
    <summary>  Nowadays video streaming is growing over the social clouds, where end-users
always want to share High Definition (HD) videos among friends. Mostly videos
were recorded via smartphones and other HD devices and short time videos have a
big file size. The big file size of videos required high bandwidth to upload
and download on the Internet and also required more time to load in a web page
for play. So avoiding this problem social cloud compress videos during the
upload for smooth play and fast loading in a web page. Compression decreases
the video quality which also decreases the quality of experience of end users.
In this paper we measure the QoS of different standard video file formats on
social clouds; they varied from each other in resolution, audio/video bitrate,
and storage size.
</summary>
    <author>
      <name>Sajida Karim</name>
    </author>
    <author>
      <name>Hui He</name>
    </author>
    <author>
      <name>Asif Ali Laghari</name>
    </author>
    <author>
      <name>Hina Madiha</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.3987056</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.3987056" rel="related"/>
    <link href="http://arxiv.org/abs/2008.12017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.12017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PF" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.12408v1</id>
    <updated>2020-08-27T23:40:22Z</updated>
    <published>2020-08-27T23:40:22Z</published>
    <title>Rate distortion optimization over large scale video corpus with machine
  learning</title>
    <summary>  We present an efficient codec-agnostic method for bitrate allocation over a
large scale video corpus with the goal of minimizing the average bitrate
subject to constraints on average and minimum quality. Our method clusters the
videos in the corpus such that videos within one cluster have similar
rate-distortion (R-D) characteristics. We train a support vector machine
classifier to predict the R-D cluster of a video using simple video complexity
features that are computationally easy to obtain. The model allows us to
classify a large sample of the corpus in order to estimate the distribution of
the number of videos in each of the clusters. We use this distribution to find
the optimal encoder operating point for each R-D cluster. Experiments with AV1
encoder show that our method can achieve the same average quality over the
corpus with $22\%$ less average bitrate.
</summary>
    <author>
      <name>Sam John</name>
    </author>
    <author>
      <name>Akshay Gadde</name>
    </author>
    <author>
      <name>Balu Adsumilli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in 2020 IEEE International Conference on Image Processing
  (ICIP)</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.12408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.12408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.12832v1</id>
    <updated>2020-08-28T20:04:23Z</updated>
    <published>2020-08-28T20:04:23Z</published>
    <title>Semantics Preserving Hierarchy based Retrieval of Indian heritage
  monuments</title>
    <summary>  Monument classification can be performed on the basis of their appearance and
shape from coarse to fine categories. Although there is much semantic
information present in the monuments which is reflected in the eras they were
built, its type or purpose, the dynasty which established it, etc.
Particularly, Indian subcontinent exhibits a huge deal of variation in terms of
architectural styles owing to its rich cultural heritage. In this paper, we
propose a framework that utilizes hierarchy to preserve semantic information
while performing image classification or image retrieval. We encode the learnt
deep embeddings to construct a dictionary of images and then utilize a
re-ranking framework on the the retrieved results using DeLF features. The
semantic information preserved in these embeddings helps to classify unknown
monuments at higher level of granularity in hierarchy. We have curated a large,
novel Indian heritage monuments dataset comprising of images of historical,
cultural and religious importance with subtypes of eras, dynasties and
architectural styles. We demonstrate the performance of the proposed framework
in image classification and retrieval tasks and compare it with other competing
methods on this dataset.
</summary>
    <author>
      <name>Ronak Gupta</name>
    </author>
    <author>
      <name>Prerana Mukherjee</name>
    </author>
    <author>
      <name>Brejesh Lall</name>
    </author>
    <author>
      <name>Varshul Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in Structuring and Understanding of Multimedia heritAge
  Contents (SUMAC2020), ACM Multimedia Workshops, Seattle, United States,
  October 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.12832v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.12832v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.03523v1</id>
    <updated>2020-09-08T05:26:16Z</updated>
    <published>2020-09-08T05:26:16Z</published>
    <title>An optimal mode selection algorithm for scalable video coding</title>
    <summary>  Scalable video coding (SVC) is extended from its predecessor advanced video
coding (AVC) because of its flexible transmission to all type of gadgets.
However, SVC is more flexible and scalable than AVC, but it is more complex in
determining the computations than AVC. The traditional full search method in
the standard H.264 SVC consumes more encoding time for computation. This
complexity in computation need to be reduced and many fast mode decision (FMD)
algorithms were developed, but many fail to balance in all the three measures
such as peak signal to noise ratio (PSNR), encoding time and bit rate. In this
paper, the proposed optimal mode selection algorithm based on the orientation
of pixels achieves better time saving, good PSNR and coding efficiency. The
proposed algorithm is compared with the standard H.264 JSVM reference software
and found to be 57.44% time saving, 0.43 dB increments in PSNR and 0.23%
compression in bit rate.
</summary>
    <author>
      <name>L. Balaji</name>
    </author>
    <author>
      <name>K. K. Thyagharajan</name>
    </author>
    <author>
      <name>C. Raja</name>
    </author>
    <author>
      <name>A. Dhanalakshmi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1504/IJCVR.2020.105685</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1504/IJCVR.2020.105685" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computational Vision and Robotics
  (IJCVR), Vol. 10, No. 2, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2009.03523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.03523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.2; E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.04646v1</id>
    <updated>2020-09-10T02:41:57Z</updated>
    <published>2020-09-10T02:41:57Z</published>
    <title>Key-Point Sequence Lossless Compression for Intelligent Video Analysis</title>
    <summary>  Feature coding has been recently considered to facilitate intelligent video
analysis for urban computing. Instead of raw videos, extracted features in the
front-end are encoded and transmitted to the back-end for further processing.
In this article, we present a lossless key-point sequence compression approach
for efficient feature coding. The essence of this predict-and-encode strategy
is to eliminate the spatial and temporal redundancies of key points in videos.
Multiple prediction modes with an adaptive mode selection method are proposed
to handle key-point sequences with various structures and motion. Experimental
results validate the effectiveness of the proposed scheme on four types of
widely used key-point sequences in video analysis.
</summary>
    <author>
      <name>Weiyao Lin</name>
    </author>
    <author>
      <name>Xiaoyi He</name>
    </author>
    <author>
      <name>Wenrui Dai</name>
    </author>
    <author>
      <name>John See</name>
    </author>
    <author>
      <name>Tushar Shinde</name>
    </author>
    <author>
      <name>Hongkai Xiong</name>
    </author>
    <author>
      <name>Lingyu Duan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/MMUL.2020.2990863</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/MMUL.2020.2990863" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted version for IEEE MultiMedia Journal</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE MultiMedia, vol. 27, no. 3, pp. 12-22, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2009.04646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.04646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.07816v1</id>
    <updated>2020-09-16T17:19:23Z</updated>
    <published>2020-09-16T17:19:23Z</published>
    <title>A Human-Computer Duet System for Music Performance</title>
    <summary>  Virtual musicians have become a remarkable phenomenon in the contemporary
multimedia arts. However, most of the virtual musicians nowadays have not been
endowed with abilities to create their own behaviors, or to perform music with
human musicians. In this paper, we firstly create a virtual violinist, who can
collaborate with a human pianist to perform chamber music automatically without
any intervention. The system incorporates the techniques from various fields,
including real-time music tracking, pose estimation, and body movement
generation. In our system, the virtual musician's behavior is generated based
on the given music audio alone, and such a system results in a low-cost,
efficient and scalable way to produce human and virtual musicians'
co-performance. The proposed system has been validated in public concerts.
Objective quality assessment approaches and possible ways to systematically
improve the system are also discussed.
</summary>
    <author>
      <name>Yuen-Jen Lin</name>
    </author>
    <author>
      <name>Hsuan-Kai Kao</name>
    </author>
    <author>
      <name>Yih-Chih Tseng</name>
    </author>
    <author>
      <name>Ming Tsai</name>
    </author>
    <author>
      <name>Li Su</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3394171.3413921</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3394171.3413921" rel="related"/>
    <link href="http://arxiv.org/abs/2009.07816v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.07816v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.08015v1</id>
    <updated>2020-09-17T02:10:05Z</updated>
    <published>2020-09-17T02:10:05Z</published>
    <title>Temporally Guided Music-to-Body-Movement Generation</title>
    <summary>  This paper presents a neural network model to generate virtual violinist's
3-D skeleton movements from music audio. Improved from the conventional
recurrent neural network models for generating 2-D skeleton data in previous
works, the proposed model incorporates an encoder-decoder architecture, as well
as the self-attention mechanism to model the complicated dynamics in body
movement sequences. To facilitate the optimization of self-attention model,
beat tracking is applied to determine effective sizes and boundaries of the
training examples. The decoder is accompanied with a refining network and a
bowing attack inference mechanism to emphasize the right-hand behavior and
bowing attack timing. Both objective and subjective evaluations reveal that the
proposed model outperforms the state-of-the-art methods. To the best of our
knowledge, this work represents the first attempt to generate 3-D violinists'
body movements considering key features in musical body movement.
</summary>
    <author>
      <name>Hsuan-Kai Kao</name>
    </author>
    <author>
      <name>Li Su</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3394171.3413848</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3394171.3413848" rel="related"/>
    <link href="http://arxiv.org/abs/2009.08015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.08015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.10708v1</id>
    <updated>2020-09-22T17:53:15Z</updated>
    <published>2020-09-22T17:53:15Z</published>
    <title>H.264/SVC Mode Decision Based on Mode Correlation and Desired Mode List</title>
    <summary>  The design of video encoders involves the implementation of fast mode
decision (FMD) algorithm to reduce computation complexity while maintaining the
performance of the coding. Although H.264/scalable video coding (SVC) achieves
high scalability and coding efficiency, it also has high complexity in
implementing its exhaustive computation. In this paper, a novel algorithm is
proposed to reduce the redundant candidate modes by making use of the
correlation among layers. The desired mode list is created based on the
probability to be the best mode for each block in the base layer and a
candidate mode selection in the enhancement layer by the correlations of modes
among the reference frame and current frame. Our algorithm is implemented in
joint scalable video model (JSVM) 9.19.15 reference software and the
performance is evaluated based on the average encoding time, peak signal to
noise ratio (PSNR) and bit rate. The experimental results show 41.89%
improvement in encoding time with minimal loss of 0.02dB in PSNR and 0.05%
increase in bit rate.
</summary>
    <author>
      <name>L. Balaji</name>
    </author>
    <author>
      <name>K. K. Thyagharajan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11633-014-0830-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11633-014-0830-5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 13 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">11(5), October 2014, 510-516</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2009.10708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.10708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="E.4; I.4.2; H.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.11129v1</id>
    <updated>2020-09-23T13:12:30Z</updated>
    <published>2020-09-23T13:12:30Z</published>
    <title>Cosine Similarity of Multimodal Content Vectors for TV Programmes</title>
    <summary>  Multimodal information originates from a variety of sources: audiovisual
files, textual descriptions, and metadata. We show how one can represent the
content encoded by each individual source using vectors, how to combine the
vectors via middle and late fusion techniques, and how to compute the semantic
similarities between the contents. Our vectorial representations are built from
spectral features and Bags of Audio Words, for audio, LSI topics and Doc2vec
embeddings for subtitles, and the categorical features, for metadata. We
implement our model on a dataset of BBC TV programmes and evaluate the fused
representations to provide recommendations. The late fused similarity matrices
significantly improve the precision and diversity of recommendations.
</summary>
    <author>
      <name>Saba Nazir</name>
    </author>
    <author>
      <name>Taner Cagali</name>
    </author>
    <author>
      <name>Chris Newell</name>
    </author>
    <author>
      <name>Mehrnoosh Sadrzadeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 figure, Machine Learning for Media Discovery (ML4MD)
  Workshop at ICML 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.11129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.11129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.4.7; I.7.0; H.5.1; H.5.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.13216v1</id>
    <updated>2020-09-28T11:10:27Z</updated>
    <published>2020-09-28T11:10:27Z</published>
    <title>Traffic model of LTE using maximum flow algorithm with binary search
  technique</title>
    <summary>  Inrecent time a rapid increase in the number of smart devices and user
applications have generated an intensity volume of data traffic from/to a
cellular network. So the Long Term Evaluation(LTE)network is facing some
issuesdifficulties ofthebase station and infrastructure in terms of upgrade and
configuration becausethere is no concept of BSC (Base Station Controller) of 2G
and RNC (Radio Network Controller) of 3G to control several BTS/NB. Only 4G
(LTE) all the eNBs areinterconnected for traffic flow from UE (user equipment)
to core switch. Determination of capacity of a linkof such a network is a
challenging job since each node offers its own traffic andat the same time
conveys traffic of other nodes.In this paper, we apply maximum flow algorithm
including the binary search techniqueto solve the traffic flow of radio
networkandinterconnected eNBs of the LTE network. The throughput of the LTE
network shown graphically under the QPSK and 16-QAM
</summary>
    <author>
      <name>Md. Zahurul Haque</name>
    </author>
    <author>
      <name>Md. Rafiqul Isla</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5281/zenodo.4159304</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5281/zenodo.4159304" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Information
  Security(IJCSIS) , Vol. 18, No. 9, September 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2009.13216v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.13216v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.13737v1</id>
    <updated>2020-09-29T02:54:52Z</updated>
    <published>2020-09-29T02:54:52Z</published>
    <title>Sequential Reinforced 360-Degree Video Adaptive Streaming with
  Cross-user Attentive Network</title>
    <summary>  In the tile-based 360-degree video streaming, predicting user's future
viewpoints and developing adaptive bitrate (ABR) algorithms are essential for
optimizing user's quality of experience (QoE). Traditional single-user based
viewpoint prediction methods fail to achieve good performance in long-term
prediction, and the recently proposed reinforcement learning (RL) based ABR
schemes applied in traditional video streaming can not be directly applied in
the tile-based 360-degree video streaming due to the exponential action space.
Therefore, we propose a sequential reinforced 360-degree video streaming scheme
with cross-user attentive network. Firstly, considering different users may
have the similar viewing preference on the same video, we propose a cross-user
attentive network (CUAN), boosting the performance of long-term viewpoint
prediction by selectively utilizing cross-user information. Secondly, we
propose a sequential RL-based (360SRL) ABR approach, transforming action space
size of each decision step from exponential to linear via introducing a
sequential decision structure. We evaluate the proposed CUAN and 360SRL using
trace-driven experiments and experimental results demonstrate that CUAN and
360SRL outperform existing viewpoint prediction and ABR approaches with a
noticeable margin.
</summary>
    <author>
      <name>Jun Fu</name>
    </author>
    <author>
      <name>Zhibo Chen</name>
    </author>
    <author>
      <name>Xiaoming Chen</name>
    </author>
    <author>
      <name>Weiping Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.13737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.13737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.14165v2</id>
    <updated>2020-11-04T01:33:21Z</updated>
    <published>2020-09-29T17:28:15Z</published>
    <title>Performance of AV1 Real-Time Mode</title>
    <summary>  With COVID-19, the interest for digital interactions has raised, putting in
turn real-time (or low-latency) codecs into a new light. Most of the codec
research has been traditionally focusing on coding efficiency, while very
little literature exist on real-time codecs. It is shown how the speed at which
content is made available impacts both latency and throughput. The authors
introduce a new test set up, integrating a paced reader, which allows to run
codec in the same condition as real-time media capture. Quality measurements
using VMAF, as well as multiple speed measurements are made on encoding of HD
and full HD video sequences, both at 25 fps and 50 fps to compare the
respective performances of several implementations of the H.264, H.265, VP8,
VP9 and AV1 codecs.
</summary>
    <author>
      <name>Ludovic Roux</name>
    </author>
    <author>
      <name>Alexandre Gouaillard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.14165v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.14165v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.00302v1</id>
    <updated>2020-10-01T11:05:47Z</updated>
    <published>2020-10-01T11:05:47Z</published>
    <title>An authorship protection technology for electronic documents based on
  image watermarking</title>
    <summary>  In the field of information technology, information security technologies
hold a special place. They ensure the security of the use of information
technology. One of the urgent tasks is the protection of electronic documents
during their transfer in information systems. This paper proposes a technology
for protecting electronic documents containing digital images. The main idea is
that the electronic document authorship protection can be implemented by
digital watermark embedding in the images that are contained in this document.
The paper considers three cases of using the proposed technology: full copying
of an electronic document, copying of images contained in the document, and
copying of text. It is shown that in all three cases the authorship
confirmation can be successfully implemented. Computational experiments are
conducted with robust watermarking algorithms that can be used within the
technology. A scenario of technology implementation is proposed, which provides
for the joint use of different class algorithms.
</summary>
    <author>
      <name>Anna Melman</name>
    </author>
    <author>
      <name>Oleg Evsutin</name>
    </author>
    <author>
      <name>Alexander Shelupanov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.00302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.00302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.02015v1</id>
    <updated>2020-10-05T13:43:46Z</updated>
    <published>2020-10-05T13:43:46Z</published>
    <title>Combined Hapto-Visual and Auditory Rendering of Cultural Heritage
  Objects</title>
    <summary>  In this work, we develop a multi-modal rendering framework comprising of
hapto-visual and auditory data. The prime focus is to haptically render point
cloud data representing virtual 3-D models of cultural significance and also to
handle their affine transformations. Cultural heritage objects could
potentially be very large and one may be required to render the object at
various scales of details. Further, surface effects such as texture and
friction are incorporated in order to provide a realistic haptic perception to
the users. Moreover, the proposed framework includes an appropriate sound
synthesis to bring out the acoustic properties of the object. It also includes
a graphical user interface with varied options such as choosing the desired
orientation of 3-D objects and selecting the desired level of spatial
resolution adaptively at runtime. A fast, point proxy-based haptic rendering
technique is proposed with proxy update loop running 100 times faster than the
required haptic update frequency of 1 kHz. The surface properties are
integrated in the system by applying a bilateral filter on the depth data of
the virtual 3-D models. Position dependent sound synthesis is incorporated with
the incorporation of appropriate audio clips.
</summary>
    <author>
      <name>Praseedha Krishnan Aniyath</name>
    </author>
    <author>
      <name>Sreeni Kamalalayam Gopalan</name>
    </author>
    <author>
      <name>Priyadarshini K</name>
    </author>
    <author>
      <name>Subhasis Chaudhuri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to ACCVw 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.02015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.02015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.03169v1</id>
    <updated>2020-10-05T14:32:47Z</updated>
    <published>2020-10-05T14:32:47Z</published>
    <title>Haptic Rendering of Cultural Heritage Objects at Different Scales</title>
    <summary>  In this work, we address the issue of a virtual representation of objects of
cultural heritage for haptic interaction. Our main focus is to provide haptic
access to artistic objects of any physical scale to the differently-abled
people. This is a low-cost system and, in conjunction with a stereoscopic
visual display, gives a better immersive experience even to the sighted
persons. To achieve this, we propose a simple multilevel, proxy-based
hapto-visual rendering technique for point cloud data, which includes the
much-desired scalability feature which enables the users to change the scale of
the objects adaptively during the haptic interaction. For the proposed haptic
rendering technique, the proxy updation loop runs at a rate 100 times faster
than the required haptic updation frequency of 1KHz. We observe that this
functionality augments very well with the realism of the experience.
</summary>
    <author>
      <name>Sreeni K. G</name>
    </author>
    <author>
      <name>Priyadarshini K</name>
    </author>
    <author>
      <name>Praseedha A. K</name>
    </author>
    <author>
      <name>Subhasis Chaudhuri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to EuroHaptics. arXiv admin note: text overlap with
  arXiv:2010.02015</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.03169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.03169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.04645v1</id>
    <updated>2020-10-09T15:39:08Z</updated>
    <published>2020-10-09T15:39:08Z</published>
    <title>MPEG Media Enablers For Richer XR Experiences</title>
    <summary>  With the advent of immersive media applications, the requirements for the
representation and the consumption of such content has dramatically increased.
The ever-increasing size of the media asset combined with the stringent
motion-to-photon latency requirement makes the equation of a high quality of
experience for XR streaming services difficult to solve. The MPEG-I standards
aim at facilitating the wide deployment of immersive applications. This paper
describes part 13, Video Decoding Interface, and part 14, Scene Description for
MPEG Media of MPEG-I which address decoder management and the virtual scene
composition, respectively. These new parts intend to make complex media
rendering operations and hardware resources management hidden from the
application, hence lowering the barrier for XR application to become mainstream
and accessible to XR experience developers and designers. Both parts are
expected to be published by ISO at the end of 2021.
</summary>
    <author>
      <name>Emmanuel Thomas</name>
    </author>
    <author>
      <name>Emmanouil Potetsianakis</name>
    </author>
    <author>
      <name>Thomas Stockhammer</name>
    </author>
    <author>
      <name>Imed Bouazizi</name>
    </author>
    <author>
      <name>Mary-Luc Champel</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IBC (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.04645v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.04645v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.3.2; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.04676v1</id>
    <updated>2020-10-09T16:53:16Z</updated>
    <published>2020-10-09T16:53:16Z</published>
    <title>A Clustering-Based Method for Automatic Educational Video Recommendation
  Using Deep Face-Features of Lecturers</title>
    <summary>  Discovering and accessing specific content within educational video bases is
a challenging task, mainly because of the abundance of video content and its
diversity. Recommender systems are often used to enhance the ability to find
and select content. But, recommendation mechanisms, especially those based on
textual information, exhibit some limitations, such as being error-prone to
manually created keywords or due to imprecise speech recognition. This paper
presents a method for generating educational video recommendation using deep
face-features of lecturers without identifying them. More precisely, we use an
unsupervised face clustering mechanism to create relations among the videos
based on the lecturer's presence. Then, for a selected educational video taken
as a reference, we recommend the ones where the presence of the same lecturers
is detected. Moreover, we rank these recommended videos based on the amount of
time the referenced lecturers were present. For this task, we achieved a mAP
value of 99.165%.
</summary>
    <author>
      <name>Paulo R. C. Mendes</name>
    </author>
    <author>
      <name>Eduardo S. Vieira</name>
    </author>
    <author>
      <name>√Ålan L. V. Guedes</name>
    </author>
    <author>
      <name>Antonio J. G. Busson</name>
    </author>
    <author>
      <name>S√©rgio Colcher</name>
    </author>
    <link href="http://arxiv.org/abs/2010.04676v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.04676v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.08737v2</id>
    <updated>2021-01-11T12:33:05Z</updated>
    <published>2020-10-17T08:12:18Z</published>
    <title>Audio-based Near-Duplicate Video Retrieval with Audio Similarity
  Learning</title>
    <summary>  In this work, we address the problem of audio-based near-duplicate video
retrieval. We propose the Audio Similarity Learning (AuSiL) approach that
effectively captures temporal patterns of audio similarity between video pairs.
For the robust similarity calculation between two videos, we first extract
representative audio-based video descriptors by leveraging transfer learning
based on a Convolutional Neural Network (CNN) trained on a large scale dataset
of audio events, and then we calculate the similarity matrix derived from the
pairwise similarity of these descriptors. The similarity matrix is subsequently
fed to a CNN network that captures the temporal structures existing within its
content. We train our network following a triplet generation process and
optimizing the triplet loss function. To evaluate the effectiveness of the
proposed approach, we have manually annotated two publicly available video
datasets based on the audio duplicity between their videos. The proposed
approach achieves very competitive results compared to three state-of-the-art
methods. Also, unlike the competing methods, it is very robust to the retrieval
of audio duplicates generated with speed transformations.
</summary>
    <author>
      <name>Pavlos Avgoustinakis</name>
    </author>
    <author>
      <name>Giorgos Kordopatis-Zilos</name>
    </author>
    <author>
      <name>Symeon Papadopoulos</name>
    </author>
    <author>
      <name>Andreas L. Symeonidis</name>
    </author>
    <author>
      <name>Ioannis Kompatsiaris</name>
    </author>
    <link href="http://arxiv.org/abs/2010.08737v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.08737v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.09235v2</id>
    <updated>2021-12-11T22:58:52Z</updated>
    <published>2020-10-19T05:59:14Z</published>
    <title>Ensemble Chinese End-to-End Spoken Language Understanding for Abnormal
  Event Detection from audio stream</title>
    <summary>  Conventional spoken language understanding (SLU) consist of two stages, the
first stage maps speech to text by automatic speech recognition (ASR), and the
second stage maps text to intent by natural language understanding (NLU).
End-to-end SLU maps speech directly to intent through a single deep learning
model. Previous end-to-end SLU models are primarily used for English
environment due to lacking large scale SLU dataset in Chines, and use only one
ASR model to extract features from speech. With the help of Kuaishou
technology, a large scale SLU dataset in Chinese is collected to detect
abnormal event in their live audio stream. Based on this dataset, this paper
proposed a ensemble end-to-end SLU model used for Chinese environment. This
ensemble SLU models extracted hierarchies features using multiple pre-trained
ASR models, leading to better representation of phoneme level and word level
information. This proposed approached achieve 9.7% increase of accuracy
compared to previous end-to-end SLU model.
</summary>
    <author>
      <name>Haoran Wei</name>
    </author>
    <author>
      <name>Fei Tao</name>
    </author>
    <author>
      <name>Runze Su</name>
    </author>
    <author>
      <name>Sen Yang</name>
    </author>
    <author>
      <name>Ji Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitting to ICASSP 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.09235v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.09235v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.10135v2</id>
    <updated>2020-11-07T00:20:28Z</updated>
    <published>2020-10-20T09:00:46Z</published>
    <title>INDCOR white paper 1: A shared vocabulary for IDN (Interactive Digital
  Narratives)</title>
    <summary>  COST Action 18230 INDCOR (Interactive Narrative Design for Complexity
Representations) is an interdisciplinary network of researchers and
practitioners intended to further the use of interactive digital narratives
(IDN1) to represent highly complex topics. IDN possess crucial advantages in
this regard, but more knowledge is needed to realize these advantages in broad
usage by media producers and the general public. The lack of a shared
vocabulary is a crucial obstacle on the path to a generalized, accessible body
of IDN knowledge. This white paper frames the situation from the perspective of
INDCOR and describes the creation of an online encyclopedia as a means to
overcome this issue. Two similar and successful projects (The Living Handbook
of Narratology and the Stanford Encyclopedia of Philosophy) serve as examples
for this effort, showing how community-authored encyclopedias can provide
high-quality content. The authors introduce a taxonomy based on an overarching
analytical framework (SPP model) as the foundational element of the
encyclopedia, and detail editorial procedures for the project, including a
peer-review process, designed to assure high academic quality and relevance of
encyclopedia entries. Also, a sample entry provides guidance for authors.
</summary>
    <author>
      <name>Hartmut Koenitz</name>
    </author>
    <author>
      <name>Mirjam Palosaari Eladhari</name>
    </author>
    <author>
      <name>Sandy Louchart</name>
    </author>
    <author>
      <name>Frank Nack</name>
    </author>
    <link href="http://arxiv.org/abs/2010.10135v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.10135v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.10144v1</id>
    <updated>2020-10-20T09:23:45Z</updated>
    <published>2020-10-20T09:23:45Z</published>
    <title>Keystroke Dynamics as Part of Lifelogging</title>
    <summary>  In this paper we present the case for including keystroke dynamics in
lifelogging. We describe how we have used a simple keystroke logging
application called Loggerman, to create a dataset of longitudinal keystroke
timing data spanning a period of more than 6 months for 4 participants. We
perform a detailed analysis of this data by examining the timing information
associated with bigrams or pairs of adjacently-typed alphabetic characters. We
show how there is very little day-on-day variation of the keystroke timing
among the top-200 bigrams for some participants and for others there is a lot
and this correlates with the amount of typing each would do on a daily basis.
We explore how daily variations could correlate with sleep score from the
previous night but find no significant relation-ship between the two. Finally
we describe the public release of this data as well including as a series of
pointers for future work including correlating keystroke dynamics with mood and
fatigue during the day.
</summary>
    <author>
      <name>Alan F. Smeaton</name>
    </author>
    <author>
      <name>Naveen Garaga Krishnamurthy</name>
    </author>
    <author>
      <name>Amruth Hebbasuru Suryanarayana</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-67835-7_16</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-67835-7_16" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to 27th International Conference on Multimedia Modeling,
  Prague, Czech Republic, June 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.10144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.10144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.10721v1</id>
    <updated>2020-10-21T02:27:31Z</updated>
    <published>2020-10-21T02:27:31Z</published>
    <title>ComboLoss for Facial Attractiveness Analysis with Squeeze-and-Excitation
  Networks</title>
    <summary>  Loss function is crucial for model training and feature representation
learning, conventional models usually regard facial attractiveness recognition
task as a regression problem, and adopt MSE loss or Huber variant loss as
supervision to train a deep convolutional neural network (CNN) to predict
facial attractiveness score. Little work has been done to systematically
compare the performance of diverse loss functions. In this paper, we firstly
systematically analyze model performance under diverse loss functions. Then a
novel loss function named ComboLoss is proposed to guide the SEResNeXt50
network. The proposed method achieves state-of-the-art performance on SCUT-FBP,
HotOrNot and SCUT-FBP5500 datasets with an improvement of 1.13%, 2.1% and 0.57%
compared with prior arts, respectively. Code and models are available at
https://github.com/lucasxlu/ComboLoss.git.
</summary>
    <author>
      <name>Lu Xu</name>
    </author>
    <author>
      <name>Jinhai Xiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Tech Report</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.10721v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.10721v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.12662v1</id>
    <updated>2020-10-23T20:52:24Z</updated>
    <published>2020-10-23T20:52:24Z</published>
    <title>Short Video-based Advertisements Evaluation System: Self-Organizing
  Learning Approach</title>
    <summary>  With the rising of short video apps, such as TikTok, Snapchat and Kwai,
advertisement in short-term user-generated videos (UGVs) has become a trending
form of advertising. Prediction of user behavior without specific user profile
is required by advertisers, as they expect to acquire advertisement performance
in advance in the scenario of cold start. Current recommender system do not
take raw videos as input; additionally, most previous work of Multi-Modal
Machine Learning may not deal with unconstrained videos like UGVs. In this
paper, we proposed a novel end-to-end self-organizing framework for user
behavior prediction. Our model is able to learn the optimal topology of neural
network architecture, as well as optimal weights, through training data. We
evaluate our proposed method on our in-house dataset. The experimental results
reveal that our model achieves the best performance in all our experiments.
</summary>
    <author>
      <name>Yunjie Zhang</name>
    </author>
    <author>
      <name>Fei Tao</name>
    </author>
    <author>
      <name>Xudong Liu</name>
    </author>
    <author>
      <name>Runze Su</name>
    </author>
    <author>
      <name>Xiaorong Mei</name>
    </author>
    <author>
      <name>Weicong Ding</name>
    </author>
    <author>
      <name>Zhichen Zhao</name>
    </author>
    <author>
      <name>Lei Yuan</name>
    </author>
    <author>
      <name>Ji Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitting to ICASSP 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.12662v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.12662v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.13260v1</id>
    <updated>2020-10-26T00:31:42Z</updated>
    <published>2020-10-26T00:31:42Z</published>
    <title>Effect of Language Proficiency on Subjective Evaluation of Noise
  Suppression Algorithms</title>
    <summary>  Speech communication systems based on Voice-over-IP technology are frequently
used by native as well as non-native speakers of a target language, e.g. in
international phone calls or telemeetings. Frequently, such calls also occur in
a noisy environment, making noise suppression modules necessary to increase
perceived quality of experience. Whereas standard tests for assessing perceived
quality make use of native listeners, we assume that noise-reduced speech and
residual noise may affect native and non-native listeners of a target language
in different ways. To test this assumption, we report results of two subjective
tests conducted with English and German native listeners who judge the quality
of speech samples recorded by native English, German, and Mandarin speakers,
which are degraded with different background noise levels and noise suppression
effects. The experiments were conducted following the standardized ITU-T Rec.
P.835 approach, however implemented in a crowdsourcing setting according to
ITU-T Rec. P.808. Our results show a significant influence of language on
speech signal ratings and, consequently, on the overall perceived quality in
specific conditions.
</summary>
    <author>
      <name>Babak Naderi</name>
    </author>
    <author>
      <name>Gabriel Mittag</name>
    </author>
    <author>
      <name>Rafael Zequeira Jim\a'enez</name>
    </author>
    <author>
      <name>Sebastian M√∂ller</name>
    </author>
    <link href="http://arxiv.org/abs/2010.13260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.13260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.AS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.04959v1</id>
    <updated>2020-11-10T07:52:35Z</updated>
    <published>2020-11-10T07:52:35Z</published>
    <title>Multi-domain Reversible Data Hiding in JPEG</title>
    <summary>  As a branch of reversible data hiding (RDH), reversible data hiding in JEPG
is particularly important. Because JPEG images are widely used, it is great
significance to study reversible data hiding algorithm for JEPG images. The
existing JEPG reversible data methods can be divided into two categories, one
is based on Discrete Cosine Transform (DCT) coefficients modification, the
other is based on Huffman table modification, the methods based on DCT
coefficient modification result in large file expansion and visual quality
distortion, while the methods based on entropy coding domain modification have
low capacity and they may lead to large file expansion. In order to effectively
solve the problems in these two kinds of methods, this paper proposes a
reversible data hiding in JPEG images methods based on multi-domain
modification. In this method, the secret data is divided into two parts by
payload distribution algorithm, part of the secret data is first embedded in
the DCT coefficient domain, and then the remaining secret data is embedded in
the entropy coding domain. Experimental results demonstrate that most JPEG
image files with this scheme have smaller file size increment and higher
payload than previous RDH schemes.
</summary>
    <author>
      <name>Zhaoxia Yin</name>
    </author>
    <author>
      <name>Hongnian Guo</name>
    </author>
    <author>
      <name>Yang Du</name>
    </author>
    <link href="http://arxiv.org/abs/2011.04959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.04959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.05203v1</id>
    <updated>2020-10-30T14:21:53Z</updated>
    <published>2020-10-30T14:21:53Z</published>
    <title>OpenKinoAI: An Open Source Framework for Intelligent Cinematography and
  Editing of Live Performances</title>
    <summary>  OpenKinoAI is an open source framework for post-production of ultra high
definition video which makes it possible to emulate professional multiclip
editing techniques for the case of single camera recordings. OpenKinoAI
includes tools for uploading raw video footage of live performances on a remote
web server, detecting, tracking and recognizing the performers in the original
material, reframing the raw video into a large choice of cinematographic
rushes, editing the rushes into movies, and annotating rushes and movies for
documentation purposes. OpenKinoAI is made available to promote research in
multiclip video editing of ultra high definition video, and to allow performing
artists and companies to use this research for archiving, documenting and
sharing their work online in an innovative fashion.
</summary>
    <author>
      <name>R√©mi Ronfard</name>
    </author>
    <author>
      <name>R√©mi Colin de Verdi√®re</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.05203v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.05203v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.06691v1</id>
    <updated>2020-11-12T23:55:12Z</updated>
    <published>2020-11-12T23:55:12Z</published>
    <title>CNN-based driving of block partitioning for intra slices encoding</title>
    <summary>  This paper provides a technical overview of a deep-learning-based encoder
method aiming at optimizing next generation hybrid video encoders for driving
the block partitioning in intra slices. An encoding approach based on
Convolutional Neural Networks is explored to partly substitute classical
heuristics-based encoder speed-ups by a systematic and automatic process. The
solution allows controlling the trade-off between complexity and coding gains,
in intra slices, with one single parameter. This algorithm was proposed at the
Call for Proposals of the Joint Video Exploration Team (JVET) on video
compression with capability beyond HEVC. In All Intra configuration, for a
given allowed topology of splits, a speed-up of $\times 2$ is obtained without
BD-rate loss, or a speed-up above $\times 4$ with a loss below 1\% in BD-rate.
</summary>
    <author>
      <name>Franck Galpin</name>
    </author>
    <author>
      <name>Fabien Racap√©</name>
    </author>
    <author>
      <name>Sunil Jaiswal</name>
    </author>
    <author>
      <name>Philippe Bordes</name>
    </author>
    <author>
      <name>Fabrice Le L√©annec</name>
    </author>
    <author>
      <name>Edouard Fran√ßois</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DCC.2019.00024</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DCC.2019.00024" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2019 Data Compression Conference (DCC)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2011.06691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.06691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.08525v1</id>
    <updated>2020-11-17T09:24:05Z</updated>
    <published>2020-11-17T09:24:05Z</published>
    <title>Building Movie Map -- A Tool for Exploring Areas in a City -- and its
  Evaluation</title>
    <summary>  We propose a new Movie Map system, with an interface for exploring cities.
The system consists of four stages; acquisition, analysis, management, and
interaction. In the acquisition stage, omnidirectional videos are taken along
streets in target areas. Frames of the video are localized on the map,
intersections are detected, and videos are segmented. Turning views at
intersections are subsequently generated. By connecting the video segments
following the specified movement in an area, we can view the streets better.
The interface allows for easy exploration of a target area, and it can show
virtual billboards of stores in the view. We conducted user studies to compare
our system to the GSV in a scenario where users could freely move and explore
to find a landmark. The experiment showed that our system had a better user
experience than GSV.
</summary>
    <author>
      <name>Naoki Sugimoto</name>
    </author>
    <author>
      <name>Yoshihito Ebine</name>
    </author>
    <author>
      <name>Kiyoharu Aizawa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3394171.3413881</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3394171.3413881" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Multimedia 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2011.08525v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.08525v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.11286v1</id>
    <updated>2020-11-23T09:01:28Z</updated>
    <published>2020-11-23T09:01:28Z</published>
    <title>MEG: Multi-Evidence GNN for Multimodal Semantic Forensics</title>
    <summary>  Fake news often involves semantic manipulations across modalities such as
image, text, location etc and requires the development of multimodal semantic
forensics for its detection. Recent research has centered the problem around
images, calling it image repurposing -- where a digitally unmanipulated image
is semantically misrepresented by means of its accompanying multimodal metadata
such as captions, location, etc. The image and metadata together comprise a
multimedia package. The problem setup requires algorithms to perform multimodal
semantic forensics to authenticate a query multimedia package using a reference
dataset of potentially related packages as evidences. Existing methods are
limited to using a single evidence (retrieved package), which ignores potential
performance improvement from the use of multiple evidences. In this work, we
introduce a novel graph neural network based model for multimodal semantic
forensics, which effectively utilizes multiple retrieved packages as evidences
and is scalable with the number of evidences. We compare the scalability and
performance of our model against existing methods. Experimental results show
that the proposed model outperforms existing state-of-the-art algorithms with
an error reduction of up to 25%.
</summary>
    <author>
      <name>Ekraam Sabir</name>
    </author>
    <author>
      <name>Ayush Jaiswal</name>
    </author>
    <author>
      <name>Wael AbdAlmageed</name>
    </author>
    <author>
      <name>Prem Natarajan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published at ICPR 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.11286v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.11286v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.14068v1</id>
    <updated>2020-11-28T04:51:16Z</updated>
    <published>2020-11-28T04:51:16Z</published>
    <title>Overview of Screen Content Coding in Recently Developed Video Coding
  Standards</title>
    <summary>  In recent years, screen content (SC) video including computer generated text,
graphics and animations, have drawn more attention than ever, as many related
applications become very popular. To address the need for efficient coding of
such contents, a number of coding tools have been specifically developed and
achieved great advances in terms of coding efficiency. The inclusion of screen
content coding (SCC) features in all the recently developed video coding
standards (namely, HEVC SCC, VVC, AVS3, AV1 and EVC) demonstrated the
importance of supporting such features. This paper provides an overview and
comparative study of screen content coding technologies, with discussions on
the performance and complexity aspects for the tools developed in these
standards.
</summary>
    <author>
      <name>Xiaozhong Xu</name>
    </author>
    <author>
      <name>Shan Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 10 figures and 8 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.14068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.14068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.00502v1</id>
    <updated>2021-01-31T17:53:16Z</updated>
    <published>2021-01-31T17:53:16Z</published>
    <title>A Machine Learning Approach to Optimal Inverse Discrete Cosine Transform
  (IDCT) Design</title>
    <summary>  The design of the optimal inverse discrete cosine transform (IDCT) to
compensate the quantization error is proposed for effective lossy image
compression in this work. The forward and inverse DCTs are designed in pair in
current image/video coding standards without taking the quantization effect
into account. Yet, the distribution of quantized DCT coefficients deviate from
that of original DCT coefficients. This is particularly obvious when the
quality factor of JPEG compressed images is small. To address this problem, we
first use a set of training images to learn the compound effect of forward DCT,
quantization and dequantization in cascade. Then, a new IDCT kernel is learned
to reverse the effect of such a pipeline. Experiments are conducted to
demonstrate that the advantage of the new method, which has a gain of
0.11-0.30dB over the standard JPEG over a wide range of quality factors.
</summary>
    <author>
      <name>Yifan Wang</name>
    </author>
    <author>
      <name>Zhanxuan Mei</name>
    </author>
    <author>
      <name>Chia-Yang Tsai</name>
    </author>
    <author>
      <name>Ioannis Katsavounidis</name>
    </author>
    <author>
      <name>C. -C. Jay Kuo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.00502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.00502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.01272v1</id>
    <updated>2021-02-02T03:09:56Z</updated>
    <published>2021-02-02T03:09:56Z</published>
    <title>Efficient Compressed Sensing Based Image Coding by Using Gray
  Transformation</title>
    <summary>  In recent years, compressed sensing (CS) based image coding has become a hot
topic in image processing field. However, since the bit depth required for
encoding each CS sample is too large, the compression performance of this
paradigm is unattractive. To address this issue, a novel CS-based image coding
system by using gray transformation is proposed. In the proposed system, we use
a gray transformation to preprocess the original image firstly and then use CS
to sample the transformed image. Since gray transformation makes the
probability distribution of CS samples centralized, the bit depth required for
encoding each CS sample is reduced significantly. Consequently, the proposed
system can considerably improve the compression performance of CS-based image
coding. Simulation results show that the proposed system outperforms the
traditional one without using gray transformation in terms of compression
performance.
</summary>
    <author>
      <name>Bo Zhang</name>
    </author>
    <author>
      <name>Di Xiao</name>
    </author>
    <author>
      <name>Lan Wang</name>
    </author>
    <author>
      <name>Sen Bai</name>
    </author>
    <author>
      <name>Lei Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.01272v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.01272v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.01893v2</id>
    <updated>2021-05-21T09:21:37Z</updated>
    <published>2021-02-03T06:28:18Z</published>
    <title>Multi-color balancing for correctly adjusting the intensity of target
  colors</title>
    <summary>  In this paper, we propose a novel multi-color balance method for reducing
color distortions caused by lighting effects. The proposed method allows us to
adjust three target-colors chosen by a user in an input image so that each
target color is the same as the corresponding destination (benchmark) one. In
contrast, white balancing is a typical technique for reducing the color
distortions, however, they cannot remove lighting effects on colors other than
white. In an experiment, the proposed method is demonstrated to be able to
remove lighting effects on selected three colors, and is compared with existing
white balance adjustments.
</summary>
    <author>
      <name>Teruaki Akazawa</name>
    </author>
    <author>
      <name>Yuma Kinoshita</name>
    </author>
    <author>
      <name>Hitoshi Kiya</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/LifeTech52111.2021.9391973</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/LifeTech52111.2021.9391973" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">\c{opyright} 2021 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Proc. of 2021 IEEE 3rd Global Conference on Life Sciences and
  Technologies (LifeTech), pp.8-12, Mar., 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2102.01893v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.01893v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.08192v1</id>
    <updated>2021-02-16T14:39:59Z</updated>
    <published>2021-02-16T14:39:59Z</published>
    <title>A Survey on 360-Degree Video: Coding, Quality of Experience and
  Streaming</title>
    <summary>  The commercialization of Virtual Reality (VR) headsets has made immersive and
360-degree video streaming the subject of intense interest in the industry and
research communities. While the basic principles of video streaming are the
same, immersive video presents a set of specific challenges that need to be
addressed. In this survey, we present the latest developments in the relevant
literature on four of the most important ones: (i) omnidirectional video coding
and compression, (ii) subjective and objective Quality of Experience (QoE) and
the factors that can affect it, (iii) saliency measurement and Field of View
(FoV) prediction, and (iv) the adaptive streaming of immersive 360-degree
videos. The final objective of the survey is to provide an overview of the
research on all the elements of an immersive video streaming system, giving the
reader an understanding of their interplay and performance.
</summary>
    <author>
      <name>Federico Chiariotti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Elsevier Computer Communications</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.08192v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.08192v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.10162v1</id>
    <updated>2021-02-19T21:23:50Z</updated>
    <published>2021-02-19T21:23:50Z</published>
    <title>Clarification of Video Retrieval Query Results by the Automated
  Insertion of Supporting Shots</title>
    <summary>  Computational Video Editing Systems output video generally follows a
particular form, e.g. conversation or music videos, in this way they are domain
specific. We describe a recent development in our video annotation and
segmentation system to support general computational video editing in which we
derive a single generic editing strategy from general cinema narrative
principles instead of using a hierarchical film gram-mar. We demonstrate how
this single principle coupled with a database of scripts derived from annotated
videos leverages the existing video editing knowledge encoded within the
editing of those sequences in a flexible and generic manner. We discuss the
cinema theory foundations for this generic editing strategy, review the
algorithms used to effect it, and goon by means of examples to show its
appropriateness in an automated system.
</summary>
    <author>
      <name>Sean Butler</name>
    </author>
    <link href="http://arxiv.org/abs/2102.10162v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.10162v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
