<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Acs.NE%26id_list%3D%26start%3D0%26max_results%3D1100" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:cs.NE&amp;id_list=&amp;start=0&amp;max_results=1100</title>
  <id>http://arxiv.org/api/ZtrKSuGGt5FH4QyQNA83Upjg5f8</id>
  <updated>2025-05-27T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">16025</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">1100</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/cs/0702055v1</id>
    <updated>2007-02-09T13:16:14Z</updated>
    <published>2007-02-09T13:16:14Z</published>
    <title>On the possibility of making the complete computer model of a human
  brain</title>
    <summary>  The development of the algorithm of a neural network building by the
corresponding parts of a DNA code is discussed.
</summary>
    <author>
      <name>A. V. Paraskevov</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0702055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.4495v1</id>
    <updated>2010-09-22T22:32:37Z</updated>
    <published>2010-09-22T22:32:37Z</published>
    <title>Unary Coding for Neural Network Learning</title>
    <summary>  This paper presents some properties of unary coding of significance for
biological learning and instantaneously trained neural networks.
</summary>
    <author>
      <name>Subhash Kak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.4495v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.4495v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.7806v1</id>
    <updated>2014-11-28T10:39:24Z</updated>
    <published>2014-11-28T10:39:24Z</published>
    <title>Two Gaussian Approaches to Black-Box Optomization</title>
    <summary>  Outline of several strategies for using Gaussian processes as surrogate
models for the covariance matrix adaptation evolution strategy (CMA-ES).
</summary>
    <author>
      <name>Lukáš Bajer</name>
    </author>
    <author>
      <name>Martin Holeňa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.7806v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.7806v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05549v1</id>
    <updated>2017-01-19T18:43:56Z</updated>
    <published>2017-01-19T18:43:56Z</published>
    <title>Deep Neural Networks - A Brief History</title>
    <summary>  Introduction to deep neural networks and their history.
</summary>
    <author>
      <name>Krzysztof J. Cios</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.05549v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05549v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02746v2</id>
    <updated>2017-07-12T20:08:12Z</updated>
    <published>2017-07-10T08:44:46Z</published>
    <title>Backpropagation in matrix notation</title>
    <summary>  In this note we calculate the gradient of the network function in matrix
notation.
</summary>
    <author>
      <name>N. M. Mishachev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, Remark 6 added</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.02746v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02746v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504056v1</id>
    <updated>2005-04-13T13:59:55Z</updated>
    <published>2005-04-13T13:59:55Z</published>
    <title>Self-Organizing Multilayered Neural Networks of Optimal Complexity</title>
    <summary>  The principles of self-organizing the neural networks of optimal complexity
is considered under the unrepresentative learning set. The method of
self-organizing the multi-layered neural networks is offered and used to train
the logical neural networks which were applied to the medical diagnostics.
</summary>
    <author>
      <name>V. Schetinin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.0798v1</id>
    <updated>2009-06-03T23:10:25Z</updated>
    <published>2009-06-03T23:10:25Z</published>
    <title>Single Neuron Memories and the Network's Proximity Matrix</title>
    <summary>  This paper extends the treatment of single-neuron memories obtained by the
B-matrix approach. The spreading of the activity within the network is
determined by the network's proximity matrix which represents the separations
amongst the neurons through the neural pathways.
</summary>
    <author>
      <name>Subhash Kak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.0798v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.0798v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.3390v1</id>
    <updated>2010-05-19T09:29:39Z</updated>
    <published>2010-05-19T09:29:39Z</published>
    <title>Critical control of a genetic algorithm</title>
    <summary>  Based on speculations coming from statistical mechanics and the conjectured
existence of critical states, I propose a simple heuristic in order to control
the mutation probability and the population size of a genetic algorithm.
</summary>
    <author>
      <name>Raphaël Cerf</name>
    </author>
    <link href="http://arxiv.org/abs/1005.3390v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.3390v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.stat-mech" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.5081v2</id>
    <updated>2011-06-28T21:32:26Z</updated>
    <published>2011-03-25T20:59:13Z</published>
    <title>Using Variable Threshold to Increase Capacity in a Feedback Neural
  Network</title>
    <summary>  The article presents new results on the use of variable thresholds to
increase the capacity of a feedback neural network. Non-binary networks are
also considered in this analysis.
</summary>
    <author>
      <name>Praveen Kuruvada</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.5081v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.5081v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.5538v1</id>
    <updated>2011-04-29T02:28:50Z</updated>
    <published>2011-04-29T02:28:50Z</published>
    <title>Complex Networks</title>
    <summary>  Introduction to the Special Issue on Complex Networks, Artificial Life
journal.
</summary>
    <author>
      <name>Carlos Gershenson</name>
    </author>
    <author>
      <name>Mikhail Prokopenko</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/artl_e_00037</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/artl_e_00037" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, in press</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Life 17(4):259--261. 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1104.5538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.5538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.6223v1</id>
    <updated>2011-06-30T13:38:35Z</updated>
    <published>2011-06-30T13:38:35Z</published>
    <title>Why 'GSA: A Gravitational Search Algorithm' Is Not Genuinely Based on
  the Law of Gravity</title>
    <summary>  Why 'GSA: A Gravitational Search Algorithm' Is Not Genuinely Based on the Law
of Gravity
</summary>
    <author>
      <name>Melvin Gauci</name>
    </author>
    <author>
      <name>Tony J. Dodd</name>
    </author>
    <author>
      <name>Roderich Gross</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11047-012-9322-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11047-012-9322-0" rel="related"/>
    <link href="http://arxiv.org/abs/1106.6223v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.6223v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.5674v1</id>
    <updated>2013-07-22T12:16:08Z</updated>
    <published>2013-07-22T12:16:08Z</published>
    <title>Solving Traveling Salesman Problem by Marker Method</title>
    <summary>  In this paper we use marker method and propose a new mutation operator that
selects the nearest neighbor among all near neighbors solving Traveling
Salesman Problem.
</summary>
    <author>
      <name>Masoumeh Vali</name>
    </author>
    <link href="http://arxiv.org/abs/1307.5674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.5674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.4675v2</id>
    <updated>2017-06-30T03:01:50Z</updated>
    <published>2013-08-16T04:36:03Z</published>
    <title>Genetic Algorithm for Solving Simple Mathematical Equality Problem</title>
    <summary>  This paper explains genetic algorithm for novice in this field. Basic
philosophy of genetic algorithm and its flowchart are described. Step by step
numerical computation of genetic algorithm for solving simple mathematical
equality problem will be briefly explained
</summary>
    <author>
      <name>Denny Hermawanto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Tutorial paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1308.4675v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.4675v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5997v2</id>
    <updated>2014-04-26T23:10:51Z</updated>
    <published>2014-04-23T22:37:56Z</published>
    <title>One weird trick for parallelizing convolutional neural networks</title>
    <summary>  I present a new way to parallelize the training of convolutional neural
networks across multiple GPUs. The method scales significantly better than all
alternatives when applied to modern convolutional neural networks.
</summary>
    <author>
      <name>Alex Krizhevsky</name>
    </author>
    <link href="http://arxiv.org/abs/1404.5997v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5997v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.02128v1</id>
    <updated>2015-01-09T13:21:11Z</updated>
    <published>2015-01-09T13:21:11Z</published>
    <title>Introduction and Ranking Results of the ICSI 2014 Competition on Single
  Objective Optimization</title>
    <summary>  This technical report includes the introduction and ranking results of the
ICSI 2014 Competition on Single Objective Optimization.
</summary>
    <author>
      <name>Ying Tan</name>
    </author>
    <author>
      <name>Junzhi Li</name>
    </author>
    <author>
      <name>Zhongyang Zheng</name>
    </author>
    <link href="http://arxiv.org/abs/1501.02128v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.02128v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04306v1</id>
    <updated>2016-06-14T10:55:53Z</updated>
    <published>2016-06-14T10:55:53Z</published>
    <title>Viral Search algorithm</title>
    <summary>  The article, after a brief introduction on genetic algorithms and their
functioning, presents a kind of genetic algorithm called Viral Search. We
present the key concepts, we formally derive the algorithm and we perform
numerical tests designed to illustrate the potential and limits.
</summary>
    <author>
      <name>Matteo Gardini</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, in Italian, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.04306v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04306v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.1534v1</id>
    <updated>2011-05-08T16:47:35Z</updated>
    <published>2011-05-08T16:47:35Z</published>
    <title>Taking the redpill: Artificial Evolution in native x86 systems</title>
    <summary>  In analogon to successful artificial evolution simulations as Tierra or
avida, this text presents a way to perform artificial evolution in a native x86
system. The implementation of the artificial chemistry and first results of
statistical experiments are presented.
</summary>
    <author>
      <name>Thomas Sperl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.1534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.1534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.01524v1</id>
    <updated>2015-03-05T03:29:16Z</updated>
    <published>2015-03-05T03:29:16Z</published>
    <title>Genetic optimization of the Hyperloop route through the Grapevine</title>
    <summary>  We demonstrate a genetic algorithm that employs a versatile fitness function
to optimize route selection for the Hyperloop, a proposed high speed passenger
transportation system.
</summary>
    <author>
      <name>Casey J. Handmer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 4 figures, 1 Mathematica notebook attachment</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.01524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.01524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.00208v1</id>
    <updated>2024-03-30T01:11:21Z</updated>
    <published>2024-03-30T01:11:21Z</published>
    <title>Discrete Natural Evolution Strategies</title>
    <summary>  Natural evolution strategies are a class of approximate-gradient black-box
optimizers that have been successfully used for continuous parameter spaces. In
this paper, we derive NES algorithms for discrete parameter spaces and
demonstrate their effectiveness in tasks involving discrete parameters.
</summary>
    <author>
      <name>Ahmad Ayaz Amin</name>
    </author>
    <link href="http://arxiv.org/abs/2404.00208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.00208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0408006v1</id>
    <updated>2004-08-01T15:04:53Z</updated>
    <published>2004-08-01T15:04:53Z</published>
    <title>Why Two Sexes?</title>
    <summary>  Evolutionary role of the separation into two sexes from a cyberneticist's
point of view. [I translated this 1965 article from Russian "Nauka i Zhizn"
(Science and Life) in 1988. In a popular form, the article puts forward several
useful ideas not all of which even today are necessarily well known or widely
accepted. Boris Lubachevsky, bdl@bell-labs.com ]
</summary>
    <author>
      <name>Vigen A. Geodakian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Nauka i zhizn (Science and Life), 1965</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0408006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0408006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504068v1</id>
    <updated>2005-04-14T10:45:06Z</updated>
    <published>2005-04-14T10:45:06Z</published>
    <title>Self-Organization of the Neuron Collective of Optimal Complexity</title>
    <summary>  The optimal complexity of neural networks is achieved when the
self-organization principles is used to eliminate the contradictions existing
in accordance with the K. Godel theorem about incompleteness of the systems
based on axiomatics. The principle of S. Beer exterior addition the Heuristic
Group Method of Data Handling by A. Ivakhnenko realized is used.
</summary>
    <author>
      <name>V. Schetinin</name>
    </author>
    <author>
      <name>A. Kostunin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NOLTA-1996, Japan</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0504068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505016v1</id>
    <updated>2005-05-07T20:56:58Z</updated>
    <published>2005-05-07T20:56:58Z</published>
    <title>Visual Character Recognition using Artificial Neural Networks</title>
    <summary>  The recognition of optical characters is known to be one of the earliest
applications of Artificial Neural Networks, which partially emulate human
thinking in the domain of artificial intelligence. In this paper, a simplified
neural approach to recognition of optical or visual characters is portrayed and
discussed. The document is expected to serve as a resource for learners and
amateur investigators in pattern recognition, neural networking and related
disciplines.
</summary>
    <author>
      <name>Shashank Araokar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, tutorial resource</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505019v1</id>
    <updated>2005-05-10T06:37:31Z</updated>
    <published>2005-05-10T06:37:31Z</published>
    <title>Artificial Neural Networks and their Applications</title>
    <summary>  The Artificial Neural network is a functional imitation of simplified model
of the biological neurons and their goal is to construct useful computers for
real world problems. The ANN applications have increased dramatically in the
last few years fired by both theoretical and practical applications in a wide
variety of applications. A brief theory of ANN is presented and potential areas
are identified and future trends are discussed.
</summary>
    <author>
      <name>Nitin Malik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0601129v1</id>
    <updated>2006-01-30T22:02:47Z</updated>
    <published>2006-01-30T22:02:47Z</published>
    <title>Instantaneously Trained Neural Networks</title>
    <summary>  This paper presents a review of instantaneously trained neural networks
(ITNNs). These networks trade learning time for size and, in the basic model, a
new hidden node is created for each training sample. Various versions of the
corner-classification family of ITNNs, which have found applications in
artificial intelligence (AI), are described. Implementation issues are also
considered.
</summary>
    <author>
      <name>Abhilash Ponnath</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0601129v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0601129v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607019v1</id>
    <updated>2006-07-06T18:49:20Z</updated>
    <published>2006-07-06T18:49:20Z</published>
    <title>Modelling the Probability Density of Markov Sources</title>
    <summary>  This paper introduces an objective function that seeks to minimise the
average total number of bits required to encode the joint state of all of the
layers of a Markov source. This type of encoder may be applied to the problem
of optimising the bottom-up (recognition model) and top-down (generative model)
connections in a multilayer neural network, and it unifies several previous
results on the optimisation of multilayer neural networks.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0607019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703134v5</id>
    <updated>2008-01-07T13:04:20Z</updated>
    <published>2007-03-27T16:57:39Z</published>
    <title>Automatic Generation of Benchmarks for Plagiarism Detection Tools using
  Grammatical Evolution</title>
    <summary>  This paper has been withdrawn by the authors due to a major rewriting.
</summary>
    <author>
      <name>Manuel Cebrian</name>
    </author>
    <author>
      <name>Manuel Alfonseca</name>
    </author>
    <author>
      <name>Alfonso Ortega</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0703134v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703134v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.1; I.2.2; D.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0708.2686v1</id>
    <updated>2007-07-24T18:50:22Z</updated>
    <published>2007-07-24T18:50:22Z</published>
    <title>The universal evolutionary computer based on super-recursive algorithms
  of evolvability</title>
    <summary>  This work exposes which mechanisms and procesess in the Nature of evolution
compute a function not computable by Turing machine. The computer with
intelligence that is not higher than one bacteria population could have, but
with efficency to solve the problems that are non-computable by Turing machine
is represented. This theoretical construction is called Universal Evolutinary
Computer and it is based on the superecursive algorithms of evolvability.
</summary>
    <author>
      <name>D. Roglic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 table, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/0708.2686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0708.2686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; H.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.3875v1</id>
    <updated>2008-02-26T19:07:53Z</updated>
    <published>2008-02-26T19:07:53Z</published>
    <title>Are complex systems hard to evolve?</title>
    <summary>  Evolutionary complexity is here measured by the number of trials/evaluations
needed for evolving a logical gate in a non-linear medium. Behavioural
complexity of the gates evolved is characterised in terms of cellular automata
behaviour. We speculate that hierarchies of behavioural and evolutionary
complexities are isomorphic up to some degree, subject to substrate specificity
of evolution and the spectrum of evolution parameters.
</summary>
    <author>
      <name>Andy Adamatzky</name>
    </author>
    <author>
      <name>Larry Bull</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/cplx.20269</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/cplx.20269" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Volume 14, Issue 6, pages 15-20, July/August 2009</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.3875v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.3875v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.1888v1</id>
    <updated>2009-04-13T00:59:10Z</updated>
    <published>2009-04-13T00:59:10Z</published>
    <title>On Fodor on Darwin on Evolution</title>
    <summary>  Jerry Fodor argues that Darwin was wrong about "natural selection" because
(1) it is only a tautology rather than a scientific law that can support
counterfactuals ("If X had happened, Y would have happened") and because (2)
only minds can select. Hence Darwin's analogy with "artificial selection" by
animal breeders was misleading and evolutionary explanation is nothing but
post-hoc historical narrative. I argue that Darwin was right on all counts.
</summary>
    <author>
      <name>Stevan Harnad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21</arxiv:comment>
    <link href="http://arxiv.org/abs/0904.1888v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.1888v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.0677v1</id>
    <updated>2009-05-05T19:45:13Z</updated>
    <published>2009-05-05T19:45:13Z</published>
    <title>Feasibility of random basis function approximators for modeling and
  control</title>
    <summary>  We discuss the role of random basis function approximators in modeling and
control. We analyze the published work on random basis function approximators
and demonstrate that their favorable error rate of convergence O(1/n) is
guaranteed only with very substantial computational resources. We also discuss
implications of our analysis for applications of neural networks in modeling
and control.
</summary>
    <author>
      <name>Ivan Tyukin</name>
    </author>
    <author>
      <name>Danil Prokhorov</name>
    </author>
    <link href="http://arxiv.org/abs/0905.0677v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.0677v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.2649v1</id>
    <updated>2009-05-16T02:34:32Z</updated>
    <published>2009-05-16T02:34:32Z</published>
    <title>An Immune System Inspired Approach to Automated Program Verification</title>
    <summary>  An immune system inspired Artificial Immune System (AIS) algorithm is
presented, and is used for the purposes of automated program verification.
Relevant immunological concepts are discussed and the field of AIS is briefly
reviewed. It is proposed to use this AIS algorithm for a specific automated
program verification task: that of predicting shape of program invariants. It
is shown that the algorithm correctly predicts program invariant shape for a
variety of benchmarked programs.
</summary>
    <author>
      <name>Soumya Banerjee</name>
    </author>
    <link href="http://arxiv.org/abs/0905.2649v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.2649v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.0516v1</id>
    <updated>2009-07-03T02:21:36Z</updated>
    <published>2009-07-03T02:21:36Z</published>
    <title>Adaptation and Self-Organization in Evolutionary Algorithms</title>
    <summary>  Abbreviated Abstract: The objective of Evolutionary Computation is to solve
practical problems (e.g. optimization, data mining) by simulating the
mechanisms of natural evolution. This thesis addresses several topics related
to adaptation and self-organization in evolving systems with the overall aims
of improving the performance of Evolutionary Algorithms (EA), understanding its
relation to natural evolution, and incorporating new mechanisms for mimicking
complex biological systems.
</summary>
    <author>
      <name>James M Whitacre</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PhD Thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.0516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.0516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0911.2324v1</id>
    <updated>2009-11-12T08:48:38Z</updated>
    <published>2009-11-12T08:48:38Z</published>
    <title>Deterministic Autopoietic Automata</title>
    <summary>  This paper studies two issues related to the paper on Computing by
Self-reproduction: Autopoietic Automata by Jiri Wiedermann. It is shown that
all results presented there extend to deterministic computations. In
particular, nondeterminism is not needed for a lineage to generate all
autopoietic automata.
</summary>
    <author>
      <name>Martin Fürer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.9.6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.9.6" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 9, 2009, pp. 49-53</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0911.2324v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0911.2324v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.2280v2</id>
    <updated>2010-09-23T16:27:39Z</updated>
    <published>2010-04-13T22:12:22Z</published>
    <title>XOR at a Single Vertex -- Artificial Dendrites</title>
    <summary>  New to neuroscience with implications for AI, the exclusive OR, or any other
Boolean gate may be biologically accomplished within a single region where
active dendrites merge. This is demonstrated below using dynamic circuit
analysis. Medical knowledge aside, this observation points to the possibility
of specially coated conductors to accomplish artificial dendrites.
</summary>
    <author>
      <name>John Robert Burger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Edited for clarity; added Kandel reference</arxiv:comment>
    <link href="http://arxiv.org/abs/1004.2280v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.2280v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.6.1; C.1.3; I.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1006.1434v1</id>
    <updated>2010-06-08T01:17:00Z</updated>
    <published>2010-06-08T01:17:00Z</published>
    <title>Computing by Means of Physics-Based Optical Neural Networks</title>
    <summary>  We report recent research on computing with biology-based neural network
models by means of physics-based opto-electronic hardware. New technology
provides opportunities for very-high-speed computation and uncovers problems
obstructing the wide-spread use of this new capability. The Computation
Modeling community may be able to offer solutions to these cross-boundary
research problems.
</summary>
    <author>
      <name>A. Steven Younger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Missouri State University</arxiv:affiliation>
    </author>
    <author>
      <name>Emmett Redd</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Missouri State University</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.26.15</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.26.15" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 26, 2010, pp. 159-167</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1006.1434v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1006.1434v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.0417v1</id>
    <updated>2010-07-02T18:05:29Z</updated>
    <published>2010-07-02T18:05:29Z</published>
    <title>Delta Learning Rule for the Active Sites Model</title>
    <summary>  This paper reports the results on methods of comparing the memory retrieval
capacity of the Hebbian neural network which implements the B-Matrix approach,
by using the Widrow-Hoff rule of learning. We then, extend the recently
proposed Active Sites model by developing a delta rule to increase memory
capacity. Also, this paper extends the binary neural network to a multi-level
(non-binary) neural network.
</summary>
    <author>
      <name>Krishna Chaithanya Lingashetty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 Pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1007.0417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.0417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.0915v1</id>
    <updated>2010-09-05T13:17:01Z</updated>
    <published>2010-09-05T13:17:01Z</published>
    <title>Results of Evolution Supervised by Genetic Algorithms</title>
    <summary>  A series of results of evolution supervised by genetic algorithms with
interest to agricultural and horticultural fields are reviewed. New obtained
original results from the use of genetic algorithms on structure-activity
relationships are reported.
</summary>
    <author>
      <name>Lorentz Jäntschi</name>
    </author>
    <author>
      <name>Sorana D. Bolboac{\ba}</name>
    </author>
    <author>
      <name>Mugur C. Bălan</name>
    </author>
    <author>
      <name>Radu E. Sestraş</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 Table, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1009.0915v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.0915v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="78M32" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.4; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.5866v1</id>
    <updated>2010-11-26T19:46:56Z</updated>
    <published>2010-11-26T19:46:56Z</published>
    <title>Evolving difficult SAT instances thanks to local search</title>
    <summary>  We propose to use local search algorithms to produce SAT instances which are
harder to solve than randomly generated k-CNF formulae. The first results,
obtained with rudimentary search algorithms, show that the approach deserves
further study. It could be used as a test of robustness for SAT solvers, and
could help to investigate how branching heuristics, learning strategies, and
other aspects of solvers impact there robustness.
</summary>
    <author>
      <name>Olivier Bailleux</name>
    </author>
    <link href="http://arxiv.org/abs/1011.5866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.5866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.1589v1</id>
    <updated>2011-08-08T00:00:13Z</updated>
    <published>2011-08-08T00:00:13Z</published>
    <title>Imitation of Life: Advanced system for native Artificial Evolution</title>
    <summary>  A model for artificial evolution in native x86 Windows systems has been
developed at the end of 2010. In this text, further improvements and additional
analogies to natural microbiologic processes are presented. Several experiments
indicate the capability of the system - and raise the question of possible
countermeasures.
</summary>
    <author>
      <name>Thomas Sperl</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.1589v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.1589v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.4080v1</id>
    <updated>2011-08-20T02:11:57Z</updated>
    <published>2011-08-20T02:11:57Z</published>
    <title>Convergence Properties of Two (μ + λ) Evolutionary
  Algorithms On OneMax and Royal Roads Test Functions</title>
    <summary>  We present a number of bounds on convergence time for two elitist
population-based Evolutionary Algorithms using a recombination operator
k-Bit-Swap and a mainstream Randomized Local Search algorithm. We study the
effect of distribution of elite species and population size.
</summary>
    <author>
      <name>Aram Ter-Sarkisov</name>
    </author>
    <author>
      <name>Stephen Marsland</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for ECTA 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.4080v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.4080v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.6441v1</id>
    <updated>2011-09-29T08:53:36Z</updated>
    <published>2011-09-29T08:53:36Z</published>
    <title>Memetic Algorithms: Parametrization and Balancing Local and Global
  Search</title>
    <summary>  This is a preprint of a book chapter from the Handbook of Memetic Algorithms,
Studies in Computational Intelligence, Vol. 379, ISBN 978-3-642-23246-6,
Springer, edited by F. Neri, C. Cotta, and P. Moscato. It is devoted to the
parametrization of memetic algorithms and how to find a good balance between
global and local search.
</summary>
    <author>
      <name>Dirk Sudholt</name>
    </author>
    <link href="http://arxiv.org/abs/1109.6441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.6441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.1353v1</id>
    <updated>2011-11-05T21:43:56Z</updated>
    <published>2011-11-05T21:43:56Z</published>
    <title>An efficient implementation of the simulated annealing heuristic for the
  quadratic assignment problem</title>
    <summary>  The quadratic assignment problem (QAP) is one of the most difficult
combinatorial optimization problems. One of the most powerful and commonly used
heuristics to obtain approximations to the optimal solution of the QAP is
simulated annealing (SA). We present an efficient implementation of the SA
heuristic which performs more than 100 times faster then existing
implementations for large problem sizes and a large number of SA iterations.
</summary>
    <author>
      <name>Gerald Paul</name>
    </author>
    <link href="http://arxiv.org/abs/1111.1353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.1353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1206.6722v1</id>
    <updated>2012-06-28T15:13:37Z</updated>
    <published>2012-06-28T15:13:37Z</published>
    <title>Piecewise Linear Topology, Evolutionary Algorithms, and Optimization
  Problems</title>
    <summary>  Schemata theory, Markov chains, and statistical mechanics have been used to
explain how evolutionary algorithms (EAs) work. Incremental success has been
achieved with all of these methods, but each has been stymied by limitations
related to its less-than-global view. We show that moving the investigation
into topological space improves our understanding of why EAs work.
</summary>
    <author>
      <name>Andrew Clark</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">PDF from Word docx, 11 pages, no figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1206.6722v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1206.6722v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.4009v1</id>
    <updated>2012-08-20T13:54:26Z</updated>
    <published>2012-08-20T13:54:26Z</published>
    <title>Learning sparse messages in networks of neural cliques</title>
    <summary>  An extension to a recently introduced binary neural network is proposed in
order to allow the learning of sparse messages, in large numbers and with high
memory efficiency. This new network is justified both in biological and
informational terms. The learning and retrieval rules are detailed and
illustrated by various simulation results.
</summary>
    <author>
      <name>Behrooz Kamary Aliabadi</name>
    </author>
    <author>
      <name>Claude Berrou</name>
    </author>
    <author>
      <name>Vincent Gripon</name>
    </author>
    <author>
      <name>Xiaoran Jiang</name>
    </author>
    <link href="http://arxiv.org/abs/1208.4009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.4009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.3909v2</id>
    <updated>2015-08-17T00:38:32Z</updated>
    <published>2012-09-18T11:12:23Z</published>
    <title>Network Routing Optimization Using Swarm Intelligence</title>
    <summary>  The aim of this paper is to highlight and explore a traditional problem,
which is the minimum spanning tree, and finding the shortest-path in network
routing, by using Swarm Intelligence. This work to be considered as an
investigation topic with combination between operations research, discrete
mathematics, and evolutionary computing aiming to solve one of networking
problems.
</summary>
    <author>
      <name>Mohamed A. El Galil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.3909v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.3909v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.0639v2</id>
    <updated>2017-03-17T20:06:29Z</updated>
    <published>2012-12-04T08:48:29Z</published>
    <title>Evaluation of Particle Swarm Optimization Algorithms for Weighted
  Max-Sat Problem: Technical Report</title>
    <summary>  An experimental evaluation is conducted to asses the performance of 4
different Particle Swarm Optimization neighborhood structures in solving
Max-Sat problem. The experiment has shown that none of the algorithms achieves
statistically significant performance over the others under confidence level of
0.05.
</summary>
    <author>
      <name>Osama Khalil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Not useful</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.0639v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.0639v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1301.0254v3</id>
    <updated>2013-05-03T16:05:32Z</updated>
    <published>2012-12-27T23:04:01Z</published>
    <title>Group theory, group actions, evolutionary algorithms, and global
  optimization</title>
    <summary>  In this paper we use group, action and orbit to understand how evolutionary
solve nonconvex optimization problems.
</summary>
    <author>
      <name>Andrew Clark</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1301.0254v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1301.0254v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.RA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90-02, 90-08, 90B99, 37D05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.1; G.1.6; G.2.0; I.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.7056v1</id>
    <updated>2013-05-30T10:28:06Z</updated>
    <published>2013-05-30T10:28:06Z</published>
    <title>Dienstplanerstellung in Krankenhaeusern mittels genetischer Algorithmen</title>
    <summary>  This thesis investigates the use of problem-specific knowledge to enhance a
genetic algorithm approach to multiple-choice optimisation problems. It shows
that such information can significantly enhance performance, but that the
choice of information and the way it is included are important factors for
success.
</summary>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Diplomarbeit, in German, Universitaet Mannheim, 1996</arxiv:comment>
    <link href="http://arxiv.org/abs/1305.7056v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.7056v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.5519v1</id>
    <updated>2013-07-21T11:29:16Z</updated>
    <published>2013-07-21T11:29:16Z</published>
    <title>Optimal Recombination in Genetic Algorithms</title>
    <summary>  This paper surveys results on complexity of the optimal recombination problem
(ORP), which consists in finding the best possible offspring as a result of a
recombination operator in a genetic algorithm, given two parent solutions. We
consider efficient reductions of the ORPs, allowing to establish polynomial
solvability or NP-hardness of the ORPs, as well as direct proofs of hardness
results.
</summary>
    <author>
      <name>Anton V. Eremeev</name>
    </author>
    <author>
      <name>Julia V. Kovalenko</name>
    </author>
    <link href="http://arxiv.org/abs/1307.5519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.5519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.6995v2</id>
    <updated>2013-08-02T21:04:24Z</updated>
    <published>2013-07-26T11:11:47Z</published>
    <title>Finite State Machine Synthesis for Evolutionary Hardware</title>
    <summary>  This article considers application of genetic algorithms for finite machine
synthesis. The resulting genetic finite state machines synthesis algorithm
allows for creation of machines with less number of states and within shorter
time. This makes it possible to use hardware-oriented genetic finite machines
synthesis algorithm in autonomous systems on reconfigurable platforms.
</summary>
    <author>
      <name>Andrey Bereza</name>
    </author>
    <author>
      <name>Maksim Lyashov</name>
    </author>
    <author>
      <name>Luis Blanco</name>
    </author>
    <link href="http://arxiv.org/abs/1307.6995v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.6995v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.4044v1</id>
    <updated>2013-12-14T13:33:48Z</updated>
    <published>2013-12-14T13:33:48Z</published>
    <title>CACO : Competitive Ant Colony Optimization, A Nature-Inspired
  Metaheuristic For Large-Scale Global Optimization</title>
    <summary>  Large-scale problems are nonlinear problems that need metaheuristics, or
global optimization algorithms. This paper reviews nature-inspired
metaheuristics, then it introduces a framework named Competitive Ant Colony
Optimization inspired by the chemical communications among insects. Then a case
study is presented to investigate the proposed framework for large-scale global
optimization.
</summary>
    <author>
      <name>M. A. El-Dosuky</name>
    </author>
    <link href="http://arxiv.org/abs/1312.4044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.4044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.4696v1</id>
    <updated>2014-01-19T16:42:57Z</updated>
    <published>2014-01-19T16:42:57Z</published>
    <title>Evolutionary Optimization for Decision Making under Uncertainty</title>
    <summary>  Optimizing decision problems under uncertainty can be done using a variety of
solution methods. Soft computing and heuristic approaches tend to be powerful
for solving such problems. In this overview article, we survey Evolutionary
Optimization techniques to solve Stochastic Programming problems - both for the
single-stage and multi-stage case.
</summary>
    <author>
      <name>Ronald Hochreiter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Keynote talk at the MENDEL 2011</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of MENDEL 2011: 107-113. 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1401.4696v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.4696v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.4699v1</id>
    <updated>2014-02-19T15:33:35Z</updated>
    <published>2014-02-19T15:33:35Z</published>
    <title>A Powerful Genetic Algorithm for Traveling Salesman Problem</title>
    <summary>  This paper presents a powerful genetic algorithm(GA) to solve the traveling
salesman problem (TSP). To construct a powerful GA, I use edge swapping(ES)
with a local search procedure to determine good combinations of building blocks
of parent solutions for generating even better offspring solutions.
Experimental results on well studied TSP benchmarks demonstrate that the
proposed GA is competitive in finding very high quality solutions on instances
with up to 16,862 cities.
</summary>
    <author>
      <name>Shujia Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.4699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.4699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.3115v1</id>
    <updated>2014-03-12T21:19:26Z</updated>
    <published>2014-03-12T21:19:26Z</published>
    <title>Memory Capacity of Neural Networks using a Circulant Weight Matrix</title>
    <summary>  This paper presents results on the memory capacity of a generalized feedback
neural network using a circulant matrix. Children are capable of learning soon
after birth which indicates that the neural networks of the brain have prior
learnt capacity that is a consequence of the regular structures in the brain's
organization. Motivated by this idea, we consider the capacity of circulant
matrices as weight matrices in a feedback network.
</summary>
    <author>
      <name>Vamsi Sashank Kotagiri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.3115v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.3115v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.1958v2</id>
    <updated>2015-08-22T00:51:00Z</updated>
    <published>2014-05-08T15:05:28Z</published>
    <title>A Self-Adaptive Network Protection System</title>
    <summary>  In this treatise we aim to build a hybrid network automated (self-adaptive)
security threats discovery and prevention system; by using unconventional
techniques and methods, including fuzzy logic and biological inspired
algorithms under the context of soft computing.
</summary>
    <author>
      <name>Mohamed Hassan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">91. arXiv admin note: text overlap with arXiv:1204.1336 by other
  authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.1958v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.1958v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2507v4</id>
    <updated>2014-08-07T05:17:20Z</updated>
    <published>2014-06-10T11:00:54Z</published>
    <title>WebAL-1: Workshop on Artificial Life and the Web 2014 Proceedings</title>
    <summary>  Proceedings of WebAL-1: Workshop on Artificial Life and the Web 2014, held at
the 14th International Conference on the Synthesis and Simulation of Living
Systems (ALIFE 14), New York, NY, 31 July 2014.
</summary>
    <author>
      <name>Tim Taylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Editors: Tim Taylor, Josh Auerbach, Josh Bongard, Jeff Clune, Simon
  Hickinbotham, Greg Hornby</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2507v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2507v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2671v1</id>
    <updated>2014-06-10T19:21:33Z</updated>
    <published>2014-06-10T19:21:33Z</published>
    <title>Conceptors: an easy introduction</title>
    <summary>  Conceptors provide an elementary neuro-computational mechanism which sheds a
fresh and unifying light on a diversity of cognitive phenomena. A number of
demanding learning and processing tasks can be solved with unprecedented ease,
robustness and accuracy. Some of these tasks were impossible to solve before.
This entirely informal paper introduces the basic principles of conceptors and
highlights some of their usages.
</summary>
    <author>
      <name>Herbert Jaeger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2671v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2671v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.3100v1</id>
    <updated>2014-06-12T02:08:31Z</updated>
    <published>2014-06-12T02:08:31Z</published>
    <title>Learning ELM network weights using linear discriminant analysis</title>
    <summary>  We present an alternative to the pseudo-inverse method for determining the
hidden to output weight values for Extreme Learning Machines performing
classification tasks. The method is based on linear discriminant analysis and
provides Bayes optimal single point estimates for the weight values.
</summary>
    <author>
      <name>Philip de Chazal</name>
    </author>
    <author>
      <name>Jonathan Tapson</name>
    </author>
    <author>
      <name>André van Schaik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In submission to the ELM 2014 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.3100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.3100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.5719v1</id>
    <updated>2014-07-22T03:11:28Z</updated>
    <published>2014-07-22T03:11:28Z</published>
    <title>Artificial Life and the Web: WebAL Comes of Age</title>
    <summary>  A brief survey is presented of the first 18 years of web-based Artificial
Life ("WebAL") research and applications, covering the period 1995-2013. The
survey is followed by a short discussion of common methodologies employed and
current technologies relevant to WebAL research. The paper concludes with a
quick look at what the future may hold for work in this exciting area.
</summary>
    <author>
      <name>Tim Taylor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at WebAL-1: Workshop on Artificial Life and the Web 2014
  (arXiv:1406.2507)</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.5719v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.5719v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.7737v1</id>
    <updated>2014-07-29T14:26:57Z</updated>
    <published>2014-07-29T14:26:57Z</published>
    <title>A CUDA-Based Real Parameter Optimization Benchmark</title>
    <summary>  Benchmarking is key for developing and comparing optimization algorithms. In
this paper, a CUDA-based real parameter optimization benchmark (cuROB) is
introduced. Test functions of diverse properties are included within cuROB and
implemented efficiently with CUDA. Speedup of one order of magnitude can be
achieved in comparison with CPU-based benchmark of CEC'14.
</summary>
    <author>
      <name>Ke Ding</name>
    </author>
    <author>
      <name>Ying Tan</name>
    </author>
    <link href="http://arxiv.org/abs/1407.7737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.7737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.3277v1</id>
    <updated>2014-11-12T18:56:45Z</updated>
    <published>2014-11-12T18:56:45Z</published>
    <title>Using Ants as a Genetic Crossover Operator in GLS to Solve STSP</title>
    <summary>  Ant Colony Algorithm (ACA) and Genetic Local Search (GLS) are two
optimization algorithms that have been successfully applied to the Traveling
Salesman Problem (TSP). In this paper we define new crossover operator then
redefine ACAs ants as operate according to defined crossover operator then put
forward our GLS that uses these ants to solve Symmetric TSP (STSP) instances.
</summary>
    <author>
      <name>Hassan Ismkhan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2010 International Conference of Soft Computing and Pattern
  Recognition</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.3277v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.3277v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.5067v1</id>
    <updated>2014-12-16T16:29:34Z</updated>
    <published>2014-12-16T16:29:34Z</published>
    <title>Analysis of Optimal Recombination in Genetic Algorithm for a Scheduling
  Problem with Setups</title>
    <summary>  In this paper, we perform an experimental study of optimal recombination
operator for makespan minimization problem on single machine with
sequence-dependent setup times ($1|s_{vu}|C_{\max}$). The computational
experiment on benchmark problems from TSPLIB library indicates practical
applicability of optimal recombination in crossover operator of genetic
algorithm for $1|s_{vu}|C_{\max}$.
</summary>
    <author>
      <name>A. V. Eremeev</name>
    </author>
    <author>
      <name>Ju. V. Kovalenko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, in Russian</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.5067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.5067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.00299v1</id>
    <updated>2015-01-01T18:37:36Z</updated>
    <published>2015-01-01T18:37:36Z</published>
    <title>Sequence Modeling using Gated Recurrent Neural Networks</title>
    <summary>  In this paper, we have used Recurrent Neural Networks to capture and model
human motion data and generate motions by prediction of the next immediate data
point at each time-step. Our RNN is armed with recently proposed Gated
Recurrent Units which has shown promising results in some sequence modeling
problems such as Machine Translation and Speech Synthesis. We demonstrate that
this model is able to capture long-term dependencies in data and generate
realistic motions.
</summary>
    <author>
      <name>Mohammad Pezeshki</name>
    </author>
    <link href="http://arxiv.org/abs/1501.00299v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.00299v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00130v1</id>
    <updated>2015-01-31T16:10:29Z</updated>
    <published>2015-01-31T16:10:29Z</published>
    <title>The Search for Computational Intelligence</title>
    <summary>  We define and explore in simulation several rules for the local evolution of
generative rules for 1D and 2D cellular automata. Our implementation uses
strategies from conceptual blending. We discuss potential applications to
modelling social dynamics.
</summary>
    <author>
      <name>Joseph Corneli</name>
    </author>
    <author>
      <name>Ewen Maclean</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages. Submitted to Social Aspects of Cognition and Computing
  symposium at AISB 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.00130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; I.6.3; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02444v1</id>
    <updated>2015-02-09T11:45:02Z</updated>
    <published>2015-02-09T11:45:02Z</published>
    <title>On the Dynamics of a Recurrent Hopfield Network</title>
    <summary>  In this research paper novel real/complex valued recurrent Hopfield Neural
Network (RHNN) is proposed. The method of synthesizing the energy landscape of
such a network and the experimental investigation of dynamics of Recurrent
Hopfield Network is discussed. Parallel modes of operation (other than fully
parallel mode) in layered RHNN is proposed. Also, certain potential
applications are proposed.
</summary>
    <author>
      <name>Rama Garimella</name>
    </author>
    <author>
      <name>Berkay Kicanaoglu</name>
    </author>
    <author>
      <name>Moncef Gabbouj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures, 1 table, submitted to IJCNN-2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1502.02444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.00444v1</id>
    <updated>2015-05-03T16:11:26Z</updated>
    <published>2015-05-03T16:11:26Z</published>
    <title>Some Theoretical Properties of a Network of Discretely Firing Neurons</title>
    <summary>  The problem of optimising a network of discretely firing neurons is
addressed. An objective function is introduced which measures the average
number of bits that are needed for the network to encode its state. When this
is minimised, it is shown that this leads to a number of results, such as
topographic mappings, piecewise linear dependence on the input of the
probability of a neuron firing, and factorial encoder networks.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.00444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.00444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.01573v1</id>
    <updated>2015-06-04T13:08:04Z</updated>
    <published>2015-06-04T13:08:04Z</published>
    <title>Programs as Polypeptides</title>
    <summary>  We describe a visual programming language for defining behaviors manifested
by reified actors in a 2D virtual world that can be compiled into programs
comprised of sequences of combinators that are themselves reified as actors.
This makes it possible to build programs that build programs from components of
a few fixed types delivered by diffusion using processes that resemble
chemistry as much as computation.
</summary>
    <author>
      <name>Lance R. Williams</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in European Conference on Artificial Life (ECAL '15), York, UK, 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.01573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.01573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02788v1</id>
    <updated>2015-08-12T01:01:11Z</updated>
    <published>2015-08-12T01:01:11Z</published>
    <title>The Effects of Hyperparameters on SGD Training of Neural Networks</title>
    <summary>  The performance of neural network classifiers is determined by a number of
hyperparameters, including learning rate, batch size, and depth. A number of
attempts have been made to explore these parameters in the literature, and at
times, to develop methods for optimizing them. However, exploration of
parameter spaces has often been limited. In this note, I report the results of
large scale experiments exploring these different parameters and their
interactions.
</summary>
    <author>
      <name>Thomas M. Breuel</name>
    </author>
    <link href="http://arxiv.org/abs/1508.02788v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02788v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.05925v1</id>
    <updated>2016-02-18T19:56:39Z</updated>
    <published>2016-02-18T19:56:39Z</published>
    <title>Encoding Data for HTM Systems</title>
    <summary>  Hierarchical Temporal Memory (HTM) is a biologically inspired machine
intelligence technology that mimics the architecture and processes of the
neocortex. In this white paper we describe how to encode data as Sparse
Distributed Representations (SDRs) for use in HTM systems. We explain several
existing encoders, which are available through the open source project called
NuPIC, and we discuss requirements for creating encoders for new types of data.
</summary>
    <author>
      <name>Scott Purdy</name>
    </author>
    <link href="http://arxiv.org/abs/1602.05925v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.05925v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.07893v3</id>
    <updated>2016-08-28T09:56:23Z</updated>
    <published>2016-03-25T12:28:02Z</published>
    <title>Investigation Into The Effectiveness Of Long Short Term Memory Networks
  For Stock Price Prediction</title>
    <summary>  The effectiveness of long short term memory networks trained by
backpropagation through time for stock price prediction is explored in this
paper. A range of different architecture LSTM networks are constructed trained
and tested.
</summary>
    <author>
      <name>Hengjian Jia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.07893v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.07893v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.01982v1</id>
    <updated>2016-08-16T02:18:33Z</updated>
    <published>2016-08-16T02:18:33Z</published>
    <title>Uniform Transformation of Non-Separable Probability Distributions</title>
    <summary>  A theoretical framework is developed to describe the transformation that
distributes probability density functions uniformly over space. In one
dimension, the cumulative distribution can be used, but does not generalize to
higher dimensions, or non-separable distributions. A potential function is
shown to link probability density functions to their transformation, and to
generalize the cumulative. A numerical method is developed to compute the
potential, and examples are shown in two dimensions.
</summary>
    <author>
      <name>Eric Kee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.01982v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.01982v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01430v2</id>
    <updated>2016-10-06T13:28:41Z</updated>
    <published>2016-10-05T14:14:51Z</published>
    <title>LAYERS: Yet another Neural Network toolkit</title>
    <summary>  Layers is an open source neural network toolkit aim at providing an easy way
to implement modern neural networks. The main user target are students and to
this end layers provides an easy scriptting language that can be early adopted.
The user has to focus only on design details as network totpology and parameter
tunning.
</summary>
    <author>
      <name>Roberto Paredes</name>
    </author>
    <author>
      <name>José-Miguel Benedí</name>
    </author>
    <link href="http://arxiv.org/abs/1610.01430v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01430v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.06212v1</id>
    <updated>2016-12-19T14:59:14Z</updated>
    <published>2016-12-19T14:59:14Z</published>
    <title>A recurrent neural network without chaos</title>
    <summary>  We introduce an exceptionally simple gated recurrent neural network (RNN)
that achieves performance comparable to well-known gated architectures, such as
LSTMs and GRUs, on the word-level language modeling task. We prove that our
model has simple, predicable and non-chaotic dynamics. This stands in stark
contrast to more standard gated architectures, whose underlying dynamical
systems exhibit chaotic behavior.
</summary>
    <author>
      <name>Thomas Laurent</name>
    </author>
    <author>
      <name>James von Brecht</name>
    </author>
    <link href="http://arxiv.org/abs/1612.06212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.06212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05923v1</id>
    <updated>2017-01-20T20:53:51Z</updated>
    <published>2017-01-20T20:53:51Z</published>
    <title>Gate-Variants of Gated Recurrent Unit (GRU) Neural Networks</title>
    <summary>  The paper evaluates three variants of the Gated Recurrent Unit (GRU) in
recurrent neural networks (RNN) by reducing parameters in the update and reset
gates. We evaluate the three variant GRU models on MNIST and IMDB datasets and
show that these GRU-RNN variant models perform as well as the original GRU RNN
model while reducing the computational expense.
</summary>
    <author>
      <name>Rahul Dey</name>
    </author>
    <author>
      <name>Fathi M. Salem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 8 Figures, 4 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.05923v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05923v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02217v2</id>
    <updated>2017-04-24T16:33:34Z</updated>
    <published>2017-02-07T22:20:43Z</published>
    <title>Multitask Evolution with Cartesian Genetic Programming</title>
    <summary>  We introduce a genetic programming method for solving multiple Boolean
circuit synthesis tasks simultaneously. This allows us to solve a set of
elementary logic functions twice as easily as with a direct, single-task
approach.
</summary>
    <author>
      <name>Eric O. Scott</name>
    </author>
    <author>
      <name>Kenneth A. De Jong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.02217v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02217v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.00207v2</id>
    <updated>2017-04-04T23:48:24Z</updated>
    <published>2017-04-01T18:22:33Z</published>
    <title>A Brownian Motion Model and Extreme Belief Machine for Modeling Sensor
  Data Measurements</title>
    <summary>  As the title suggests, we will describe (and justify through the presentation
of some of the relevant mathematics) prediction methodologies for sensor
measurements. This exposition will mainly be concerned with the mathematics
related to modeling the sensor measurements.
</summary>
    <author>
      <name>Robert A. Murphy</name>
    </author>
    <link href="http://arxiv.org/abs/1704.00207v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.00207v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="60J70" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01365v1</id>
    <updated>2017-05-03T11:29:28Z</updated>
    <published>2017-05-03T11:29:28Z</published>
    <title>Quantified advantage of discontinuous weight selection in approximations
  with deep neural networks</title>
    <summary>  We consider approximations of 1D Lipschitz functions by deep ReLU networks of
a fixed width. We prove that without the assumption of continuous weight
selection the uniform approximation error is lower than with this assumption at
least by a factor logarithmic in the size of the network.
</summary>
    <author>
      <name>Dmitry Yarotsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, submitted to J. Approx. Theory</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.01365v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01365v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.08675v1</id>
    <updated>2017-06-27T05:28:06Z</updated>
    <published>2017-06-27T05:28:06Z</published>
    <title>Proceedings of the First International Workshop on Deep Learning and
  Music</title>
    <summary>  Proceedings of the First International Workshop on Deep Learning and Music,
joint with IJCNN, Anchorage, US, May 17-18, 2017
</summary>
    <author>
      <name>Dorien Herremans</name>
    </author>
    <author>
      <name>Ching-Hua Chuan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.22227.99364/1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.22227.99364/1" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Workshop on Deep Learning
  and Music, joint with IJCNN, Anchorage, US, May 17-18, 2017</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.08675v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.08675v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.02617v1</id>
    <updated>2017-07-09T18:34:45Z</updated>
    <published>2017-07-09T18:34:45Z</published>
    <title>Deepest Neural Networks</title>
    <summary>  This paper shows that a long chain of perceptrons (that is, a multilayer
perceptron, or MLP, with many hidden layers of width one) can be a universal
classifier. The classification procedure is not necessarily computationally
efficient, but the technique throws some light on the kind of computations
possible with narrow and deep MLPs.
</summary>
    <author>
      <name>Raul Rojas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.02617v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.02617v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.04498v1</id>
    <updated>2017-06-16T12:29:41Z</updated>
    <published>2017-06-16T12:29:41Z</published>
    <title>Self-adaptive node-based PCA encodings</title>
    <summary>  In this paper we propose an algorithm, Simple Hebbian PCA, and prove that it
is able to calculate the principal component analysis (PCA) in a distributed
fashion across nodes. It simplifies existing network structures by removing
intralayer weights, essentially cutting the number of weights that need to be
trained in half.
</summary>
    <author>
      <name>Leonard Johard</name>
    </author>
    <author>
      <name>Victor Rivera</name>
    </author>
    <author>
      <name>Manuel Mazzara</name>
    </author>
    <author>
      <name>JooYoung Lee</name>
    </author>
    <link href="http://arxiv.org/abs/1708.04498v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.04498v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.02797v1</id>
    <updated>2017-09-06T07:26:59Z</updated>
    <published>2017-09-06T07:26:59Z</published>
    <title>On the exact relationship between the denoising function and the data
  distribution</title>
    <summary>  We prove an exact relationship between the optimal denoising function and the
data distribution in the case of additive Gaussian noise, showing that
denoising implicitly models the structure of data allowing it to be exploited
in the unsupervised learning of representations. This result generalizes a
known relationship [2], which is valid only in the limit of small corruption
noise.
</summary>
    <author>
      <name>Heikki Arponen</name>
    </author>
    <author>
      <name>Matti Herranen</name>
    </author>
    <author>
      <name>Harri Valpola</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.02797v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.02797v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.04036v1</id>
    <updated>2017-10-11T12:38:59Z</updated>
    <published>2017-10-11T12:38:59Z</published>
    <title>Porcellio scaber algorithm (PSA) for solving constrained optimization
  problems</title>
    <summary>  In this paper, we extend a bio-inspired algorithm called the porcellio scaber
algorithm (PSA) to solve constrained optimization problems, including a
constrained mixed discrete-continuous nonlinear optimization problem. Our
extensive experiment results based on benchmark optimization problems show that
the PSA has a better performance than many existing methods or algorithms. The
results indicate that the PSA is a promising algorithm for constrained
optimization.
</summary>
    <author>
      <name>Yinyan Zhang</name>
    </author>
    <author>
      <name>Shuai Li</name>
    </author>
    <author>
      <name>Hongliang Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.04036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.04036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.00964v2</id>
    <updated>2018-06-12T07:13:08Z</updated>
    <published>2017-12-04T09:13:16Z</published>
    <title>Drift Analysis</title>
    <summary>  Drift analysis is one of the major tools for analysing evolutionary
algorithms and nature-inspired search heuristics. In this chapter we give an
introduction to drift analysis and give some examples of how to use it for the
analysis of evolutionary algorithms.
</summary>
    <author>
      <name>Johannes Lengler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This article will become a chapter in a book on Theory of
  Evolutionary Algorithms that will be published by Springer, edited by
  Benjamin Doerr and Frank Neumann</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.00964v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.00964v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08729v1</id>
    <updated>2018-01-21T17:24:31Z</updated>
    <published>2018-01-21T17:24:31Z</published>
    <title>Review: Metaheuristic Search-Based Fuzzy Clustering Algorithms</title>
    <summary>  Fuzzy clustering is a famous unsupervised learning method used to collecting
similar data elements within cluster according to some similarity measurement.
But, clustering algorithms suffer from some drawbacks. Among the main weakness
including, selecting the initial cluster centres and the appropriate clusters
number is normally unknown. These weaknesses are considered the most
challenging tasks in clustering algorithms. This paper introduces a
comprehensive review of metahueristic search to solve fuzzy clustering
algorithms problems.
</summary>
    <author>
      <name>Waleed Alomoush</name>
    </author>
    <author>
      <name>Ayat Alrosan</name>
    </author>
    <link href="http://arxiv.org/abs/1802.08729v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08729v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.04187v1</id>
    <updated>2018-04-11T19:57:06Z</updated>
    <published>2018-04-11T19:57:06Z</published>
    <title>Coevolutionary Neural Population Models</title>
    <summary>  We present a method for using neural networks to model evolutionary
population dynamics, and draw parallels to recent deep learning advancements in
which adversarially-trained neural networks engage in coevolutionary
interactions. We conduct experiments which demonstrate that models from
evolutionary game theory are capable of describing the behavior of these neural
population systems.
</summary>
    <author>
      <name>Nick Moran</name>
    </author>
    <author>
      <name>Jordan Pollack</name>
    </author>
    <link href="http://arxiv.org/abs/1804.04187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.04187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01632v1</id>
    <updated>2018-04-22T13:32:44Z</updated>
    <published>2018-04-22T13:32:44Z</published>
    <title>Why the Firefly Algorithm Works?</title>
    <summary>  Firefly algorithm is a nature-inspired optimization algorithm and there have
been significant developments since its appearance about ten years ago. This
chapter summarizes the latest developments about the firefly algorithm and its
variants as well as their diverse applications. Future research directions are
also highlighted.
</summary>
    <author>
      <name>Xin-She Yang</name>
    </author>
    <author>
      <name>Xingshi He</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-67669-2_11</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-67669-2_11" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Book chapter, 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W20, 90C30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06230v1</id>
    <updated>2018-07-17T05:34:07Z</updated>
    <published>2018-07-17T05:34:07Z</published>
    <title>Genetic algorithms in Forth</title>
    <summary>  A method for automatically finding a program (bytecode) realizing the given
algorithm is developed. The algorithm is specified as a set of tests
(input\_data) $ \rightarrow $ (output\_data). Genetic methods made it possible
to find the implementation of relatively complex algorithms: sorting, decimal
digits, GCD, LCM, factorial, prime divisors, binomial coefficients, and others.
The algorithms are implemented on a highly simplified version of Forth
language.
</summary>
    <author>
      <name>S. I. Khashin</name>
    </author>
    <author>
      <name>S. E. Vaganov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.06230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.07074v1</id>
    <updated>2018-10-12T19:59:48Z</updated>
    <published>2018-10-12T19:59:48Z</published>
    <title>Why We Do Not Evolve Software? Analysis of Evolutionary Algorithms</title>
    <summary>  In this paper, we review the state-of-the-art results in evolutionary
computation and observe that we do not evolve non trivial software from scratch
and with no human intervention. A number of possible explanations are
considered, but we conclude that computational complexity of the problem
prevents it from being solved as currently attempted. A detailed analysis of
necessary and available computational resources is provided to support our
findings.
</summary>
    <author>
      <name>Roman V. Yampolskiy</name>
    </author>
    <link href="http://arxiv.org/abs/1810.07074v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.07074v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.05290v1</id>
    <updated>2018-11-13T13:55:18Z</updated>
    <published>2018-11-13T13:55:18Z</published>
    <title>Towards the Design of Aerostat Wind Turbine Arrays through AI</title>
    <summary>  A new form of aerostat wind generation system which contains an array of
interacting turbines is proposed. The design of the balloon turbine components
is undertaken through the combination of artificial intelligence and rapid
prototyping techniques such that the need for highly accurate
models/simulations of the lift and wake dynamics is removed/reduced. Initial
small-scale wind tunnel testing to determine design and algorithmic
fundamentals will be presented.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <author>
      <name>Neil Phillips</name>
    </author>
    <link href="http://arxiv.org/abs/1811.05290v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.05290v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.00983v3</id>
    <updated>2019-01-28T13:41:54Z</updated>
    <published>2019-01-04T05:21:29Z</published>
    <title>Brief Review of Computational Intelligence Algorithms</title>
    <summary>  Computational Intelligence algorithms have gained a lot of attention of
researchers in the recent years due to their ability to deliver near optimal
solutions.
</summary>
    <author>
      <name>Satyarth Vaidya</name>
    </author>
    <author>
      <name>Arshveer Kaur</name>
    </author>
    <author>
      <name>Lavika Goel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">major error and re-ordering needed</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.00983v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.00983v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.11598v1</id>
    <updated>2019-03-27T11:44:09Z</updated>
    <published>2019-03-27T11:44:09Z</published>
    <title>A Simple Haploid-Diploid Evolutionary Algorithm</title>
    <summary>  It has recently been suggested that evolution exploits a form of fitness
landscape smoothing within eukaryotic sex due to the haploid-diploid cycle.
This short paper presents a simple modification to the standard evolutionary
computing algorithm to similarly benefit from the process. Using the well-known
NK model of fitness landscapes it is shown that the benefit emerges as
ruggedness is increased.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1903.07429,
  arXiv:1811.04073, arXiv:1808.03471</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.11598v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.11598v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.09419v1</id>
    <updated>2019-05-22T10:07:06Z</updated>
    <published>2019-05-22T10:07:06Z</published>
    <title>Effect of shapes of activation functions on predictability in the echo
  state network</title>
    <summary>  We investigate prediction accuracy for time series of Echo state networks
with respect to several kinds of activation functions. As a result, we found
that some kinds of activation functions with an appropriate nonlinearity show
high performance compared to the conventional sigmoid function.
</summary>
    <author>
      <name>Hanten Chang</name>
    </author>
    <author>
      <name>Shinji Nakaoka</name>
    </author>
    <author>
      <name>Hiroyasu Ando</name>
    </author>
    <link href="http://arxiv.org/abs/1905.09419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.09419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.10988v1</id>
    <updated>2019-07-25T11:58:41Z</updated>
    <published>2019-07-25T11:58:41Z</published>
    <title>Benchmarking HillVallEA for the GECCO 2019 Competition on Multimodal
  Optimization</title>
    <summary>  This report presents benchmarking results of the Hill-Valley Evolutionary
Algorithm version 2019 (HillVallEA19) on the CEC2013 niching benchmark suite
under the restrictions of the GECCO 2019 niching competition on multimodal
optimization. Performance is compared to algorithms that participated in
previous editions of the niching competition.
</summary>
    <author>
      <name>S. C. Maree</name>
    </author>
    <author>
      <name>T. Alderliesten</name>
    </author>
    <author>
      <name>P. A. N. Bosman</name>
    </author>
    <link href="http://arxiv.org/abs/1907.10988v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.10988v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.11554v2</id>
    <updated>2019-07-29T09:54:55Z</updated>
    <published>2019-07-26T13:04:53Z</published>
    <title>Autoencoding with a Learning Classifier System: Initial Results</title>
    <summary>  Autoencoders enable data dimensionality reduction and a key component of many
(deep) learning systems. This short paper introduces a form of Holland's
Learning Classifier System (LCS) to perform autoencoding building upon a
previously presented form of LCS that utilises unsupervised learning for
clustering. Initial results using a neural network representation suggest it is
an effective approach to reduction.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <link href="http://arxiv.org/abs/1907.11554v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.11554v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.02130v1</id>
    <updated>2019-07-30T16:57:38Z</updated>
    <published>2019-07-30T16:57:38Z</published>
    <title>Deep learning research landscape &amp; roadmap in a nutshell: past, present
  and future -- Towards deep cortical learning</title>
    <summary>  The past, present and future of deep learning is presented in this work.
Given this landscape &amp; roadmap, we predict that deep cortical learning will be
the convergence of deep learning &amp; cortical learning which builds an artificial
cortical column ultimately.
</summary>
    <author>
      <name>Aras R. Dargazany</name>
    </author>
    <link href="http://arxiv.org/abs/1908.02130v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.02130v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.00490v1</id>
    <updated>2019-11-01T17:57:03Z</updated>
    <published>2019-11-01T17:57:03Z</published>
    <title>Variations of Genetic Algorithms</title>
    <summary>  The goal of this project is to develop the Genetic Algorithms (GA) for
solving the Schaffer F6 function in fewer than 4000 function evaluations on a
total of 30 runs. Four types of Genetic Algorithms (GA) are presented -
Generational GA (GGA), Steady-State (mu+1)-GA (SSGA), Steady-Generational
(mu,mu)-GA (SGGA), and (mu+mu)-GA.
</summary>
    <author>
      <name>Alison Jenkins</name>
    </author>
    <author>
      <name>Vinika Gupta</name>
    </author>
    <author>
      <name>Alexis Myrick</name>
    </author>
    <author>
      <name>Mary Lenoir</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">genetic algorithm, elitism, generational, steady-state</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.00490v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.00490v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91E40" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.01673v1</id>
    <updated>2020-02-05T07:47:07Z</updated>
    <published>2020-02-05T07:47:07Z</published>
    <title>Convergence analysis of particle swarm optimization using stochastic
  Lyapunov functions and quantifier elimination</title>
    <summary>  This paper adds to the discussion about theoretical aspects of particle swarm
stability by proposing to employ stochastic Lyapunov functions and to determine
the convergence set by quantifier elimination. We present a computational
procedure and show that this approach leads to reevaluation and extension of
previously know stability regions for PSO using a Lyapunov approach under
stagnation assumptions.
</summary>
    <author>
      <name>Maximilian Gerwien</name>
    </author>
    <author>
      <name>Rick Voßwinkel</name>
    </author>
    <author>
      <name>Hendrik Richter</name>
    </author>
    <link href="http://arxiv.org/abs/2002.01673v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.01673v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.04747v1</id>
    <updated>2021-02-20T09:36:00Z</updated>
    <published>2021-02-20T09:36:00Z</published>
    <title>Info-Evo: Using Information Geometry to Guide Evolutionary Program
  Learning</title>
    <summary>  A novel optimization strategy, Info-Evo, is described, in which natural
gradient search using nonparametric Fisher information is used to provide
ongoing guidance to an evolutionary learning algorithm, so that the
evolutionary process preferentially moves in the directions identified as
"shortest paths" according to the natural gradient. Some specifics regarding
the application of this approach to automated program learning are reviewed,
including a strategy for integrating Info-Evo into the MOSES program learning
framework.
</summary>
    <author>
      <name>Ben Goertzel</name>
    </author>
    <link href="http://arxiv.org/abs/2103.04747v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.04747v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.00014v1</id>
    <updated>2021-05-31T16:27:50Z</updated>
    <published>2021-05-31T16:27:50Z</published>
    <title>Diffusion Self-Organizing Map on the Hypersphere</title>
    <summary>  We discuss a diffusion based implementation of the self-organizing map on the
unit hypersphere. We show that this approach can be efficiently implemented
using just linear algebra methods, we give a python numpy implementation, and
we illustrate the approach using the well known MNIST dataset.
</summary>
    <author>
      <name>M. Andrecut</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.00014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.00014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.11804v1</id>
    <updated>2021-06-17T22:21:46Z</updated>
    <published>2021-06-17T22:21:46Z</published>
    <title>Evo* 2021 -- Late-Breaking Abstracts Volume</title>
    <summary>  Volume with the Late-Breaking Abstracts submitted to the Evo* 2021
Conference, held online from 7 to 9 of April 2021. These papers present ongoing
research and preliminary results investigating on the application of different
approaches of Bioinspired Methods (mainly Evolutionary Computation) to
different problems, most of them real world ones.
</summary>
    <author>
      <name>A. M. Mora</name>
    </author>
    <author>
      <name>A. I. Esparcia-Alcázar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LBAs accepted in Evo* 2021. Part of the Conference Proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.11804v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.11804v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05, 68W20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.0; I.2; K.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.15295v1</id>
    <updated>2021-06-29T12:16:19Z</updated>
    <published>2021-06-29T12:16:19Z</published>
    <title>Reliable and Fast Recurrent Neural Network Architecture Optimization</title>
    <summary>  This article introduces Random Error Sampling-based Neuroevolution (RESN), a
novel automatic method to optimize recurrent neural network architectures. RESN
combines an evolutionary algorithm with a training-free evaluation approach.
The results show that RESN achieves state-of-the-art error performance while
reducing by half the computational time.
</summary>
    <author>
      <name>Andrés Camero</name>
    </author>
    <author>
      <name>Jamal Toutouh</name>
    </author>
    <author>
      <name>Enrique Alba</name>
    </author>
    <link href="http://arxiv.org/abs/2106.15295v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.15295v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.06318v1</id>
    <updated>2021-08-13T17:11:05Z</updated>
    <published>2021-08-13T17:11:05Z</published>
    <title>Neuromorphic Processing: A Unifying Tutorial</title>
    <summary>  All systolic or distributed neuromorphic architectures require
power-efficient processing nodes. In this paper, a unifying tutorial is
presented which implements multiple neuromorphic processing elements using a
systematic analog approach including synapse, neuron and astrocyte models. It
is shown that the proposed approach can successfully synthesize
multidimensional dynamical systems into analog circuitry with minimum effort.
</summary>
    <author>
      <name>Hamid Soleimani</name>
    </author>
    <author>
      <name>Emmanuel. M. Drakakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2007.13941</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.06318v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.06318v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.10670v1</id>
    <updated>2022-03-20T22:56:18Z</updated>
    <published>2022-03-20T22:56:18Z</published>
    <title>Fully Convolutional Fractional Scaling</title>
    <summary>  We introduce a fully convolutional fractional scaling component, FCFS. Fully
convolutional networks can be applied to any size input and previously did not
support non-integer scaling. Our architecture is simple with an efficient
single layer implementation. Examples and code implementations of three common
scaling methods are published.
</summary>
    <author>
      <name>Michael Soloveitchik</name>
    </author>
    <author>
      <name>Michael Werman</name>
    </author>
    <link href="http://arxiv.org/abs/2203.10670v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.10670v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.13202v1</id>
    <updated>2022-03-16T13:11:50Z</updated>
    <published>2022-03-16T13:11:50Z</published>
    <title>Multi Expression Programming for solving classification problems</title>
    <summary>  Multi Expression Programming (MEP) is a Genetic Programming variant which
encodes multiple solutions in a single chromosome. This paper introduces and
deeply describes several strategies for solving binary and multi-class
classification problems within the \textit{multi solutions per chromosome}
paradigm of MEP. Extensive experiments on various classification problems are
performed. MEP shows similar or better performances than other methods used for
comparison (namely Artificial Neural Networks and Linear Genetic Programming).
</summary>
    <author>
      <name>Mihai Oltean</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.13202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.13202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.05437v1</id>
    <updated>2022-04-11T23:10:42Z</updated>
    <published>2022-04-11T23:10:42Z</published>
    <title>Implementing Online Reinforcement Learning with Temporal Neural Networks</title>
    <summary>  A Temporal Neural Network (TNN) architecture for implementing efficient
online reinforcement learning is proposed and studied via simulation. The
proposed T-learning system is composed of a frontend TNN that implements online
unsupervised clustering and a backend TNN that implements online reinforcement
learning. The reinforcement learning paradigm employs biologically plausible
neo-Hebbian three-factor learning rules. As a working example, a prototype
implementation of the cart-pole problem (balancing an inverted pendulum) is
studied via simulation.
</summary>
    <author>
      <name>James E. Smith</name>
    </author>
    <link href="http://arxiv.org/abs/2204.05437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.05437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T07" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.6082v1</id>
    <updated>2012-10-22T22:59:58Z</updated>
    <published>2012-10-22T22:59:58Z</published>
    <title>Interplay: Dispersed Activation in Neural Networks</title>
    <summary>  This paper presents a multi-point stimulation of a Hebbian neural network
with investigation of the interplay between the stimulus waves through the
neurons of the network. Equilibrium of the resulting memory is achieved for
recall of specific memory data at a rate faster than single point stimulus. The
interplay of the intersecting stimuli appears to parallel the clarification
process of recall in biological systems.
</summary>
    <author>
      <name>Richard L. Churchill</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1210.6082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.6082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.5667v1</id>
    <updated>2013-06-24T16:35:37Z</updated>
    <published>2013-06-24T16:35:37Z</published>
    <title>Using Genetic Programming to Model Software</title>
    <summary>  We study a generic program to investigate the scope for automatically
customising it for a vital current task, which was not considered when it was
first written. In detail, we show genetic programming (GP) can evolve models of
aspects of BLAST's output when it is used to map Solexa Next-Gen DNA sequences
to the human genome.
</summary>
    <author>
      <name>W. B. Langdon</name>
    </author>
    <author>
      <name>M. Harman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">As UCL computer science Technical Report RN/13/12</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.5667v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.5667v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.00402v1</id>
    <updated>2019-06-02T13:44:45Z</updated>
    <published>2019-06-02T13:44:45Z</published>
    <title>Push and Pull Search Embedded in an M2M Framework for Solving
  Constrained Multi-objective Optimization Problems</title>
    <summary>  In dealing with constrained multi-objective optimization problems (CMOPs), a
key issue of multi-objective evolutionary algorithms (MOEAs) is to balance the
convergence and diversity of working populations.
</summary>
    <author>
      <name>Zhun Fan</name>
    </author>
    <author>
      <name>Zhaojun Wang</name>
    </author>
    <author>
      <name>Wenji Li</name>
    </author>
    <author>
      <name>Yutong Yuan</name>
    </author>
    <author>
      <name>Yugen You</name>
    </author>
    <author>
      <name>Zhi Yang</name>
    </author>
    <author>
      <name>Fuzan Sun</name>
    </author>
    <author>
      <name>Jie Ruan</name>
    </author>
    <author>
      <name>Zhaocheng Li</name>
    </author>
    <link href="http://arxiv.org/abs/1906.00402v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.00402v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.01241v1</id>
    <updated>2019-06-04T07:20:37Z</updated>
    <published>2019-06-04T07:20:37Z</published>
    <title>Kinetic Market Model: An Evolutionary Algorithm</title>
    <summary>  This research proposes the econophysics kinetic market model as an
evolutionary algorithm's instance. The immediate results from this proposal is
a new replacement rule for family competition genetic algorithms. It also
represents a starting point to adding evolvable entities to kinetic market
models.
</summary>
    <author>
      <name>Evandro Luquini</name>
    </author>
    <author>
      <name>Nizam Omar</name>
    </author>
    <link href="http://arxiv.org/abs/1906.01241v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.01241v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3, I.2.6, I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.09588v1</id>
    <updated>2019-08-25T21:59:26Z</updated>
    <published>2019-08-25T21:59:26Z</published>
    <title>What are Neural Networks made of?</title>
    <summary>  The success of Deep Learning methods is not well understood, though various
attempts at explaining it have been made, typically centered on properties of
stochastic gradient descent. Even less clear is why certain neural network
architectures perform better than others. We provide a potential opening with
the hypothesis that neural network training is a form of Genetic Programming.
</summary>
    <author>
      <name>Rene Schaub</name>
    </author>
    <link href="http://arxiv.org/abs/1909.09588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.09588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.01827v1</id>
    <updated>2020-09-03T17:48:06Z</updated>
    <published>2020-09-03T17:48:06Z</published>
    <title>Tree Neural Networks in HOL4</title>
    <summary>  We present an implementation of tree neural networks within the proof
assistant HOL4. Their architecture makes them naturally suited for
approximating functions whose domain is a set of formulas. We measure the
performance of our implementation and compare it with other machine learning
predictors on the tasks of evaluating arithmetical expressions and estimating
the truth of propositional formulas.
</summary>
    <author>
      <name>Thibault Gauthier</name>
    </author>
    <link href="http://arxiv.org/abs/2009.01827v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.01827v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.10460v1</id>
    <updated>2020-09-22T11:32:20Z</updated>
    <published>2020-09-22T11:32:20Z</published>
    <title>Multi-threaded Memory Efficient Crossover in C++ for Generational
  Genetic Programming</title>
    <summary>  C++ code snippets from a multi-core parallel memory-efficient crossover for
genetic programming are given. They may be adapted for separate generation
evolutionary algorithms where large chromosomes or small RAM require no more
than M + (2 times nthreads) simultaneously active individuals.
</summary>
    <author>
      <name>W. B. Langdon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.10460v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.10460v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.00918v2</id>
    <updated>2021-02-23T12:42:21Z</updated>
    <published>2020-10-02T10:53:46Z</published>
    <title>Are Artificial Dendrites useful in NeuroEvolution?</title>
    <summary>  The significant role of dendritic processing within neuronal networks has
become increasingly clear. This letter explores the effects of including a
simple dendrite-inspired mechanism into neuroevolution. The phenomenon of
separate dendrite activation thresholds on connections is allowed to emerge
under an evolutionary process. It is shown how such processing can be
positively selected for, particularly for connections between the hidden and
output layer, and increases performance.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <link href="http://arxiv.org/abs/2010.00918v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.00918v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.02538v1</id>
    <updated>2021-12-13T15:24:33Z</updated>
    <published>2021-12-13T15:24:33Z</published>
    <title>Improving Surrogate Gradient Learning in Spiking Neural Networks via
  Regularization and Normalization</title>
    <summary>  Spiking neural networks (SNNs) are different from the classical networks used
in deep learning: the neurons communicate using electrical impulses called
spikes, just like biological neurons. SNNs are appealing for AI technology,
because they could be implemented on low power neuromorphic chips. However,
SNNs generally remain less accurate than their analog counterparts. In this
report, we examine various regularization and normalization techniques with the
goal of improving surrogate gradient learning in SNNs.
</summary>
    <author>
      <name>Nandan Meda</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Bachelor Thesis</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.02538v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.02538v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.13280v3</id>
    <updated>2022-07-08T11:37:35Z</updated>
    <published>2022-06-27T13:16:08Z</published>
    <title>Expressive power of binary and ternary neural networks</title>
    <summary>  We show that deep sparse ReLU networks with ternary weights and deep ReLU
networks with binary weights can approximate $\beta$-H\"older functions on
$[0,1]^d$. Also, for any interval $[a,b)\subset\mathbb{R}$, continuous
functions on $[0,1]^d$ can be approximated by networks of depth $2$ with binary
activation function $\mathds{1}_{[a,b)}$.
</summary>
    <author>
      <name>Aleksandr Beknazaryan</name>
    </author>
    <link href="http://arxiv.org/abs/2206.13280v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.13280v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.00555v1</id>
    <updated>2022-08-01T01:07:02Z</updated>
    <published>2022-08-01T01:07:02Z</published>
    <title>Evo* 2022 -- Late-Breaking Abstracts Volume</title>
    <summary>  Volume with the Late-Breaking Abstracts submitted to the Evo* 2022
Conference, held in Madrid (Spain), from 20 to 22 of April. These papers
present ongoing research and preliminary results investigating on the
application of different approaches of Bioinspired Methods (mainly Evolutionary
Computation) to different problems, most of them real world ones.
</summary>
    <author>
      <name>A. M. Mora</name>
    </author>
    <author>
      <name>A. I. Esparcia-Alcázar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LBAs accepted in Evo* 2022. Part of the Conference Proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.00555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.00555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05, 68W20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.0; I.2; K.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.09663v1</id>
    <updated>2022-09-19T09:17:31Z</updated>
    <published>2022-09-19T09:17:31Z</published>
    <title>Autonomous Visual Navigation A Biologically Inspired Approach</title>
    <summary>  Inspired by the navigational behavior observed in the animal kingdom and
especially the navigational behavior of the ants, we attempt to simulate it in
an artificial environment by implementing different kinds of biomimetic
algorithms.
</summary>
    <author>
      <name>Sotirios Athanasoulias</name>
    </author>
    <author>
      <name>Andy Philippides</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">57 pages, 39 figures, Computer Science &amp; Artificial Intelligence
  Dissertation, University of Sussex</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.09663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.09663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92-10, 92-11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.03217v1</id>
    <updated>2022-10-06T21:24:43Z</updated>
    <published>2022-10-06T21:24:43Z</published>
    <title>Genetic algorithm formulation and tuning with use of test functions</title>
    <summary>  This work discusses single-objective constrained genetic algorithm with
floating-point, integer, binary and permutation representation. Floating-point
genetic algorithm tuning with use of test functions is done and leads to a
parameterization with comparatively outstanding performance.
</summary>
    <author>
      <name>Tomasz Tarkowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figure, 2 tables. For associated software repository, see
  https://github.com/ttarkowski/quile</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.03217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.03217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.10232v1</id>
    <updated>2022-10-19T01:17:50Z</updated>
    <published>2022-10-19T01:17:50Z</published>
    <title>Application of Decision Tree Classifier in Detection of Specific Denial
  of Service Attacks with Genetic Algorithm Based Feature Selection on NSL-KDD</title>
    <summary>  Using a Genetic Algorithm and Decision Tree Classifier, the features of the
NSL-KDD dataset are reduced using combinatorial optimization to determine the
minimum features required to accurately classify Denial of Service attacks
within the NSL-KDD dataset.
</summary>
    <author>
      <name>Deanna Wilborne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, 10 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.10232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.10232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.11627v1</id>
    <updated>2023-02-22T20:07:15Z</updated>
    <published>2023-02-22T20:07:15Z</published>
    <title>Particle Swarm Optimization in 3D Medical Image Registration: A
  Systematic Review</title>
    <summary>  Medical image registration seeks to find an optimal spatial transformation
that best aligns the underlying anatomical structures. These problems usually
require the optimization of a similarity metric. Swarm Intelligence techniques
are very effective and efficient optimization methods. This systematic review
focuses on 3D medical image registration using Particle Swarm Optimization
</summary>
    <author>
      <name>Lucia Ballerini</name>
    </author>
    <link href="http://arxiv.org/abs/2302.11627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.11627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.14152v1</id>
    <updated>2023-04-27T12:49:31Z</updated>
    <published>2023-04-27T12:49:31Z</published>
    <title>Spiking Neural Network Decision Feedback Equalization for IM/DD Systems</title>
    <summary>  A spiking neural network (SNN) equalizer with a decision feedback structure
is applied to an IM/DD link with various parameters. The SNN outperforms linear
and artificial neural network (ANN) based equalizers.
</summary>
    <author>
      <name>Alexander von Bank</name>
    </author>
    <author>
      <name>Eike-Manuel Edelmann</name>
    </author>
    <author>
      <name>Laurent Schmalen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for publication at SPPCom 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.14152v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.14152v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.10191v1</id>
    <updated>2023-04-30T02:01:54Z</updated>
    <published>2023-04-30T02:01:54Z</published>
    <title>Sparsifying Spiking Networks through Local Rhythms</title>
    <summary>  It has been well-established that within conventional neural networks, many
of the values produced at each layer are zero. In this work, I demonstrate that
spiking neural networks can prevent the transmission of spikes representing
values close to zero using local information. This can reduce the amount of
energy required for communication and computation in these networks while
preserving accuracy. Additionally, this demonstrates a novel application of
biologically observed spiking rhythms.
</summary>
    <author>
      <name>Wilkie Olin-Ammentorp</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.10191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.10191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.00049v1</id>
    <updated>2023-10-31T18:01:58Z</updated>
    <published>2023-10-31T18:01:58Z</published>
    <title>On the Kolmogorov neural networks</title>
    <summary>  In this paper, we show that the Kolmogorov two hidden layer neural network
model with a continuous, discontinuous bounded or unbounded activation function
in the second hidden layer can precisely represent continuous, discontinuous
bounded and all unbounded multivariate functions, respectively.
</summary>
    <author>
      <name>Aysu Ismayilova</name>
    </author>
    <author>
      <name>Vugar Ismailov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 1 figure; this article uses material from arXiv:2012.03016</arxiv:comment>
    <link href="http://arxiv.org/abs/2311.00049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.00049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="46A22, 46E10, 46N60, 68T05, 92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.00425v1</id>
    <updated>2024-02-01T08:48:12Z</updated>
    <published>2024-02-01T08:48:12Z</published>
    <title>Genetic Programming Theory and Practice: A Fifteen-Year Trajectory</title>
    <summary>  The GPTP workshop series, which began in 2003, has served over the years as a
focal meeting for genetic programming (GP) researchers. As such, we think it
provides an excellent source for studying the development of GP over the past
fifteen years. We thus present herein a trajectory of the thematic developments
in the field of GP.
</summary>
    <author>
      <name>Moshe Sipper</name>
    </author>
    <author>
      <name>Jason H. Moore</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10710-019-09353-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10710-019-09353-5" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Genetic Programming and Evolvable Machines (2020) 21:169-179</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2402.00425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.00425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.13950v1</id>
    <updated>2024-03-20T19:42:11Z</updated>
    <published>2024-03-20T19:42:11Z</published>
    <title>Evo* 2023 -- Late-Breaking Abstracts Volume</title>
    <summary>  Volume with the Late-Breaking Abstracts submitted to the Evo* 2023
Conference, held in Brno (Czech Republic), from 12 to 14 of April. These papers
present ongoing research and preliminary results investigating on the
application of different approaches of Bioinspired Methods (mainly Evolutionary
Computation) to different problems, most of them real world ones.
</summary>
    <author>
      <name>A. M. Mora</name>
    </author>
    <author>
      <name>A. I. Esparcia-Alcázar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LBAs accepted in Evo* 2023. Part of the Conference Proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.13950v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.13950v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05, 68W20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.0; I.2; K.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.00680v1</id>
    <updated>2024-02-19T18:37:27Z</updated>
    <published>2024-02-19T18:37:27Z</published>
    <title>Comparative approach: Electric distribution optimization with loss
  minimization algorithm and particle swarm optimization</title>
    <summary>  Power systems are very large and complex, it can be influenced by many
unexpected events this makes power system optimization problems difficult to
solve, hence methods for solving these problems ought to be an active research
topic. This review presents an overview of important mathematical comparaison
of loss minimization algorithm and particle swarm optimization algorithm in
terms of the performances of electric distribution.
</summary>
    <author>
      <name>Soufiane Bouabbadi</name>
    </author>
    <link href="http://arxiv.org/abs/2405.00680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.00680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.13433v1</id>
    <updated>2024-05-22T08:19:55Z</updated>
    <published>2024-05-22T08:19:55Z</published>
    <title>Towards Exploratory Quality Diversity Landscape Analysis</title>
    <summary>  This work is a preliminary study on using Exploratory Landscape Analysis
(ELA) for Quality Diversity (QD) problems. We seek to understand whether ELA
features can potentially be used to characterise QD problems paving the way for
automating QD algorithm selection. Our results demonstrate that ELA features
are affected by QD optimisation differently than random sampling, and more
specifically, by the choice of variation operator, behaviour function, archive
size and problem dimensionality.
</summary>
    <author>
      <name>Kyriacos Mosphilis</name>
    </author>
    <author>
      <name>Vassilis Vassiliades</name>
    </author>
    <link href="http://arxiv.org/abs/2405.13433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.13433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.11946v1</id>
    <updated>2024-07-19T23:00:38Z</updated>
    <published>2024-07-19T23:00:38Z</published>
    <title>A Technical Note on the Architectural Effects on Maximum Dependency
  Lengths of Recurrent Neural Networks</title>
    <summary>  This work proposes a methodology for determining the maximum dependency
length of a recurrent neural network (RNN), and then studies the effects of
architectural changes, including the number and neuron count of layers, on the
maximum dependency lengths of traditional RNN, gated recurrent unit (GRU), and
long-short term memory (LSTM) models.
</summary>
    <author>
      <name>Jonathan S. Kent</name>
    </author>
    <author>
      <name>Michael M. Murray</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 12 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.11946v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.11946v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.03573v1</id>
    <updated>2025-01-07T06:54:10Z</updated>
    <published>2025-01-07T06:54:10Z</published>
    <title>Neural Cellular Automata and Deep Equilibrium Models</title>
    <summary>  This essay discusses the connections and differences between two emerging
paradigms in deep learning, namely Neural Cellular Automata and Deep
Equilibrium Models, and train a simple Deep Equilibrium Convolutional model to
demonstrate the inherent similarity of NCA and DEQ based methods. Finally, this
essay speculates about ways to combine theoretical and practical aspects of
both approaches for future research.
</summary>
    <author>
      <name>Zhibai Jia</name>
    </author>
    <link href="http://arxiv.org/abs/2501.03573v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.03573v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.04141v1</id>
    <updated>2025-01-07T21:09:16Z</updated>
    <published>2025-01-07T21:09:16Z</published>
    <title>Hardware-In-The-Loop Training of a 4f Optical Correlator with
  Logarithmic Complexity Reduction for CNNs</title>
    <summary>  This work evaluates a forward-only learning algorithm on the MNIST dataset
with hardware-in-the-loop training of a 4f optical correlator, achieving 87.6%
accuracy with O(n2) complexity, compared to backpropagation, which achieves
88.8% accuracy with O(n2 log n) complexity.
</summary>
    <author>
      <name>Lorenzo Pes</name>
    </author>
    <author>
      <name>Maryam Dehbashizadeh Chehreghan</name>
    </author>
    <author>
      <name>Rick Luiken</name>
    </author>
    <author>
      <name>Sander Stuijk</name>
    </author>
    <author>
      <name>Ripalta Stabile</name>
    </author>
    <author>
      <name>Federico Corradi</name>
    </author>
    <link href="http://arxiv.org/abs/2501.04141v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.04141v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.13409v1</id>
    <updated>2025-05-19T17:45:10Z</updated>
    <published>2025-05-19T17:45:10Z</published>
    <title>Recombinant dynamical systems</title>
    <summary>  We describe a connectionist model that attempts to capture a notion of
experience-based problem solving or task learning, whereby solutions to newly
encountered problems are composed from remembered solutions to prior problems.
We apply this model to the computational problem of \emph{efficient sequence
generation}, a problem for which there is no obvious gradient descent
procedure, and for which not all posable problem instances are solvable.
Empirical tests show promising evidence of utility.
</summary>
    <author>
      <name>Saul Kato</name>
    </author>
    <link href="http://arxiv.org/abs/2505.13409v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.13409v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.4570v2</id>
    <updated>2010-02-17T04:26:06Z</updated>
    <published>2009-05-28T08:09:46Z</published>
    <title>Weak Evolvability Equals Strong Evolvability</title>
    <summary>  An updated version will be uploaded later.
</summary>
    <author>
      <name>Yang Yu</name>
    </author>
    <author>
      <name>Zhi-Hua Zhou</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the authors</arxiv:comment>
    <link href="http://arxiv.org/abs/0905.4570v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.4570v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809111v1</id>
    <updated>1998-09-28T03:48:22Z</updated>
    <published>1998-09-28T03:48:22Z</published>
    <title>Evolution of Neural Networks to Play the Game of Dots-and-Boxes</title>
    <summary>  Dots-and-Boxes is a child's game which remains analytically unsolved. We
implement and evolve artificial neural networks to play this game, evaluating
them against simple heuristic players. Our networks do not evaluate or predict
the final outcome of the game, but rather recommend moves at each stage.
Superior generalisation of play by co-evolved populations is found, and a
comparison made with networks trained by back-propagation using simple
heuristics as an oracle.
</summary>
    <author>
      <name>Lex Weaver</name>
    </author>
    <author>
      <name>Terry Bossomaier</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures, LaTeX 2.09 (works with LaTeX2e)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Alife V: Poster Presentations, May 16-18 1996, pages 43-50</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9809111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9811030v1</id>
    <updated>1998-11-24T22:51:20Z</updated>
    <published>1998-11-24T22:51:20Z</published>
    <title>Generating Segment Durations in a Text-To-Speech System: A Hybrid
  Rule-Based/Neural Network Approach</title>
    <summary>  A combination of a neural network with rule firing information from a
rule-based system is used to generate segment durations for a text-to-speech
system. The system shows a slight improvement in performance over a neural
network system without the rule firing information. Synthesized speech using
segment durations was accepted by listeners as having about the same quality as
speech generated using segment durations extracted from natural speech.
</summary>
    <author>
      <name>Gerald Corrigan</name>
    </author>
    <author>
      <name>Noel Massey</name>
    </author>
    <author>
      <name>Orhan Karaali</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, PostScript</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Eurospeech (1997) 2675-2678. Rhodes, Greece</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9811030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9811030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9811031v1</id>
    <updated>1998-11-24T23:33:12Z</updated>
    <published>1998-11-24T23:33:12Z</published>
    <title>Speech Synthesis with Neural Networks</title>
    <summary>  Text-to-speech conversion has traditionally been performed either by
concatenating short samples of speech or by using rule-based systems to convert
a phonetic representation of speech into an acoustic representation, which is
then converted into speech. This paper describes a system that uses a
time-delay neural network (TDNN) to perform this phonetic-to-acoustic mapping,
with another neural network to control the timing of the generated speech. The
neural network system requires less memory than a concatenation system, and
performed well in tests comparing it to commercial systems using other
technologies.
</summary>
    <author>
      <name>Orhan Karaali</name>
    </author>
    <author>
      <name>Gerald Corrigan</name>
    </author>
    <author>
      <name>Ira Gerson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, PostScript</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">World Congress on Neural Networks (1996) 45-50. San Diego</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9811031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9811031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9811032v1</id>
    <updated>1998-11-24T23:51:56Z</updated>
    <published>1998-11-24T23:51:56Z</published>
    <title>Text-To-Speech Conversion with Neural Networks: A Recurrent TDNN
  Approach</title>
    <summary>  This paper describes the design of a neural network that performs the
phonetic-to-acoustic mapping in a speech synthesis system. The use of a
time-domain neural network architecture limits discontinuities that occur at
phone boundaries. Recurrent data input also helps smooth the output parameter
tracks. Independent testing has demonstrated that the voice quality produced by
this system compares favorably with speech from existing commercial
text-to-speech systems.
</summary>
    <author>
      <name>Orhan Karaali</name>
    </author>
    <author>
      <name>Gerald Corrigan</name>
    </author>
    <author>
      <name>Ira Gerson</name>
    </author>
    <author>
      <name>Noel Massey</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, PostScript</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of Eurospeech (1997) 561-564. Rhodes, Greece</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9811032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9811032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9812002v1</id>
    <updated>1998-12-03T09:08:03Z</updated>
    <published>1998-12-03T09:08:03Z</published>
    <title>Training Reinforcement Neurocontrollers Using the Polytope Algorithm</title>
    <summary>  A new training algorithm is presented for delayed reinforcement learning
problems that does not assume the existence of a critic model and employs the
polytope optimization algorithm to adjust the weights of the action network so
that a simple direct measure of the training performance is maximized.
Experimental results from the application of the method to the pole balancing
problem indicate improved training performance compared with critic-based and
genetic reinforcement approaches.
</summary>
    <author>
      <name>A. Likas</name>
    </author>
    <author>
      <name>I. E. Lagaris</name>
    </author>
    <link href="http://arxiv.org/abs/cs/9812002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9812002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0102015v1</id>
    <updated>2001-02-20T13:08:15Z</updated>
    <published>2001-02-20T13:08:15Z</published>
    <title>Non-convex cost functionals in boosting algorithms and methods for panel
  selection</title>
    <summary>  In this document we propose a new improvement for boosting techniques as
proposed in Friedman '99 by the use of non-convex cost functional. The idea is
to introduce a correlation term to better deal with forecasting of additive
time series. The problem is discussed in a theoretical way to prove the
existence of minimizing sequence, and in a numerical way to propose a new
"ArgMin" algorithm. The model has been used to perform the touristic presence
forecast for the winter season 1999/2000 in Trentino (italian Alps).
</summary>
    <author>
      <name>Marco Visentin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0102015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0102015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6;G.1.2;G.3;I.6.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0108011v1</id>
    <updated>2001-08-21T15:00:47Z</updated>
    <published>2001-08-21T15:00:47Z</published>
    <title>On Classes of Functions for which No Free Lunch Results Hold</title>
    <summary>  In a recent paper it was shown that No Free Lunch results hold for any subset
F of the set of all possible functions from a finite set X to a finite set Y
iff F is closed under permutation of X. In this article, we prove that the
number of those subsets can be neglected compared to the overall number of
possible subsets. Further, we present some arguments why problem classes
relevant in practice are not likely to be closed under permutation.
</summary>
    <author>
      <name>Christian Igel</name>
    </author>
    <author>
      <name>Marc Toussaint</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure, see http://www.neuroinformatik.ruhr-uni-bochum.de/</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0108011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0108011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0202009v1</id>
    <updated>2002-02-11T11:04:08Z</updated>
    <published>2002-02-11T11:04:08Z</published>
    <title>Non-negative sparse coding</title>
    <summary>  Non-negative sparse coding is a method for decomposing multivariate data into
non-negative sparse components. In this paper we briefly describe the
motivation behind this type of data representation and its relation to standard
sparse coding and non-negative matrix factorization. We then give a simple yet
efficient multiplicative algorithm for finding the optimal values of the hidden
components. In addition, we show how the basis vectors can be learned from the
observed data. Simulations demonstrate the effectiveness of the proposed
method.
</summary>
    <author>
      <name>Patrik O. Hoyer</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0202009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0202009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0202034v1</id>
    <updated>2002-02-20T17:12:03Z</updated>
    <published>2002-02-20T17:12:03Z</published>
    <title>Covariance Plasticity and Regulated Criticality</title>
    <summary>  We propose that a regulation mechanism based on Hebbian covariance plasticity
may cause the brain to operate near criticality. We analyze the effect of such
a regulation on the dynamics of a network with excitatory and inhibitory
neurons and uniform connectivity within and across the two populations. We show
that, under broad conditions, the system converges to a critical state lying at
the common boundary of three regions in parameter space; these correspond to
three modes of behavior: high activity, low activity, oscillation.
</summary>
    <author>
      <name>Elie Bienenstock</name>
    </author>
    <author>
      <name>Daniel Lehmann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 8 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Complex Systems, 1(4) (1998) pp. 361-384</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0202034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0202034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0212042v1</id>
    <updated>2002-12-12T22:39:39Z</updated>
    <published>2002-12-12T22:39:39Z</published>
    <title>Increasing Evolvability Considered as a Large-Scale Trend in Evolution</title>
    <summary>  Evolvability is the capacity to evolve. This paper introduces a simple
computational model of evolvability and demonstrates that, under certain
conditions, evolvability can increase indefinitely, even when there is no
direct selection for evolvability. The model shows that increasing evolvability
implies an accelerating evolutionary pace. It is suggested that the conditions
for indefinitely increasing evolvability are satisfied in biological and
cultural evolution. We claim that increasing evolvability is a large-scale
trend in evolution. This hypothesis leads to testable predictions about
biological and cultural evolution.
</summary>
    <author>
      <name>Peter D. Turney</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National Research Council of Canada</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 1999 Genetic and Evolutionary Computation
  Conference Workshop Program, (1999), 43-46</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0212042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0212042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.3; I.6.8; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402014v1</id>
    <updated>2004-02-09T19:44:33Z</updated>
    <published>2004-02-09T19:44:33Z</published>
    <title>Self-Organising Networks for Classification: developing Applications to
  Science Analysis for Astroparticle Physics</title>
    <summary>  Physics analysis in astroparticle experiments requires the capability of
recognizing new phenomena; in order to establish what is new, it is important
to develop tools for automatic classification, able to compare the final result
with data from different detectors. A typical example is the problem of Gamma
Ray Burst detection, classification, and possible association to known sources:
for this task physicists will need in the next years tools to associate data
from optical databases, from satellite experiments (EGRET, GLAST), and from
Cherenkov telescopes (MAGIC, HESS, CANGAROO, VERITAS).
</summary>
    <author>
      <name>A. De Angelis</name>
    </author>
    <author>
      <name>P. Boinee</name>
    </author>
    <author>
      <name>M. Frailis</name>
    </author>
    <author>
      <name>E. Milotti</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.physa.2004.02.023</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.physa.2004.02.023" rel="related"/>
    <link href="http://arxiv.org/abs/cs/0402014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402047v1</id>
    <updated>2004-02-19T19:37:32Z</updated>
    <published>2004-02-19T19:37:32Z</published>
    <title>Parameter-less Optimization with the Extended Compact Genetic Algorithm
  and Iterated Local Search</title>
    <summary>  This paper presents a parameter-less optimization framework that uses the
extended compact genetic algorithm (ECGA) and iterated local search (ILS), but
is not restricted to these algorithms. The presented optimization algorithm
(ILS+ECGA) comes as an extension of the parameter-less genetic algorithm (GA),
where the parameters of a selecto-recombinative GA are eliminated. The approach
that we propose is tested on several well known problems. In the absence of
domain knowledge, it is shown that ILS+ECGA is a robust and easy-to-use
optimization method.
</summary>
    <author>
      <name>Claudio F. Lima</name>
    </author>
    <author>
      <name>Fernando G. Lobo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, submitted to gecco 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; I.2.6; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402049v1</id>
    <updated>2004-02-20T16:36:20Z</updated>
    <published>2004-02-20T16:36:20Z</published>
    <title>An architecture for massive parallelization of the compact genetic
  algorithm</title>
    <summary>  This paper presents an architecture which is suitable for a massive
parallelization of the compact genetic algorithm. The resulting scheme has
three major advantages. First, it has low synchronization costs. Second, it is
fault tolerant, and third, it is scalable.
  The paper argues that the benefits that can be obtained with the proposed
approach is potentially higher than those obtained with traditional parallel
genetic algorithms. In addition, the ideas suggested in the paper may also be
relevant towards parallelizing more complex probabilistic model building
genetic algorithms.
</summary>
    <author>
      <name>Fernando G. Lobo</name>
    </author>
    <author>
      <name>Claudio F. Lima</name>
    </author>
    <author>
      <name>Hugo Martires</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, submitted to gecco 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.4; G.1.6; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402050v1</id>
    <updated>2004-02-20T16:52:11Z</updated>
    <published>2004-02-20T16:52:11Z</published>
    <title>A philosophical essay on life and its connections with genetic
  algorithms</title>
    <summary>  This paper makes a number of connections between life and various facets of
genetic and evolutionary algorithms research. Specifically, it addresses the
topics of adaptation, multiobjective optimization, decision making, deception,
and search operators, among others. It argues that human life, from birth to
death, is an adaptive or dynamic optimization problem where people are
continuously searching for happiness. More important, the paper speculates that
genetic algorithms can be used as a source of inspiration for helping people
make decisions in their everyday life.
</summary>
    <author>
      <name>Fernando G. Lobo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, submitted to gecco 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8; J.4; K.4.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404019v1</id>
    <updated>2004-04-07T07:08:09Z</updated>
    <published>2004-04-07T07:08:09Z</published>
    <title>Optimizing genetic algorithm strategies for evolving networks</title>
    <summary>  This paper explores the use of genetic algorithms for the design of networks,
where the demands on the network fluctuate in time. For varying network
constraints, we find the best network using the standard genetic algorithm
operators such as inversion, mutation and crossover. We also examine how the
choice of genetic algorithm operators affects the quality of the best network
found. Such networks typically contain redundancy in servers, where several
servers perform the same task and pleiotropy, where servers perform multiple
tasks. We explore this trade-off between pleiotropy versus redundancy on the
cost versus reliability as a measure of the quality of the network.
</summary>
    <author>
      <name>Matthew J. Berryman</name>
    </author>
    <author>
      <name>Andrew Allison</name>
    </author>
    <author>
      <name>Derek Abbott</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.548122</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.548122" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0404019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; C.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404042v2</id>
    <updated>2004-04-22T09:00:52Z</updated>
    <published>2004-04-21T16:05:27Z</published>
    <title>Extraction of topological features from communication network
  topological patterns using self-organizing feature maps</title>
    <summary>  Different classes of communication network topologies and their
representation in the form of adjacency matrix and its eigenvalues are
presented. A self-organizing feature map neural network is used to map
different classes of communication network topological patterns. The neural
network simulation results are reported.
</summary>
    <author>
      <name>W. Ali</name>
    </author>
    <author>
      <name>R. J. Mondragon</name>
    </author>
    <author>
      <name>F. Alavi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 5 figures, To be appeared in IEE Electronics Letter Journal</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0404042v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404042v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.2; I.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412023v1</id>
    <updated>2004-12-06T20:23:15Z</updated>
    <published>2004-12-06T20:23:15Z</published>
    <title>Multidimensional data classification with artificial neural networks</title>
    <summary>  Multi-dimensional data classification is an important and challenging problem
in many astro-particle experiments. Neural networks have proved to be versatile
and robust in multi-dimensional data classification. In this article we shall
study the classification of gamma from the hadrons for the MAGIC Experiment.
Two neural networks have been used for the classification task. One is
Multi-Layer Perceptron based on supervised learning and other is
Self-Organising Map (SOM), which is based on unsupervised learning technique.
The results have been shown and the possible ways of combining these networks
have been proposed to yield better and faster classification results.
</summary>
    <author>
      <name>P. Boinee</name>
    </author>
    <author>
      <name>F. Barbarino</name>
    </author>
    <author>
      <name>A. De Angelis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures, Submitted to EURASIP Journal on Applied Signal
  Processing, 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0412023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; K.3.2; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412109v1</id>
    <updated>2004-12-24T13:41:10Z</updated>
    <published>2004-12-24T13:41:10Z</published>
    <title>Global minimization of a quadratic functional: neural network approach</title>
    <summary>  The problem of finding out the global minimum of a multiextremal functional
is discussed. One frequently faces with such a functional in various
applications. We propose a procedure, which depends on the dimensionality of
the problem polynomially. In our approach we use the eigenvalues and
eigenvectors of the connection matrix.
</summary>
    <author>
      <name>L. B. Litinskii</name>
    </author>
    <author>
      <name>B. M. Magomedov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Lecture on 7th International Conference on Pattern
  Recognition and Image Analysis PRIA-07-2004, St. Petersburg, Russia</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0412109v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412109v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412110v1</id>
    <updated>2004-12-24T13:48:44Z</updated>
    <published>2004-12-24T13:48:44Z</published>
    <title>Q-valued neural network as a system of fast identification and pattern
  recognition</title>
    <summary>  An effective neural network algorithm of the perceptron type is proposed. The
algorithm allows us to identify strongly distorted input vector reliably. It is
shown that its reliability and processing speed are orders of magnitude higher
than that of full connected neural networks. The processing speed of our
algorithm exceeds the one of the stack fast-access retrieval algorithm that is
modified for working when there are noises in the input channel.
</summary>
    <author>
      <name>D. I. Alieva</name>
    </author>
    <author>
      <name>B. V. Kryzhanovsky</name>
    </author>
    <author>
      <name>V. M. Kryzhanovsky</name>
    </author>
    <author>
      <name>A. B. Fonarev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Presentation on the 7th International Conference on Pattern
  Recognition and Image Analysis PRIA-07-2004, St. Petersburg, Russia</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0412110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0501005v1</id>
    <updated>2005-01-03T18:55:47Z</updated>
    <published>2005-01-03T18:55:47Z</published>
    <title>Portfolio selection using neural networks</title>
    <summary>  In this paper we apply a heuristic method based on artificial neural networks
in order to trace out the efficient frontier associated to the portfolio
selection problem. We consider a generalization of the standard Markowitz
mean-variance model which includes cardinality and bounding constraints. These
constraints ensure the investment in a given number of different assets and
limit the amount of capital to be invested in each asset. We present some
experimental results obtained with the neural network heuristic and we compare
them to those obtained with three previous heuristic methods.
</summary>
    <author>
      <name>Alberto Fernandez</name>
    </author>
    <author>
      <name>Sergio Gomez</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.cor.2005.06.017</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.cor.2005.06.017" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages; submitted to "Computers &amp; Operations Research"</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computers &amp; Operations Research 34 (2007) 1177-1191</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0501005v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0501005v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502057v1</id>
    <updated>2005-02-12T22:29:45Z</updated>
    <published>2005-02-12T22:29:45Z</published>
    <title>Decomposable Problems, Niching, and Scalability of Multiobjective
  Estimation of Distribution Algorithms</title>
    <summary>  The paper analyzes the scalability of multiobjective estimation of
distribution algorithms (MOEDAs) on a class of boundedly-difficult
additively-separable multiobjective optimization problems. The paper
illustrates that even if the linkage is correctly identified, massive
multimodality of the search problems can easily overwhelm the nicher and lead
to exponential scale-up. Facetwise models are subsequently used to propose a
growth rate of the number of differing substructures between the two objectives
to avoid the niching method from being overwhelmed and lead to polynomial
scalability of MOEDAs.
</summary>
    <author>
      <name>Kumara Sastry</name>
    </author>
    <author>
      <name>Martin Pelikan</name>
    </author>
    <author>
      <name>David E. Goldberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Genetic and Evolutionary Computation Conference,
  GECCO-2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0502057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502096v1</id>
    <updated>2005-02-28T16:40:34Z</updated>
    <published>2005-02-28T16:40:34Z</published>
    <title>Property analysis of symmetric travelling salesman problem instances
  acquired through evolution</title>
    <summary>  We show how an evolutionary algorithm can successfully be used to evolve a
set of difficult to solve symmetric travelling salesman problem instances for
two variants of the Lin-Kernighan algorithm. Then we analyse the instances in
those sets to guide us towards deferring general knowledge about the efficiency
of the two variants in relation to structural properties of the symmetric
travelling sale sman problem.
</summary>
    <author>
      <name>J. I. van Hemert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in G. Raidl and J. Gottlieb, editors, Evolutionary
  Computation in Combinatorial Optimization, Springer Lecture Notes on Computer
  Science, pages 122-131. Springer-Verlag, Berlin, 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0502096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G1.6;I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504052v1</id>
    <updated>2005-04-13T13:22:49Z</updated>
    <published>2005-04-13T13:22:49Z</published>
    <title>Learning Multi-Class Neural-Network Models from Electroencephalograms</title>
    <summary>  We describe a new algorithm for learning multi-class neural-network models
from large-scale clinical electroencephalograms (EEGs). This algorithm trains
hidden neurons separately to classify all the pairs of classes. To find best
pairwise classifiers, our algorithm searches for input variables which are
relevant to the classification problem. Despite patient variability and heavily
overlapping classes, a 16-class model learnt from EEGs of 65 sleeping newborns
correctly classified 80.8% of the training and 80.1% of the testing examples.
Additionally, the neural-network model provides a probabilistic interpretation
of decisions.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <author>
      <name>Joachim Schult</name>
    </author>
    <author>
      <name>Burkhart Scheidt</name>
    </author>
    <author>
      <name>Valery Kuriakin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">KES-2003</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0504052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504053v1</id>
    <updated>2005-04-13T13:28:15Z</updated>
    <published>2005-04-13T13:28:15Z</published>
    <title>A Neural-Network Technique for Recognition of Filaments in Solar Images</title>
    <summary>  We describe a new neural-network technique developed for an automated
recognition of solar filaments visible in the hydrogen H-alpha line full disk
spectroheliograms. This technique allows neural networks learn from a few image
fragments labelled manually to recognize the single filaments depicted on a
local background. The trained network is able to recognize filaments depicted
on the backgrounds with variations in brightness caused by atmospherics
distortions. Despite the difference in backgrounds in our experiments the
neural network has properly recognized filaments in the testing image
fragments. Using a parabolic activation function we extend this technique to
recognize multiple solar filaments which may appear in one fragment.
</summary>
    <author>
      <name>V. V. Zharkova</name>
    </author>
    <author>
      <name>V. Schetinin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504055v1</id>
    <updated>2005-04-13T13:57:56Z</updated>
    <published>2005-04-13T13:57:56Z</published>
    <title>A Learning Algorithm for Evolving Cascade Neural Networks</title>
    <summary>  A new learning algorithm for Evolving Cascade Neural Networks (ECNNs) is
described. An ECNN starts to learn with one input node and then adding new
inputs as well as new hidden neurons evolves it. The trained ECNN has a nearly
minimal number of input and hidden neurons as well as connections. The
algorithm was successfully applied to classify artifacts and normal segments in
clinical electroencephalograms (EEGs). The EEG segments were visually labeled
by EEG-viewer. The trained ECNN has correctly classified 96.69% of the testing
segments. It is slightly better than a standard fully connected neural network.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Processing Letter 17:21-31, 2003. Kluwer</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0504055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504058v1</id>
    <updated>2005-04-13T14:06:32Z</updated>
    <published>2005-04-13T14:06:32Z</published>
    <title>Polynomial Neural Networks Learnt to Classify EEG Signals</title>
    <summary>  A neural network based technique is presented, which is able to successfully
extract polynomial classification rules from labeled electroencephalogram (EEG)
signals. To represent the classification rules in an analytical form, we use
the polynomial neural networks trained by a modified Group Method of Data
Handling (GMDH). The classification rules were extracted from clinical EEG data
that were recorded from an Alzheimer patient and the sudden death risk
patients. The third data is EEG recordings that include the normal and artifact
segments. These EEG data were visually identified by medical experts. The
extracted polynomial rules verified on the testing EEG data allow to correctly
classify 72% of the risk group patients and 96.5% of the segments. These rules
performs slightly better than standard feedforward neural networks.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504067v1</id>
    <updated>2005-04-14T10:36:54Z</updated>
    <published>2005-04-14T10:36:54Z</published>
    <title>An Evolving Cascade Neural Network Technique for Cleaning Sleep
  Electroencephalograms</title>
    <summary>  Evolving Cascade Neural Networks (ECNNs) and a new training algorithm capable
of selecting informative features are described. The ECNN initially learns with
one input node and then evolves by adding new inputs as well as new hidden
neurons. The resultant ECNN has a near minimal number of hidden neurons and
inputs. The algorithm is successfully used for training ECNN to recognise
artefacts in sleep electroencephalograms (EEGs) which were visually labelled by
EEG-viewers. In our experiments, the ECNN outperforms the standard
neural-network as well as evolutionary techniques.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Natural Computing Application, 2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0504067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504070v1</id>
    <updated>2005-04-14T10:49:55Z</updated>
    <published>2005-04-14T10:49:55Z</published>
    <title>The Combined Technique for Detection of Artifacts in Clinical
  Electroencephalograms of Sleeping Newborns</title>
    <summary>  In this paper we describe a new method combining the polynomial neural
network and decision tree techniques in order to derive comprehensible
classification rules from clinical electroencephalograms (EEGs) recorded from
sleeping newborns. These EEGs are heavily corrupted by cardiac, eye movement,
muscle and noise artifacts and as a consequence some EEG features are
irrelevant to classification problems. Combining the polynomial network and
decision tree techniques, we discover comprehensible classification rules
whilst also attempting to keep their classification error down. This technique
is shown to outperform a number of commonly used machine learning technique
applied to automatically recognize artifacts in the sleep EEGs.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <author>
      <name>Joachim Schult</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505003v1</id>
    <updated>2005-04-30T16:13:32Z</updated>
    <published>2005-04-30T16:13:32Z</published>
    <title>A New Kind of Hopfield Networks for Finding Global Optimum</title>
    <summary>  The Hopfield network has been applied to solve optimization problems over
decades. However, it still has many limitations in accomplishing this task.
Most of them are inherited from the optimization algorithms it implements. The
computation of a Hopfield network, defined by a set of difference equations,
can easily be trapped into one local optimum or another, sensitive to initial
conditions, perturbations, and neuron update orders. It doesn't know how long
it will take to converge, as well as if the final solution is a global optimum,
or not. In this paper, we present a Hopfield network with a new set of
difference equations to fix those problems. The difference equations directly
implement a new powerful optimization algorithm.
</summary>
    <author>
      <name>Xiaofei Huang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, accepted by International Joint Conference on Neural
  Networks 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505065v2</id>
    <updated>2005-05-27T04:41:11Z</updated>
    <published>2005-05-24T14:54:06Z</published>
    <title>A dissipative particle swarm optimization</title>
    <summary>  A dissipative particle swarm optimization is developed according to the
self-organization of dissipative structure. The negative entropy is introduced
to construct an opening dissipative system that is far-from-equilibrium so as
to driving the irreversible evolution process with better fitness. The testing
of two multimodal functions indicates it improves the performance effectively
</summary>
    <author>
      <name>Xiao-Feng Xie</name>
    </author>
    <author>
      <name>Wen-Jun Zhang</name>
    </author>
    <author>
      <name>Zhi-Lian Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2002 Congress on Evolutionary Computation, 2002.
  Volume: 2, On page(s): 1456-1461</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505065v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505065v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505067v1</id>
    <updated>2005-05-25T01:28:18Z</updated>
    <published>2005-05-25T01:28:18Z</published>
    <title>Optimizing semiconductor devices by self-organizing particle swarm</title>
    <summary>  A self-organizing particle swarm is presented. It works in dissipative state
by employing the small inertia weight, according to experimental analysis on a
simplified model, which with fast convergence. Then by recognizing and
replacing inactive particles according to the process deviation information of
device parameters, the fluctuation is introduced so as to driving the
irreversible evolution process with better fitness. The testing on benchmark
functions and an application example for device optimization with designed
fitness function indicates it improves the performance effectively.
</summary>
    <author>
      <name>Xiao-Feng Xie</name>
    </author>
    <author>
      <name>Wen-Jun Zhang</name>
    </author>
    <author>
      <name>De-Chun Bi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Congress on Evolutionary Computation, 2004. CEC2004. Volume: 2, On
  page(s): 2017- 2022 Vol.2</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505068v1</id>
    <updated>2005-05-25T01:32:30Z</updated>
    <published>2005-05-25T01:32:30Z</published>
    <title>Handling equality constraints by adaptive relaxing rule for swarm
  algorithms</title>
    <summary>  The adaptive constraints relaxing rule for swarm algorithms to handle with
the problems with equality constraints is presented. The feasible space of such
problems may be similiar to ridge function class, which is hard for applying
swarm algorithms. To enter the solution space more easily, the relaxed quasi
feasible space is introduced and shrinked adaptively. The experimental results
on benchmark functions are compared with the performance of other algorithms,
which show its efficiency.
</summary>
    <author>
      <name>Xiao-Feng Xie</name>
    </author>
    <author>
      <name>Wen-Jun Zhang</name>
    </author>
    <author>
      <name>De-Chun Bi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Congress on Evolutionary Computation, 2004. CEC2004. Volume: 2, On
  page(s): 2012- 2016 Vol.2</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505069v1</id>
    <updated>2005-05-25T01:36:07Z</updated>
    <published>2005-05-25T01:36:07Z</published>
    <title>Handling boundary constraints for numerical optimization by particle
  swarm flying in periodic search space</title>
    <summary>  The periodic mode is analyzed together with two conventional boundary
handling modes for particle swarm. By providing an infinite space that
comprises periodic copies of original search space, it avoids possible
disorganizing of particle swarm that is induced by the undesired mutations at
the boundary. The results on benchmark functions show that particle swarm with
periodic mode is capable of improving the search performance significantly, by
compared with that of conventional modes and other algorithms.
</summary>
    <author>
      <name>Wen-Jun Zhang</name>
    </author>
    <author>
      <name>Xiao-Feng Xie</name>
    </author>
    <author>
      <name>De-Chun Bi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Congress on Evolutionary Computation, 2004. CEC2004. Volume: 2, On
  page(s): 2307- 2311 Vol.2</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505070v1</id>
    <updated>2005-05-25T01:39:55Z</updated>
    <published>2005-05-25T01:39:55Z</published>
    <title>SWAF: Swarm Algorithm Framework for Numerical Optimization</title>
    <summary>  A swarm algorithm framework (SWAF), realized by agent-based modeling, is
presented to solve numerical optimization problems. Each agent is a bare bones
cognitive architecture, which learns knowledge by appropriately deploying a set
of simple rules in fast and frugal heuristics. Two essential categories of
rules, the generate-and-test and the problem-formulation rules, are
implemented, and both of the macro rules by simple combination and subsymbolic
deploying of multiple rules among them are also studied. Experimental results
on benchmark problems are presented, and performance comparison between SWAF
and other existing algorithms indicates that it is efficiently.
</summary>
    <author>
      <name>Xiao-Feng Xie</name>
    </author>
    <author>
      <name>Wen-Jun Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Genetic and Evolutionary Computation Conference (GECCO), Part I,
  2004: 238-250 (LNCS 3102)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0508117v1</id>
    <updated>2005-08-26T10:38:36Z</updated>
    <published>2005-08-26T10:38:36Z</published>
    <title>Long-term neuronal behavior caused by two synaptic modification
  mechanisms</title>
    <summary>  We report the first results of simulating the coupling of neuronal,
astrocyte, and cerebrovascular activity. It is suggested that the dynamics of
the system is different from systems that only include neurons. In the
neuron-vascular coupling, distribution of synapse strengths affects neuronal
behavior and thus balance of the blood flow; oscillations are induced in the
neuron-to-astrocyte coupling.
</summary>
    <author>
      <name>Xi Shen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Imperial College London, United Kingdom</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe De Wilde</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Heriot-Watt University, United Kingdom</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0508117v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0508117v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0511027v1</id>
    <updated>2005-11-07T19:09:01Z</updated>
    <published>2005-11-07T19:09:01Z</published>
    <title>Discrete Network Dynamics. Part 1: Operator Theory</title>
    <summary>  An operator algebra implementation of Markov chain Monte Carlo algorithms for
simulating Markov random fields is proposed. It allows the dynamics of networks
whose nodes have discrete state spaces to be specified by the action of an
update operator that is composed of creation and annihilation operators. This
formulation of discrete network dynamics has properties that are similar to
those of a quantum field theory of bosons, which allows reuse of many
conceptual and theoretical structures from QFT. The equilibrium behaviour of
one of these generalised MRFs and of the adaptive cluster expansion network
(ACEnet) are shown to be equivalent, which provides a way of unifying these two
theories.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0511027v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0511027v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.2; G.2.1; G.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0512019v1</id>
    <updated>2005-12-05T11:12:42Z</updated>
    <published>2005-12-05T11:12:42Z</published>
    <title>Amazing geometry of genetic space or are genetic algorithms convergent?</title>
    <summary>  There is no proof yet of convergence of Genetic Algorithms. We do not supply
it too. Instead, we present some thoughts and arguments to convince the Reader,
that Genetic Algorithms are essentially bound for success. For this purpose, we
consider only the crossover operators, single- or multiple-point, together with
selection procedure. We also give a proof that the soft selection is superior
to other selection schemes.
</summary>
    <author>
      <name>Marek W. Gutowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, presented on VII KAEiOG (VII Domestic Conference on
  Evolutionary Algorithms and Global Optimization), May 24-26, 2004, Kazimierz
  Dolny, Poland</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0512019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0512019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; G.1.6; G.4; I.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0602036v1</id>
    <updated>2006-02-10T06:32:29Z</updated>
    <published>2006-02-10T06:32:29Z</published>
    <title>Réseaux d'Automates de Caianiello Revisité</title>
    <summary>  We exhibit a family of neural networks of McCulloch and Pitts of size $2nk+2$
which can be simulated by a neural networks of Caianiello of size $2n+2$ and
memory length $k$. This simulation allows us to find again one of the result of
the following article: [Cycles exponentiels des r\'{e}seaux de Caianiello et
compteurs en arithm\'{e}tique redondante, Technique et Science Informatiques
Vol. 19, pages 985-1008] on the existence of neural networks of Caianiello of
size $2n+2$ and memory length $k$ which describes a cycle of length $k \times
2^{nk}$.
</summary>
    <author>
      <name>René Ndoundam</name>
    </author>
    <author>
      <name>Maurice Tchuente</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0602036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0602036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0603015v1</id>
    <updated>2006-03-02T23:59:19Z</updated>
    <published>2006-03-02T23:59:19Z</published>
    <title>The Basic Kak Neural Network with Complex Inputs</title>
    <summary>  The Kak family of neural networks is able to learn patterns quickly, and this
speed of learning can be a decisive advantage over other competing models in
many applications. Amongst the implementations of these networks are those
using reconfigurable networks, FPGAs and optical networks. In some
applications, it is useful to use complex data, and it is with that in mind
that this introduction to the basic Kak network with complex inputs is being
presented. The training algorithm is prescriptive and the network weights are
assigned simply upon examining the inputs. The input is mapped using quaternary
encoding for purpose of efficienty. This network family is part of a larger
hierarchy of learning schemes that include quantum models.
</summary>
    <author>
      <name>Pritam Rajagopal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 9 figures, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0603015v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0603015v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607007v4</id>
    <updated>2009-10-04T15:23:16Z</updated>
    <published>2006-07-03T04:03:23Z</published>
    <title>Theory of sexes by Geodakian as it is advanced by Iskrin</title>
    <summary>  In 1960s V.Geodakian proposed a theory that explains sexes as a mechanism for
evolutionary adaptation of the species to changing environmental conditions. In
2001 V.Iskrin refined and augmented the concepts of Geodakian and gave a new
and interesting explanation to several phenomena which involve sex, and sex
ratio, including the war-years phenomena. He also introduced a new concept of
the "catastrophic sex ratio." This note is an attempt to digest technical
aspects of the new ideas by Iskrin.
</summary>
    <author>
      <name>Boris D. Lubachevsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0607007v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607007v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607090v1</id>
    <updated>2006-07-18T21:01:43Z</updated>
    <published>2006-07-18T21:01:43Z</published>
    <title>Neural Networks with Complex and Quaternion Inputs</title>
    <summary>  This article investigates Kak neural networks, which can be instantaneously
trained, for complex and quaternion inputs. The performance of the basic
algorithm has been analyzed and shown how it provides a plausible model of
human perception and understanding of images. The motivation for studying
quaternion inputs is their use in representing spatial rotations that find
applications in computer graphics, robotics, global navigation, computer vision
and the spatial orientation of instruments. The problem of efficient mapping of
data in quaternion neural networks is examined. Some problems that need to be
addressed before quaternion neural networks find applications are identified.
</summary>
    <author>
      <name>Adityan Rishiyur</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0607090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0610041v1</id>
    <updated>2006-10-09T11:42:27Z</updated>
    <published>2006-10-09T11:42:27Z</published>
    <title>A Computational Model of Spatial Memory Anticipation during Visual
  Search</title>
    <summary>  Some visual search tasks require to memorize the location of stimuli that
have been previously scanned. Considerations about the eye movements raise the
question of how we are able to maintain a coherent memory, despite the frequent
drastically changes in the perception. In this article, we present a
computational model that is able to anticipate the consequences of the eye
movements on the visual perception in order to update a spatial memory
</summary>
    <author>
      <name>Jérémy Fix</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Julien Vitay</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <author>
      <name>Nicolas Rougier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - LORIA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Anticipatory Behavior in Adaptive Learning Systems 2006
  (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0610041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0610041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611006v1</id>
    <updated>2006-11-02T00:47:57Z</updated>
    <published>2006-11-02T00:47:57Z</published>
    <title>Evolving controllers for simulated car racing</title>
    <summary>  This paper describes the evolution of controllers for racing a simulated
radio-controlled car around a track, modelled on a real physical track. Five
different controller architectures were compared, based on neural networks,
force fields and action sequences. The controllers use either egocentric (first
person), Newtonian (third person) or no information about the state of the car
(open-loop controller). The only controller that was able to evolve good racing
behaviour was based on a neural network acting on egocentric inputs.
</summary>
    <author>
      <name>Julian Togelius</name>
    </author>
    <author>
      <name>Simon M. Lucas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Won the CEC 2005 Best Student Paper Award</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2005 Congress on Evolutionary Computation,
  pages 1906-1913</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0611006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611032v2</id>
    <updated>2007-04-08T23:01:48Z</updated>
    <published>2006-11-07T20:25:28Z</published>
    <title>V-like formations in flocks of artificial birds</title>
    <summary>  We consider flocks of artificial birds and study the emergence of V-like
formations during flight. We introduce a small set of fully distributed
positioning rules to guide the birds' movements and demonstrate, by means of
simulations, that they tend to lead to stabilization into several of the
well-known V-like formations that have been observed in nature. We also provide
quantitative indicators that we believe are closely related to achieving V-like
formations, and study their behavior over a large set of independent
simulations.
</summary>
    <author>
      <name>Andre Nathan</name>
    </author>
    <author>
      <name>Valmir C. Barbosa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/artl.2008.14.2.179</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/artl.2008.14.2.179" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Life 14 (2008), 179-188</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0611032v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611032v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611077v1</id>
    <updated>2006-11-16T03:27:16Z</updated>
    <published>2006-11-16T03:27:16Z</published>
    <title>Evolutionary Optimization in an Algorithmic Setting</title>
    <summary>  Evolutionary processes proved very useful for solving optimization problems.
In this work, we build a formalization of the notion of cooperation and
competition of multiple systems working toward a common optimization goal of
the population using evolutionary computation techniques. It is justified that
evolutionary algorithms are more expressive than conventional recursive
algorithms. Three subclasses of evolutionary algorithms are proposed here:
bounded finite, unbounded finite and infinite types. Some results on
completeness, optimality and search decidability for the above classes are
presented. A natural extension of Evolutionary Turing Machine model developed
in this paper allows one to mathematically represent and study properties of
cooperation and competition in a population of optimized species.
</summary>
    <author>
      <name>Mark Burgin</name>
    </author>
    <author>
      <name>Eugene Eberbach</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0611077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611104v1</id>
    <updated>2006-11-21T12:54:29Z</updated>
    <published>2006-11-21T12:54:29Z</published>
    <title>Learning and discrimination through STDP in a top-down modulated
  associative memory</title>
    <summary>  This article underlines the learning and discrimination capabilities of a
model of associative memory based on artificial networks of spiking neurons.
Inspired from neuropsychology and neurobiology, the model implements top-down
modulations, as in neocortical layer V pyramidal neurons, with a learning rule
based on synaptic plasticity (STDP), for performing a multimodal association
learning task. A temporal correlation method of analysis proves the ability of
the model to associate specific activity patterns to different samples of
stimulation. Even in the absence of initial learning and with continuously
varying weights, the activity patterns become stable enough for discrimination.
</summary>
    <author>
      <name>Anthony Mouraud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISC, GRIMAAG</arxiv:affiliation>
    </author>
    <author>
      <name>Hélène Paugam-Moisy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISC</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 14 European Symposium on Artificial Neural Networks
  (ESANN 2006) (03/2006) 611-616</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0611104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.3587v1</id>
    <updated>2007-09-22T15:54:37Z</updated>
    <published>2007-09-22T15:54:37Z</published>
    <title>Self-organizing maps and symbolic data</title>
    <summary>  In data analysis new forms of complex data have to be considered like for
example (symbolic data, functional data, web data, trees, SQL query and
multimedia data, ...). In this context classical data analysis for knowledge
discovery based on calculating the center of gravity can not be used because
input are not $\mathbb{R}^p$ vectors. In this paper, we present an application
on real world symbolic data using the self-organizing map. To this end, we
propose an extension of the self-organizing map that can handle symbolic data.
</summary>
    <author>
      <name>Aïcha El Golli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <author>
      <name>Brieuc Conan-Guez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <author>
      <name>Fabrice Rossi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Symbolic Data Analysis 2, 1 (2004)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0709.3587v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.3587v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.3642v1</id>
    <updated>2007-09-23T14:10:48Z</updated>
    <published>2007-09-23T14:10:48Z</published>
    <title>Functional Multi-Layer Perceptron: a Nonlinear Tool for Functional Data
  Analysis</title>
    <summary>  In this paper, we study a natural extension of Multi-Layer Perceptrons (MLP)
to functional inputs. We show that fundamental results for classical MLP can be
extended to functional MLP. We obtain universal approximation results that show
the expressive power of functional MLP is comparable to that of numerical MLP.
We obtain consistency results which imply that the estimation of optimal
parameters for functional MLP is statistically well defined. We finally show on
simulated and real world data that the proposed model performs in a very
satisfactory way.
</summary>
    <author>
      <name>Fabrice Rossi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE</arxiv:affiliation>
    </author>
    <author>
      <name>Brieuc Conan-Guez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neunet.2004.07.001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neunet.2004.07.001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://www.sciencedirect.com/science/journal/08936080</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks 18, 1 (2005) 45--60</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0709.3642v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.3642v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.0213v1</id>
    <updated>2007-10-01T06:51:42Z</updated>
    <published>2007-10-01T06:51:42Z</published>
    <title>Optimising the topology of complex neural networks</title>
    <summary>  In this paper, we study instances of complex neural networks, i.e. neural
netwo rks with complex topologies. We use Self-Organizing Map neural networks
whose n eighbourhood relationships are defined by a complex network, to
classify handwr itten digits. We show that topology has a small impact on
performance and robus tness to neuron failures, at least at long learning
times. Performance may howe ver be increased (by almost 10%) by artificial
evolution of the network topo logy. In our experimental conditions, the evolved
networks are more random than their parents, but display a more heterogeneous
degree distribution.
</summary>
    <author>
      <name>Fei Jiang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs, INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Hugues Berry</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <author>
      <name>Marc Schoenauer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans ECCS'07 (2007)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.0213v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.0213v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.2227v1</id>
    <updated>2007-10-11T12:09:33Z</updated>
    <published>2007-10-11T12:09:33Z</published>
    <title>A System for Predicting Subcellular Localization of Yeast Genome Using
  Neural Network</title>
    <summary>  The subcellular location of a protein can provide valuable information about
its function. With the rapid increase of sequenced genomic data, the need for
an automated and accurate tool to predict subcellular localization becomes
increasingly important. Many efforts have been made to predict protein
subcellular localization. This paper aims to merge the artificial neural
networks and bioinformatics to predict the location of protein in yeast genome.
We introduce a new subcellular prediction method based on a backpropagation
neural network. The results show that the prediction within an error limit of 5
to 10 percentage can be achieved with the system.
</summary>
    <author>
      <name>Sabu M. Thampi</name>
    </author>
    <author>
      <name>K. Chandra Sekaran</name>
    </author>
    <link href="http://arxiv.org/abs/0710.2227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.2227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4725v1</id>
    <updated>2007-10-25T09:37:48Z</updated>
    <published>2007-10-25T09:37:48Z</published>
    <title>Fault-Trajectory Approach for Fault Diagnosis on Analog Circuits</title>
    <summary>  This issue discusses the fault-trajectory approach suitability for fault
diagnosis on analog networks. Recent works have shown promising results
concerning a method based on this concept for ATPG for diagnosing faults on
analog networks. Such method relies on evolutionary techniques, where a generic
algorithm (GA) is coded to generate a set of optimum frequencies capable to
disclose faults.
</summary>
    <author>
      <name>Carlos Eduardo Savioli</name>
    </author>
    <author>
      <name>Claudio C. Czendrodi</name>
    </author>
    <author>
      <name>Jose Vicente Calvano</name>
    </author>
    <author>
      <name>Antonio Carneiro De Mesquita Filho</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted on behalf of EDAA (http://www.edaa.com/)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Design, Automation and Test in Europe - DATE'05, Munich :
  Allemagne (2005)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0710.4725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.2630v1</id>
    <updated>2007-12-17T19:59:42Z</updated>
    <published>2007-12-17T19:59:42Z</published>
    <title>Evolving XSLT stylesheets</title>
    <summary>  This paper introduces a procedure based on genetic programming to evolve XSLT
programs (usually called stylesheets or logicsheets). XSLT is a general
purpose, document-oriented functional language, generally used to transform XML
documents (or, in general, solve any problem that can be coded as an XML
document). The proposed solution uses a tree representation for the stylesheets
as well as diverse specific operators in order to obtain, in the studied cases
and a reasonable time, a XSLT stylesheet that performs the transformation.
Several types of representation have been compared, resulting in different
performance and degree of success.
</summary>
    <author>
      <name>Nestor Zorzano</name>
    </author>
    <author>
      <name>Daniel Merino</name>
    </author>
    <author>
      <name>J. L. J. Laredo</name>
    </author>
    <author>
      <name>J. P. Sevilla</name>
    </author>
    <author>
      <name>Pablo Garcia</name>
    </author>
    <author>
      <name>J. J. Merelo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">First draft, preparing for WCCI 2008</arxiv:comment>
    <link href="http://arxiv.org/abs/0712.2630v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.2630v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.1883v1</id>
    <updated>2008-01-12T08:02:12Z</updated>
    <published>2008-01-12T08:02:12Z</published>
    <title>D-optimal Bayesian Interrogation for Parameter and Noise Identification
  of Recurrent Neural Networks</title>
    <summary>  We introduce a novel online Bayesian method for the identification of a
family of noisy recurrent neural networks (RNNs). We develop Bayesian active
learning technique in order to optimize the interrogating stimuli given past
experiences. In particular, we consider the unknown parameters as stochastic
variables and use the D-optimality principle, also known as `\emph{infomax
method}', to choose optimal stimuli. We apply a greedy technique to maximize
the information gain concerning network parameters at each time step. We also
derive the D-optimal estimation of the additive noise that perturbs the
dynamical system of the RNN. Our analytical results are approximation-free. The
analytic derivation gives rise to attractive quadratic update rules.
</summary>
    <author>
      <name>Barnabas Poczos</name>
    </author>
    <author>
      <name>Andras Lorincz</name>
    </author>
    <link href="http://arxiv.org/abs/0801.1883v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.1883v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.3113v1</id>
    <updated>2008-01-21T00:34:55Z</updated>
    <published>2008-01-21T00:34:55Z</published>
    <title>iBOA: The Incremental Bayesian Optimization Algorithm</title>
    <summary>  This paper proposes the incremental Bayesian optimization algorithm (iBOA),
which modifies standard BOA by removing the population of solutions and using
incremental updates of the Bayesian network. iBOA is shown to be able to learn
and exploit unrestricted Bayesian networks using incremental techniques for
updating both the structure as well as the parameters of the probabilistic
model. This represents an important step toward the design of competent
incremental estimation of distribution algorithms that can solve difficult
nearly decomposable problems scalably and reliably.
</summary>
    <author>
      <name>Martin Pelikan</name>
    </author>
    <author>
      <name>Kumara Sastry</name>
    </author>
    <author>
      <name>David E. Goldberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Also available at the MEDAL web site, http://medal.cs.umsl.edu/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Genetic and Evolutionary Computation Conference
  (GECCO-2008), ACM Press, 455-462</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.3113v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.3113v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.8; G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.4287v2</id>
    <updated>2008-03-03T17:05:58Z</updated>
    <published>2008-01-28T14:19:12Z</published>
    <title>Movie Recommendation Systems Using An Artificial Immune System</title>
    <summary>  We apply the Artificial Immune System (AIS) technology to the Collaborative
Filtering (CF) technology when we build the movie recommendation system. Two
different affinity measure algorithms of AIS, Kendall tau and Weighted Kappa,
are used to calculate the correlation coefficients for this movie
recommendation system. From the testing we think that Weighted Kappa is more
suitable than Kendall tau for movie problems.
</summary>
    <author>
      <name>Qi Chen</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">6th International Conference in Adaptive Computing in Design and
  Manufacture (ACDM 2004), Bristol, UK, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.4287v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.4287v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.0252v1</id>
    <updated>2008-02-02T15:10:35Z</updated>
    <published>2008-02-02T15:10:35Z</published>
    <title>Accélération des cartes auto-organisatrices sur tableau de
  dissimilarités par séparation et évaluation</title>
    <summary>  In this paper, a new implementation of the adaptation of Kohonen
self-organising maps (SOM) to dissimilarity matrices is proposed. This
implementation relies on the branch and bound principle to reduce the algorithm
running time. An important property of this new approach is that the obtained
algorithm produces exactly the same results as the standard algorithm.
</summary>
    <author>
      <name>Brieuc Conan-Guez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITA</arxiv:affiliation>
    </author>
    <author>
      <name>Fabrice Rossi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A para\^itre</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">REVUE DES NOUVELLES TECHNOLOGIES DE L'INFORMATION (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.0252v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.0252v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.0861v1</id>
    <updated>2008-02-06T18:50:16Z</updated>
    <published>2008-02-06T18:50:16Z</published>
    <title>Using Bayesian Blocks to Partition Self-Organizing Maps</title>
    <summary>  Self organizing maps (SOMs) are widely-used for unsupervised classification.
For this application, they must be combined with some partitioning scheme that
can identify boundaries between distinct regions in the maps they produce. We
discuss a novel partitioning scheme for SOMs based on the Bayesian Blocks
segmentation algorithm of Scargle [1998]. This algorithm minimizes a cost
function to identify contiguous regions over which the values of the attributes
can be represented as approximately constant. Because this cost function is
well-defined and largely independent of assumptions regarding the number and
structure of clusters in the original sample space, this partitioning scheme
offers significant advantages over many conventional methods. Sample code is
available.
</summary>
    <author>
      <name>Paul R. Gazis</name>
    </author>
    <author>
      <name>Jeffrey D. Scargle</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0802.0861v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.0861v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.2138v1</id>
    <updated>2008-02-15T04:53:33Z</updated>
    <published>2008-02-15T04:53:33Z</published>
    <title>Support Vector classifiers for Land Cover Classification</title>
    <summary>  Support vector machines represent a promising development in machine learning
research that is not widely used within the remote sensing community. This
paper reports the results of Multispectral(Landsat-7 ETM+) and Hyperspectral
DAIS)data in which multi-class SVMs are compared with maximum likelihood and
artificial neural network methods in terms of classification accuracy. Our
results show that the SVM achieves a higher level of classification accuracy
than either the maximum likelihood or the neural classifier, and that the
support vector machine can be used with small training datasets and
high-dimensional data.
</summary>
    <author>
      <name>Mahesh Pal</name>
    </author>
    <author>
      <name>Paul M. Mather</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/01431160802007624</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/01431160802007624" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 1 figure, Published in MapIndia Conference 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/0802.2138v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.2138v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.3235v3</id>
    <updated>2009-07-02T17:24:41Z</updated>
    <published>2008-02-21T23:41:09Z</published>
    <title>Characterization of the convergence of stationary Fokker-Planck learning</title>
    <summary>  The convergence properties of the stationary Fokker-Planck algorithm for the
estimation of the asymptotic density of stochastic search processes is studied.
Theoretical and empirical arguments for the characterization of convergence of
the estimation in the case of separable and nonseparable nonlinear optimization
problems are given. Some implications of the convergence of stationary
Fokker-Planck learning for the inference of parameters in artificial neural
network models are outlined.
</summary>
    <author>
      <name>Arturo Berrones</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neucom.2008.12.042</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neucom.2008.12.042" rel="related"/>
    <link href="http://arxiv.org/abs/0802.3235v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.3235v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.0353v1</id>
    <updated>2008-04-02T13:56:10Z</updated>
    <published>2008-04-02T13:56:10Z</published>
    <title>Graphical Estimation of Permeability Using RST&amp;NFIS</title>
    <summary>  This paper pursues some applications of Rough Set Theory (RST) and
neural-fuzzy model to analysis of "lugeon data". In the manner, using Self
Organizing Map (SOM) as a pre-processing the data are scaled and then the
dominant rules by RST, are elicited. Based on these rules variations of
permeability in the different levels of Shivashan dam, Iran has been
highlighted. Then, via using a combining of SOM and an adaptive Neuro-Fuzzy
Inference System (NFIS) another analysis on the data was carried out. Finally,
a brief comparison between the obtained results of RST and SOM-NFIS (briefly
SONFIS) has been rendered.
</summary>
    <author>
      <name>H. Owladeghaffari</name>
    </author>
    <author>
      <name>K. Shahriar W. Pedrycz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages;NAFIPS08</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.0353v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.0353v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.1133v1</id>
    <updated>2008-04-07T20:11:24Z</updated>
    <published>2008-04-07T20:11:24Z</published>
    <title>Prospective Algorithms for Quantum Evolutionary Computation</title>
    <summary>  This effort examines the intersection of the emerging field of quantum
computing and the more established field of evolutionary computation. The goal
is to understand what benefits quantum computing might offer to computational
intelligence and how computational intelligence paradigms might be implemented
as quantum programs to be run on a future quantum computer. We critically
examine proposed algorithms and methods for implementing computational
intelligence paradigms, primarily focused on heuristic optimization methods
including and related to evolutionary computation, with particular regard for
their potential for eventual implementation on quantum computing hardware.
</summary>
    <author>
      <name>Donald A. Sofge</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.1133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.1133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.4237v1</id>
    <updated>2008-04-26T16:55:59Z</updated>
    <published>2008-04-26T16:55:59Z</published>
    <title>Explaining the Logical Nature of Electrical Solitons in Neural Circuits</title>
    <summary>  Neurons are modeled electrically based on ferroelectric membranes thin enough
to permit charge transfer, conjectured to be the tunneling result of thermally
energetic ions and random electrons. These membranes can be triggered to
produce electrical solitons, the main signals for brain associative memory and
logical processing. Dendritic circuits are modeled, and electrical solitons are
simulated to demonstrate the nature of soliton propagation, soliton reflection,
the collision of solitons, as well as soliton OR gates, AND gates, XOR gates
and NOT gates.
</summary>
    <author>
      <name>John Robert Burger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.4237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.4237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.4808v1</id>
    <updated>2008-04-30T12:23:05Z</updated>
    <published>2008-04-30T12:23:05Z</published>
    <title>Solving Time of Least Square Systems in Sigma-Pi Unit Networks</title>
    <summary>  The solving of least square systems is a useful operation in
neurocomputational modeling of learning, pattern matching, and pattern
recognition. In these last two cases, the solution must be obtained on-line,
thus the time required to solve a system in a plausible neural architecture is
critical. This paper presents a recurrent network of Sigma-Pi neurons, whose
solving time increases at most like the logarithm of the system size, and of
its condition number, which provides plausible computation times for biological
systems.
</summary>
    <author>
      <name>Pierre Courrieu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LPC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Nombre de pages: 7</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Information Processing - Letters and Reviews 4, 3 (2004)
  39-45</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0804.4808v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.4808v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.0231v4</id>
    <updated>2008-05-18T06:38:04Z</updated>
    <published>2008-05-02T13:55:37Z</published>
    <title>CMA-ES with Two-Point Step-Size Adaptation</title>
    <summary>  We combine a refined version of two-point step-size adaptation with the
covariance matrix adaptation evolution strategy (CMA-ES). Additionally, we
suggest polished formulae for the learning rate of the covariance matrix and
the recombination weights. In contrast to cumulative step-size adaptation or to
the 1/5-th success rule, the refined two-point adaptation (TPA) does not rely
on any internal model of optimality. In contrast to conventional
self-adaptation, the TPA will achieve a better target step-size in particular
with large populations. The disadvantage of TPA is that it relies on two
additional objective function
</summary>
    <author>
      <name>Nikolaus Hansen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Futurs</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/0805.0231v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.0231v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.0697v1</id>
    <updated>2008-05-06T11:06:49Z</updated>
    <published>2008-05-06T11:06:49Z</published>
    <title>Stochastic Optimization Approaches for Solving Sudoku</title>
    <summary>  In this paper the Sudoku problem is solved using stochastic search techniques
and these are: Cultural Genetic Algorithm (CGA), Repulsive Particle Swarm
Optimization (RPSO), Quantum Simulated Annealing (QSA) and the Hybrid method
that combines Genetic Algorithm with Simulated Annealing (HGASA). The results
obtained show that the CGA, QSA and HGASA are able to solve the Sudoku puzzle
with CGA finding a solution in 28 seconds, while QSA finding a solution in 65
seconds and HGASA in 1.447 seconds. This is mainly because HGASA combines the
parallel searching of GA with the flexibility of SA. The RPSO was found to be
unable to solve the puzzle.
</summary>
    <author>
      <name>Meir Perez</name>
    </author>
    <author>
      <name>Tshilidzi Marwala</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.eswa.2012.04.019</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.eswa.2012.04.019" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0805.0697v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.0697v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.1153v1</id>
    <updated>2008-05-08T12:10:21Z</updated>
    <published>2008-05-08T12:10:21Z</published>
    <title>Contact state analysis using NFIS and SOM</title>
    <summary>  This paper reports application of neuro- fuzzy inference system (NFIS) and
self organizing feature map neural networks (SOM) on detection of contact state
in a block system. In this manner, on a simple system, the evolution of contact
states, by parallelization of DDA, has been investigated. So, a comparison
between NFIS and SOM results has been presented. The results show applicability
of the proposed methods, by different accuracy, on detection of contact's
distribution.
</summary>
    <author>
      <name>H. Owladeghaffari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. International Symposium on Computational Mechanics (ISCM2007),
  Yao ZH &amp; Yuan MW (eds.), Beijing: Tsinghua University Press &amp; Springer, July
  30-August 1, 2007, Beijing, China,</arxiv:comment>
    <link href="http://arxiv.org/abs/0805.1153v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.1153v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0806.3646v2</id>
    <updated>2008-08-09T21:45:53Z</updated>
    <published>2008-06-23T10:04:14Z</published>
    <title>Round Trip Time Prediction Using the Symbolic Function Network Approach</title>
    <summary>  In this paper, we develop a novel approach to model the Internet round trip
time using a recently proposed symbolic type neural network model called
symbolic function network. The developed predictor is shown to have good
generalization performance and simple representation compared to the multilayer
perceptron based predictors.
</summary>
    <author>
      <name>George S. Eskander</name>
    </author>
    <author>
      <name>Amir Atiya</name>
    </author>
    <author>
      <name>Kil To Chong</name>
    </author>
    <author>
      <name>Hyongsuk Kim</name>
    </author>
    <author>
      <name>Sung Goo Yoo</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ISITC, pp. 3-7, 2007 International Symposium on Information
  Technology Convergence (ISITC 2007), 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0806.3646v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0806.3646v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.1378v1</id>
    <updated>2008-08-09T22:05:48Z</updated>
    <published>2008-08-09T22:05:48Z</published>
    <title>A Novel Symbolic Type Neural Network Model- Application to River Flow
  Forecasting</title>
    <summary>  In this paper we introduce a new symbolic type neural tree network called
symbolic function network (SFN) that is based on using elementary functions to
model systems in a symbolic form. The proposed formulation permits feature
selection, functional selection, and flexible structure. We applied this model
on the River Flow forecasting problem. The results found to be superior in both
fitness and sparsity.
</summary>
    <author>
      <name>George S. Eskander</name>
    </author>
    <author>
      <name>Amir F. Atiya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in ICENCO2007, Cairo, December 2007</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.1378v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.1378v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0809.4622v1</id>
    <updated>2008-09-26T13:12:36Z</updated>
    <published>2008-09-26T13:12:36Z</published>
    <title>A computational approach to the covert and overt deployment of spatial
  attention</title>
    <summary>  Popular computational models of visual attention tend to neglect the
influence of saccadic eye movements whereas it has been shown that the primates
perform on average three of them per seconds and that the neural substrate for
the deployment of attention and the execution of an eye movement might
considerably overlap. Here we propose a computational model in which the
deployment of attention with or without a subsequent eye movement emerges from
local, distributed and numerical computations.
</summary>
    <author>
      <name>Jérémy Fix</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - Loria</arxiv:affiliation>
    </author>
    <author>
      <name>Nicolas P. Rougier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - Loria, University of Colorado, Boulder</arxiv:affiliation>
    </author>
    <author>
      <name>Frédéric Alexandre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lorraine - Loria</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans NeuroComp 2008 : 2i\`eme Conf\'erence Fran\c{c}aise de
  Neurosciences Computationnelles (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0809.4622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0809.4622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.3356v1</id>
    <updated>2008-10-19T00:38:06Z</updated>
    <published>2008-10-19T00:38:06Z</published>
    <title>The Fundamental Problem with the Building Block Hypothesis</title>
    <summary>  Skepticism of the building block hypothesis (BBH) has previously been
expressed on account of the weak theoretical foundations of this hypothesis and
the anomalies in the empirical record of the simple genetic algorithm. In this
paper we hone in on a more fundamental cause for skepticism--the extraordinary
strength of some of the assumptions that undergird the BBH. Specifically, we
focus on assumptions made about the distribution of fitness over the genome
set, and argue that these assumptions are unacceptably strong. As most of these
assumptions have been embraced by the designers of so-called "competent"
genetic algorithms, our critique is relevant to an appraisal of such algorithms
as well.
</summary>
    <author>
      <name>Keki Burjorjee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary version. 26 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.3356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.3356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0810.3492v1</id>
    <updated>2008-10-20T08:36:22Z</updated>
    <published>2008-10-20T08:36:22Z</published>
    <title>The Connectivity of NK Landscapes' Basins: A Network Analysis</title>
    <summary>  We propose a network characterization of combinatorial fitness landscapes by
adapting the notion of inherent networks proposed for energy surfaces. We use
the well-known family of NK landscapes as an example. In our case the inherent
network is the graph where the vertices represent the local maxima in the
landscape, and the edges account for the transition probabilities between their
corresponding basins of attraction. We exhaustively extracted such networks on
representative small NK landscape instances, and performed a statistical
characterization of their properties. We found that most of these network
properties can be related to the search difficulty on the underlying NK
landscapes with varying values of K.
</summary>
    <author>
      <name>Sébastien Verel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Gabriela Ochoa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISI</arxiv:affiliation>
    </author>
    <author>
      <name>Marco Tomassini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISI</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Life XI, Winchester : France (2008)</arxiv:comment>
    <link href="http://arxiv.org/abs/0810.3492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0810.3492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.0882v1</id>
    <updated>2008-12-04T09:12:14Z</updated>
    <published>2008-12-04T09:12:14Z</published>
    <title>Elagage d'un perceptron multicouches : utilisation de l'analyse de la
  variance de la sensibilité des paramètres</title>
    <summary>  The stucture determination of a neural network for the modelisation of a
system remain the core of the problem. Within this framework, we propose a
pruning algorithm of the network based on the use of the analysis of the
sensitivity of the variance of all the parameters of the network. This
algorithm will be tested on two examples of simulation and its performances
will be compared with three other algorithms of pruning of the literature
</summary>
    <author>
      <name>Philippe Thomas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <author>
      <name>André Thomas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Conf\'erence Internationale Francophone d'Automatique CIFA'08,
  Bucarest : Roumanie (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0812.0882v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.0882v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0812.1094v1</id>
    <updated>2008-12-05T08:49:53Z</updated>
    <published>2008-12-05T08:49:53Z</published>
    <title>Sélection de la structure d'un perceptron multicouches pour la
  réduction dun modèle de simulation d'une scierie</title>
    <summary>  Simulation is often used to evaluate the relevance of a Directing Program of
Production (PDP) or to evaluate its impact on detailed sc\'enarii of
scheduling. Within this framework, we propose to reduce the complexity of a
model of simulation by exploiting a multilayer perceptron. A main phase of the
modeling of one system using a multilayer perceptron remains the determination
of the structure of the network. We propose to compare and use various pruning
algorithms in order to determine the optimal structure of the network used to
reduce the complexity of the model of simulation of our case of application: a
sawmill.
</summary>
    <author>
      <name>Philippe Thomas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <author>
      <name>André Thomas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Conf\'erence Internationale Francophone d'Automatique CIFA'08,
  Bucarest : Roumanie (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0812.1094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0812.1094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.0317v1</id>
    <updated>2009-01-03T17:35:49Z</updated>
    <published>2009-01-03T17:35:49Z</published>
    <title>Design of a P System based Artificial Graph Chemistry</title>
    <summary>  Artificial Chemistries (ACs) are symbolic chemical metaphors for the
exploration of Artificial Life, with specific focus on the origin of life. In
this work we define a P system based artificial graph chemistry to understand
the principles leading to the evolution of life-like structures in an AC set up
and to develop a unified framework to characterize and classify symbolic
artificial chemistries by devising appropriate formalism to capture semantic
and organizational information. An extension of P system is considered by
associating probabilities with the rules providing the topological framework
for the evolution of a labeled undirected graph based molecular reaction
semantics.
</summary>
    <author>
      <name>Janardan Misra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0901.0317v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.0317v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0901.0597v4</id>
    <updated>2010-09-13T12:40:44Z</updated>
    <published>2009-01-06T06:36:54Z</published>
    <title>On the Optimal Convergence Probability of Univariate Estimation of
  Distribution Algorithms</title>
    <summary>  In this paper, we obtain bounds on the probability of convergence to the
optimal solution for the compact Genetic Algorithm (cGA) and the Population
Based Incremental Learning (PBIL). We also give a sufficient condition for
convergence of these algorithms to the optimal solution and compute a range of
possible values of the parameters of these algorithms for which they converge
to the optimal solution with a confidence level.
</summary>
    <author>
      <name>Reza Rastegar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">evolutionary computation</arxiv:comment>
    <link href="http://arxiv.org/abs/0901.0597v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0901.0597v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0902.1690v1</id>
    <updated>2009-02-10T16:43:06Z</updated>
    <published>2009-02-10T16:43:06Z</published>
    <title>Back analysis of microplane model parameters using soft computing
  methods</title>
    <summary>  A new procedure based on layered feed-forward neural networks for the
microplane material model parameters identification is proposed in the present
paper. Novelties are usage of the Latin Hypercube Sampling method for the
generation of training sets, a systematic employment of stochastic sensitivity
analysis and a genetic algorithm-based training of a neural network by an
evolutionary algorithm. Advantages and disadvantages of this approach together
with possible extensions are thoroughly discussed and analyzed.
</summary>
    <author>
      <name>A. Kucerova</name>
    </author>
    <author>
      <name>M. Leps</name>
    </author>
    <author>
      <name>J. Zeman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 27 figures, 7 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CAMES: Computer Assisted Mechanics and Engineering Sciences, 14
  (2), 219-242, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0902.1690v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0902.1690v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0903.2516v1</id>
    <updated>2009-03-14T00:18:08Z</updated>
    <published>2009-03-14T00:18:08Z</published>
    <title>Effect of Degree Distribution on Evolutionary Search</title>
    <summary>  This paper introduces a method to generate hierarchically modular networks
with prescribed node degree list and proposes a metric to measure network
modularity based on the notion of edge distance. The generated networks are
used as test problems to explore the effect of modularity and degree
distribution on evolutionary algorithm performance. Results from the
experiments (i) confirm a previous finding that modularity increases the
performance advantage of genetic algorithms over hill climbers, and (ii)
support a new conjecture that test problems with modularized constraint
networks having heavy-tailed right-skewed degree distributions are more easily
solved than test problems with modularized constraint networks having
bell-shaped normal degree distributions.
</summary>
    <author>
      <name>Susan Khor</name>
    </author>
    <link href="http://arxiv.org/abs/0903.2516v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0903.2516v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0904.3650v1</id>
    <updated>2009-04-23T10:44:21Z</updated>
    <published>2009-04-23T10:44:21Z</published>
    <title>The use of invariant moments in hand-written character recognition</title>
    <summary>  The goal of this paper is to present the implementation of a Radial Basis
Function neural network with built-in knowledge to recognize hand-written
characters. The neural network includes in its architecture gates controlled by
an attraction/repulsion system of coefficients. These coefficients are derived
from a preprocessing stage which groups the characters according to their
ascendant, central, or descendent components. The neural network is trained
using data from invariant moment functions. Results are compared with those
obtained using a K nearest neighbor method on the same moment data.
</summary>
    <author>
      <name>Dan L. Lacrama</name>
    </author>
    <author>
      <name>Ioan Snep</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages,exposed on 1st "European Conference on Computer Sciences &amp;
  Applications" - XA2006, Timisoara, Romania</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series IV (2006), 91-102</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0904.3650v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0904.3650v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0905.3771v1</id>
    <updated>2009-05-22T22:09:59Z</updated>
    <published>2009-05-22T22:09:59Z</published>
    <title>Memory Retrieved from Single Neurons</title>
    <summary>  The paper examines the problem of accessing a vector memory from a single
neuron in a Hebbian neural network. It begins with the review of the author's
earlier method, which is different from the Hopfield model in that it recruits
neighboring neurons by spreading activity, making it possible for single or
group of neurons to become associated with vector memories. Some open issues
associated with this approach are identified. It is suggested that fragments
that generate stored memories could be associated with single neurons through
local spreading activity.
</summary>
    <author>
      <name>Subhash Kak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/0905.3771v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0905.3771v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.1900v1</id>
    <updated>2009-06-10T09:56:29Z</updated>
    <published>2009-06-10T09:56:29Z</published>
    <title>How deals with discrete data for the reduction of simulation models
  using neural network</title>
    <summary>  Simulation is useful for the evaluation of a Master Production/distribution
Schedule (MPS). Also, the goal of this paper is the study of the design of a
simulation model by reducing its complexity. According to theory of
constraints, we want to build reduced models composed exclusively by
bottlenecks and a neural network. Particularly a multilayer perceptron, is
used. The structure of the network is determined by using a pruning procedure.
This work focuses on the impact of discrete data on the results and compares
different approaches to deal with these data. This approach is applied to
sawmill internal supply chain
</summary>
    <author>
      <name>Philippe Thomas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <author>
      <name>André Thomas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CRAN</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">13th IFAC Symp. On Information Control Problems in Manufacturing
  INCOM'09, Moscou : Russie (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0906.1900v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.1900v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0906.4846v1</id>
    <updated>2009-06-26T06:25:07Z</updated>
    <published>2009-06-26T06:25:07Z</published>
    <title>A genetic algorithm for structure-activity relationships: software
  implementation</title>
    <summary>  The design and the implementation of a genetic algorithm are described. The
applicability domain is on structure-activity relationships expressed as
multiple linear regressions and predictor variables are from families of
structure-based molecular descriptors. An experiment to compare different
selection and survival strategies was designed and realized. The genetic
algorithm was run using the designed experiment on a set of 206 polychlorinated
biphenyls searching on structure-activity relationships having known the
measured octanol-water partition coefficients and a family of molecular
descriptors. The experiment shows that different selection and survival
strategies create different partitions on the entire population of all possible
genotypes.
</summary>
    <author>
      <name>Lorentz Jantschi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages; 10 figures; 14 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/0906.4846v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0906.4846v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.3; I.6.4; J.2; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.0229v1</id>
    <updated>2009-07-01T19:54:39Z</updated>
    <published>2009-07-01T19:54:39Z</published>
    <title>A new model of artificial neuron: cyberneuron and its use</title>
    <summary>  This article describes a new type of artificial neuron, called the authors
"cyberneuron". Unlike classical models of artificial neurons, this type of
neuron used table substitution instead of the operation of multiplication of
input values for the weights. This allowed to significantly increase the
information capacity of a single neuron, but also greatly simplify the process
of learning. Considered an example of the use of "cyberneuron" with the task of
detecting computer viruses.
</summary>
    <author>
      <name>S. V. Polikarpov</name>
    </author>
    <author>
      <name>V. S. Dergachev</name>
    </author>
    <author>
      <name>K. E. Rumyantsev</name>
    </author>
    <author>
      <name>D. M. Golubchikov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 23 figures, in Russian</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.0229v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.0229v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.0520v1</id>
    <updated>2009-07-03T02:32:34Z</updated>
    <published>2009-07-03T02:32:34Z</published>
    <title>Computational Scenario-based Capability Planning</title>
    <summary>  Scenarios are pen-pictures of plausible futures, used for strategic planning.
The aim of this investigation is to expand the horizon of scenario-based
planning through computational models that are able to aid the analyst in the
planning process. The investigation builds upon the advances of Information and
Communication Technology (ICT) to create a novel, flexible and customizable
computational capability-based planning methodology that is practical and
theoretically sound. We will show how evolutionary computation, in particular
evolutionary multi-objective optimization, can play a central role - both as an
optimizer and as a source for innovation.
</summary>
    <author>
      <name>Hussein Abbass</name>
    </author>
    <author>
      <name>Axel Bender</name>
    </author>
    <author>
      <name>Helen Dam</name>
    </author>
    <author>
      <name>Stephen Baker</name>
    </author>
    <author>
      <name>James M Whitacre</name>
    </author>
    <author>
      <name>Ruhul Sarker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1389095.1389378</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1389095.1389378" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">GECCO-2008, Atlanta, GA, USA</arxiv:comment>
    <link href="http://arxiv.org/abs/0907.0520v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.0520v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0907.0592v1</id>
    <updated>2009-07-03T11:02:25Z</updated>
    <published>2009-07-03T11:02:25Z</published>
    <title>Credit Assignment in Adaptive Evolutionary Algorithms</title>
    <summary>  In this paper, a new method for assigning credit to search operators is
presented. Starting with the principle of optimizing search bias, search
operators are selected based on an ability to create solutions that are
historically linked to future generations. Using a novel framework for defining
performance measurements, distributing credit for performance, and the
statistical interpretation of this credit, a new adaptive method is developed
and shown to outperform a variety of adaptive and non-adaptive competitors.
</summary>
    <author>
      <name>James M. Whitacre</name>
    </author>
    <author>
      <name>Tuan Q. Pham</name>
    </author>
    <author>
      <name>Ruhul A. Sarker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/1143997.1144206</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/1143997.1144206" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Genetic And Evolutionary Computation Conference, 2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0907.0592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0907.0592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.1597v1</id>
    <updated>2009-08-11T23:45:08Z</updated>
    <published>2009-08-11T23:45:08Z</published>
    <title>A quantum diffusion network</title>
    <summary>  Wong's diffusion network is a stochastic, zero-input Hopfield network with a
Gibbs stationary distribution over a bounded, connected continuum. Previously,
logarithmic thermal annealing was demonstrated for the diffusion network and
digital versions of it were studied and applied to imaging. Recently, "quantum"
annealed Markov chains have garnered significant attention because of their
improved performance over "pure" thermal annealing. In this note, a joint
quantum and thermal version of Wong's diffusion network is described and its
convergence properties are studied. Different choices for "auxiliary" functions
are discussed, including those of the kinetic type previously associated with
quantum annealing.
</summary>
    <author>
      <name>George Kesidis</name>
    </author>
    <link href="http://arxiv.org/abs/0908.1597v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.1597v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0908.3184v2</id>
    <updated>2009-11-19T18:31:27Z</updated>
    <published>2009-08-21T19:53:54Z</published>
    <title>Location of Single Neuron Memories in a Hebbian Network</title>
    <summary>  This paper reports the results of an experiment on the use of Kak's B-Matrix
approach to spreading activity in a Hebbian neural network. Specifically, it
concentrates on the memory retrieval from single neurons and compares the
performance of the B-Matrix approach to that of the traditional approach.
</summary>
    <author>
      <name>Krishna Chaithanya Lingashetty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 11 figures, Presented at the Conference on Theoretical and
  Applied Computer Science 2009(TACS'09), Stillwater, Oklahoma. Corrected
  results and formatting changes</arxiv:comment>
    <link href="http://arxiv.org/abs/0908.3184v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0908.3184v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.0646v1</id>
    <updated>2009-10-04T22:09:18Z</updated>
    <published>2009-10-04T22:09:18Z</published>
    <title>Digital Business Ecosystems: Natural Science Paradigms</title>
    <summary>  A primary motivation for research in Digital Ecosystems is the desire to
exploit the self-organising properties of natural ecosystems. Ecosystems arc
thought to be robust, scalable architectures that can automatically solve
complex, dynamic problems. However, the biological processes that contribute to
these properties have not been made explicit in Digital Ecosystem research.
Here, we introduce how biological properties contribute to the self-organising
features of natural ecosystems. These properties include populations of
evolving agents, a complex dynamic environment, and spatial distributions which
generate local interactions. The potential for exploiting these properties in
artificial systems is then considered.
</summary>
    <author>
      <name>Gerard Briscoe</name>
    </author>
    <author>
      <name>Suzanne Sadedin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0910.0646v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.0646v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0910.4116v1</id>
    <updated>2009-10-21T15:02:12Z</updated>
    <published>2009-10-21T15:02:12Z</published>
    <title>Swarm Intelligence</title>
    <summary>  Biologically inspired computing is an area of computer science which uses the
advantageous properties of biological systems. It is the amalgamation of
computational intelligence and collective intelligence. Biologically inspired
mechanisms have already proved successful in achieving major advances in a wide
range of problems in computing and communication systems. The consortium of
bio-inspired computing are artificial neural networks, evolutionary algorithms,
swarm intelligence, artificial immune systems, fractal geometry, DNA computing
and quantum computing, etc. This article gives an introduction to swarm
intelligence.
</summary>
    <author>
      <name>Sabu M. Thampi</name>
    </author>
    <link href="http://arxiv.org/abs/0910.4116v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0910.4116v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.0936v1</id>
    <updated>2009-12-04T21:33:10Z</updated>
    <published>2009-12-04T21:33:10Z</published>
    <title>Neural-estimator for the surface emission rate of atmospheric gases</title>
    <summary>  The emission rate of minority atmospheric gases is inferred by a new approach
based on neural networks. The neural network applied is the multi-layer
perceptron with backpropagation algorithm for learning. The identification of
these surface fluxes is an inverse problem. A comparison between the new
neural-inversion and regularized inverse solution id performed. The results
obtained from the neural networks are significantly better. In addition, the
inversion with the neural netwroks is fster than regularized approaches, after
training.
</summary>
    <author>
      <name>F. F. Paes</name>
    </author>
    <author>
      <name>H. F. Campos Velho</name>
    </author>
    <link href="http://arxiv.org/abs/0912.0936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.0936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0912.1534v2</id>
    <updated>2010-01-18T21:20:22Z</updated>
    <published>2009-12-08T16:29:24Z</published>
    <title>Evolutionary multi-stage financial scenario tree generation</title>
    <summary>  Multi-stage financial decision optimization under uncertainty depends on a
careful numerical approximation of the underlying stochastic process, which
describes the future returns of the selected assets or asset categories.
Various approaches towards an optimal generation of discrete-time,
discrete-state approximations (represented as scenario trees) have been
suggested in the literature. In this paper, a new evolutionary algorithm to
create scenario trees for multi-stage financial optimization models will be
presented. Numerical results and implementation details conclude the paper.
</summary>
    <author>
      <name>Ronald Hochreiter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-12242-2_19</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-12242-2_19" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Computer Science 6025:182-191. 2010.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0912.1534v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0912.1534v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.PM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.0745v1</id>
    <updated>2010-02-03T14:02:39Z</updated>
    <published>2010-02-03T14:02:39Z</published>
    <title>Using CODEQ to Train Feed-forward Neural Networks</title>
    <summary>  CODEQ is a new, population-based meta-heuristic algorithm that is a hybrid of
concepts from chaotic search, opposition-based learning, differential evolution
and quantum mechanics. CODEQ has successfully been used to solve different
types of problems (e.g. constrained, integer-programming, engineering) with
excellent results. In this paper, CODEQ is used to train feed-forward neural
networks. The proposed method is compared with particle swarm optimization and
differential evolution algorithms on three data sets with encouraging results.
</summary>
    <author>
      <name>Mahamed G. H. Omran</name>
    </author>
    <author>
      <name>Faisal al-Adwani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1002.0745v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.0745v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1002.2012v1</id>
    <updated>2010-02-10T01:16:21Z</updated>
    <published>2010-02-10T01:16:21Z</published>
    <title>Implementing Genetic Algorithms on Arduino Micro-Controllers</title>
    <summary>  Since their conception in 1975, Genetic Algorithms have been an extremely
popular approach to find exact or approximate solutions to optimization and
search problems. Over the last years there has been an enhanced interest in the
field with related techniques, such as grammatical evolution, being developed.
Unfortunately, work on developing genetic optimizations for low-end embedded
architectures hasn't embraced the same enthusiasm. This short paper tackles
that situation by demonstrating how genetic algorithms can be implemented in
Arduino Duemilanove, a 16 MHz open-source micro-controller, with limited
computation power and storage resources. As part of this short paper, the
libraries used in this implementation are released into the public domain under
a GPL license.
</summary>
    <author>
      <name>Nuno Alves</name>
    </author>
    <link href="http://arxiv.org/abs/1002.2012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1002.2012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.0358v1</id>
    <updated>2010-03-01T14:32:11Z</updated>
    <published>2010-03-01T14:32:11Z</published>
    <title>Deep Big Simple Neural Nets Excel on Handwritten Digit Recognition</title>
    <summary>  Good old on-line back-propagation for plain multi-layer perceptrons yields a
very low 0.35% error rate on the famous MNIST handwritten digits benchmark. All
we need to achieve this best result so far are many hidden layers, many neurons
per layer, numerous deformed training images, and graphics cards to greatly
speed up learning.
</summary>
    <author>
      <name>Dan Claudiu Ciresan</name>
    </author>
    <author>
      <name>Ueli Meier</name>
    </author>
    <author>
      <name>Luca Maria Gambardella</name>
    </author>
    <author>
      <name>Juergen Schmidhuber</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/NECO_a_00052</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/NECO_a_00052" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 2 figures, 4 listings</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Computation, Volume 22, Number 12, December 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1003.0358v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.0358v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1003.2724v1</id>
    <updated>2010-03-13T16:36:47Z</updated>
    <published>2010-03-13T16:36:47Z</published>
    <title>Particle Swarm Optimization Based Diophantine Equation Solver</title>
    <summary>  The paper introduces particle swarm optimization as a viable strategy to find
numerical solution of Diophantine equation, for which there exists no general
method of finding solutions. The proposed methodology uses a population of
integer particles. The candidate solutions in the feasible space are optimized
to have better positions through particle best and global best positions. The
methodology, which follows fully connected neighborhood topology, can offer
many solutions of such equations.
</summary>
    <author>
      <name>Siby Abraham</name>
    </author>
    <author>
      <name>Sugata Sanyal</name>
    </author>
    <author>
      <name>Mukund Sanglikar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 Pages, 12 Figures, 5 Tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1003.2724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1003.2724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.3557v1</id>
    <updated>2010-04-20T20:17:41Z</updated>
    <published>2010-04-20T20:17:41Z</published>
    <title>Neuroevolutionary optimization</title>
    <summary>  This paper presents an application of evolutionary search procedures to
artificial neural networks. Here, we can distinguish among three kinds of
evolution in artificial neural networks, i.e. the evolution of connection
weights, of architectures, and of learning rules. We review each kind of
evolution in detail and analyse critical issues related to different
evolutions. This article concentrates on finding the suitable way of using
evolutionary algorithms for optimizing the artificial neural network
parameters.
</summary>
    <author>
      <name>Eva Volna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues online at
  http://ijcsi.org/articles/Neuroevolutionary-optimization.php</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI, Volume 7, Issue 2, March 2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.3557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.3557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.4610v1</id>
    <updated>2010-04-26T19:18:48Z</updated>
    <published>2010-04-26T19:18:48Z</published>
    <title>Mobility Prediction in Wireless Ad Hoc Networks using Neural Networks</title>
    <summary>  Mobility prediction allows estimating the stability of paths in a mobile
wireless Ad Hoc networks. Identifying stable paths helps to improve routing by
reducing the overhead and the number of connection interruptions. In this
paper, we introduce a neural network based method for mobility prediction in Ad
Hoc networks. This method consists of a multi-layer and recurrent neural
network using back propagation through time algorithm for training.
</summary>
    <author>
      <name>Heni Kaaniche</name>
    </author>
    <author>
      <name>Farouk Kamoun</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Heni Kaaniche and Farouk Kamoun, "Mobility Prediction in Wireless Ad
  Hoc Networks using Neural Networks", Journal of Telecommunications, Volume 2,
  Issue 1, p95-101, April 2010</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Telecommunications, Volume 2, Issue 1, p95-101, April
  2010</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1004.4610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.4610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1004.5262v1</id>
    <updated>2010-04-29T12:25:16Z</updated>
    <published>2010-04-29T12:25:16Z</published>
    <title>On Application of the Local Search and the Genetic Algorithms Techniques
  to Some Combinatorial Optimization Problems</title>
    <summary>  In this paper the approach to solving several combinatorial optimization
problems using the local search and the genetic algorithm techniques is
proposed. Initially this approach was developed in purpose to overcome some
difficulties inhibiting the application of above mentioned techniques to the
problems of the Questionnaire Theory. But when the algorithms were developed it
became clear that them could be successfully applied also to the Minimum Set
Cover, the 0-1-Knapsack and probably to other combinatorial optimization
problems.
</summary>
    <author>
      <name>Anton Bondarenko</name>
    </author>
    <link href="http://arxiv.org/abs/1004.5262v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1004.5262v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C27, 68P10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1005.4446v1</id>
    <updated>2010-05-24T22:15:57Z</updated>
    <published>2010-05-24T22:15:57Z</published>
    <title>Genetic algorithms and the art of Zen</title>
    <summary>  In this paper we present a novel genetic algorithm (GA) solution to a simple
yet challenging commercial puzzle game known as the Zen Puzzle Garden (ZPG). We
describe the game in detail, before presenting a suitable encoding scheme and
fitness function for candidate solutions. We then compare the performance of
the genetic algorithm with that of the A* algorithm. Our results show that the
GA is competitive with informed search in terms of solution quality, and
significantly out-performs it in terms of computational resource requirements.
We conclude with a brief discussion of the implications of our findings for
game solving and other "real world" problems.
</summary>
    <author>
      <name>Jack Coldridge</name>
    </author>
    <author>
      <name>Martyn Amos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1005.4446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1005.4446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.4221v2</id>
    <updated>2010-10-01T23:55:16Z</updated>
    <published>2010-07-23T21:30:18Z</published>
    <title>Building Blocks Propagation in Quantum-Inspired Genetic Algorithm</title>
    <summary>  This paper presents an analysis of building blocks propagation in
Quantum-Inspired Genetic Algorithm, which belongs to a new class of
metaheuristics drawing their inspiration from both biological evolution and
unitary evolution of quantum systems. The expected number of quantum
chromosomes matching a schema has been analyzed and a random variable
corresponding to this issue has been introduced. The results have been compared
with Simple Genetic Algorithm. Also, it has been presented how selected binary
quantum chromosomes cover a domain of one-dimensional fitness function.
</summary>
    <author>
      <name>Robert Nowotniak</name>
    </author>
    <author>
      <name>Jacek Kucharski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1007.4221v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.4221v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1007.4636v2</id>
    <updated>2010-11-15T08:52:23Z</updated>
    <published>2010-07-27T08:18:52Z</published>
    <title>Computational Complexity Analysis of Simple Genetic Programming On Two
  Problems Modeling Isolated Program Semantics</title>
    <summary>  Analyzing the computational complexity of evolutionary algorithms for binary
search spaces has significantly increased their theoretical understanding. With
this paper, we start the computational complexity analysis of genetic
programming. We set up several simplified genetic programming algorithms and
analyze them on two separable model problems, ORDER and MAJORITY, each of which
captures an important facet of typical genetic programming problems. Both
analyses give first rigorous insights on aspects of genetic programming design,
highlighting in particular the impact of accepting or rejecting neutral moves
and the importance of a local mutation operator.
</summary>
    <author>
      <name>Greg Durrett</name>
    </author>
    <author>
      <name>Frank Neumann</name>
    </author>
    <author>
      <name>Una-May O'Reilly</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1007.4636v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1007.4636v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1009.5031v1</id>
    <updated>2010-09-25T19:44:51Z</updated>
    <published>2010-09-25T19:44:51Z</published>
    <title>A Genetic Algorithm for the Multi-Pickup and Delivery Problem with time
  windows</title>
    <summary>  In This paper we present a genetic algorithm for the multi-pickup and
delivery problem with time windows (m-PDPTW). The m-PDPTW is an optimization
vehicles routing problem which must meet requests for transport between
suppliers and customers satisfying precedence, capacity and time constraints.
This paper purposes a brief literature review of the PDPTW, present our
approach based on genetic algorithms to minimizing the total travel distance
and thereafter the total travel cost, by showing that an encoding represents
the parameters of each individual.
</summary>
    <author>
      <name>Imen Harbaoui Dridi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAGIS</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Kammarti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACS</arxiv:affiliation>
    </author>
    <author>
      <name>Mekki Ksouri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACS</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Borne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAGIS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Studies in Informatics and Control 18, 2 (2009) pages 173-180</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1009.5031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1009.5031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.0771v2</id>
    <updated>2013-02-01T07:35:40Z</updated>
    <published>2010-10-05T06:01:24Z</published>
    <title>Genetic Algorithm for Mulicriteria Optimization of a Multi-Pickup and
  Delivery Problem with Time Windows</title>
    <summary>  In This paper we present a genetic algorithm for mulicriteria optimization of
a multipickup and delivery problem with time windows (m-PDPTW). The m-PDPTW is
an optimization vehicles routing problem which must meet requests for transport
between suppliers and customers satisfying precedence, capacity and time
constraints. This paper purposes a brief literature review of the PDPTW,
present an approach based on genetic algorithms and Pareto dominance method to
give a set of satisfying solutions to the m-PDPTW minimizing total travel cost,
total tardiness time and the vehicles number.
</summary>
    <author>
      <name>Imen Harbaoui Dridi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACS</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Kammarti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACS</arxiv:affiliation>
    </author>
    <author>
      <name>Mekki Ksouri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACS</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Borne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAGIS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">INCOM'09 IFAC, Russie, F\'ed\'eration De (2009)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1010.0771v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.0771v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1010.0979v1</id>
    <updated>2010-10-05T18:54:43Z</updated>
    <published>2010-10-05T18:54:43Z</published>
    <title>Un Algorithme génétique pour le problème de ramassage et de
  livraison avec fenêtres de temps à plusieurs véhicules</title>
    <summary>  The PDPTW is an optimization vehicles routing problem which must meet
requests for transport between suppliers and customers satisfying precedence,
capacity and time constraints. We present, in this paper, a genetic algorithm
for optimization of a multi pickup and delivery problem with time windows
(m-PDPTW). We purposes a brief literature review of the PDPTW, present an
approach based on genetic algorithms to give a satisfying solution to the
m-PDPTW minimizing the total travel cost.
</summary>
    <author>
      <name>Imen Harbaoui Dridi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAGIS, ACS</arxiv:affiliation>
    </author>
    <author>
      <name>Ryan Kammarti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACS</arxiv:affiliation>
    </author>
    <author>
      <name>Pierre Borne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LAGIS</arxiv:affiliation>
    </author>
    <author>
      <name>Mekki Ksouri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ACS</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CIFA, Roumanie (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1010.0979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1010.0979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1011.4682v1</id>
    <updated>2010-11-21T17:39:27Z</updated>
    <published>2010-11-21T17:39:27Z</published>
    <title>Analysis of attractor distances in Random Boolean Networks</title>
    <summary>  We study the properties of the distance between attractors in Random Boolean
Networks, a prominent model of genetic regulatory networks. We define three
distance measures, upon which attractor distance matrices are constructed and
their main statistic parameters are computed. The experimental analysis shows
that ordered networks have a very clustered set of attractors, while chaotic
networks' attractors are scattered; critical networks show, instead, a pattern
with characteristics of both ordered and chaotic networks.
</summary>
    <author>
      <name>Andrea Roli</name>
    </author>
    <author>
      <name>Stefano Benedettini</name>
    </author>
    <author>
      <name>Roberto Serra</name>
    </author>
    <author>
      <name>Marco Villani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 6 figures. Presented at WIRN 2010 - Italian workshop on
  neural networks, May 2010. To appear in a volume published by IOS Press</arxiv:comment>
    <link href="http://arxiv.org/abs/1011.4682v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1011.4682v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.0952v1</id>
    <updated>2010-12-04T22:11:48Z</updated>
    <published>2010-12-04T22:11:48Z</published>
    <title>Faster Black-Box Algorithms Through Higher Arity Operators</title>
    <summary>  We extend the work of Lehre and Witt (GECCO 2010) on the unbiased black-box
model by considering higher arity variation operators. In particular, we show
that already for binary operators the black-box complexity of \leadingones
drops from $\Theta(n^2)$ for unary operators to $O(n \log n)$. For \onemax, the
$\Omega(n \log n)$ unary black-box complexity drops to O(n) in the binary case.
For $k$-ary operators, $k \leq n$, the \onemax-complexity further decreases to
$O(n/\log k)$.
</summary>
    <author>
      <name>Benjamin Doerr</name>
    </author>
    <author>
      <name>Daniel Johannsen</name>
    </author>
    <author>
      <name>Timo Kötzing</name>
    </author>
    <author>
      <name>Per Kristian Lehre</name>
    </author>
    <author>
      <name>Markus Wagner</name>
    </author>
    <author>
      <name>Carola Winzen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at FOGA 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.0952v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.0952v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.3656v1</id>
    <updated>2010-12-16T16:21:42Z</updated>
    <published>2010-12-16T16:21:42Z</published>
    <title>Adaptive Cluster Expansion (ACE): A Multilayer Network for Estimating
  Probability Density Functions</title>
    <summary>  We derive an adaptive hierarchical method of estimating high dimensional
probability density functions. We call this method of density estimation the
"adaptive cluster expansion" or ACE for short. We present an application of
this approach, based on a multilayer topographic mapping network, that
adaptively estimates the joint probability density function of the pixel values
of an image, and presents this result as a "probability image". We apply this
to the problem of identifying statistically anomalous regions in otherwise
statistically homogeneous images.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 14 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.3656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.3656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.3724v1</id>
    <updated>2010-12-16T19:30:20Z</updated>
    <published>2010-12-16T19:30:20Z</published>
    <title>The Development of Dominance Stripes and Orientation Maps in a
  Self-Organising Visual Cortex Network (VICON)</title>
    <summary>  A self-organising neural network is presented that is based on a rigorous
Bayesian analysis of the information contained in individual neural firing
events. This leads to a visual cortex network (VICON) that has many of the
properties emerge when a mammalian visual cortex is exposed to data arriving
from two imaging sensors (i.e. the two retinae), such as dominance stripes and
orientation maps.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">33 pages, 19 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.3724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.3724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1012.4173v1</id>
    <updated>2010-12-19T14:48:55Z</updated>
    <published>2010-12-19T14:48:55Z</published>
    <title>A Self-Organising Neural Network for Processing Data from Multiple
  Sensors</title>
    <summary>  This paper shows how a folded Markov chain network can be applied to the
problem of processing data from multiple sensors, with an emphasis on the
special case of 2 sensors. It is necessary to design the network so that it can
transform a high dimensional input vector into a posterior probability, for
which purpose the partitioned mixture distribution network is ideally suited.
The underlying theory is presented in detail, and a simple numerical simulation
is given that shows the emergence of ocular dominance stripes.
</summary>
    <author>
      <name>S P Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1012.4173v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1012.4173v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1101.5997v1</id>
    <updated>2011-01-31T15:47:05Z</updated>
    <published>2011-01-31T15:47:05Z</published>
    <title>New Model for Multi-Objective Evolutionary Algorithms</title>
    <summary>  Multi-Objective Evolutionary Algorithms (MOEAs) have been proved efficient to
deal with Multi-objective Optimization Problems (MOPs). Until now tens of MOEAs
have been proposed. The unified mode would provide a more systematic approach
to build new MOEAs. Here a new model is proposed which includes two sub-models
based on two classes of different schemas of MOEAs. According to the new model,
some representatives algorithms are decomposed and some interesting issues are
discussed.
</summary>
    <author>
      <name>Bojin Zheng</name>
    </author>
    <author>
      <name>Yuanxiang Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,ICCS 2007</arxiv:comment>
    <link href="http://arxiv.org/abs/1101.5997v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1101.5997v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1102.2559v1</id>
    <updated>2011-02-13T05:09:32Z</updated>
    <published>2011-02-13T05:09:32Z</published>
    <title>Toward Measuring the Scaling of Genetic Programming</title>
    <summary>  Several genetic programming systems are created, each solving a different
problem. In these systems, the median number of generations G needed to evolve
a working program is measured. The behavior of G is observed as the difficulty
of the problem is increased. In these systems, the density D of working
programs in the universe of all possible programs is measured. The relationship
G ~ 1/sqrt(D) is observed to approximately hold for two program-like systems.
For parallel systems (systems that look like several independent programs
evolving in parallel), the relationship G ~ 1/(n ln n) is observed to
approximately hold. Finally, systems that are anti-parallel are considered.
</summary>
    <author>
      <name>Mike Stimpson</name>
    </author>
    <link href="http://arxiv.org/abs/1102.2559v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1102.2559v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.2741v1</id>
    <updated>2011-03-14T18:58:59Z</updated>
    <published>2011-03-14T18:58:59Z</published>
    <title>Memory Retrieval in the B-Matrix Neural Network</title>
    <summary>  This paper is an extension to the memory retrieval procedure of the B-Matrix
approach [6],[17] to neural network learning. The B-Matrix is a part of the
interconnection matrix generated from the Hebbian neural network, and in memory
retrieval, the B-matrix is clamped with a small fragment of the memory. The
fragment gradually enlarges by means of feedback, until the entire vector is
obtained. In this paper, we propose the use of delta learning to enhance the
retrieval rate of the stored memories.
</summary>
    <author>
      <name>Prerana Laddha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 Pages, 4 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.2741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.2741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.4820v1</id>
    <updated>2011-03-24T17:59:10Z</updated>
    <published>2011-03-24T17:59:10Z</published>
    <title>Design and classification of dynamic multi-objective optimization
  problems</title>
    <summary>  In this work we provide a formal model for the different time-dependent
components that can appear in dynamic multi-objective optimization problems,
along with a classification of these components. Four main classes are
identified, corresponding to the influence of the parameters, objective
functions, previous states of the dynamic system and, last, environment
changes, which in turn lead to online optimization problems. For illustration
purposes, examples are provided for each class identified - by no means
standing as the most representative ones or exhaustive in scope.
</summary>
    <author>
      <name>Alexandru-Adrian Tantar</name>
    </author>
    <author>
      <name>Emilia Tantar</name>
    </author>
    <author>
      <name>Pascal Bouvry</name>
    </author>
    <link href="http://arxiv.org/abs/1103.4820v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.4820v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1103.5797v2</id>
    <updated>2011-05-06T06:07:01Z</updated>
    <published>2011-03-29T23:52:30Z</published>
    <title>Computational Complexity Results for Genetic Programming and the Sorting
  Problem</title>
    <summary>  Genetic Programming (GP) has found various applications. Understanding this
type of algorithm from a theoretical point of view is a challenging task. The
first results on the computational complexity of GP have been obtained for
problems with isolated program semantics. With this paper, we push forward the
computational complexity analysis of GP on a problem with dependent program
semantics. We study the well-known sorting problem in this context and analyze
rigorously how GP can deal with different measures of sortedness.
</summary>
    <author>
      <name>Markus Wagner</name>
    </author>
    <author>
      <name>Frank Neumann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1103.5797v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1103.5797v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.0775v2</id>
    <updated>2011-06-16T05:42:09Z</updated>
    <published>2011-04-05T08:46:33Z</published>
    <title>Evolving Pacing Strategies for Team Pursuit Track Cycling</title>
    <summary>  Team pursuit track cycling is a bicycle racing sport held on velodromes and
is part of the Summer Olympics. It involves the use of strategies to minimize
the overall time that a team of cyclists needs to complete a race. We present
an optimisation framework for team pursuit track cycling and show how to evolve
strategies using metaheuristics for this interesting real-world problem. Our
experimental results show that these heuristics lead to significantly better
strategies than state-of-art strategies that are currently used by teams of
cyclists.
</summary>
    <author>
      <name>Markus Wagner</name>
    </author>
    <author>
      <name>Jareth Day</name>
    </author>
    <author>
      <name>Diora Jordan</name>
    </author>
    <author>
      <name>Trent Kroeger</name>
    </author>
    <author>
      <name>Frank Neumann</name>
    </author>
    <link href="http://arxiv.org/abs/1104.0775v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.0775v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1104.2644v1</id>
    <updated>2011-04-13T23:24:49Z</updated>
    <published>2011-04-13T23:24:49Z</published>
    <title>Idealized Dynamic Population Sizing for Uniformly Scaled Problems</title>
    <summary>  This paper explores an idealized dynamic population sizing strategy for
solving additive decomposable problems of uniform scale. The method is designed
on top of the foundations of existing population sizing theory for this class
of problems, and is carefully compared with an optimal fixed population sized
genetic algorithm. The resulting strategy should be close to a lower bound in
terms of what can be achieved, performance-wise, by self-adjusting population
sizing algorithms for this class of problems.
</summary>
    <author>
      <name>Fernando G. Lobo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, submitted to ACM GECCO-2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1104.2644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1104.2644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.0118v2</id>
    <updated>2011-06-02T08:23:54Z</updated>
    <published>2011-06-01T08:29:28Z</published>
    <title>1st International Workshop on Distributed Evolutionary Computation in
  Informal Environments</title>
    <summary>  Online conference proceedings for the IWDECIE workshop, taking place in New
Orleans on June 5th, 2011. The workshop focuses on non-conventional
implementations of bioinspired algorithms and its conceptual implications.
</summary>
    <author>
      <name>Juan-J. Merelo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">editors</arxiv:affiliation>
    </author>
    <author>
      <name>Maribel García-Arenas</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">editors</arxiv:affiliation>
    </author>
    <author>
      <name>Juan-Luis J. Laredo</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">editors</arxiv:affiliation>
    </author>
    <author>
      <name>Francisco Fernández de la Vega</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">editors</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Five papers, workshop took place together with CEC 2011 in New
  Orleans (LA, USA). http://geneura.ugr.es/~iwdecie</arxiv:comment>
    <link href="http://arxiv.org/abs/1106.0118v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.0118v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1106.6185v1</id>
    <updated>2011-06-30T11:08:05Z</updated>
    <published>2011-06-30T11:08:05Z</published>
    <title>Effects of Compensation, Connectivity and Tau in a Computational Model
  of Alzheimer's Disease</title>
    <summary>  This work updates an existing, simplistic computational model of Alzheimer's
Disease (AD) to investigate the behaviour of synaptic compensatory mechanisms
in neural networks with small-world connectivity, and varying methods of
calculating compensation. It additionally introduces a method for simulating
tau neurofibrillary pathology, resulting in a more dramatic damage profile.
Small-world connectivity is shown to have contrasting effects on capacity,
retrieval time, and robustness to damage, whilst the use of more
easily-obtained remote memories rather than recent memories for synaptic
compensation is found to lead to rapid network damage.
</summary>
    <author>
      <name>Mark Rowan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, submitted to International Joint Conference on Neural
  Networks 2011</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">The 2011 International Joint Conference on Neural Networks
  (IJCNN), (2011) 543--550</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1106.6185v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1106.6185v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.4083v2</id>
    <updated>2011-08-24T07:43:10Z</updated>
    <published>2011-08-20T02:40:33Z</published>
    <title>Convergence of a Recombination-Based Elitist Evolutionary Algorithm on
  the Royal Roads Test Function</title>
    <summary>  We present an analysis of the performance of an elitist Evolutionary
algorithm using a recombination operator known as 1-Bit-Swap on the Royal Roads
test function based on a population. We derive complete, approximate and
asymptotic convergence rates for the algorithm. The complete model shows the
benefit of the size of the population and re- combination pool.
</summary>
    <author>
      <name>Aram Ter-Sarkisov</name>
    </author>
    <author>
      <name>Stephen Marsland</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted for AI 2011: 24th Australasian Joint Conference on
  Artificial Intelligence</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.4083v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.4083v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.4548v1</id>
    <updated>2011-08-23T10:47:23Z</updated>
    <published>2011-08-23T10:47:23Z</published>
    <title>Ant Colony Optimization of Rough Set for HV Bushings Fault Detection</title>
    <summary>  Most transformer failures are attributed to bushings failures. Hence it is
necessary to monitor the condition of bushings. In this paper three methods are
developed to monitor the condition of oil filled bushing. Multi-layer
perceptron (MLP), Radial basis function (RBF) and Rough Set (RS) models are
developed and combined through majority voting to form a committee. The MLP
performs better that the RBF and the RS is terms of classification accuracy.
The RBF is the fasted to train. The committee performs better than the
individual models. The diversity of models is measured to evaluate their
similarity when used in the committee.
</summary>
    <author>
      <name>J. L. Mpanza</name>
    </author>
    <author>
      <name>T. Marwala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fourth International Workshop on Advanced Computational Intelligence
  (IWACI 2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.4548v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.4548v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.4618v1</id>
    <updated>2011-08-23T14:44:44Z</updated>
    <published>2011-08-23T14:44:44Z</published>
    <title>Artificial Neural Network and Rough Set for HV Bushings Condition
  Monitoring</title>
    <summary>  Most transformer failures are attributed to bushings failures. Hence it is
necessary to monitor the condition of bushings. In this paper three methods are
developed to monitor the condition of oil filled bushing. Multi-layer
perceptron (MLP), Radial basis function (RBF) and Rough Set (RS) models are
developed and combined through majority voting to form a committee. The MLP
performs better that the RBF and the RS is terms of classification accuracy.
The RBF is the fasted to train. The committee performs better than the
individual models. The diversity of models is measured to evaluate their
similarity when used in the committee.
</summary>
    <author>
      <name>LJ Mpanza</name>
    </author>
    <author>
      <name>T. Marwala</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE INES 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.4618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.4618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.1211v1</id>
    <updated>2011-09-06T15:19:48Z</updated>
    <published>2011-09-06T15:19:48Z</published>
    <title>An Efficient Preprocessing Methodology for Discovering Patterns and
  Clustering of Web Users using a Dynamic ART1 Neural Network</title>
    <summary>  In this paper, a complete preprocessing methodology for discovering patterns
in web usage mining process to improve the quality of data by reducing the
quantity of data has been proposed. A dynamic ART1 neural network clustering
algorithm to group users according to their Web access patterns with its neat
architecture is also proposed. Several experiments are conducted and the
results show the proposed methodology reduces the size of Web log files down to
73-82% of the initial size and the proposed ART1 algorithm is dynamic and
learns relatively stable quality clusters.
</summary>
    <author>
      <name>C. Ramya</name>
    </author>
    <author>
      <name>G. Kavitha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages; International Conference on Information Processing,
  august-2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.1211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.1211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1109.2034v2</id>
    <updated>2013-08-22T12:21:24Z</updated>
    <published>2011-09-09T14:59:59Z</published>
    <title>Learning Sequence Neighbourhood Metrics</title>
    <summary>  Recurrent neural networks (RNNs) in combination with a pooling operator and
the neighbourhood components analysis (NCA) objective function are able to
detect the characterizing dynamics of sequences and embed them into a
fixed-length vector space of arbitrary dimensionality. Subsequently, the
resulting features are meaningful and can be used for visualization or nearest
neighbour classification in linear time. This kind of metric learning for
sequential data enables the use of algorithms tailored towards fixed length
vector spaces such as R^n.
</summary>
    <author>
      <name>Justin Bayer</name>
    </author>
    <author>
      <name>Christian Osendorfer</name>
    </author>
    <author>
      <name>Patrick van der Smagt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Neural Networks and Machine Learning ICANN 2012 Springer
  Berlin Heidelberg 2012. 531-538</arxiv:comment>
    <link href="http://arxiv.org/abs/1109.2034v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1109.2034v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.0477v1</id>
    <updated>2011-10-03T20:03:46Z</updated>
    <published>2011-10-03T20:03:46Z</published>
    <title>Distributed Evolutionary Graph Partitioning</title>
    <summary>  We present a novel distributed evolutionary algorithm, KaFFPaE, to solve the
Graph Partitioning Problem, which makes use of KaFFPa (Karlsruhe Fast Flow
Partitioner). The use of our multilevel graph partitioner KaFFPa provides new
effective crossover and mutation operators. By combining these with a scalable
communication protocol we obtain a system that is able to improve the best
known partitioning results for many inputs in a very short amount of time. For
example, in Walshaw's well known benchmark tables we are able to improve or
recompute 76% of entries for the tables with 1%, 3% and 5% imbalance.
</summary>
    <author>
      <name>Peter Sanders</name>
    </author>
    <author>
      <name>Christian Schulz</name>
    </author>
    <link href="http://arxiv.org/abs/1110.0477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.0477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1110.1038v1</id>
    <updated>2011-10-05T16:52:26Z</updated>
    <published>2011-10-05T16:52:26Z</published>
    <title>Using Genetic Algorithm in the Evolutionary Design of Sequential Logic
  Circuits</title>
    <summary>  Evolvable hardware (EHW) is a set of techniques that are based on the idea of
combining reconfiguration hardware systems with evolutionary algorithms. In
other word, EHW has two sections; the reconfigurable hardware and evolutionary
algorithm where the configurations are under the control of an evolutionary
algorithm. This paper, suggests a method to design and optimize the synchronous
sequential circuits. Genetic algorithm (GA) was applied as evolutionary
algorithm. In this approach, for building input combinational logic circuit of
each DFF, and also output combinational logic circuit, the cell arrays have
been used. The obtained results show that our method can reduce the average
number of generations by limitation the search space.
</summary>
    <author>
      <name>Parisa Soleimani</name>
    </author>
    <author>
      <name>Reza Sabbaghi-Nadooshan</name>
    </author>
    <author>
      <name>Sattar Mirzakuchaki</name>
    </author>
    <author>
      <name>Mahdi Bagheri</name>
    </author>
    <link href="http://arxiv.org/abs/1110.1038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1110.1038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1111.2988v1</id>
    <updated>2011-11-13T04:46:28Z</updated>
    <published>2011-11-13T04:46:28Z</published>
    <title>Application of PSO, Artificial Bee Colony and Bacterial Foraging
  Optimization algorithms to economic load dispatch: An analysis</title>
    <summary>  This paper illustrates successful implementation of three evolutionary
algorithms, namely- Particle Swarm Optimization(PSO), Artificial Bee Colony
(ABC) and Bacterial Foraging Optimization (BFO) algorithms to economic load
dispatch problem (ELD). Power output of each generating unit and optimum fuel
cost obtained using all three algorithms have been compared. The results
obtained show that ABC and BFO algorithms converge to optimal fuel cost with
reduced computational time when compared to PSO for the two example problems
considered.
</summary>
    <author>
      <name>Anant Baijal</name>
    </author>
    <author>
      <name>Vikram Singh Chauhan</name>
    </author>
    <author>
      <name>T. Jayabarathi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 4, No 1, 2011, 467-470</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1111.2988v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1111.2988v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.4737v1</id>
    <updated>2012-01-20T20:30:59Z</updated>
    <published>2012-01-20T20:30:59Z</published>
    <title>Production System Rules as Protein Complexes from Genetic Regulatory
  Networks</title>
    <summary>  This short paper introduces a new way by which to design production system
rules. An indirect encoding scheme is presented which views such rules as
protein complexes produced by the temporal behaviour of an artificial genetic
regulatory network. This initial study begins by using a simple Boolean
regulatory network to produce traditional ternary-encoded rules before moving
to a fuzzy variant to produce real-valued rules. Competitive performance is
shown with related genetic regulatory networks and rule-based systems on
benchmark problems.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <link href="http://arxiv.org/abs/1201.4737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.4737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1201.4908v1</id>
    <updated>2012-01-24T03:16:43Z</updated>
    <published>2012-01-24T03:16:43Z</published>
    <title>Self-Organisation of Evolving Agent Populations in Digital Ecosystems</title>
    <summary>  We investigate the self-organising behaviour of Digital Ecosystems, because a
primary motivation for our research is to exploit the self-organising
properties of biological ecosystems. We extended a definition for the
complexity, grounded in the biological sciences, providing a measure of the
information in an organism's genome. Next, we extended a definition for the
stability, originating from the computer sciences, based upon convergence to an
equilibrium distribution. Finally, we investigated a definition for the
diversity, relative to the selection pressures provided by the user requests.
We conclude with a summary and discussion of the achievements, including the
experimental results.
</summary>
    <author>
      <name>Gerard Briscoe</name>
    </author>
    <author>
      <name>Philippe De Wilde</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">50 pages, 25 figures, journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1201.4908v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1201.4908v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.2249v1</id>
    <updated>2012-02-10T12:57:34Z</updated>
    <published>2012-02-10T12:57:34Z</published>
    <title>Supervised Learning in Multilayer Spiking Neural Networks</title>
    <summary>  The current article introduces a supervised learning algorithm for multilayer
spiking neural networks. The algorithm presented here overcomes some
limitations of existing learning algorithms as it can be applied to neurons
firing multiple spikes and it can in principle be applied to any linearisable
neuron model. The algorithm is applied successfully to various benchmarks, such
as the XOR problem and the Iris data set, as well as complex classifications
problems. The simulations also show the flexibility of this supervised learning
algorithm which permits different encodings of the spike timing patterns,
including precise spike trains encoding.
</summary>
    <author>
      <name>Ioana Sporea</name>
    </author>
    <author>
      <name>André Grüning</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/NECO_a_00396</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/NECO_a_00396" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Compuation February 2013, Vol. 25, No. 2, Pages 473-509</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.2249v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.2249v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.4170v1</id>
    <updated>2012-02-19T16:56:45Z</updated>
    <published>2012-02-19T16:56:45Z</published>
    <title>Classification by Ensembles of Neural Networks</title>
    <summary>  We introduce a new procedure for training of artificial neural networks by
using the approximation of an objective function by arithmetic mean of an
ensemble of selected randomly generated neural networks, and apply this
procedure to the classification (or pattern recognition) problem. This approach
differs from the standard one based on the optimization theory. In particular,
any neural network from the mentioned ensemble may not be an approximation of
the objective function.
</summary>
    <author>
      <name>S. V. Kozyrev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, LaTeX</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">p-Adic Numbers, Ultrametric Analysis and Applications, 2012, Vol.
  4, No. 1, pp. 27-33</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.4170v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.4170v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.5284v3</id>
    <updated>2012-04-02T04:27:11Z</updated>
    <published>2012-02-23T20:21:57Z</published>
    <title>Elitism Levels Traverse Mechanism For The Derivation of Upper Bounds on
  Unimodal Functions</title>
    <summary>  In this article we present an Elitism Levels Traverse Mechanism that we
designed to find bounds on population-based Evolutionary algorithms solving
unimodal functions. We prove its efficiency theoretically and test it on OneMax
function deriving bounds c{\mu}n log n - O({\mu} n). This analysis can be
generalized to any similar algorithm using variants of tournament selection and
genetic operators that flip or swap only 1 bit in each string.
</summary>
    <author>
      <name>Aram Ter-Sarkisov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">accepted to Congress on Evolutionary Computation (WCCI/CEC) 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1202.5284v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.5284v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1202.5953v1</id>
    <updated>2012-02-27T14:36:43Z</updated>
    <published>2012-02-27T14:36:43Z</published>
    <title>On an Ethical Use of Neural Networks: A Case Study on a North Indian
  Raga</title>
    <summary>  The paper gives an artificial neural network (ANN) approach to time series
modeling, the data being instance versus notes (characterized by pitch)
depicting the structure of a North Indian raga, namely, Bageshree. Respecting
the sentiments of the artists' community, the paper argues why it is more
ethical to model a structure than try and "manufacture" an artist by training
the neural network to copy performances of artists. Indian Classical Music
centers on the ragas, where emotion and devotion are both important and neither
can be substituted by such "calculated artistry" which the ANN generated copies
are ultimately up to.
</summary>
    <author>
      <name>Ripunjai Kumar Shukla</name>
    </author>
    <author>
      <name>Soubhik Chakraborty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series VII/2 (2009), 41-56</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1202.5953v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1202.5953v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.0197v2</id>
    <updated>2012-03-06T09:02:14Z</updated>
    <published>2012-03-01T14:25:50Z</published>
    <title>Statistical Approach for Selecting Elite Ants</title>
    <summary>  Applications of ACO algorithms to obtain better solutions for combinatorial
optimization problems have become very popular in recent years. In ACO
algorithms, group of agents repeatedly perform well defined actions and
collaborate with other ants in order to accomplish the defined task. In this
paper, we introduce new mechanisms for selecting the Elite ants dynamically
based on simple statistical tools. We also investigate the performance of newly
proposed mechanisms.
</summary>
    <author>
      <name>G. S. Raghavendra</name>
    </author>
    <author>
      <name>N. Prasanna Kumar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Ann. Univ. Tibiscus Comp. Sci. Series IX/2 (2011), 69-90</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.0197v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.0197v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.3097v1</id>
    <updated>2012-03-14T14:32:45Z</updated>
    <published>2012-03-14T14:32:45Z</published>
    <title>A Comparative Study of Adaptive Crossover Operators for Genetic
  Algorithms to Resolve the Traveling Salesman Problem</title>
    <summary>  Genetic algorithm includes some parameters that should be adjusting so that
the algorithm can provide positive results. Crossover operators play very
important role by constructing competitive Genetic Algorithms (GAs). In this
paper, the basic conceptual features and specific characteristics of various
crossover operators in the context of the Traveling Salesman Problem (TSP) are
discussed. The results of experimental comparison of more than six different
crossover operators for the TSP are presented. The experiment results show that
OX operator enables to achieve a better solutions than other operators tested.
</summary>
    <author>
      <name>Otman Abdoun</name>
    </author>
    <author>
      <name>Jaafar Abouchabaka</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications (0975 - 8887)
  Volume 31 - No.11, October 2011</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.3097v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.3097v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.4416v1</id>
    <updated>2012-03-20T12:59:15Z</updated>
    <published>2012-03-20T12:59:15Z</published>
    <title>On Training Deep Boltzmann Machines</title>
    <summary>  The deep Boltzmann machine (DBM) has been an important development in the
quest for powerful "deep" probabilistic models. To date, simultaneous or joint
training of all layers of the DBM has been largely unsuccessful with existing
training methods. We introduce a simple regularization scheme that encourages
the weight vectors associated with each hidden unit to have similar norms. We
demonstrate that this regularization can be easily combined with standard
stochastic maximum likelihood to yield an effective training strategy for the
simultaneous training of all layers of the deep Boltzmann machine.
</summary>
    <author>
      <name>Guillaume Desjardins</name>
    </author>
    <author>
      <name>Aaron Courville</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1203.4416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.4416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.4881v1</id>
    <updated>2012-03-22T04:22:36Z</updated>
    <published>2012-03-22T04:22:36Z</published>
    <title>Computational Complexity Analysis of Multi-Objective Genetic Programming</title>
    <summary>  The computational complexity analysis of genetic programming (GP) has been
started recently by analyzing simple (1+1) GP algorithms for the problems ORDER
and MAJORITY. In this paper, we study how taking the complexity as an
additional criteria influences the runtime behavior. We consider
generalizations of ORDER and MAJORITY and present a computational complexity
analysis of (1+1) GP using multi-criteria fitness functions that take into
account the original objective and the complexity of a syntax tree as a
secondary measure. Furthermore, we study the expected time until
population-based multi-objective genetic programming algorithms have computed
the Pareto front when taking the complexity of a syntax tree as an equally
important objective.
</summary>
    <author>
      <name>Frank Neumann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A conference version has been accepted for GECCO 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1203.4881v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.4881v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1203.5028v1</id>
    <updated>2012-03-22T16:09:51Z</updated>
    <published>2012-03-22T16:09:51Z</published>
    <title>Hybridizing PSM and RSM Operator for Solving NP-Complete Problems:
  Application to Travelling Salesman Problem</title>
    <summary>  In this paper, we present a new mutation operator, Hybrid Mutation (HPRM),
for a genetic algorithm that generates high quality solutions to the Traveling
Salesman Problem (TSP). The Hybrid Mutation operator constructs an offspring
from a pair of parents by hybridizing two mutation operators, PSM and RSM. The
efficiency of the HPRM is compared as against some existing mutation operators;
namely, Reverse Sequence Mutation (RSM) and Partial Shuffle Mutation (PSM) for
BERLIN52 as instance of TSPLIB. Experimental results show that the new mutation
operator is better than the RSM and PSM.
</summary>
    <author>
      <name>Otman Abdoun</name>
    </author>
    <author>
      <name>Chakir Tajani</name>
    </author>
    <author>
      <name>Jaafar Abouchabka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ISSN (Online): 1694-0814</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 1, No 1, 2012, 374-378</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1203.5028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1203.5028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.0262v2</id>
    <updated>2012-04-26T23:17:34Z</updated>
    <published>2012-04-01T20:17:32Z</published>
    <title>Managing contextual artificial neural networks with a service-based
  mediator</title>
    <summary>  Today, a wide variety of probabilistic and expert AI systems used to analyze
real world inputs such as unstructured text, sounds, images, and statistical
data. However, all these systems exist on different platforms, with different
implementations, and with very different, often very specific goals in mind.
This paper introduces a concept for a mediator framework for such systems and
seeks to show several architectures which would support it, potential benefits
in combining the signals of disparate networks for formalized, high level logic
and signal processing, and its possible academic and industrial uses.
</summary>
    <author>
      <name>Greg Fish</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.0262v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.0262v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="97R40" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.6.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2321v6</id>
    <updated>2013-08-30T22:11:40Z</updated>
    <published>2012-04-11T02:25:34Z</published>
    <title>Derivation of Upper Bounds on Optimization Time of Population-Based
  Evolutionary Algorithm on a Function with Fitness Plateaus Using Elitism
  Levels Traverse Mechanism</title>
    <summary>  In this article a tool for the analysis of population-based EAs is used to
derive asymptotic upper bounds on the optimization time of the algorithm
solving Royal Roads problem, a test function with plateaus of fitness. In
addition to this, limiting distribution of a certain subset of the population
is approximated.
</summary>
    <author>
      <name>Aram Ter-Sarkisov</name>
    </author>
    <author>
      <name>Stephen Marsland</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper will be replaced by a new version with a different title</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.2321v6" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2321v6" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1204.2601v1</id>
    <updated>2012-04-12T01:45:21Z</updated>
    <published>2012-04-12T01:45:21Z</published>
    <title>Detecting lateral genetic material transfer</title>
    <summary>  The bioinformatical methods to detect lateral gene transfer events are mainly
based on functional coding DNA characteristics. In this paper, we propose the
use of DNA traits not depending on protein coding requirements. We introduce
several semilocal variables that depend on DNA primary sequence and that
reflect thermodynamic as well as physico-chemical magnitudes that are able to
tell apart the genome of different organisms. After combining these variables
in a neural classificator, we obtain results whose power of resolution go as
far as to detect the exchange of genomic material between bacteria that are
phylogenetically close.
</summary>
    <author>
      <name>C. Calderón</name>
    </author>
    <author>
      <name>L. Delaye</name>
    </author>
    <author>
      <name>V. Mireles</name>
    </author>
    <author>
      <name>P. Miramontes</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Applied Computational Intelligence and Soft Computing</arxiv:comment>
    <link href="http://arxiv.org/abs/1204.2601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1204.2601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1207.4632v1</id>
    <updated>2012-07-19T12:08:43Z</updated>
    <published>2012-07-19T12:08:43Z</published>
    <title>Clustering of Local Optima in Combinatorial Fitness Landscapes</title>
    <summary>  Using the recently proposed model of combinatorial landscapes: local optima
networks, we study the distribution of local optima in two classes of instances
of the quadratic assignment problem. Our results indicate that the two problem
instance classes give rise to very different configuration spaces. For the
so-called real-like class, the optima networks possess a clear modular
structure, while the networks belonging to the class of random uniform
instances are less well partitionable into clusters. We briefly discuss the
consequences of the findings for heuristically searching the corresponding
problem spaces.
</summary>
    <author>
      <name>Gabriela Ochoa</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lille - Nord Europe</arxiv:affiliation>
    </author>
    <author>
      <name>Sébastien Verel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lille - Nord Europe</arxiv:affiliation>
    </author>
    <author>
      <name>Fabio Daolio</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISI</arxiv:affiliation>
    </author>
    <author>
      <name>Marco Tomassini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISI</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-25566-3_35</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-25566-3_35" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Learning and Intelligent OptimizatioN Conference (LION 5), Rome :
  Italy (2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1207.4632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1207.4632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.6028v1</id>
    <updated>2012-08-13T11:32:03Z</updated>
    <published>2012-08-13T11:32:03Z</published>
    <title>Design of Low Noise Amplifiers Using Particle Swarm Optimization</title>
    <summary>  This short paper presents a work on the design of low noise microwave
amplifiers using particle swarm optimization (PSO) technique. Particle Swarm
Optimization is used as a method that is applied to a single stage amplifier
circuit to meet two criteria: desired gain and desired low noise. The aim is to
get the best optimized design using the predefined constraints for gain and low
noise values. The code is written to apply the algorithm to meet the desired
goals and the obtained results are verified using different simulators. The
results obtained show that PSO can be applied very efficiently for this kind of
design problems with multiple constraints.
</summary>
    <author>
      <name>Sadik Ulker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Artificial Intelligence &amp; Applications
  vol 3, no 4, July 2012, 99-106</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1208.6028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.6028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.2717v1</id>
    <updated>2012-09-12T20:17:05Z</updated>
    <published>2012-09-12T20:17:05Z</published>
    <title>Comparison Study for Clonal Selection Algorithm and Genetic Algorithm</title>
    <summary>  Two metaheuristic algorithms namely Artificial Immune Systems (AIS) and
Genetic Algorithms are classified as computational systems inspired by
theoretical immunology and genetics mechanisms. In this work we examine the
comparative performances of two algorithms. A special selection algorithm,
Clonal Selection Algorithm (CLONALG), which is a subset of Artificial Immune
Systems, and Genetic Algorithms are tested with certain benchmark functions. It
is shown that depending on type of a function Clonal Selection Algorithm and
Genetic Algorithm have better performance over each other.
</summary>
    <author>
      <name>Ezgi Deniz Ulker</name>
    </author>
    <author>
      <name>Sadik Ulker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijcsit.2012.4410</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijcsit.2012.4410" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 12 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 4, No 4, August 2012 pp 107-118</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1209.2717v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.2717v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1209.5339v1</id>
    <updated>2012-09-24T17:26:29Z</updated>
    <published>2012-09-24T17:26:29Z</published>
    <title>Developing Improved Greedy Crossover to Solve Symmetric Traveling
  Salesman Problem</title>
    <summary>  The Traveling Salesman Problem (TSP) is one of the most famous optimization
problems. Greedy crossover designed by Greffenstette et al, can be used while
Symmetric TSP (STSP) is resolved by Genetic Algorithm (GA). Researchers have
proposed several versions of greedy crossover. Here we propose improved version
of it. We compare our greedy crossover with some of recent crossovers, we use
our greedy crossover and some recent crossovers in GA then compare crossovers
on speed and accuracy.
</summary>
    <author>
      <name>Hassan Ismkhan</name>
    </author>
    <author>
      <name>Kamran Zamanifar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science Issues, Volume 9, Issue 4,
  No 3, July 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1209.5339v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1209.5339v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.0660v1</id>
    <updated>2012-11-04T04:49:24Z</updated>
    <published>2012-11-04T04:49:24Z</published>
    <title>Generation of Two-Layer Monotonic Functions</title>
    <summary>  The problem of implementing a class of functions with particular conditions
by using monotonic multilayer functions is considered. A genetic algorithm is
used to create monotonic functions of a certain class, and these are
implemented with two-layer monotonic functions. The existence of a solution to
the given problem suggests that from two monotone functions, a monotonic
function with the same dimensions can be created. A new algorithm based on the
genetic algorithm is proposed, which easily implemented two-layer monotonic
functions of a specific class for up to six variables.
</summary>
    <author>
      <name>Yukihiro Kamada</name>
    </author>
    <author>
      <name>Kiyonori Miyasaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.0660v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.0660v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.2361v2</id>
    <updated>2013-03-22T06:09:29Z</updated>
    <published>2012-11-11T00:26:22Z</published>
    <title>Genetic Algorithm for Designing a Convenient Facility Layout for a
  Circular Flow Path</title>
    <summary>  In this paper, we present a heuristic for designing facility layouts that are
convenient for designing a unidirectional loop for material handling. We use
genetic algorithm where the objective function and crossover and mutation
operators have all been designed specifically for this purpose. Our design is
made under flexible bay structure and comparisons are made with other layouts
from the literature that were designed under flexible bay structure.
</summary>
    <author>
      <name>Hossein Jahandideh</name>
    </author>
    <author>
      <name>Ardavan Asef-Vaziri</name>
    </author>
    <author>
      <name>Mohammad Modarres</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to the 2013 IEEE Symposium Series on Computational
  Intelligence: Swarm Intelligence Symposium. This paper has been withdrawn by
  the author, by the request of the supervisor, to be updated, fixed, and
  combined with other papers</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.2361v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.2361v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.3451v1</id>
    <updated>2012-11-14T22:28:56Z</updated>
    <published>2012-11-14T22:28:56Z</published>
    <title>Memory Capacity of a Random Neural Network</title>
    <summary>  This paper considers the problem of information capacity of a random neural
network. The network is represented by matrices that are square and
symmetrical. The matrices have a weight which determines the highest and lowest
possible value found in the matrix. The examined matrices are randomly
generated and analyzed by a computer program. We find the surprising result
that the capacity of the network is a maximum for the binary random neural
network and it does not change as the number of quantization levels associated
with the weights increases.
</summary>
    <author>
      <name>Matt Stowe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.3451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.3451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.5098v1</id>
    <updated>2012-11-21T17:32:52Z</updated>
    <published>2012-11-21T17:32:52Z</published>
    <title>Scaling Genetic Programming for Source Code Modification</title>
    <summary>  In Search Based Software Engineering, Genetic Programming has been used for
bug fixing, performance improvement and parallelisation of programs through the
modification of source code. Where an evolutionary computation algorithm, such
as Genetic Programming, is to be applied to similar code manipulation tasks,
the complexity and size of source code for real-world software poses a
scalability problem. To address this, we intend to inspect how the Software
Engineering concepts of modularity, granularity and localisation of change can
be reformulated as additional mechanisms within a Genetic Programming
algorithm.
</summary>
    <author>
      <name>Brendan Cody-Kenny</name>
    </author>
    <author>
      <name>Stephen Barrett</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Accepted for Graduate Student Workshop, GECCO 2012,
  Retracted by Authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1211.5098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.5098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.6410v1</id>
    <updated>2012-11-24T01:30:36Z</updated>
    <published>2012-11-24T01:30:36Z</published>
    <title>New Hoopoe Heuristic Optimization</title>
    <summary>  Most optimization problems in real life applications are often highly
nonlinear. Local optimization algorithms do not give the desired performance.
So, only global optimization algorithms should be used to obtain optimal
solutions. This paper introduces a new nature-inspired metaheuristic
optimization algorithm, called Hoopoe Heuristic (HH). In this paper, we will
study HH and validate it against some test functions. Investigations show that
it is very promising and could be seen as an optimization of the powerful
algorithm of cuckoo search. Finally, we discuss the features of Hoopoe
Heuristic and propose topics for further studies.
</summary>
    <author>
      <name>Mohammed El-Dosuky</name>
    </author>
    <author>
      <name>Ahmed EL-Bassiouny</name>
    </author>
    <author>
      <name>Taher Hamza</name>
    </author>
    <author>
      <name>Magdy Rashad</name>
    </author>
    <link href="http://arxiv.org/abs/1211.6410v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.6410v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1211.7184v1</id>
    <updated>2012-11-30T08:49:46Z</updated>
    <published>2012-11-30T08:49:46Z</published>
    <title>Erratum: Simplified Drift Analysis for Proving Lower Bounds in
  Evolutionary Computation</title>
    <summary>  This erratum points out an error in the simplified drift theorem (SDT)
[Algorithmica 59(3), 369-386, 2011]. It is also shown that a minor modification
of one of its conditions is sufficient to establish a valid result. In many
respects, the new theorem is more general than before. We no longer assume a
Markov process nor a finite search space. Furthermore, the proof of the theorem
is more compact than the previous ones. Finally, previous applications of the
SDT are revisited. It turns out that all of these either meet the modified
condition directly or by means of few additional arguments.
</summary>
    <author>
      <name>Pietro S. Oliveto</name>
    </author>
    <author>
      <name>Carsten Witt</name>
    </author>
    <link href="http://arxiv.org/abs/1211.7184v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1211.7184v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1212.2529v1</id>
    <updated>2012-12-11T16:40:45Z</updated>
    <published>2012-12-11T16:40:45Z</published>
    <title>On The Delays In Spiking Neural P Systems</title>
    <summary>  In this work we extend and improve the results done in a previous work on
simulating Spiking Neural P systems (SNP systems in short) with delays using
SNP systems without delays. We simulate the former with the latter over
sequential, iteration, join, and split routing. Our results provide
constructions so that both systems halt at exactly the same time, start with
only one spike, and produce the same number of spikes to the environment after
halting.
</summary>
    <author>
      <name>Francis George C. Cabarle</name>
    </author>
    <author>
      <name>Kelvin C. Buño</name>
    </author>
    <author>
      <name>Henry N. Adorna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 6th Symposium on the Mathematical Aspects of
  Computer Science (SMACS2012), Boracay, Philippines. 6 figures, 6 pages, 2
  columns</arxiv:comment>
    <link href="http://arxiv.org/abs/1212.2529v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1212.2529v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="97P20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.5374v4</id>
    <updated>2015-04-01T05:59:58Z</updated>
    <published>2013-02-21T18:57:09Z</published>
    <title>A Weight-coded Evolutionary Algorithm for the Multidimensional Knapsack
  Problem</title>
    <summary>  A revised weight-coded evolutionary algorithm (RWCEA) is proposed for solving
multidimensional knapsack problems. This RWCEA uses a new decoding method and
incorporates a heuristic method in initialization. Computational results show
that the RWCEA performs better than a weight-coded evolutionary algorithm
proposed by Raidl (1999) and to some existing benchmarks, it can yield better
results than the ones reported in the OR-library.
</summary>
    <author>
      <name>Quan Yuan</name>
    </author>
    <author>
      <name>Zhixin Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Applied Mathematics and Computation on April 8, 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1302.5374v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.5374v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90B50" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.2096v1</id>
    <updated>2013-03-08T20:16:02Z</updated>
    <published>2013-03-08T20:16:02Z</published>
    <title>Gene-Machine, a new search heuristic algorithm</title>
    <summary>  This paper introduces Gene-Machine, an efficient and new search heuristic
algorithm, based in the building-block hypothesis. It is inspired by natural
evolution, but does not use some of the concepts present in genetic algorithms
like population, mutation and generation. This heuristic exhibits good
performance in comparison with genetic algorithms, and can be used to generate
useful solutions to optimization and search problems.
</summary>
    <author>
      <name>Alfredo Garcia Woods</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">GeneMachine uses the chromosome notion, genes and evolution, but it
  differs from genetic algorithms, in that it does not use mutation, nor
  population of individuals, neither the notion of generation</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.2096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.2096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1303.6310v3</id>
    <updated>2013-06-05T15:46:53Z</updated>
    <published>2013-03-25T20:53:09Z</published>
    <title>A hybrid bat algorithm</title>
    <summary>  Swarm intelligence is a very powerful technique to be used for optimization
purposes. In this paper we present a new swarm intelligence algorithm, based on
the bat algorithm. The Bat algorithm is hybridized with differential evolution
strategies. Besides showing very promising results of the standard benchmark
functions, this hybridization also significantly improves the original bat
algorithm.
</summary>
    <author>
      <name>Iztok Fister Jr.</name>
    </author>
    <author>
      <name>Dušan Fister</name>
    </author>
    <author>
      <name>Xin-She Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Electrotechnical review, 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1303.6310v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1303.6310v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3610v1</id>
    <updated>2013-04-12T11:54:35Z</updated>
    <published>2013-04-12T11:54:35Z</published>
    <title>Modified Soft Brood Crossover in Genetic Programming</title>
    <summary>  Premature convergence is one of the important issues while using Genetic
Programming for data modeling. It can be avoided by improving population
diversity. Intelligent genetic operators can help to improve the population
diversity. Crossover is an important operator in Genetic Programming. So, we
have analyzed number of intelligent crossover operators and proposed an
algorithm with the modification of soft brood crossover operator. It will help
to improve the population diversity and reduce the premature convergence. We
have performed experiments on three different symbolic regression problems.
Then we made the performance comparison of our proposed crossover (Modified
Soft Brood Crossover) with the existing soft brood crossover and subtree
crossover operators.
</summary>
    <author>
      <name>Hardik M. Parekh</name>
    </author>
    <author>
      <name>Vipul K. Dabhi</name>
    </author>
    <link href="http://arxiv.org/abs/1304.3610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3612v1</id>
    <updated>2013-04-12T12:08:07Z</updated>
    <published>2013-04-12T12:08:07Z</published>
    <title>A Novel Metaheuristics To Solve Mixed Shop Scheduling Problems</title>
    <summary>  This paper represents the metaheuristics proposed for solving a class of Shop
Scheduling problem. The Bacterial Foraging Optimization algorithm is featured
with Ant Colony Optimization algorithm and proposed as a natural inspired
computing approach to solve the Mixed Shop Scheduling problem. The Mixed Shop
is the combination of Job Shop, Flow Shop and Open Shop scheduling problems.
The sample instances for all mentioned Shop problems are used as test data and
Mixed Shop survive its computational complexity to minimize the makespan. The
computational results show that the proposed algorithm is gentler to solve and
performs better than the existing algorithms.
</summary>
    <author>
      <name>V. Ravibabu</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal in Foundations of Computer Science &amp;
  Technology, March2013, Volume 3, Number 2, ISSN : 1839-7662</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1304.3612v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3612v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1304.3892v1</id>
    <updated>2013-04-14T08:56:10Z</updated>
    <published>2013-04-14T08:56:10Z</published>
    <title>An accelerated CLPSO algorithm</title>
    <summary>  The particle swarm approach provides a low complexity solution to the
optimization problem among various existing heuristic algorithms. Recent
advances in the algorithm resulted in improved performance at the cost of
increased computational complexity, which is undesirable. Literature shows that
the particle swarm optimization algorithm based on comprehensive learning
provides the best complexity-performance trade-off. We show how to reduce the
complexity of this algorithm further, with a slight but acceptable performance
loss. This enhancement allows the application of the algorithm in time critical
applications, such as, real-time tracking, equalization etc.
</summary>
    <author>
      <name>Muhammad Omer Bin Saeed</name>
    </author>
    <author>
      <name>Muhammad Saqib Sohail</name>
    </author>
    <author>
      <name>Syed Zeeshan Rizvi</name>
    </author>
    <author>
      <name>Mobien Shoaib</name>
    </author>
    <author>
      <name>Asrar Ul Haq Sheikh</name>
    </author>
    <link href="http://arxiv.org/abs/1304.3892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1304.3892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.2490v2</id>
    <updated>2014-04-15T09:23:36Z</updated>
    <published>2013-05-11T09:57:15Z</published>
    <title>Combining Drift Analysis and Generalized Schema Theory to Design
  Efficient Hybrid and/or Mixed Strategy EAs</title>
    <summary>  Hybrid and mixed strategy EAs have become rather popular for tackling various
complex and NP-hard optimization problems. While empirical evidence suggests
that such algorithms are successful in practice, rather little theoretical
support for their success is available, not mentioning a solid mathematical
foundation that would provide guidance towards an efficient design of this type
of EAs. In the current paper we develop a rigorous mathematical framework that
suggests such designs based on generalized schema theory, fitness levels and
drift analysis. An example-application for tackling one of the classical
NP-hard problems, the "single-machine scheduling problem" is presented.
</summary>
    <author>
      <name>Boris Mitavskiy</name>
    </author>
    <author>
      <name>Jun He</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CEC.2013.6557808</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CEC.2013.6557808" rel="related"/>
    <link href="http://arxiv.org/abs/1305.2490v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.2490v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1305.6537v1</id>
    <updated>2013-05-28T15:42:51Z</updated>
    <published>2013-05-28T15:42:51Z</published>
    <title>A Cooperative Coevolutionary Genetic Algorithm for Learning Bayesian
  Network Structures</title>
    <summary>  We propose a cooperative coevolutionary genetic algorithm for learning
Bayesian network structures from fully observable data sets. Since this problem
can be decomposed into two dependent subproblems, that is to find an ordering
of the nodes and an optimal connectivity matrix, our algorithm uses two
subpopulations, each one representing a subtask. We describe the empirical
results obtained with simulations of the Alarm and Insurance networks. We show
that our algorithm outperforms the deterministic algorithm K2.
</summary>
    <author>
      <name>Arthur Carvalho</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2001576.2001729</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2001576.2001729" rel="related"/>
    <link href="http://arxiv.org/abs/1305.6537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1305.6537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.1372v2</id>
    <updated>2013-08-19T09:11:13Z</updated>
    <published>2013-07-04T15:22:35Z</published>
    <title>Clustering of Complex Networks and Community Detection Using Group
  Search Optimization</title>
    <summary>  Group Search Optimizer(GSO) is one of the best algorithms, is very new in the
field of Evolutionary Computing. It is very robust and efficient algorithm,
which is inspired by animal searching behaviour. The paper describes an
application of GSO to clustering of networks. We have tested GSO against five
standard benchmark datasets, GSO algorithm is proved very competitive in terms
of accuracy and convergence speed.
</summary>
    <author>
      <name>G. Kishore Kumar</name>
    </author>
    <author>
      <name>V. K. Jayaraman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.1372v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.1372v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3463v4</id>
    <updated>2014-03-28T02:19:00Z</updated>
    <published>2013-07-12T14:07:09Z</published>
    <title>Non-Elitist Genetic Algorithm as a Local Search Method</title>
    <summary>  Sufficient conditions are found under which the iterated non-elitist genetic
algorithm with tournament selection first visits a local optimum in
polynomially bounded time on average. It is shown that these conditions are
satisfied on a class of problems with guaranteed local optima (GLO) if
appropriate parameters of the algorithm are chosen.
</summary>
    <author>
      <name>Anton Eremeev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract of the talk presented at Dagstuhl Seminar "Theory
  of Evolutionary Algorithms" (Dagstuhl, Germany, 30 June - 5 July 2013)</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.3463v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3463v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.3782v3</id>
    <updated>2016-04-22T18:45:01Z</updated>
    <published>2013-07-14T21:03:39Z</published>
    <title>Handwritten Digits Recognition using Deep Convolutional Neural Network:
  An Experimental Study using EBlearn</title>
    <summary>  In this paper, results of an experimental study of a deep convolution neural
network architecture which can classify different handwritten digits using
EBLearn library are reported. The purpose of this neural network is to classify
input images into 10 different classes or digits (0-9) and to explore new
findings. The input dataset used consists of digits images of size 32X32 in
grayscale (MNIST dataset).
</summary>
    <author>
      <name>Karim M. Mahmoud</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to some errors and
  incomplete study</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.3782v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.3782v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.4274v1</id>
    <updated>2013-07-16T13:45:24Z</updated>
    <published>2013-07-16T13:45:24Z</published>
    <title>The Fitness Level Method with Tail Bounds</title>
    <summary>  The fitness-level method, also called the method of f-based partitions, is an
intuitive and widely used technique for the running time analysis of randomized
search heuristics. It was originally defined to prove upper and lower bounds on
the expected running time. Recently, upper tail bounds were added to the
technique; however, these tail bounds only apply to running times that are at
least twice as large as the expectation.
  We remove this restriction and supplement the fitness-level method with sharp
tail bounds, including lower tails. As an exemplary application, we prove that
the running time of randomized local search on OneMax is sharply concentrated
around n ln n - 0.1159 n.
</summary>
    <author>
      <name>Carsten Witt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.4274v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.4274v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.5534v1</id>
    <updated>2013-07-21T15:34:24Z</updated>
    <published>2013-07-21T15:34:24Z</published>
    <title>A New Optimization Approach Based on Rotational Mutation and Crossover
  Operator</title>
    <summary>  Evaluating a global optimal point in many global optimization problems in
large space is required to more calculations. In this paper, there is presented
a new approach for the continuous functions optimization with rotational
mutation and crossover operator. This proposed method (RMC) starts from the
point which has best fitness value by elitism mechanism and after that
rotational mutation and crossover operator are used to reach optimal point. RMC
method is implemented by GA (Briefly RMCGA) and is compared with other
wellknown algorithms such as: DE, PGA, Grefensstette and Eshelman[15,16] and
numerical and simulating results show that RMCGA achieve global optimal point
with more decision by smaller generations.
</summary>
    <author>
      <name>Masoumeh Vali</name>
    </author>
    <link href="http://arxiv.org/abs/1307.5534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.5534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.5679v1</id>
    <updated>2013-07-22T12:25:21Z</updated>
    <published>2013-07-22T12:25:21Z</published>
    <title>Sub-Dividing Genetic Method for Optimization Problems</title>
    <summary>  Nowadays, optimization problem have more application in all major but they
have problem in computation. Computation global point in continuous functions
have high calculation and this became clearer in large space .In this paper, we
proposed Sub- Dividing Genetic Method(SGM) that have less computation than
other method for achieving global points . This method userotation mutation and
crossover based sub-division method that sub diving method is used for minimize
search space and rotation mutation with crossover is used for finding global
optimal points. In experimental, SGM algorithm is implemented on De Jong
function. The numerical examples show that SGM is performed more optimal than
other methods such as Grefensstette, Random Value, and PNG.
</summary>
    <author>
      <name>Masoumeh Vali</name>
    </author>
    <link href="http://arxiv.org/abs/1307.5679v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.5679v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1307.8104v1</id>
    <updated>2013-07-30T19:51:12Z</updated>
    <published>2013-07-30T19:51:12Z</published>
    <title>Neural Network Capacity for Multilevel Inputs</title>
    <summary>  This paper examines the memory capacity of generalized neural networks.
Hopfield networks trained with a variety of learning techniques are
investigated for their capacity both for binary and non-binary alphabets. It is
shown that the capacity can be much increased when multilevel inputs are used.
New learning strategies are proposed to increase Hopfield network capacity, and
the scalability of these methods is also examined in respect to size of the
network. The ability to recall entire patterns from stimulation of a single
neuron is examined for the increased capacity networks.
</summary>
    <author>
      <name>Matt Stowe</name>
    </author>
    <author>
      <name>Subhash Kak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages,17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1307.8104v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1307.8104v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.1603v4</id>
    <updated>2018-02-01T07:45:48Z</updated>
    <published>2013-08-07T15:29:09Z</published>
    <title>A Note on Topology Preservation in Classification, and the Construction
  of a Universal Neuron Grid</title>
    <summary>  It will be shown that according to theorems of K. Menger, every neuron grid
if identified with a curve is able to preserve the adopted qualitative
structure of a data space. Furthermore, if this identification is made, the
neuron grid structure can always be mapped to a subset of a universal neuron
grid which is constructable in three space dimensions. Conclusions will be
drawn for established neuron grid types as well as neural fields.
</summary>
    <author>
      <name>Dietmar Volz</name>
    </author>
    <link href="http://arxiv.org/abs/1308.1603v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.1603v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92F99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1308.3400v1</id>
    <updated>2013-08-14T18:05:13Z</updated>
    <published>2013-08-14T18:05:13Z</published>
    <title>Guiding Designs of Self-Organizing Swarms: Interactive and Automated
  Approaches</title>
    <summary>  Self-organization of heterogeneous particle swarms is rich in its dynamics
but hard to design in a traditional top-down manner, especially when many types
of kinetically distinct particles are involved. In this chapter, we discuss how
we have been addressing this problem by (1) utilizing and enhancing interactive
evolutionary design methods and (2) realizing spontaneous evolution of self
organizing swarms within an artificial ecosystem.
</summary>
    <author>
      <name>Hiroki Sayama</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 16 figures, 3 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Guided Self-Organization: Inception, Springer, 2014, pp.365-387</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1308.3400v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1308.3400v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1309.2183v1</id>
    <updated>2013-09-09T15:03:59Z</updated>
    <published>2013-09-09T15:03:59Z</published>
    <title>Application of Artificial Neural Networks in Estimating Participation in
  Elections</title>
    <summary>  It is approved that artificial neural networks can be considerable effective
in anticipating and analyzing flows in which traditional methods and statics
are not able to solve. in this article, by using two-layer feedforward network
with tan-sigmoid transmission function in input and output layers, we can
anticipate participation rate of public in kohgiloye and boyerahmad province in
future presidential election of islamic republic of iran with 91% accuracy. the
assessment standards of participation such as confusion matrix and roc diagrams
have been approved our claims.
</summary>
    <author>
      <name>Seyyed Reza Khaze</name>
    </author>
    <author>
      <name>Mohammad Masdari</name>
    </author>
    <author>
      <name>Sohrab Hojjatkhah</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijitmc.2013.1303</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijitmc.2013.1303" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Information Technology, Modeling and
  Computing (IJITMC) Vol.1, No.3,August 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1309.2183v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1309.2183v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1311.1090v1</id>
    <updated>2013-11-05T15:33:30Z</updated>
    <published>2013-11-05T15:33:30Z</published>
    <title>Polyhedrons and Perceptrons Are Functionally Equivalent</title>
    <summary>  Mathematical definitions of polyhedrons and perceptron networks are
discussed. The formalization of polyhedrons is done in a rather traditional
way. For networks, previously proposed systems are developed. Perceptron
networks in disjunctive normal form (DNF) and conjunctive normal forms (CNF)
are introduced. The main theme is that single output perceptron neural networks
and characteristic functions of polyhedrons are one and the same class of
functions. A rigorous formulation and proof that three layers suffice is
obtained. The various constructions and results are among several steps
required for algorithms that replace incremental and statistical learning with
more efficient, direct and exact geometric methods for calculation of
perceptron architecture and weights.
</summary>
    <author>
      <name>Daniel Crespin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 0 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1311.1090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1311.1090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5548v1</id>
    <updated>2013-12-19T13:45:45Z</updated>
    <published>2013-12-19T13:45:45Z</published>
    <title>My First Deep Learning System of 1991 + Deep Learning Timeline 1962-2013</title>
    <summary>  Deep Learning has attracted significant attention in recent years. Here I
present a brief overview of my first Deep Learner of 1991, and its historic
context, with a timeline of Deep Learning highlights.
</summary>
    <author>
      <name>Jürgen Schmidhuber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages. As a machine learning researcher I am obsessed with proper
  credit assignment. This draft is the result of an experiment in rapid massive
  open online peer review. Since 20 September 2013, subsequent revisions
  published under http://www.deeplearning.me have absorbed many suggestions for
  improvements by experts</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.5548v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5548v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.0523v1</id>
    <updated>2014-01-02T20:15:00Z</updated>
    <published>2014-01-02T20:15:00Z</published>
    <title>Solving Poisson Equation by Genetic Algorithms</title>
    <summary>  This paper deals with a method for solving Poisson Equation (PE) based on
genetic algorithms and grammatical evolution. The method forms generations of
solutions expressed in an analytical form. Several examples of PE are tested
and in most cases the exact solution is recovered. But, when the solution
cannot be expressed in an analytical form, our method produces a satisfactory
solution with a good level of accuracy
</summary>
    <author>
      <name>Khalid Jebari</name>
    </author>
    <author>
      <name>Mohammed Madiafi</name>
    </author>
    <author>
      <name>Abdelaziz El Moujahid</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications Volume 83, No 5,
  December 2013</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1401.0523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.0523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.0858v1</id>
    <updated>2014-01-05T01:37:45Z</updated>
    <published>2014-01-05T01:37:45Z</published>
    <title>Multimodal Optimization by Sparkling Squid Populations</title>
    <summary>  The swarm intelligence of animals is a natural paradigm to apply to
optimization problems. Ant colony, bee colony, firefly and bat algorithms are
amongst those that have been demonstrated to efficiently to optimize complex
constraints. This paper proposes the new Sparkling Squid Algorithm (SSA) for
multimodal optimization, inspired by the intelligent swarm behavior of its
namesake. After an introduction, formulation and discussion of its
implementation, it will be compared to other popular metaheuristics. Finally,
applications to well - known problems such as image registration and the
traveling salesperson problem will be discussed.
</summary>
    <author>
      <name>Videh Seksaria</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 4 figues</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.0858v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.0858v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.2468v1</id>
    <updated>2014-01-10T21:09:36Z</updated>
    <published>2014-01-10T21:09:36Z</published>
    <title>N2Sky - Neural Networks as Services in the Clouds</title>
    <summary>  We present the N2Sky system, which provides a framework for the exchange of
neural network specific knowledge, as neural network paradigms and objects, by
a virtual organization environment. It follows the sky computing paradigm
delivering ample resources by the usage of federated Clouds. N2Sky is a novel
Cloud-based neural network simulation environment, which follows a pure service
oriented approach. The system implements a transparent environment aiming to
enable both novice and experienced users to do neural network research easily
and comfortably. N2Sky is built using the RAVO reference architecture of
virtual organizations which allows itself naturally integrating into the Cloud
service stack (SaaS, PaaS, and IaaS) of service oriented architectures.
</summary>
    <author>
      <name>Erich Schikuta</name>
    </author>
    <author>
      <name>Erwin Mann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">extended version of paper published at IJCNN 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.2468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.2468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.5; I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.2949v1</id>
    <updated>2014-01-10T12:46:56Z</updated>
    <published>2014-01-10T12:46:56Z</published>
    <title>Exploiting generalisation symmetries in accuracy-based learning
  classifier systems: An initial study</title>
    <summary>  Modern learning classifier systems typically exploit a niched genetic
algorithm to facilitate rule discovery. When used for reinforcement learning,
such rules represent generalisations over the state-action-reward space. Whilst
encouraging maximal generality, the niching can potentially hinder the
formation of generalisations in the state space which are symmetrical, or very
similar, over different actions. This paper introduces the use of rules which
contain multiple actions, maintaining accuracy and reward metrics for each
action. It is shown that problem symmetries can be exploited, improving
performance, whilst not degrading performance when symmetries are reduced.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 13 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.2949v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.2949v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.3607v2</id>
    <updated>2014-02-07T11:55:12Z</updated>
    <published>2014-01-15T14:37:48Z</published>
    <title>A Brief History of Learning Classifier Systems: From CS-1 to XCS</title>
    <summary>  Modern Learning Classifier Systems can be characterized by their use of rule
accuracy as the utility metric for the search algorithm(s) discovering useful
rules. Such searching typically takes place within the restricted space of
co-active rules for efficiency. This paper gives an historical overview of the
evolution of such systems up to XCS, and then some of the subsequent
developments of XCS to different types of learning.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">37 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1401.3607v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.3607v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.4674v1</id>
    <updated>2014-01-19T14:46:09Z</updated>
    <published>2014-01-19T14:46:09Z</published>
    <title>Evolving Accuracy: A Genetic Algorithm to Improve Election Night
  Forecasts</title>
    <summary>  In this paper, we apply genetic algorithms to the field of electoral studies.
Forecasting election results is one of the most exciting and demanding tasks in
the area of market research, especially due to the fact that decisions have to
be made within seconds on live television. We show that the proposed method
outperforms currently applied approaches and thereby provide an argument to
tighten the intersection between computer science and social science,
especially political science, further. We scrutinize the performance of our
algorithm's runtime behavior to evaluate its applicability in the field.
Numerical results with real data from a local election in the Austrian province
of Styria from 2010 substantiate the applicability of the proposed approach.
</summary>
    <author>
      <name>Ronald Hochreiter</name>
    </author>
    <author>
      <name>Christoph Waldhauser</name>
    </author>
    <link href="http://arxiv.org/abs/1401.4674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.4674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1401.5246v1</id>
    <updated>2014-01-21T10:01:27Z</updated>
    <published>2014-01-21T10:01:27Z</published>
    <title>Genetic Algorithms and its use with back-propagation network</title>
    <summary>  Genetic algorithms are considered as one of the most efficient search
techniques. Although they do not offer an optimal solution, their ability to
reach a suitable solution in considerably short time gives them their
respectable role in many AI techniques. This work introduces genetic algorithms
and describes their characteristics. Then a novel method using genetic
algorithm in best training set generation and selection for a back-propagation
network is proposed. This work also offers a new extension to the original
genetic algorithms
</summary>
    <author>
      <name>Ayman M. Bahaa-Eldin</name>
    </author>
    <author>
      <name>A. M. A. Wahdan</name>
    </author>
    <author>
      <name>H. M. K. Mahdi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">AIN Shams University, Faculty of Engineering Scientific Bulletin,
  Volume 35, Issue 3, pp 337-348 (2000)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1401.5246v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1401.5246v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.0708v1</id>
    <updated>2014-02-04T12:25:31Z</updated>
    <published>2014-02-04T12:25:31Z</published>
    <title>Microstrip Coupler Design Using Bat Algorithm</title>
    <summary>  Evolutionary and swarm algorithms have found many applications in design
problems since todays computing power enables these algorithms to find
solutions to complicated design problems very fast. Newly proposed hybrid
algorithm, bat algorithm, has been applied for the design of microwave
microstrip couplers for the first time. Simulation results indicate that the
bat algorithm is a very fast algorithm and it produces very reliable results.
</summary>
    <author>
      <name>Ezgi Deniz Ulker</name>
    </author>
    <author>
      <name>Sadik Ulker</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijaia.2014.5110</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijaia.2014.5110" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 4 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Artificial Intelligence &amp; Applications
  (IJAIA), vol. 5, no. 1, January 2014, pp. 127-133</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.0708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.0708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.5428v1</id>
    <updated>2014-02-21T21:18:58Z</updated>
    <published>2014-02-21T21:18:58Z</published>
    <title>An Evolutionary approach for solving Shrödinger Equation</title>
    <summary>  The purpose of this paper is to present a method of solving the Shr\"odinger
Equation (SE) by Genetic Algorithms and Grammatical Evolution. The method forms
generations of trial solutions expressed in an analytical form. We illustrate
the effectiveness of this method providing, for example, the results of its
application to a quantum system minimal energy, and we compare these results
with those produced by traditional analytical methods
</summary>
    <author>
      <name>Khalid jebari</name>
    </author>
    <author>
      <name>Mohammed Madiafi</name>
    </author>
    <author>
      <name>Abdelaziz Elmoujahid</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1401.0523</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.5428v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.5428v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.6556v1</id>
    <updated>2014-02-26T14:39:57Z</updated>
    <published>2014-02-26T14:39:57Z</published>
    <title>Evolutionary solving of the debts' clearing problem</title>
    <summary>  The debts' clearing problem is about clearing all the debts in a group of n
entities (persons, companies etc.) using a minimal number of money transaction
operations. The problem is known to be NP-hard in the strong sense. As for many
intractable problems, techniques from the field of artificial intelligence are
useful in finding solutions close to optimum for large inputs. An evolutionary
algorithm for solving the debts' clearing problem is proposed.
</summary>
    <author>
      <name>Csaba Patcas</name>
    </author>
    <author>
      <name>Attila Bartha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1402.6556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.6556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="97R40" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8; G.2.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.1073v1</id>
    <updated>2014-03-05T11:05:16Z</updated>
    <published>2014-03-05T11:05:16Z</published>
    <title>Artificial Neuron Modelling Based on Wave Shape</title>
    <summary>  This paper describes a new model for an artificial neural network processing
unit or neuron. It is slightly different to a traditional feedforward network
by the fact that it favours a mechanism of trying to match the wave-like
'shape' of the input with the shape of the output against specific value error
corrections. The expectation is then that a best fit shape can be transposed
into the desired output values more easily. This allows for notions of
reinforcement through resonance and also the construction of synapses.
</summary>
    <author>
      <name>Kieran Greer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">BRAIN. Broad Research in Artificial Intelligence and Neuroscience,
  Volume 4, Issues 1-4, October 2013, pp. 20 - 25, ISSN 2067-3957 (online),
  ISSN 2068 - 0473 (print)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1403.1073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.1073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.1727v1</id>
    <updated>2014-03-07T11:49:23Z</updated>
    <published>2014-03-07T11:49:23Z</published>
    <title>On the Sequence of State Configurations in the Garden of Eden</title>
    <summary>  Autonomous threshold element circuit networks are used to investigate the
structure of neural networks. With these circuits, as the transition functions
are threshold functions, it is necessary to consider the existence of sequences
of state configurations that cannot be transitioned. In this study, we focus on
all logical functions of four or fewer variables, and we discuss the periodic
sequences and transient series that transition from all sequences of state
configurations. Furthermore, by using the sequences of state configurations in
the Garden of Eden, we show that it is easy to obtain functions that determine
the operation of circuit networks.
</summary>
    <author>
      <name>Yukihiro Kamada</name>
    </author>
    <author>
      <name>Kiyonori Miyasaki</name>
    </author>
    <link href="http://arxiv.org/abs/1403.1727v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.1727v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.4871v1</id>
    <updated>2014-03-19T16:24:13Z</updated>
    <published>2014-03-19T16:24:13Z</published>
    <title>Evolutionary Algorithm for Drug Discovery Interim Design Report</title>
    <summary>  A software program which aims to provide an exploration capability over the
Search Space of potential drug molecules. The program explores the search space
by generating random molecules, determining their fitness and then breeding a
new generation from the fittest individuals. The search space, in theory any
combination of any elements in any order, is constrained by the use of a subset
of elements and a list of fragments, molecular parts that are known to be
useful in drug development. The resultant molecules from each generation are
stored in a searchable database, so that the user can browse through previous
generations looking for interesting molecules.
</summary>
    <author>
      <name>Mark Shackelford</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Pages, Interim Design Document</arxiv:comment>
    <link href="http://arxiv.org/abs/1403.4871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.4871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.7178v1</id>
    <updated>2014-03-27T19:16:50Z</updated>
    <published>2014-03-27T19:16:50Z</published>
    <title>Offshore Wind Farm Layout Optimization Using Adapted Genetic Algorithm:
  A different perspective</title>
    <summary>  In this paper we study the problem of optimal layout of an offshore wind farm
to minimize the wake effect impacts. Considering the specific requirements of
concerned offshore wind farm, we propose an adaptive genetic algorithm (AGA)
which introduces location swaps to replace random crossovers in conventional
GAs. That way the total number of turbines in the resulting layout will be
effectively kept to the initially specified value. We experiment the proposed
AGA method on three cases with free wind speed of 12 m/s, 20 m/s, and a typical
offshore wind distribution setting respectively. Numerical results verify the
effectiveness of our proposed algorithm which achieves a much faster
convergence compared to conventional GA algorithms.
</summary>
    <author>
      <name>Feng Liu</name>
    </author>
    <author>
      <name>Zhifang Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1403.7178v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.7178v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1403.7752v2</id>
    <updated>2015-01-23T19:12:05Z</updated>
    <published>2014-03-30T13:11:55Z</published>
    <title>Auto-encoders: reconstruction versus compression</title>
    <summary>  We discuss the similarities and differences between training an auto-encoder
to minimize the reconstruction error, and training the same auto-encoder to
compress the data via a generative model. Minimizing a codelength for the data
using an auto-encoder is equivalent to minimizing the reconstruction error plus
some correcting terms which have an interpretation as either a denoising or
contractive property of the decoding function. These terms are related but not
identical to those used in denoising or contractive auto-encoders [Vincent et
al. 2010, Rifai et al. 2011]. In particular, the codelength viewpoint fully
determines an optimal noise level for the denoising criterion.
</summary>
    <author>
      <name>Yann Ollivier</name>
    </author>
    <link href="http://arxiv.org/abs/1403.7752v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1403.7752v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.0695v1</id>
    <updated>2014-04-02T20:28:51Z</updated>
    <published>2014-04-02T20:28:51Z</published>
    <title>Multi-objective Flower Algorithm for Optimization</title>
    <summary>  Flower pollination algorithm is a new nature-inspired algorithm, based on the
characteristics of flowering plants. In this paper, we extend this flower
algorithm to solve multi-objective optimization problems in engineering. By
using the weighted sum method with random weights, we show that the proposed
multi-objective flower algorithm can accurately find the Pareto fronts for a
set of test functions. We then solve a bi-objective disc brake design problem,
which indeed converges quickly.
</summary>
    <author>
      <name>Xin-She Yang</name>
    </author>
    <author>
      <name>M. Karamanoglu</name>
    </author>
    <author>
      <name>Xingshi He</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.procs.2013.05.251</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.procs.2013.05.251" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 figures. arXiv admin note: substantial text overlap with
  arXiv:1312.5673</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">X. S. Yang, M. Karamanoglu, X. S. He, Multi-objective Flower
  Algorithm for Optimization, Procedia Computer Science, vol. 18, pp. 861-868
  (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.0695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.0695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C26" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.0868v1</id>
    <updated>2014-04-03T11:46:42Z</updated>
    <published>2014-04-03T11:46:42Z</published>
    <title>A Novel Genetic Algorithm using Helper Objectives for the 0-1 Knapsack
  Problem</title>
    <summary>  The 0-1 knapsack problem is a well-known combinatorial optimisation problem.
Approximation algorithms have been designed for solving it and they return
provably good solutions within polynomial time. On the other hand, genetic
algorithms are well suited for solving the knapsack problem and they find
reasonably good solutions quickly. A naturally arising question is whether
genetic algorithms are able to find solutions as good as approximation
algorithms do. This paper presents a novel multi-objective optimisation genetic
algorithm for solving the 0-1 knapsack problem. Experiment results show that
the new algorithm outperforms its rivals, the greedy algorithm, mixed strategy
genetic algorithm, and greedy algorithm + mixed strategy genetic algorithm.
</summary>
    <author>
      <name>Jun He</name>
    </author>
    <author>
      <name>Feidun He</name>
    </author>
    <author>
      <name>Hongbin Dong</name>
    </author>
    <link href="http://arxiv.org/abs/1404.0868v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.0868v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.1999v1</id>
    <updated>2014-04-08T03:41:50Z</updated>
    <published>2014-04-08T03:41:50Z</published>
    <title>Notes on Generalized Linear Models of Neurons</title>
    <summary>  Experimental neuroscience increasingly requires tractable models for
analyzing and predicting the behavior of neurons and networks. The generalized
linear model (GLM) is an increasingly popular statistical framework for
analyzing neural data that is flexible, exhibits rich dynamic behavior and is
computationally tractable (Paninski, 2004; Pillow et al., 2008; Truccolo et
al., 2005). What follows is a brief summary of the primary equations governing
the application of GLM's to spike trains with a few sentences linking this work
to the larger statistical literature. Latter sections include extensions of a
basic GLM to model spatio-temporal receptive fields as well as network activity
in an arbitrary numbers of neurons.
</summary>
    <author>
      <name>Jonathon Shlens</name>
    </author>
    <link href="http://arxiv.org/abs/1404.1999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.1999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1404.5767v1</id>
    <updated>2014-04-23T09:53:47Z</updated>
    <published>2014-04-23T09:53:47Z</published>
    <title>Codynamic Fitness Landscapes of Coevolutionary Minimal Substrates</title>
    <summary>  Coevolutionary minimal substrates are simple and abstract models that allow
studying the relationships and codynamics between objective and subjective
fitness. Using these models an approach is presented for defining and analyzing
fitness landscapes of coevolutionary problems. We devise similarity measures of
codynamic fitness landscapes and experimentally study minimal substrates of
test--based and compositional problems for both cooperative and competitive
interaction.
</summary>
    <author>
      <name>Hendrik Richter</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CEC.2014.6900272</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CEC.2014.6900272" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In: Proc. IEEE Congress on Evolutionary Computation, IEEE CEC
  2014, (Ed.: C. A. Coello, Coello), IEEE Press, Piscataway, NJ, 2014,
  2692-2699</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1404.5767v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1404.5767v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.4510v1</id>
    <updated>2014-05-18T14:25:56Z</updated>
    <published>2014-05-18T14:25:56Z</published>
    <title>A Memetic Algorithm for the Linear Ordering Problem with Cumulative
  Costs</title>
    <summary>  This paper introduces an effective memetic algorithm for the linear ordering
problem with cumulative costs. The proposed algorithm combines an order-based
recombination operator with an improved forward-backward local search procedure
and employs a solution quality based replacement criterion for pool updating.
Extensive experiments on 118 well-known benchmark instances show that the
proposed algorithm achieves competitive results by identifying 46 new upper
bounds. Furthermore, some critical ingredients of our algorithm are analyzed to
understand the source of its performance.
</summary>
    <author>
      <name>Tao Ye</name>
    </author>
    <author>
      <name>Kan Zhou</name>
    </author>
    <author>
      <name>Zhipeng Lu</name>
    </author>
    <author>
      <name>Jin-Kao Hao</name>
    </author>
    <link href="http://arxiv.org/abs/1405.4510v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.4510v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.4894v1</id>
    <updated>2014-04-25T12:59:40Z</updated>
    <published>2014-04-25T12:59:40Z</published>
    <title>Optimization of OFDM radar waveforms using genetic algorithms</title>
    <summary>  In this paper, we present our investigations on the use of single objective
and multiobjective genetic algorithms based optimisation algorithms to improve
the design of OFDM pulses for radar. We discuss these optimization procedures
in the scope of a waveform design intended for two different radar processing
solutions. Lastly, we show how the encoding solution is suited to permit the
optimizations of waveform for OFDM radar related challenges such as enhanced
detection.
</summary>
    <author>
      <name>Gabriel Lellouch</name>
    </author>
    <author>
      <name>Amit Kumar Mishra</name>
    </author>
    <link href="http://arxiv.org/abs/1405.4894v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.4894v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1405.7777v1</id>
    <updated>2014-05-30T05:24:22Z</updated>
    <published>2014-05-30T05:24:22Z</published>
    <title>Online and Adaptive Pseudoinverse Solutions for ELM Weights</title>
    <summary>  The ELM method has become widely used for classification and regressions
problems as a result of its accuracy, simplicity and ease of use. The solution
of the hidden layer weights by means of a matrix pseudoinverse operation is a
significant contributor to the utility of the method; however, the conventional
calculation of the pseudoinverse by means of a singular value decomposition
(SVD) is not always practical for large data sets or for online updates to the
solution. In this paper we discuss incremental methods for solving the
pseudoinverse which are suitable for ELM. We show that careful choice of
methods allows us to optimize for accuracy, ease of computation, or
adaptability of the solution.
</summary>
    <author>
      <name>André van Schaik</name>
    </author>
    <author>
      <name>Jonathan Tapson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication in Neurocomputing</arxiv:comment>
    <link href="http://arxiv.org/abs/1405.7777v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1405.7777v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2539v1</id>
    <updated>2014-06-10T13:22:13Z</updated>
    <published>2014-06-10T13:22:13Z</published>
    <title>Maximizing Diversity for Multimodal Optimization</title>
    <summary>  Most multimodal optimization algorithms use the so called \textit{niching
methods}~\cite{mahfoud1995niching} in order to promote diversity during
optimization, while others, like \textit{Artificial Immune
Systems}~\cite{de2010conceptual} try to find multiple solutions as its main
objective. One of such algorithms, called
\textit{dopt-aiNet}~\cite{de2005artificial}, introduced the Line Distance that
measures the distance between two solutions regarding their basis of
attraction. In this short abstract I propose the use of the Line Distance
measure as the main objective-function in order to locate multiple optima at
once in a population.
</summary>
    <author>
      <name>Fabricio Olivetti de Franca</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submitted to PPSN'14 Workshop Advances in Multimodal Optimization</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2539v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2539v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.2614v4</id>
    <updated>2015-02-26T07:06:17Z</updated>
    <published>2014-06-07T20:20:11Z</published>
    <title>Application and Verification of Algorithm Learning Based Neural Network</title>
    <summary>  This paper has been withdrawn by the author due to a crucial accuracy error
in Fig. 5. For precise performance of ALBNN please refer to Yoon et al.'s work
in the following article. Yoon, H., Park, C. S., Kim, J. S., &amp; Baek, J. G.
(2013). Algorithm learning based neural network integrating feature selection
and classification. Expert Systems with Applications, 40(1), 231-241.
http://www.sciencedirect.com/science/article/pii/S0957417412008731
</summary>
    <author>
      <name>Rizwana Kalsoom</name>
    </author>
    <author>
      <name>Moomal Qureshi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to a crucial accuracy
  error in Fig. 5</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.2614v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.2614v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.4518v1</id>
    <updated>2014-06-15T15:20:17Z</updated>
    <published>2014-06-15T15:20:17Z</published>
    <title>A Heuristic Method to Generate Better Initial Population for
  Evolutionary Methods</title>
    <summary>  Initial population plays an important role in heuristic algorithms such as GA
as it help to decrease the time those algorithms need to achieve an acceptable
result. Furthermore, it may influence the quality of the final answer given by
evolutionary algorithms. In this paper, we shall introduce a heuristic method
to generate a target based initial population which possess two mentioned
characteristics. The efficiency of the proposed method has been shown by
presenting the results of our tests on the benchmarks.
</summary>
    <author>
      <name>Erfan Khaji</name>
    </author>
    <author>
      <name>Amin Satlikh Mohammadi</name>
    </author>
    <link href="http://arxiv.org/abs/1406.4518v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.4518v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0265v1</id>
    <updated>2014-07-01T14:50:36Z</updated>
    <published>2014-07-01T14:50:36Z</published>
    <title>Supervised learning in Spiking Neural Networks with Limited Precision:
  SNN/LP</title>
    <summary>  A new supervised learning algorithm, SNN/LP, is proposed for Spiking Neural
Networks. This novel algorithm uses limited precision for both synaptic weights
and synaptic delays; 3 bits in each case. Also a genetic algorithm is used for
the supervised training. The results are comparable or better than previously
published work. The results are applicable to the realization of large scale
hardware neural networks. One of the trained networks is implemented in
programmable hardware.
</summary>
    <author>
      <name>Evangelos Stromatias</name>
    </author>
    <author>
      <name>John Marsland</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, originally submitted to IJCNN 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.0265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0698v1</id>
    <updated>2014-07-02T16:41:07Z</updated>
    <published>2014-07-02T16:41:07Z</published>
    <title>Continuous On-line Evolution of Agent Behaviours with Cartesian Genetic
  Programming</title>
    <summary>  Evolutionary Computation has been successfully used to synthesise controllers
for embodied agents and multi-agent systems in general. Notwithstanding this,
continuous on-line adaptation by the means of evolutionary algorithms is still
under-explored, especially outside the evolutionary robotics domain. In this
paper, we present an on-line evolutionary programming algorithm that searches
in the agent design space for the appropriate behavioural policies to cope with
the underlying environment. We discuss the current problems of continuous agent
adaptation, present our on-line evolution testbed for evolutionary simulation.
</summary>
    <author>
      <name>Davide Nunes</name>
    </author>
    <author>
      <name>Luis Antunes</name>
    </author>
    <link href="http://arxiv.org/abs/1407.0698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.0977v1</id>
    <updated>2014-07-02T15:22:11Z</updated>
    <published>2014-07-02T15:22:11Z</published>
    <title>Higher-Order Quantum-Inspired Genetic Algorithms</title>
    <summary>  This paper presents a theory and an empirical evaluation of Higher-Order
Quantum-Inspired Genetic Algorithms. Fundamental notions of the theory have
been introduced, and a novel Order-2 Quantum-Inspired Genetic Algorithm (QIGA2)
has been presented. Contrary to all QIGA algorithms which represent quantum
genes as independent qubits, in higher-order QIGAs quantum registers are used
to represent genes strings which allows modelling of genes relations using
quantum phenomena. Performance comparison has been conducted on a benchmark of
20 deceptive combinatorial optimization problems. It has been presented that
using higher quantum orders is beneficial for genetic algorithm efficiency, and
the new QIGA2 algorithm outperforms the old QIGA algorithm which was tuned in
highly compute intensive metaoptimization process.
</summary>
    <author>
      <name>Robert Nowotniak</name>
    </author>
    <author>
      <name>Jacek Kucharski</name>
    </author>
    <link href="http://arxiv.org/abs/1407.0977v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.0977v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.3077v1</id>
    <updated>2014-07-11T09:18:20Z</updated>
    <published>2014-07-11T09:18:20Z</published>
    <title>Charge Scheduling of an Energy Storage System under Time-of-use Pricing
  and a Demand Charge</title>
    <summary>  A real-coded genetic algorithm is used to schedule the charging of an energy
storage system (ESS), operated in tandem with renewable power by an electricity
consumer who is subject to time-of-use pricing and a demand charge. Simulations
based on load and generation profiles of typical residential customers show
that an ESS scheduled by our algorithm can reduce electricity costs by
approximately 17%, compared to a system without an ESS, and by 8% compared to a
scheduling algorithm based on net power.
</summary>
    <author>
      <name>Yourim Yoon</name>
    </author>
    <author>
      <name>Yong-Hyuk Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.3077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.3077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.5739v1</id>
    <updated>2014-07-22T05:48:08Z</updated>
    <published>2014-07-22T05:48:08Z</published>
    <title>Global optimization using Lévy flights</title>
    <summary>  This paper studies a class of enhanced diffusion processes in which random
walkers perform L\'evy flights and apply it for global optimization. L\'evy
flights offer controlled balance between exploitation and exploration. We
develop four optimization algorithms based on such properties. We compare new
algorithms with the well-known Simulated Annealing on hard test functions and
the results are very promising.
</summary>
    <author>
      <name>Truyen Tran</name>
    </author>
    <author>
      <name>Trung Thanh Nguyen</name>
    </author>
    <author>
      <name>Hoang Linh Nguyen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures, 4 algorithms,Proceedings of Second National
  Symposium on Research, Development and Application of Information and
  Communication Technology (ICT.rda'04), Hanoi, Sept 24-25, 2004</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.5739v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.5739v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1407.7211v1</id>
    <updated>2014-07-27T11:45:21Z</updated>
    <published>2014-07-27T11:45:21Z</published>
    <title>An evolutionary solver for linear integer programming</title>
    <summary>  In this paper we introduce an evolutionary algorithm for the solution of
linear integer programs. The strategy is based on the separation of the
variables into the integer subset and the continuous subset; the integer
variables are fixed by the evolutionary system, and the continuous ones are
determined in function of them, by a linear program solver.
  We report results obtained for some standard benchmark problems, and compare
them with those obtained by branch-and-bound. The performance of the
evolutionary algorithm is promising. Good feasible solutions were generally
obtained, and in some of the difficult benchmark tests it outperformed
branch-and-bound.
</summary>
    <author>
      <name>João Pedro Pedroso</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1407.7211v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1407.7211v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="80M50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6, I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.4077v1</id>
    <updated>2014-08-18T17:49:28Z</updated>
    <published>2014-08-18T17:49:28Z</published>
    <title>Brain: Biological noise-based logic</title>
    <summary>  Neural spikes in the brain form stochastic sequences, i.e., belong to the
class of pulse noises. This stochasticity is a counterintuitive feature because
extracting information - such as the commonly supposed neural information of
mean spike frequency - requires long times for reasonably low error
probability. The mystery could be solved by noise-based logic, wherein
randomness has an important function and allows large speed enhancements for
special-purpose tasks, and the same mechanism is at work for the brain logic
version of this concept.
</summary>
    <author>
      <name>Laszlo B. Kish</name>
    </author>
    <author>
      <name>Claes-Goran Granqvist</name>
    </author>
    <author>
      <name>Sergey M. Bezrukov</name>
    </author>
    <author>
      <name>Tamas Horvath</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-94-017-9548-7_45</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-94-017-9548-7_45" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">paper in press</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Advances in Cognitive Neurodynamics 2015, pp 319-322</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1408.4077v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.4077v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.5403v1</id>
    <updated>2014-08-22T12:15:54Z</updated>
    <published>2014-08-22T12:15:54Z</published>
    <title>Neural Mechanism of Language</title>
    <summary>  This paper is based on our previous work on neural coding. It is a
self-organized model supported by existing evidences. Firstly, we briefly
introduce this model in this paper, and then we explain the neural mechanism of
language and reasoning with it. Moreover, we find that the position of an area
determines its importance. Specifically, language relevant areas are in the
capital position of the cortical kingdom. Therefore they are closely related
with autonomous consciousness and working memories. In essence, language is a
miniature of the real world. Briefly, this paper would like to bridge the gap
between molecule mechanism of neurons and advanced functions such as language
and reasoning.
</summary>
    <author>
      <name>Peilei Liu</name>
    </author>
    <author>
      <name>Ting Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1408.5403v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.5403v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1408.6741v1</id>
    <updated>2014-08-28T14:55:16Z</updated>
    <published>2014-08-28T14:55:16Z</published>
    <title>Memcomputing and Swarm Intelligence</title>
    <summary>  We explore the relation between memcomputing, namely computing with and in
memory, and swarm intelligence algorithms. In particular, we show that one can
design memristive networks to solve short-path optimization problems that can
also be solved by ant-colony algorithms. By employing appropriate memristive
elements one can demonstrate an almost one-to-one correspondence between
memcomputing and ant colony optimization approaches. However, the memristive
network has the capability of finding the solution in one deterministic step,
compared to the stochastic multi-step ant colony optimization. This result
paves the way for nanoscale hardware implementations of several swarm
intelligence algorithms that are presently explored, from scheduling problems
to robotics.
</summary>
    <author>
      <name>Y. V. Pershin</name>
    </author>
    <author>
      <name>M. Di Ventra</name>
    </author>
    <link href="http://arxiv.org/abs/1408.6741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1408.6741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mes-hall" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.0334v1</id>
    <updated>2014-09-01T08:59:27Z</updated>
    <published>2014-09-01T08:59:27Z</published>
    <title>Storing sequences in binary tournament-based neural networks</title>
    <summary>  An extension to a recently introduced architecture of clique-based neural
networks is presented. This extension makes it possible to store sequences with
high efficiency. To obtain this property, network connections are provided with
orientation and with flexible redundancy carried by both spatial and temporal
redundancy, a mechanism of anticipation being introduced in the model. In
addition to the sequence storage with high efficiency, this new scheme also
offers biological plausibility. In order to achieve accurate sequence
retrieval, a double layered structure combining hetero-association and
auto-association is also proposed.
</summary>
    <author>
      <name>Xiaoran Jiang</name>
    </author>
    <author>
      <name>Vincent Gripon</name>
    </author>
    <author>
      <name>Claude Berrou</name>
    </author>
    <author>
      <name>Michael Rabbat</name>
    </author>
    <link href="http://arxiv.org/abs/1409.0334v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.0334v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.1715v1</id>
    <updated>2014-09-05T10:01:41Z</updated>
    <published>2014-09-05T10:01:41Z</published>
    <title>An Experimental Study of Adaptive Control for Evolutionary Algorithms</title>
    <summary>  The balance of exploration versus exploitation (EvE) is a key issue on
evolutionary computation. In this paper we will investigate how an adaptive
controller aimed to perform Operator Selection can be used to dynamically
manage the EvE balance required by the search, showing that the search
strategies determined by this control paradigm lead to an improvement of
solution quality found by the evolutionary algorithm.
</summary>
    <author>
      <name>Giacomo di Tollo</name>
    </author>
    <author>
      <name>Frédéric Lardeux</name>
    </author>
    <author>
      <name>Jorge Maturana</name>
    </author>
    <author>
      <name>Frédéric Saubion</name>
    </author>
    <link href="http://arxiv.org/abs/1409.1715v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.1715v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.2329v5</id>
    <updated>2015-02-19T14:46:00Z</updated>
    <published>2014-09-08T13:08:00Z</published>
    <title>Recurrent Neural Network Regularization</title>
    <summary>  We present a simple regularization technique for Recurrent Neural Networks
(RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful
technique for regularizing neural networks, does not work well with RNNs and
LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show
that it substantially reduces overfitting on a variety of tasks. These tasks
include language modeling, speech recognition, image caption generation, and
machine translation.
</summary>
    <author>
      <name>Wojciech Zaremba</name>
    </author>
    <author>
      <name>Ilya Sutskever</name>
    </author>
    <author>
      <name>Oriol Vinyals</name>
    </author>
    <link href="http://arxiv.org/abs/1409.2329v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.2329v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.4244v1</id>
    <updated>2014-09-12T12:07:47Z</updated>
    <published>2014-09-12T12:07:47Z</published>
    <title>An OvS-MultiObjective Algorithm Approach for Lane Reversal Problem</title>
    <summary>  The lane reversal has proven to be a useful method to mitigate traffic
congestion during rush hour or in case of specific events that affect high
traffic volumes. In this work we propose a methodology that is placed within
optimization via Simulation, by means of which a multi-objective genetic
algorithm and simulations of traffic are used to determine the configuration of
ideal lane reversal.
</summary>
    <author>
      <name>Enrique Gabriel Baquela</name>
    </author>
    <author>
      <name>Ana Carolina Olivera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ALIO/EURO 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.4244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.4244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.7478v1</id>
    <updated>2014-09-26T06:32:52Z</updated>
    <published>2014-09-26T06:32:52Z</published>
    <title>An Analysis on Selection for High-Resolution Approximations in
  Many-Objective Optimization</title>
    <summary>  This work studies the behavior of three elitist multi- and many-objective
evolutionary algorithms generating a high-resolution approximation of the
Pareto optimal set. Several search-assessment indicators are defined to trace
the dynamics of survival selection and measure the ability to simultaneously
keep optimal solutions and discover new ones under different population sizes,
set as a fraction of the size of the Pareto optimal set.
</summary>
    <author>
      <name>Hernan Aguirre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lille - Nord Europe, LIFL</arxiv:affiliation>
    </author>
    <author>
      <name>Arnaud Liefooghe</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Lille - Nord Europe, LIFL</arxiv:affiliation>
    </author>
    <author>
      <name>Sébastien Verel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LISIC</arxiv:affiliation>
    </author>
    <author>
      <name>Kiyoshi Tanaka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">apperas in Parallel Problem Solving from Nature - PPSN XIII,
  Ljubljana : Slovenia (2014)</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.7478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.7478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1409.8191v1</id>
    <updated>2014-09-29T17:08:21Z</updated>
    <published>2014-09-29T17:08:21Z</published>
    <title>A Neural Networks Committee for the Contextual Bandit Problem</title>
    <summary>  This paper presents a new contextual bandit algorithm, NeuralBandit, which
does not need hypothesis on stationarity of contexts and rewards. Several
neural networks are trained to modelize the value of rewards knowing the
context. Two variants, based on multi-experts approach, are proposed to choose
online the parameters of multi-layer perceptrons. The proposed algorithms are
successfully tested on a large dataset with and without stationarity of
rewards.
</summary>
    <author>
      <name>Robin Allesiardo</name>
    </author>
    <author>
      <name>Raphael Feraud</name>
    </author>
    <author>
      <name>Djallel Bouneffouf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21st International Conference on Neural Information Processing</arxiv:comment>
    <link href="http://arxiv.org/abs/1409.8191v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1409.8191v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.5401v2</id>
    <updated>2014-12-10T16:01:39Z</updated>
    <published>2014-10-20T19:28:26Z</published>
    <title>Neural Turing Machines</title>
    <summary>  We extend the capabilities of neural networks by coupling them to external
memory resources, which they can interact with by attentional processes. The
combined system is analogous to a Turing Machine or Von Neumann architecture
but is differentiable end-to-end, allowing it to be efficiently trained with
gradient descent. Preliminary results demonstrate that Neural Turing Machines
can infer simple algorithms such as copying, sorting, and associative recall
from input and output examples.
</summary>
    <author>
      <name>Alex Graves</name>
    </author>
    <author>
      <name>Greg Wayne</name>
    </author>
    <author>
      <name>Ivo Danihelka</name>
    </author>
    <link href="http://arxiv.org/abs/1410.5401v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.5401v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.6413v1</id>
    <updated>2014-10-23T16:54:39Z</updated>
    <published>2014-10-23T16:54:39Z</published>
    <title>Initialization of multilayer forecasting artifical neural networks</title>
    <summary>  In this paper, a new method was developed for initialising artificial neural
networks predicting dynamics of time series. Initial weighting coefficients
were determined for neurons analogously to the case of a linear prediction
filter. Moreover, to improve the accuracy of the initialization method for a
multilayer neural network, some variants of decomposition of the transformation
matrix corresponding to the linear prediction filter were suggested. The
efficiency of the proposed neural network prediction method by forecasting
solutions of the Lorentz chaotic system is shown in this paper.
</summary>
    <author>
      <name>Vladimir V. Bochkarev</name>
    </author>
    <author>
      <name>Yulia S. Maslennikova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Uchenye Zapiski Kazanskogo Universiteta. Seriya
  Fiziko-Matematicheskie Nauki, 2010, vol. 152, no. 1, pp. 7-14. (In Russian)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1410.6413v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.6413v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
    <category term="62M45, 62M10, 68T05" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.7326v3</id>
    <updated>2015-11-03T21:21:41Z</updated>
    <published>2014-10-27T17:46:28Z</published>
    <title>Neuroevolution in Games: State of the Art and Open Challenges</title>
    <summary>  This paper surveys research on applying neuroevolution (NE) to games. In
neuroevolution, artificial neural networks are trained through evolutionary
algorithms, taking inspiration from the way biological brains evolved. We
analyse the application of NE in games along five different axes, which are the
role NE is chosen to play in a game, the different types of neural networks
used, the way these networks are evolved, how the fitness is determined and
what type of input the network receives. The article also highlights important
open research challenges in the field.
</summary>
    <author>
      <name>Sebastian Risi</name>
    </author>
    <author>
      <name>Julian Togelius</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">- Added more references - Corrected typos - Added an overview table
  (Table 1)</arxiv:comment>
    <link href="http://arxiv.org/abs/1410.7326v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.7326v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1410.7883v1</id>
    <updated>2014-10-29T05:56:00Z</updated>
    <published>2014-10-29T05:56:00Z</published>
    <title>Sub-threshold CMOS Spiking Neuron Circuit Design for Navigation Inspired
  by C. elegans Chemotaxis</title>
    <summary>  We demonstrate a spiking neural network for navigation motivated by the
chemotaxis network of Caenorhabditis elegans. Our network uses information
regarding temporal gradients in the tracking variable's concentration to make
navigational decisions. The gradient information is determined by mimicking the
underlying mechanisms of the ASE neurons of C. elegans. Simulations show that
our model is able to forage and track a target set-point in extremely noisy
environments. We develop a VLSI implementation for the main gradient detector
neurons, which could be integrated with standard comparator circuitry to
develop a robust circuit for navigation and contour tracking.
</summary>
    <author>
      <name>Shibani Santurkar</name>
    </author>
    <author>
      <name>Bipin Rajendran</name>
    </author>
    <link href="http://arxiv.org/abs/1410.7883v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1410.7883v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.2897v1</id>
    <updated>2014-11-11T17:42:26Z</updated>
    <published>2014-11-11T17:42:26Z</published>
    <title>Accelerating the ANT Colony Optimization By Smart ANTs, Using Genetic
  Operator</title>
    <summary>  This paper research review Ant colony optimization (ACO) and Genetic
Algorithm (GA), both are two powerful meta-heuristics. This paper explains some
major defects of these two algorithm at first then proposes a new model for ACO
in which, artificial ants use a quick genetic operator and accelerate their
actions in selecting next state. Experimental results show that proposed hybrid
algorithm is effective and its performance including speed and accuracy beats
other version.
</summary>
    <author>
      <name>Hassan Ismkhan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal on Computational Science &amp; Applications,
  Volume: 4 - volume NO: 2 - Issue: April 2014</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.2897v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.2897v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.4297v1</id>
    <updated>2014-11-05T15:58:47Z</updated>
    <published>2014-11-05T15:58:47Z</published>
    <title>Application of Multi-core Parallel Programming to a Combination of Ant
  Colony Optimization and Genetic Algorithm</title>
    <summary>  This Paper will deal with a combination of Ant Colony and Genetic Programming
Algorithm to optimize Travelling Salesmen problem (NP-Hard). However, the
complexity of the algorithm requires considerable computational time and
resources. Parallel implementation can reduce the computational time. In this
paper, emphasis in the parallelizing section is given to Multi-core
architecture and Multi-Processor Systems which is developed and used almost
everywhere today and hence, multi-core parallelization to the combination of
algorithm is achieved by OpenMP library by Intel Corporation.
</summary>
    <author>
      <name>Rishita Kalyani</name>
    </author>
    <link href="http://arxiv.org/abs/1411.4297v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.4297v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.5737v5</id>
    <updated>2015-10-06T00:03:43Z</updated>
    <published>2014-11-21T01:21:17Z</published>
    <title>Fuzzy Adaptive Resonance Theory, Diffusion Maps and their applications
  to Clustering and Biclustering</title>
    <summary>  In this paper, we describe an algorithm FARDiff (Fuzzy Adaptive Resonance
Dif- fusion) which combines Diffusion Maps and Fuzzy Adaptive Resonance Theory
to do clustering on high dimensional data. We describe some applications of
this method and some problems for future research.
</summary>
    <author>
      <name>S. B. Damelin</name>
    </author>
    <author>
      <name>Y. Gu</name>
    </author>
    <author>
      <name>D. C. Wunsch II</name>
    </author>
    <author>
      <name>R. Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in Math.Model.Nat.Phenom</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Math.Model.Nat.Phenom. Vol. 10, No 3, 2015, pp. 206-211</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1411.5737v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.5737v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="94A15, 62H30, 60J20, 68T05, 68T45, 68T10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1411.6768v1</id>
    <updated>2014-11-25T08:52:14Z</updated>
    <published>2014-11-25T08:52:14Z</published>
    <title>Hypotheses of neural code and the information model of the
  neuron-detector</title>
    <summary>  This paper deals with the problem of neural code solving. On the basis of the
formulated hypotheses the information model of a neuron-detector is suggested,
the detector being one of the basic elements of an artificial neural network
(ANN). The paper subjects the connectionist paradigm of ANN building to
criticism and suggests a new presentation paradigm for ANN building and
neuroelements (NE) learning. The adequacy of the suggested model is proved by
the fact that is does not contradict the modern propositions of neuropsychology
and neurophysiology.
</summary>
    <author>
      <name>Yuri Parzhin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1411.6768v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1411.6768v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3555v1</id>
    <updated>2014-12-11T06:46:53Z</updated>
    <published>2014-12-11T06:46:53Z</published>
    <title>Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
  Modeling</title>
    <summary>  In this paper we compare different types of recurrent units in recurrent
neural networks (RNNs). Especially, we focus on more sophisticated units that
implement a gating mechanism, such as a long short-term memory (LSTM) unit and
a recently proposed gated recurrent unit (GRU). We evaluate these recurrent
units on the tasks of polyphonic music modeling and speech signal modeling. Our
experiments revealed that these advanced recurrent units are indeed better than
more traditional recurrent units such as tanh units. Also, we found GRU to be
comparable to LSTM.
</summary>
    <author>
      <name>Junyoung Chung</name>
    </author>
    <author>
      <name>Caglar Gulcehre</name>
    </author>
    <author>
      <name>KyungHyun Cho</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in NIPS 2014 Deep Learning and Representation Learning
  Workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.3555v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3555v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3714v2</id>
    <updated>2014-12-13T00:57:57Z</updated>
    <published>2014-12-11T16:35:27Z</published>
    <title>Feature Weight Tuning for Recursive Neural Networks</title>
    <summary>  This paper addresses how a recursive neural network model can automatically
leave out useless information and emphasize important evidence, in other words,
to perform "weight tuning" for higher-level representation acquisition. We
propose two models, Weighted Neural Network (WNN) and Binary-Expectation Neural
Network (BENN), which automatically control how much one specific unit
contributes to the higher-level representation. The proposed model can be
viewed as incorporating a more powerful compositional function for embedding
acquisition in recursive neural networks. Experimental results demonstrate the
significant improvement over standard neural models.
</summary>
    <author>
      <name>Jiwei Li</name>
    </author>
    <link href="http://arxiv.org/abs/1412.3714v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3714v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6122v1</id>
    <updated>2014-12-02T22:58:54Z</updated>
    <published>2014-12-02T22:58:54Z</published>
    <title>Spread Unary Coding</title>
    <summary>  Unary coding is useful but it is redundant in its standard form. Unary coding
can also be seen as spatial coding where the value of the number is determined
by its place in an array. Motivated by biological finding that several neurons
in the vicinity represent the same number, we propose a variant of unary
numeration in its spatial form, where each number is represented by several 1s.
We call this spread unary coding where the number of 1s used is the spread of
the code. Spread unary coding is associated with saturation of the Hamming
distance between code words.
</summary>
    <author>
      <name>Subhash Kak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.6122v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6122v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6144v1</id>
    <updated>2014-12-14T23:12:05Z</updated>
    <published>2014-12-14T23:12:05Z</published>
    <title>The Computational Theory of Intelligence: Applications to Genetic
  Programming and Turing Machines</title>
    <summary>  In this paper, we continue the efforts of the Computational Theory of
Intelligence (CTI) by extending concepts to include computational processes in
terms of Genetic Algorithms (GA's) and Turing Machines (TM's). Active, Passive,
and Hybrid Computational Intelligence processes are also introduced and
discussed. We consider the ramifications of the assumptions of CTI with regard
to the qualities of reproduction and virility. Applications to Biology,
Computer Science and Cyber Security are also discussed.
</summary>
    <author>
      <name>Daniel Kovach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Total of 5 figures. This paper was originally presented at RAMSA 2013
  in Visakhaptnam, India in December 2013</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.6144v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6144v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92DXX, 68TXX, 03Dxx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.0; J.3; F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7009v3</id>
    <updated>2015-04-09T01:54:33Z</updated>
    <published>2014-12-22T14:57:05Z</published>
    <title>Generative Class-conditional Autoencoders</title>
    <summary>  Recent work by Bengio et al. (2013) proposes a sampling procedure for
denoising autoencoders which involves learning the transition operator of a
Markov chain. The transition operator is typically unimodal, which limits its
capacity to model complex data. In order to perform efficient sampling from
conditional distributions, we extend this work, both theoretically and
algorithmically, to gated autoencoders (Memisevic, 2013), The proposed model is
able to generate convincing class-conditional samples when trained on both the
MNIST and TFD datasets.
</summary>
    <author>
      <name>Jan Rudy</name>
    </author>
    <author>
      <name>Graham Taylor</name>
    </author>
    <link href="http://arxiv.org/abs/1412.7009v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7009v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.7955v1</id>
    <updated>2014-12-26T16:41:04Z</updated>
    <published>2014-12-26T16:41:04Z</published>
    <title>Unsupervised Learning through Prediction in a Model of Cortex</title>
    <summary>  We propose a primitive called PJOIN, for "predictive join," which combines
and extends the operations JOIN and LINK, which Valiant proposed as the basis
of a computational theory of cortex. We show that PJOIN can be implemented in
Valiant's model. We also show that, using PJOIN, certain reasonably complex
learning and pattern matching tasks can be performed, in a way that involves
phenomena which have been observed in cognition and the brain, namely
memory-based prediction and downward traffic in the cortical hierarchy.
</summary>
    <author>
      <name>Christos H. Papadimitriou</name>
    </author>
    <author>
      <name>Santosh S. Vempala</name>
    </author>
    <link href="http://arxiv.org/abs/1412.7955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.7955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.8291v1</id>
    <updated>2014-12-29T09:51:20Z</updated>
    <published>2014-12-29T09:51:20Z</published>
    <title>Improving approximate RPCA with a k-sparsity prior</title>
    <summary>  A process centric view of robust PCA (RPCA) allows its fast approximate
implementation based on a special form o a deep neural network with weights
shared across all layers. However, empirically this fast approximation to RPCA
fails to find representations that are parsemonious. We resolve these bad local
minima by relaxing the elementwise L1 and L2 priors and instead utilize a
structure inducing k-sparsity prior. In a discriminative classification task
the newly learned representations outperform these from the original
approximate RPCA formulation significantly.
</summary>
    <author>
      <name>Maximilian Karl</name>
    </author>
    <author>
      <name>Christian Osendorfer</name>
    </author>
    <link href="http://arxiv.org/abs/1412.8291v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.8291v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.04010v1</id>
    <updated>2015-01-16T15:22:19Z</updated>
    <published>2015-01-16T15:22:19Z</published>
    <title>Coevolutionary intransitivity in games: A landscape analysis</title>
    <summary>  Intransitivity is supposed to be a main reason for deficits in coevolutionary
progress and inheritable superiority. Besides, coevolutionary dynamics is
characterized by interactions yielding subjective fitness, but aiming at
solutions that are superior with respect to an objective measurement. Such an
approximation of objective fitness may be, for instance, generalization
performance. In the paper a link between rating-- and ranking--based measures
of intransitivity and fitness landscapes that can address the dichotomy between
subjective and objective fitness is explored. The approach is illustrated by
numerical experiments involving a simple random game with continuously tunable
degree of randomness.
</summary>
    <author>
      <name>Hendrik Richter</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In: Applications of Evolutionary Computation - EvoApplications
  2015, (Eds.: A. M. Mora, G. Squillero), Lecture Notes in Computer Science,
  Vol. 9028, Springer-Verlag, Berlin, 2015, 869-881</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1501.04010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.04010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1501.06633v3</id>
    <updated>2015-01-30T23:50:49Z</updated>
    <published>2015-01-27T01:19:12Z</published>
    <title>maxDNN: An Efficient Convolution Kernel for Deep Learning with Maxwell
  GPUs</title>
    <summary>  This paper describes maxDNN, a computationally efficient convolution kernel
for deep learning with the NVIDIA Maxwell GPU. maxDNN reaches 96.3%
computational efficiency on typical deep learning network architectures. The
design combines ideas from cuda-convnet2 with the Maxas SGEMM assembly code. We
only address forward propagation (FPROP) operation of the network, but we
believe that the same techniques used here will be effective for backward
propagation (BPROP) as well.
</summary>
    <author>
      <name>Andrew Lavin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1501.06633v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1501.06633v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00193v1</id>
    <updated>2015-02-01T04:39:30Z</updated>
    <published>2015-02-01T04:39:30Z</published>
    <title>Evolutionary Artificial Neural Network Based on Chemical Reaction
  Optimization</title>
    <summary>  Evolutionary algorithms (EAs) are very popular tools to design and evolve
artificial neural networks (ANNs), especially to train them. These methods have
advantages over the conventional backpropagation (BP) method because of their
low computational requirement when searching in a large solution space. In this
paper, we employ Chemical Reaction Optimization (CRO), a newly developed global
optimization method, to replace BP in training neural networks. CRO is a
population-based metaheuristics mimicking the transition of molecules and their
interactions in a chemical reaction. Simulation results show that CRO
outperforms many EA strategies commonly used to train neural networks.
</summary>
    <author>
      <name>James J. Q. Yu</name>
    </author>
    <author>
      <name>Albert Y. S. Lam</name>
    </author>
    <author>
      <name>Victor O. K. Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CEC.2011.5949872</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CEC.2011.5949872" rel="related"/>
    <link href="http://arxiv.org/abs/1502.00193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00195v1</id>
    <updated>2015-02-01T04:48:18Z</updated>
    <published>2015-02-01T04:48:18Z</published>
    <title>Sensor Deployment for Air Pollution Monitoring Using Public
  Transportation System</title>
    <summary>  Air pollution monitoring is a very popular research topic and many monitoring
systems have been developed. In this paper, we formulate the Bus Sensor
Deployment Problem (BSDP) to select the bus routes on which sensors are
deployed, and we use Chemical Reaction Optimization (CRO) to solve BSDP. CRO is
a recently proposed metaheuristic designed to solve a wide range of
optimization problems. Using the real world data, namely Hong Kong Island bus
route data, we perform a series of simulations and the results show that CRO is
capable of solving this optimization problem efficiently.
</summary>
    <author>
      <name>James J. Q. Yu</name>
    </author>
    <author>
      <name>Victor O. K. Li</name>
    </author>
    <author>
      <name>Albert Y. S. Lam</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CEC.2012.6256495</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CEC.2012.6256495" rel="related"/>
    <link href="http://arxiv.org/abs/1502.00195v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00195v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.00199v1</id>
    <updated>2015-02-01T04:56:13Z</updated>
    <published>2015-02-01T04:56:13Z</published>
    <title>Chemical Reaction Optimization for the Set Covering Problem</title>
    <summary>  The set covering problem (SCP) is one of the representative combinatorial
optimization problems, having many practical applications. This paper
investigates the development of an algorithm to solve SCP by employing chemical
reaction optimization (CRO), a general-purpose metaheuristic. It is tested on a
wide range of benchmark instances of SCP. The simulation results indicate that
this algorithm gives outstanding performance compared with other heuristics and
metaheuristics in solving SCP.
</summary>
    <author>
      <name>James J. Q. Yu</name>
    </author>
    <author>
      <name>Albert Y. S. Lam</name>
    </author>
    <author>
      <name>Victor O. K. Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CEC.2014.6900233</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CEC.2014.6900233" rel="related"/>
    <link href="http://arxiv.org/abs/1502.00199v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.00199v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.02478v1</id>
    <updated>2015-02-09T13:29:48Z</updated>
    <published>2015-02-09T13:29:48Z</published>
    <title>Efficient batchwise dropout training using submatrices</title>
    <summary>  Dropout is a popular technique for regularizing artificial neural networks.
Dropout networks are generally trained by minibatch gradient descent with a
dropout mask turning off some of the units---a different pattern of dropout is
applied to every sample in the minibatch. We explore a very simple alternative
to the dropout mask. Instead of masking dropped out units by setting them to
zero, we perform matrix multiplication using a submatrix of the weight
matrix---unneeded hidden units are never calculated. Performing dropout
batchwise, so that one pattern of dropout is used for each sample in a
minibatch, we can substantially reduce training times. Batchwise dropout can be
used with fully-connected and convolutional neural networks.
</summary>
    <author>
      <name>Ben Graham</name>
    </author>
    <author>
      <name>Jeremy Reizenstein</name>
    </author>
    <author>
      <name>Leigh Robinson</name>
    </author>
    <link href="http://arxiv.org/abs/1502.02478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.02478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.03699v1</id>
    <updated>2015-02-12T15:24:19Z</updated>
    <published>2015-02-12T15:24:19Z</published>
    <title>Analysis of Solution Quality of a Multiobjective Optimization-based
  Evolutionary Algorithm for Knapsack Problem</title>
    <summary>  Multi-objective optimisation is regarded as one of the most promising ways
for dealing with constrained optimisation problems in evolutionary
optimisation. This paper presents a theoretical investigation of a
multi-objective optimisation evolutionary algorithm for solving the 0-1
knapsack problem. Two initialisation methods are considered in the algorithm:
local search initialisation and greedy search initialisation. Then the solution
quality of the algorithm is analysed in terms of the approximation ratio.
</summary>
    <author>
      <name>Jun He</name>
    </author>
    <author>
      <name>Yong Wang</name>
    </author>
    <author>
      <name>Yuren Zhou</name>
    </author>
    <link href="http://arxiv.org/abs/1502.03699v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.03699v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.00941v2</id>
    <updated>2015-04-07T22:39:18Z</updated>
    <published>2015-04-03T21:22:52Z</published>
    <title>A Simple Way to Initialize Recurrent Networks of Rectified Linear Units</title>
    <summary>  Learning long term dependencies in recurrent networks is difficult due to
vanishing and exploding gradients. To overcome this difficulty, researchers
have developed sophisticated optimization techniques and network architectures.
In this paper, we propose a simpler solution that use recurrent neural networks
composed of rectified linear units. Key to our solution is the use of the
identity matrix or its scaled version to initialize the recurrent weight
matrix. We find that our solution is comparable to LSTM on our four benchmarks:
two toy problems involving long-range temporal structures, a large language
modeling problem and a benchmark speech recognition problem.
</summary>
    <author>
      <name>Quoc V. Le</name>
    </author>
    <author>
      <name>Navdeep Jaitly</name>
    </author>
    <author>
      <name>Geoffrey E. Hinton</name>
    </author>
    <link href="http://arxiv.org/abs/1504.00941v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.00941v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.02590v1</id>
    <updated>2015-04-10T08:53:03Z</updated>
    <published>2015-04-10T08:53:03Z</published>
    <title>Study of Some Recent Crossovers Effects on Speed and Accuracy of Genetic
  Algorithm, Using Symmetric Travelling Salesman Problem</title>
    <summary>  The Travelling Salesman Problem (TSP) is one of the most famous optimization
problems. The Genetic Algorithm (GA) is one of metaheuristics that have been
applied to TSP. The Crossover and mutation operators are two important elements
of GA. There are many TSP solver crossover operators. In this paper, we state
implementation of some recent TSP solver crossovers at first and then we use
each of them in GA to solve some Symmetric TSP (STSP) instances and finally
compare their effects on speed and accuracy of presented GA.
</summary>
    <author>
      <name>Hassan Ismkhan</name>
    </author>
    <author>
      <name>Kamran Zamanifar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1209.5339</arxiv:comment>
    <link href="http://arxiv.org/abs/1504.02590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.02590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1505.01980v1</id>
    <updated>2015-05-08T10:20:06Z</updated>
    <published>2015-05-08T10:20:06Z</published>
    <title>Evolving Boolean Networks with RNA Editing</title>
    <summary>  The editing of transcribed RNA by other molecules such that the form of the
final product differs from that specified in the corresponding DNA sequence is
ubiquitous. This paper uses an abstract, tunable Boolean genetic regulatory
network model to explore aspects of RNA editing. In particular, it is shown how
dynamically altering expressed sequences via a guide RNA-inspired mechanism can
be selected for by simulated evolution under various single and multicellular
scenarios.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1306.4793,
  arXiv:1303.7220</arxiv:comment>
    <link href="http://arxiv.org/abs/1505.01980v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1505.01980v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.02361v1</id>
    <updated>2015-06-08T06:41:37Z</updated>
    <published>2015-06-08T06:41:37Z</published>
    <title>Microscopic approach of a time elapsed neural model</title>
    <summary>  The spike trains are the main components of the information processing in the
brain. To model spike trains several point processes have been investigated in
the literature. And more macroscopic approaches have also been studied, using
partial differential equation models. The main aim of the present article is to
build a bridge between several point processes models (Poisson, Wold, Hawkes)
that have been proved to statistically fit real spike trains data and
age-structured partial differential equations as introduced by Pakdaman,
Perthame and Salort.
</summary>
    <author>
      <name>Julien Chevallier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">JAD</arxiv:affiliation>
    </author>
    <author>
      <name>Maria J. Caceres</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LJLL, MAMBA</arxiv:affiliation>
    </author>
    <author>
      <name>Marie Doumic</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LJLL, MAMBA</arxiv:affiliation>
    </author>
    <author>
      <name>Patricia Reynaud-Bouret</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">JAD</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/1506.02361v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.02361v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.05082v2</id>
    <updated>2015-06-29T14:25:18Z</updated>
    <published>2015-06-10T15:38:17Z</published>
    <title>A review of landmark articles in the field of co-evolutionary computing</title>
    <summary>  Coevolution is a powerful tool in evolutionary computing that mitigates some
of its endemic problems, namely stagnation in local optima and lack of
convergence in high dimensionality problems. Since its inception in 1990, there
are multiple articles that have contributed greatly to the development and
improvement of the coevolutionary techniques. In this report we review some of
those landmark articles dwelving in the techniques they propose and how they
fit to conform robust evolutionary algorithms
</summary>
    <author>
      <name>Noe Casas</name>
    </author>
    <link href="http://arxiv.org/abs/1506.05082v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.05082v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.05849v1</id>
    <updated>2015-06-18T23:11:40Z</updated>
    <published>2015-06-18T23:11:40Z</published>
    <title>An Iterative Convolutional Neural Network Algorithm Improves Electron
  Microscopy Image Segmentation</title>
    <summary>  To build the connectomics map of the brain, we developed a new algorithm that
can automatically refine the Membrane Detection Probability Maps (MDPM)
generated to perform automatic segmentation of electron microscopy (EM) images.
To achieve this, we executed supervised training of a convolutional neural
network to recover the removed center pixel label of patches sampled from a
MDPM. MDPM can be generated from other machine learning based algorithms
recognizing whether a pixel in an image corresponds to the cell membrane. By
iteratively applying this network over MDPM for multiple rounds, we were able
to significantly improve membrane segmentation results.
</summary>
    <author>
      <name>Xundong Wu</name>
    </author>
    <link href="http://arxiv.org/abs/1506.05849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.05849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.06848v1</id>
    <updated>2015-06-23T03:24:05Z</updated>
    <published>2015-06-23T03:24:05Z</published>
    <title>A Feature-Based Analysis on the Impact of Set of Constraints for
  e-Constrained Differential Evolution</title>
    <summary>  Different types of evolutionary algorithms have been developed for
constrained continuous optimization. We carry out a feature-based analysis of
evolved constrained continuous optimization instances to understand the
characteristics of constraints that make problems hard for evolutionary
algorithm. In our study, we examine how various sets of constraints can
influence the behaviour of e-Constrained Differential Evolution. Investigating
the evolved instances, we obtain knowledge of what type of constraints and
their features make a problem difficult for the examined algorithm.
</summary>
    <author>
      <name>Shayan Poursoltan</name>
    </author>
    <author>
      <name>FranK Neumann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 Pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.06848v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.06848v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.07980v1</id>
    <updated>2015-06-26T07:44:38Z</updated>
    <published>2015-06-26T07:44:38Z</published>
    <title>A Java Implementation of the SGA, UMDA, ECGA, and HBOA</title>
    <summary>  The Simple Genetic Algorithm, the Univariate Marginal Distribution Algorithm,
the Extended Compact Genetic Algorithm, and the Hierarchical Bayesian
Optimization Algorithm are all well known Evolutionary Algorithms.
  In this report we present a Java implementation of these four algorithms with
detailed instructions on how to use each of them to solve a given set of
optimization problems. Additionally, it is explained how to implement and
integrate new problems within the provided set. The source and binary files of
the Java implementations are available for free download at
https://github.com/JoseCPereira/2015EvolutionaryAlgorithmsJava.
</summary>
    <author>
      <name>José C. Pereira</name>
    </author>
    <author>
      <name>Fernando G. Lobo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1506.07980v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.07980v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1506.08004v1</id>
    <updated>2015-06-26T09:13:57Z</updated>
    <published>2015-06-26T09:13:57Z</published>
    <title>ASOC: An Adaptive Parameter-free Stochastic Optimization Techinique for
  Continuous Variables</title>
    <summary>  Stochastic optimization is an important task in many optimization problems
where the tasks are not expressible as convex optimization problems. In the
case of non-convex optimization problems, various different stochastic
algorithms like simulated annealing, evolutionary algorithms, and tabu search
are available. Most of these algorithms require user-defined parameters
specific to the problem in order to find out the optimal solution. Moreover, in
many situations, iterative fine-tunings are required for the user-defined
parameters, and therefore these algorithms cannot adapt if the search space and
the optima changes over time. In this paper we propose an \underline{a}daptive
parameter-free \underline{s}tochastic \underline{o}ptimization technique for
\underline{c}ontinuous random variables called ASOC.
</summary>
    <author>
      <name>Jayanta Basak</name>
    </author>
    <link href="http://arxiv.org/abs/1506.08004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1506.08004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.00088v1</id>
    <updated>2015-07-01T01:54:43Z</updated>
    <published>2015-07-01T01:54:43Z</published>
    <title>Evaluation of Genotypic Diversity Measurements Exploited in Real-Coded
  Representation</title>
    <summary>  Numerous genotypic diversity measures (GDMs) are available in the literature
to assess the convergence status of an evolutionary algorithm (EA) or describe
its search behavior. In a recent study, the authors of this paper drew
attention to the need for a GDM validation framework. In response, this study
proposes three requirements (monotonicity in individual varieties, twinning,
and monotonicity in distance) that can clearly portray any GDMs. These
diversity requirements are analysed by means of controlled population
arrangements. In this paper four GDMs are evaluated with the proposed
validation framework. The results confirm that properly evaluating population
diversity is a rather difficult task, as none of the analysed GDMs complies
with all the diversity requirements.
</summary>
    <author>
      <name>Guillaume Corriveau</name>
    </author>
    <author>
      <name>Raynald Guilbault</name>
    </author>
    <author>
      <name>Antoine Tahan</name>
    </author>
    <author>
      <name>Robert Sabourin</name>
    </author>
    <link href="http://arxiv.org/abs/1507.00088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.00088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.01687v1</id>
    <updated>2015-07-07T06:54:47Z</updated>
    <published>2015-07-07T06:54:47Z</published>
    <title>Developing Postfix-GP Framework for Symbolic Regression Problems</title>
    <summary>  This paper describes Postfix-GP system, postfix notation based Genetic
Programming (GP), for solving symbolic regression problems. It presents an
object-oriented architecture of Postfix-GP framework. It assists the user in
understanding of the implementation details of various components of
Postfix-GP. Postfix-GP provides graphical user interface which allows user to
configure the experiment, to visualize evolved solutions, to analyze GP run,
and to perform out-of-sample predictions. The use of Postfix-GP is demonstrated
by solving the benchmark symbolic regression problem. Finally, features of
Postfix-GP framework are compared with that of other GP systems.
</summary>
    <author>
      <name>Vipul K. Dabhi</name>
    </author>
    <author>
      <name>Sanjay Chaudhary</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCT.2015.114</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCT.2015.114" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.01687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.01687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.02491v1</id>
    <updated>2015-07-09T13:10:38Z</updated>
    <published>2015-07-09T13:10:38Z</published>
    <title>Parameter Sensitivity Analysis of Social Spider Algorithm</title>
    <summary>  Social Spider Algorithm (SSA) is a recently proposed general-purpose
real-parameter metaheuristic designed to solve global numerical optimization
problems. This work systematically benchmarks SSA on a suite of 11 functions
with different control parameters. We conduct parameter sensitivity analysis of
SSA using advanced non-parametric statistical tests to generate statistically
significant conclusion on the best performing parameter settings. The
conclusion can be adopted in future work to reduce the effort in parameter
tuning. In addition, we perform a success rate test to reveal the impact of the
control parameters on the convergence speed of the algorithm.
</summary>
    <author>
      <name>James J. Q. Yu</name>
    </author>
    <author>
      <name>Victor O. K. Li</name>
    </author>
    <link href="http://arxiv.org/abs/1507.02491v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.02491v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.02492v1</id>
    <updated>2015-07-09T13:11:13Z</updated>
    <published>2015-07-09T13:11:13Z</published>
    <title>Adaptive Chemical Reaction Optimization for Global Numerical
  Optimization</title>
    <summary>  A newly proposed chemical-reaction-inspired metaheurisic, Chemical Reaction
Optimization (CRO), has been applied to many optimization problems in both
discrete and continuous domains. To alleviate the effort in tuning parameters,
this paper reduces the number of optimization parameters in canonical CRO and
develops an adaptive scheme to evolve them. Our proposed Adaptive CRO (ACRO)
adapts better to different optimization problems. We perform simulations with
ACRO on a widely-used benchmark of continuous problems. The simulation results
show that ACRO has superior performance over canonical CRO.
</summary>
    <author>
      <name>James J. Q. Yu</name>
    </author>
    <author>
      <name>Albert Y. S. Lam</name>
    </author>
    <author>
      <name>Victor O. K. Li</name>
    </author>
    <link href="http://arxiv.org/abs/1507.02492v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.02492v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.02672v2</id>
    <updated>2015-11-24T09:22:23Z</updated>
    <published>2015-07-09T19:52:19Z</published>
    <title>Semi-Supervised Learning with Ladder Networks</title>
    <summary>  We combine supervised learning with unsupervised learning in deep neural
networks. The proposed model is trained to simultaneously minimize the sum of
supervised and unsupervised cost functions by backpropagation, avoiding the
need for layer-wise pre-training. Our work builds on the Ladder network
proposed by Valpola (2015), which we extend by combining the model with
supervision. We show that the resulting model reaches state-of-the-art
performance in semi-supervised MNIST and CIFAR-10 classification, in addition
to permutation-invariant MNIST classification with all labels.
</summary>
    <author>
      <name>Antti Rasmus</name>
    </author>
    <author>
      <name>Harri Valpola</name>
    </author>
    <author>
      <name>Mikko Honkala</name>
    </author>
    <author>
      <name>Mathias Berglund</name>
    </author>
    <author>
      <name>Tapani Raiko</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Revised denoising function, updated results, fixed typos</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.02672v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.02672v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08007v2</id>
    <updated>2016-08-26T06:59:33Z</updated>
    <published>2015-07-29T02:24:55Z</published>
    <title>On Proportions of Fit Individuals in Population of Evolutionary
  Algorithm with Tournament Selection</title>
    <summary>  In this paper, we consider a fitness-level model of a non-elitist
mutation-only evolutionary algorithm (EA) with tournament selection. The model
provides upper and lower bounds for the expected proportion of the individuals
with fitness above given thresholds. In the case of so-called monotone
mutation, the obtained bounds imply that increasing the tournament size
improves the EA performance. As corollaries, we obtain an exponentially
vanishing tail bound for the Randomized Local Search on unimodal functions and
polynomial upper bounds on the runtime of EAs on 2-SAT problem and on a family
of Set Cover problems proposed by E. Balas.
</summary>
    <author>
      <name>Anton Eremeev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submited to Evolutionary Computation journal</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.08007v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08007v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.08937v1</id>
    <updated>2015-07-31T16:38:25Z</updated>
    <published>2015-07-31T16:38:25Z</published>
    <title>Efficient and robust calibration of the Heston option pricing model for
  American options using an improved Cuckoo Search Algorithm</title>
    <summary>  In this paper an improved Cuckoo Search Algorithm is developed to allow for
an efficient and robust calibration of the Heston option pricing model for
American options. Calibration of stochastic volatility models like the Heston
is significantly harder than classical option pricing models as more parameters
have to be estimated. The difficult task of calibrating one of these models to
American Put options data is the main objective of this paper. Numerical
results are shown to substantiate the suitability of the chosen method to
tackle this problem.
</summary>
    <author>
      <name>Stefan Haring</name>
    </author>
    <author>
      <name>Ronald Hochreiter</name>
    </author>
    <link href="http://arxiv.org/abs/1507.08937v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.08937v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02774v1</id>
    <updated>2015-08-11T23:31:49Z</updated>
    <published>2015-08-11T23:31:49Z</published>
    <title>Benchmarking of LSTM Networks</title>
    <summary>  LSTM (Long Short-Term Memory) recurrent neural networks have been highly
successful in a number of application areas. This technical report describes
the use of the MNIST and UW3 databases for benchmarking LSTM networks and
explores the effect of different architectural and hyperparameter choices on
performance. Significant findings include: (1) LSTM performance depends
smoothly on learning rates, (2) batching and momentum has no significant effect
on performance, (3) softmax training outperforms least square training, (4)
peephole units are not useful, (5) the standard non-linearities (tanh and
sigmoid) perform best, (6) bidirectional training combined with CTC performs
better than other methods.
</summary>
    <author>
      <name>Thomas M. Breuel</name>
    </author>
    <link href="http://arxiv.org/abs/1508.02774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02790v1</id>
    <updated>2015-08-12T01:11:47Z</updated>
    <published>2015-08-12T01:11:47Z</published>
    <title>On the Convergence of SGD Training of Neural Networks</title>
    <summary>  Neural networks are usually trained by some form of stochastic gradient
descent (SGD)). A number of strategies are in common use intended to improve
SGD optimization, such as learning rate schedules, momentum, and batching.
These are motivated by ideas about the occurrence of local minima at different
scales, valleys, and other phenomena in the objective function. Empirical
results presented here suggest that these phenomena are not significant factors
in SGD optimization of MLP-related objective functions, and that the behavior
of stochastic gradient descent in these problems is better described as the
simultaneous convergence at different rates of many, largely non-interacting
subproblems
</summary>
    <author>
      <name>Thomas M. Breuel</name>
    </author>
    <link href="http://arxiv.org/abs/1508.02790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.02792v1</id>
    <updated>2015-08-12T01:23:35Z</updated>
    <published>2015-08-12T01:23:35Z</published>
    <title>Possible Mechanisms for Neural Reconfigurability and their Implications</title>
    <summary>  The paper introduces a biologically and evolutionarily plausible neural
architecture that allows a single group of neurons, or an entire cortical
pathway, to be dynamically reconfigured to perform multiple, potentially very
different computations. The paper shows that reconfigurability can account for
the observed stochastic and distributed coding behavior of neurons and provides
a parsimonious explanation for timing phenomena in psychophysical experiments.
It also shows that reconfigurable pathways correspond to classes of statistical
classifiers that include decision lists, decision trees, and hierarchical
Bayesian methods. Implications for the interpretation of neurophysiological and
psychophysical results are discussed, and future experiments for testing the
reconfigurability hypothesis are explored.
</summary>
    <author>
      <name>Thomas M. Breuel</name>
    </author>
    <link href="http://arxiv.org/abs/1508.02792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.02792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.05752v1</id>
    <updated>2015-08-24T10:47:47Z</updated>
    <published>2015-08-24T10:47:47Z</published>
    <title>An evolutionary approach to the identification of Cellular Automata
  based on partial observations</title>
    <summary>  In this paper we consider the identification problem of Cellular Automata
(CAs). The problem is defined and solved in the context of partial observations
with time gaps of unknown length, i.e. pre-recorded, partial configurations of
the system at certain, unknown time steps. A solution method based on a
modified variant of a Genetic Algorithm (GA) is proposed and illustrated with
brief experimental results.
</summary>
    <author>
      <name>Witold Bołt</name>
    </author>
    <author>
      <name>Jan M. Baetens</name>
    </author>
    <author>
      <name>Bernard De Baets</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE CEC 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1508.05752v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.05752v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1508.07741v1</id>
    <updated>2015-08-31T09:42:33Z</updated>
    <published>2015-08-31T09:42:33Z</published>
    <title>Model Guided Sampling Optimization for Low-dimensional Problems</title>
    <summary>  Optimization of very expensive black-box functions requires utilization of
maximum information gathered by the process of optimization. Model Guided
Sampling Optimization (MGSO) forms a more robust alternative to Jones'
Gaussian-process-based EGO algorithm. Instead of EGO's maximizing expected
improvement, the MGSO uses sampling the probability of improvement which is
shown to be helpful against trapping in local minima. Further, the MGSO can
reach close-to-optimum solutions faster than standard optimization algorithms
on low dimensional or smooth problems.
</summary>
    <author>
      <name>Lukas Bajer</name>
    </author>
    <author>
      <name>Martin Holena</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Bajer, L. &amp; Holena, M. Model Guided Sampling Optimization for
  Low-dimensional Problems. in ICAART 2015 Proceedings of the International
  Conference on Agents and Artificial Intelligence, Volume 2 451-456
  (SCITEPRESS, Lisbon, 2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1508.07741v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1508.07741v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.01126v1</id>
    <updated>2015-09-03T15:28:55Z</updated>
    <published>2015-09-03T15:28:55Z</published>
    <title>Training of CC4 Neural Network with Spread Unary Coding</title>
    <summary>  This paper adapts the corner classification algorithm (CC4) to train the
neural networks using spread unary inputs. This is an important problem as
spread unary appears to be at the basis of data representation in biological
learning. The modified CC4 algorithm is tested using the pattern classification
experiment and the results are found to be good. Specifically, we show that the
number of misclassified points is not particularly sensitive to the chosen
radius of generalization.
</summary>
    <author>
      <name>Pushpa Sree Potluri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.01126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.01126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.02512v1</id>
    <updated>2015-09-08T19:59:19Z</updated>
    <published>2015-09-08T19:59:19Z</published>
    <title>DeepCough: A Deep Convolutional Neural Network in A Wearable Cough
  Detection System</title>
    <summary>  In this paper, we present a system that employs a wearable acoustic sensor
and a deep convolutional neural network for detecting coughs. We evaluate the
performance of our system on 14 healthy volunteers and compare it to that of
other cough detection systems that have been reported in the literature.
Experimental results show that our system achieves a classification sensitivity
of 95.1% and a specificity of 99.5%.
</summary>
    <author>
      <name>Justice Amoh</name>
    </author>
    <author>
      <name>Kofi Odame</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BioCAS-2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.02512v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.02512v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.05982v2</id>
    <updated>2015-09-22T20:51:05Z</updated>
    <published>2015-09-20T09:03:48Z</published>
    <title>Denoising without access to clean data using a partitioned autoencoder</title>
    <summary>  Training a denoising autoencoder neural network requires access to truly
clean data, a requirement which is often impractical. To remedy this, we
introduce a method to train an autoencoder using only noisy data, having
examples with and without the signal class of interest. The autoencoder learns
a partitioned representation of signal and noise, learning to reconstruct each
separately. We illustrate the method by denoising birdsong audio (available
abundantly in uncontrolled noisy datasets) using a convolutional autoencoder.
</summary>
    <author>
      <name>Dan Stowell</name>
    </author>
    <author>
      <name>Richard E. Turner</name>
    </author>
    <link href="http://arxiv.org/abs/1509.05982v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.05982v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1509.08255v2</id>
    <updated>2015-10-08T16:13:44Z</updated>
    <published>2015-09-28T09:54:08Z</published>
    <title>Encoding Reality: Prediction-Assisted Cortical Learning Algorithm in
  Hierarchical Temporal Memory</title>
    <summary>  In the decade since Jeff Hawkins proposed Hierarchical Temporal Memory (HTM)
as a model of neocortical computation, the theory and the algorithms have
evolved dramatically. This paper presents a detailed description of HTM's
Cortical Learning Algorithm (CLA), including for the first time a rigorous
mathematical formulation of all aspects of the computations. Prediction
Assisted CLA (paCLA), a refinement of the CLA is presented, which is both
closer to the neuroscience and adds significantly to the computational power.
Finally, we summarise the key functions of neocortex which are expressed in
paCLA implementations.
</summary>
    <author>
      <name>Fergal Byrne</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Updated reference to unofficial revision of Hawkins and Ahmad, 2011</arxiv:comment>
    <link href="http://arxiv.org/abs/1509.08255v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1509.08255v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00419v1</id>
    <updated>2015-10-01T20:34:37Z</updated>
    <published>2015-10-01T20:34:37Z</published>
    <title>An Asynchronous Implementation of the Limited Memory CMA-ES</title>
    <summary>  We present our asynchronous implementation of the LM-CMA-ES algorithm, which
is a modern evolution strategy for solving complex large-scale continuous
optimization problems. Our implementation brings the best results when the
number of cores is relatively high and the computational complexity of the
fitness function is also high. The experiments with benchmark functions show
that it is able to overcome its origin on the Sphere function, reaches certain
thresholds faster on the Rosenbrock and Ellipsoid function, and surprisingly
performs much better than the original version on the Rastrigin function.
</summary>
    <author>
      <name>Viktor Arkhipov</name>
    </author>
    <author>
      <name>Maxim Buzdalov</name>
    </author>
    <author>
      <name>Anatoly Shalyto</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICMLA.2015.97</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICMLA.2015.97" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures, 4 tables; this is a full version of a paper which
  has been accepted as a poster to IEEE ICMLA conference 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.00419v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00419v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C56" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.02693v1</id>
    <updated>2015-10-09T15:04:11Z</updated>
    <published>2015-10-09T15:04:11Z</published>
    <title>Feedforward Sequential Memory Neural Networks without Recurrent Feedback</title>
    <summary>  We introduce a new structure for memory neural networks, called feedforward
sequential memory networks (FSMN), which can learn long-term dependency without
using recurrent feedback. The proposed FSMN is a standard feedforward neural
networks equipped with learnable sequential memory blocks in the hidden layers.
In this work, we have applied FSMN to several language modeling (LM) tasks.
Experimental results have shown that the memory blocks in FSMN can learn
effective representations of long history. Experiments have shown that FSMN
based language models can significantly outperform not only feedforward neural
network (FNN) based LMs but also the popular recurrent neural network (RNN)
LMs.
</summary>
    <author>
      <name>ShiLiang Zhang</name>
    </author>
    <author>
      <name>Hui Jiang</name>
    </author>
    <author>
      <name>Si Wei</name>
    </author>
    <author>
      <name>LiRong Dai</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.02693v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.02693v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.05711v2</id>
    <updated>2015-10-28T08:42:54Z</updated>
    <published>2015-10-19T22:38:09Z</published>
    <title>Qualitative Projection Using Deep Neural Networks</title>
    <summary>  Deep neural networks (DNN) abstract by demodulating the output of linear
filters. In this article, we refine this definition of abstraction to show that
the inputs of a DNN are abstracted with respect to the filters. Or, to restate,
the abstraction is qualified by the filters. This leads us to introduce the
notion of qualitative projection. We use qualitative projection to abstract
MNIST hand-written digits with respect to the various dogs, horses, planes and
cars of the CIFAR dataset. We then classify the MNIST digits according to the
magnitude of their dogness, horseness, planeness and carness qualities,
illustrating the generality of qualitative projection.
</summary>
    <author>
      <name>Andrew J. R. Simpson</name>
    </author>
    <link href="http://arxiv.org/abs/1510.05711v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.05711v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06248v1</id>
    <updated>2015-11-19T16:47:01Z</updated>
    <published>2015-11-19T16:47:01Z</published>
    <title>Critical Parameters in Particle Swarm Optimisation</title>
    <summary>  Particle swarm optimisation is a metaheuristic algorithm which finds
reasonable solutions in a wide range of applied problems if suitable parameters
are used. We study the properties of the algorithm in the framework of random
dynamical systems which, due to the quasi-linear swarm dynamics, yields
analytical results for the stability properties of the particles. Such
considerations predict a relationship between the parameters of the algorithm
that marks the edge between convergent and divergent behaviours. Comparison
with simulations indicates that the algorithm performs best near this margin of
instability.
</summary>
    <author>
      <name>J. Michael Herrmann</name>
    </author>
    <author>
      <name>Adam Erskine</name>
    </author>
    <author>
      <name>Thomas Joyce</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.06248v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06248v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.06987v5</id>
    <updated>2022-03-30T03:26:02Z</updated>
    <published>2015-11-22T10:05:33Z</published>
    <title>Evolutionary algorithms</title>
    <summary>  This manuscript contains an outline of lectures course "Evolutionary
Algorithms" read by the author. The course covers Canonic Genetic Algorithm and
various other genetic algorithms as well as evolutionary strategies, genetic
programming, tabu search and the class of evolutionary algorithms in general.
Some facts, such as the Rotation Property of crossover, the Schemata Theorem,
GA performance as a local search and "almost surely" convergence of
evolutionary algorithms are given with complete proofs. The text is in Russian.
</summary>
    <author>
      <name>Anton V. Eremeev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Outline of lectures course "Evolutionary Algorithms" (in Russian)</arxiv:comment>
    <link href="http://arxiv.org/abs/1511.06987v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.06987v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1511.07889v2</id>
    <updated>2015-12-17T16:30:06Z</updated>
    <published>2015-11-24T21:18:33Z</published>
    <title>rnn : Recurrent Library for Torch</title>
    <summary>  The rnn package provides components for implementing a wide range of
Recurrent Neural Networks. It is built withing the framework of the Torch
distribution for use with the nn package. The components have evolved from 3
iterations, each adding to the flexibility and capability of the package. All
component modules inherit either the AbstractRecurrent or AbstractSequencer
classes. Strong unit testing, continued backwards compatibility and access to
supporting material are the principles followed during its development. The
package is compared against existing implementations of two published papers.
</summary>
    <author>
      <name>Nicholas Léonard</name>
    </author>
    <author>
      <name>Sagar Waghmare</name>
    </author>
    <author>
      <name>Yang Wang</name>
    </author>
    <author>
      <name>Jin-Hwa Kim</name>
    </author>
    <link href="http://arxiv.org/abs/1511.07889v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1511.07889v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01332v1</id>
    <updated>2015-12-04T07:51:48Z</updated>
    <published>2015-12-04T07:51:48Z</published>
    <title>Q-Networks for Binary Vector Actions</title>
    <summary>  In this paper reinforcement learning with binary vector actions was
investigated. We suggest an effective architecture of the neural networks for
approximating an action-value function with binary vector actions. The proposed
architecture approximates the action-value function by a linear function with
respect to the action vector, but is still non-linear with respect to the state
input. We show that this approximation method enables the efficient calculation
of greedy action selection and softmax action selection. Using this
architecture, we suggest an online algorithm based on Q-learning. The empirical
results in the grid world and the blocker task suggest that our approximation
architecture would be effective for the RL problems with large discrete action
sets.
</summary>
    <author>
      <name>Naoto Yoshida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures, accepted for Deep Reinforcement Learning
  Workshop, NIPS 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.01332v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01332v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.01596v3</id>
    <updated>2016-04-22T03:20:41Z</updated>
    <published>2015-12-04T23:58:47Z</published>
    <title>Creation of a Deep Convolutional Auto-Encoder in Caffe</title>
    <summary>  The development of a deep (stacked) convolutional auto-encoder in the Caffe
deep learning framework is presented in this paper. We describe simple
principles which we used to create this model in Caffe. The proposed model of
convolutional auto-encoder does not have pooling/unpooling layers yet. The
results of our experimental research show comparable accuracy of dimensionality
reduction in comparison with a classic auto-encoder on the example of MNIST
dataset.
</summary>
    <author>
      <name>Volodymyr Turchenko</name>
    </author>
    <author>
      <name>Artur Luczak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 7 figures, 5 tables, 34 references in the list; Added
  references, corrected Table 3, changed several paragraphs in the text</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.01596v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.01596v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02047v2</id>
    <updated>2015-12-09T03:22:10Z</updated>
    <published>2015-12-07T14:03:43Z</published>
    <title>Level-Based Analysis of Genetic Algorithms for Combinatorial
  Optimization</title>
    <summary>  The paper is devoted to upper bounds on run-time of Non-Elitist Genetic
Algorithms until some target subset of solutions is visited for the first time.
In particular, we consider the sets of optimal solutions and the sets of local
optima as the target subsets. Previously known upper bounds are improved by
means of drift analysis. Finally, we propose conditions ensuring that a
Non-Elitist Genetic Algorithm efficiently finds approximate solutions with
constant approximation ratio on the class of combinatorial optimization
problems with guaranteed local optima (GLO).
</summary>
    <author>
      <name>Duc-Cuong Dang</name>
    </author>
    <author>
      <name>Anton V. Eremeev</name>
    </author>
    <author>
      <name>Per Kristian Lehre</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.02047v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02047v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.02100v1</id>
    <updated>2015-12-07T15:53:48Z</updated>
    <published>2015-12-07T15:53:48Z</published>
    <title>Digital Genesis: Computers, Evolution and Artificial Life</title>
    <summary>  The application of evolution in the digital realm, with the goal of creating
artificial intelligence and artificial life, has a history as long as that of
the digital computer itself. We illustrate the intertwined history of these
ideas, starting with the early theoretical work of John von Neumann and the
pioneering experimental work of Nils Aall Barricelli. We argue that
evolutionary thinking and artificial life will continue to play an integral
role in the future development of the digital world.
</summary>
    <author>
      <name>Tim Taylor</name>
    </author>
    <author>
      <name>Alan Dorin</name>
    </author>
    <author>
      <name>Kevin Korb</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended abstract of talk presented at the 7th Munich-Sydney-Tilburg
  Philosophy of Science Conference: Evolutionary Thinking, University of
  Sydney, 20-22 March 2014. Presentation slides from talk available at
  http://www.tim-taylor.com/papers/digital-genesis-presentation.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.02100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.02100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1512.05509v1</id>
    <updated>2015-12-17T09:45:51Z</updated>
    <published>2015-12-17T09:45:51Z</published>
    <title>An Empirical Comparison of Neural Architectures for Reinforcement
  Learning in Partially Observable Environments</title>
    <summary>  This paper explores the performance of fitted neural Q iteration for
reinforcement learning in several partially observable environments, using
three recurrent neural network architectures: Long Short-Term Memory, Gated
Recurrent Unit and MUT1, a recurrent neural architecture evolved from a pool of
several thousands candidate architectures. A variant of fitted Q iteration,
based on Advantage values instead of Q values, is also explored. The results
show that GRU performs significantly better than LSTM and MUT1 for most of the
problems considered, requiring less training episodes and less CPU time before
learning a very good policy. Advantage learning also tends to produce better
results.
</summary>
    <author>
      <name>Denis Steckelmacher</name>
    </author>
    <author>
      <name>Peter Vrancx</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented at the 27th Benelux Conference on Artificial Intelligence</arxiv:comment>
    <link href="http://arxiv.org/abs/1512.05509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1512.05509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.03809v1</id>
    <updated>2015-11-03T07:21:44Z</updated>
    <published>2015-11-03T07:21:44Z</published>
    <title>Artificial neural network approach for condition-based maintenance</title>
    <summary>  In this research, computerized maintenance management will be investigated.
The rise of maintenance cost forced the research community to look for more
effective ways to schedule maintenance operations. Using computerized models to
come up with optimal maintenance policy has led to better equipment utilization
and lower costs. This research adopts Condition-Based Maintenance model where
the maintenance decision is generated based on equipment conditions. Artificial
Neural Network technique is proposed to capture and analyze equipment condition
signals which lead to higher level of knowledge gathering. This knowledge is
used to accurately estimate equipment failure time. Based on these estimations,
an optimal maintenance management policy can be achieved.
</summary>
    <author>
      <name>Mostafa Sayyed</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">108 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1601.03809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.03809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.06580v1</id>
    <updated>2016-01-25T12:49:28Z</updated>
    <published>2016-01-25T12:49:28Z</published>
    <title>Is swarm intelligence able to create mazes?</title>
    <summary>  In this paper, the idea of applying Computational Intelligence in the process
of creation board games, in particular mazes, is presented. For two different
algorithms the proposed idea has been examined. The results of the experiments
are shown and discussed to present advantages and disadvantages.
</summary>
    <author>
      <name>Dawid Polap</name>
    </author>
    <author>
      <name>Marcin Wozniak</name>
    </author>
    <author>
      <name>Christian Napoli</name>
    </author>
    <author>
      <name>Emiliano Tramontana</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1515/eletel-2015-0039</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1515/eletel-2015-0039" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Electronics and Telecommunications, Vol.
  6, n. 4, pp. 305-310 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.06580v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.06580v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05, 68T10, 68T45, 68U10, 68W25, 68W99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.10; I.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1601.07446v1</id>
    <updated>2016-01-27T16:54:51Z</updated>
    <published>2016-01-27T16:54:51Z</published>
    <title>A First Attempt to Cloud-Based User Verification in Distributed System</title>
    <summary>  In this paper, the idea of client verification in distributed systems is
presented. The proposed solution presents a sample system where client
verification through cloud resources using input signature is discussed. For
different signatures the proposed method has been examined. Research results
are presented and discussed to show potential advantages.
</summary>
    <author>
      <name>Marcin Wozniak</name>
    </author>
    <author>
      <name>Dawid Polap</name>
    </author>
    <author>
      <name>Grzegorz Borowik</name>
    </author>
    <author>
      <name>Christian Napoli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/APCASE.2015.47</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/APCASE.2015.47" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final version published on: Asia-Pacific Conference on Computer Aided
  System Engineering (APCASE), pp. 226-231 (2015)</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Asia-Pacific Conference on Computer Aided System Engineering
  (APCASE), pp. 226-231 (2015)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1601.07446v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1601.07446v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05, 68T10, 68T45, 68U10, 68W25, 68W99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6, I.2.10, I.4.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.01321v1</id>
    <updated>2016-02-03T14:46:35Z</updated>
    <published>2016-02-03T14:46:35Z</published>
    <title>A continuum among logarithmic, linear, and exponential functions, and
  its potential to improve generalization in neural networks</title>
    <summary>  We present the soft exponential activation function for artificial neural
networks that continuously interpolates between logarithmic, linear, and
exponential functions. This activation function is simple, differentiable, and
parameterized so that it can be trained as the rest of the network is trained.
We hypothesize that soft exponential has the potential to improve neural
network learning, as it can exactly calculate many natural operations that
typical neural networks can only approximate, including addition,
multiplication, inner product, distance, polynomials, and sinusoids.
</summary>
    <author>
      <name>Luke B. Godfrey</name>
    </author>
    <author>
      <name>Michael S. Gashler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 8 figures, conference, In Proceedings of Knowledge Discovery
  and Information Retrieval (KDIR) 2015, Lisbon, Portugal, December 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.01321v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.01321v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.03032v2</id>
    <updated>2016-05-19T14:18:19Z</updated>
    <published>2016-02-09T15:26:26Z</published>
    <title>Associative Long Short-Term Memory</title>
    <summary>  We investigate a new method to augment recurrent neural networks with extra
memory without increasing the number of network parameters. The system has an
associative memory based on complex-valued vectors and is closely related to
Holographic Reduced Representations and Long Short-Term Memory networks.
Holographic Reduced Representations have limited capacity: as they store more
information, each retrieval becomes noisier due to interference. Our system in
contrast creates redundant copies of stored information, which enables
retrieval with reduced noise. Experiments demonstrate faster learning on
multiple memorization tasks.
</summary>
    <author>
      <name>Ivo Danihelka</name>
    </author>
    <author>
      <name>Greg Wayne</name>
    </author>
    <author>
      <name>Benigno Uria</name>
    </author>
    <author>
      <name>Nal Kalchbrenner</name>
    </author>
    <author>
      <name>Alex Graves</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICML-2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.03032v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.03032v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.04335v1</id>
    <updated>2016-02-13T14:09:41Z</updated>
    <published>2016-02-13T14:09:41Z</published>
    <title>Learning Over Long Time Lags</title>
    <summary>  The advantage of recurrent neural networks (RNNs) in learning dependencies
between time-series data has distinguished RNNs from other deep learning
models. Recently, many advances are proposed in this emerging field. However,
there is a lack of comprehensive review on memory models in RNNs in the
literature. This paper provides a fundamental review on RNNs and long short
term memory (LSTM) model. Then, provides a surveys of recent advances in
different memory enhancements and learning techniques for capturing long term
dependencies in RNNs.
</summary>
    <author>
      <name>Hojjat Salehinejad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a draft article, in preparation to submit for peer-review</arxiv:comment>
    <link href="http://arxiv.org/abs/1602.04335v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.04335v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.04723v1</id>
    <updated>2016-02-15T16:16:56Z</updated>
    <published>2016-02-15T16:16:56Z</published>
    <title>Efficient Representation of Low-Dimensional Manifolds using Deep
  Networks</title>
    <summary>  We consider the ability of deep neural networks to represent data that lies
near a low-dimensional manifold in a high-dimensional space. We show that deep
networks can efficiently extract the intrinsic, low-dimensional coordinates of
such data. We first show that the first two layers of a deep network can
exactly embed points lying on a monotonic chain, a special type of piecewise
linear manifold, mapping them to a low-dimensional Euclidean space. Remarkably,
the network can do this using an almost optimal number of parameters. We also
show that this network projects nearby points onto the manifold and then embeds
them with little error. We then extend these results to more general manifolds.
</summary>
    <author>
      <name>Ronen Basri</name>
    </author>
    <author>
      <name>David Jacobs</name>
    </author>
    <link href="http://arxiv.org/abs/1602.04723v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.04723v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.06057v1</id>
    <updated>2016-02-19T07:09:33Z</updated>
    <published>2016-02-19T07:09:33Z</published>
    <title>Uniresolution representations of white-matter data from CoCoMac</title>
    <summary>  Tracing data as collated by CoCoMac, a seminal neuroinformatics database, is
at multiple resolutions -- white matter tracts were studied for areas and their
subdivisions by different reports. Network theoretic analysis of this
multi-resolution data often assumes that the data at various resolutions is
equivalent, which may not be correct. In this paper we propose three methods to
resolve the multi-resolution issue such that the resultant networks have
connectivity data at only one resolution. The different resultant networks are
compared in terms of their network analysis metrics and degree distributions.
</summary>
    <author>
      <name>Raghavendra Singh</name>
    </author>
    <link href="http://arxiv.org/abs/1602.06057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.06057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1602.07884v1</id>
    <updated>2016-02-25T11:04:09Z</updated>
    <published>2016-02-25T11:04:09Z</published>
    <title>Firefly Algorithm for optimization problems with non-continuous
  variables: A Review and Analysis</title>
    <summary>  Firefly algorithm is a swarm based metaheuristic algorithm inspired by the
flashing behavior of fireflies. It is an effective and an easy to implement
algorithm. It has been tested on different problems from different disciplines
and found to be effective. Even though the algorithm is proposed for
optimization problems with continuous variables, it has been modified and used
for problems with non-continuous variables, including binary and integer valued
problems. In this paper a detailed review of this modifications of firefly
algorithm for problems with non-continuous variables will be discussed. The
strength and weakness of the modifications along with possible future works
will be presented.
</summary>
    <author>
      <name>Surafel Luleseged Tilahun</name>
    </author>
    <author>
      <name>Jean Medard T Ngnotchouye</name>
    </author>
    <link href="http://arxiv.org/abs/1602.07884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1602.07884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.00930v2</id>
    <updated>2016-03-08T21:26:58Z</updated>
    <published>2016-03-02T23:44:03Z</published>
    <title>Super Mario as a String: Platformer Level Generation Via LSTMs</title>
    <summary>  The procedural generation of video game levels has existed for at least 30
years, but only recently have machine learning approaches been used to generate
levels without specifying the rules for generation. A number of these have
looked at platformer levels as a sequence of characters and performed
generation using Markov chains. In this paper we examine the use of Long
Short-Term Memory recurrent neural networks (LSTMs) for the purpose of
generating levels trained from a corpus of Super Mario Brothers levels. We
analyze a number of different data representations and how the generated levels
fit into the space of human authored Super Mario Brothers levels.
</summary>
    <author>
      <name>Adam Summerville</name>
    </author>
    <author>
      <name>Michael Mateas</name>
    </author>
    <link href="http://arxiv.org/abs/1603.00930v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.00930v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08551v1</id>
    <updated>2016-03-28T20:28:09Z</updated>
    <published>2016-03-28T20:28:09Z</published>
    <title>Genetic cellular neural networks for generating three-dimensional
  geometry</title>
    <summary>  There are a number of ways to procedurally generate interesting
three-dimensional shapes, and a method where a cellular neural network is
combined with a mesh growth algorithm is presented here. The aim is to create a
shape from a genetic code in such a way that a crude search can find
interesting shapes. Identical neural networks are placed at each vertex of a
mesh which can communicate with neural networks on neighboring vertices. The
output of the neural networks determine how the mesh grows, allowing
interesting shapes to be produced emergently, mimicking some of the complexity
of biological organism development. Since the neural networks' parameters can
be freely mutated, the approach is amenable for use in a genetic algorithm.
</summary>
    <author>
      <name>Hugo Martay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.08551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.09002v2</id>
    <updated>2018-05-28T17:37:54Z</updated>
    <published>2016-03-29T23:48:27Z</published>
    <title>Dataflow Matrix Machines as a Generalization of Recurrent Neural
  Networks</title>
    <summary>  Dataflow matrix machines are a powerful generalization of recurrent neural
networks. They work with multiple types of arbitrary linear streams, multiple
types of powerful neurons, and allow to incorporate higher-order constructions.
We expect them to be useful in machine learning and probabilistic programming,
and in the synthesis of dynamic systems and of deterministic and probabilistic
programs.
</summary>
    <author>
      <name>Michael Bukatin</name>
    </author>
    <author>
      <name>Steve Matthews</name>
    </author>
    <author>
      <name>Andrey Radul</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages position paper (v2 - update references)</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.09002v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.09002v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.00462v1</id>
    <updated>2016-04-02T05:04:12Z</updated>
    <published>2016-04-02T05:04:12Z</published>
    <title>Centralized and Decentralized Global Outer-synchronization of Asymmetric
  Recurrent Time-varying Neural Network by Data-sampling</title>
    <summary>  In this paper, we discuss the outer-synchronization of the asymmetrically
connected recurrent time-varying neural networks. By both centralized and
decentralized discretization data sampling principles, we derive several
sufficient conditions based on diverse vector norms that guarantee that any two
trajectories from different initial values of the identical neural network
system converge together. The lower bounds of the common time intervals between
data samples in centralized and decentralized principles are proved to be
positive, which guarantees exclusion of Zeno behavior. A numerical example is
provided to illustrate the efficiency of the theoretical results.
</summary>
    <author>
      <name>Wenlian Lu</name>
    </author>
    <author>
      <name>Ren Zheng</name>
    </author>
    <author>
      <name>Tianping Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.00462v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.00462v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.01088v2</id>
    <updated>2016-07-29T00:14:24Z</updated>
    <published>2016-04-04T23:19:00Z</published>
    <title>Optimal Parameter Settings for the $(1+(λ, λ))$ Genetic
  Algorithm</title>
    <summary>  The $(1+(\lambda,\lambda))$ genetic algorithm is one of the few algorithms
for which a super-constant speed-up through the use of crossover could be
proven. So far, this algorithm has been used with parameters based also on
intuitive considerations. In this work, we rigorously regard the whole
parameter space and show that the asymptotic time complexity proven by Doerr
and Doerr (GECCO 2015) for the intuitive choice is best possible among all
settings for population size, mutation probability, and crossover bias.
</summary>
    <author>
      <name>Benjamin Doerr</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Extended version of a paper that appeared at GECCO'16</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.01088v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.01088v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.02313v5</id>
    <updated>2017-01-31T21:24:36Z</updated>
    <published>2016-04-08T11:39:31Z</published>
    <title>Norm-preserving Orthogonal Permutation Linear Unit Activation Functions
  (OPLU)</title>
    <summary>  We propose a novel activation function that implements piece-wise orthogonal
non-linear mappings based on permutations. It is straightforward to implement,
and very computationally efficient, also it has little memory requirements. We
tested it on two toy problems for feedforward and recurrent networks, it shows
similar performance to tanh and ReLU. OPLU activation function ensures norm
preservance of the backpropagated gradients, therefore it is potentially good
for the training of deep, extra deep, and recurrent neural networks.
</summary>
    <author>
      <name>Artem Chernodub</name>
    </author>
    <author>
      <name>Dimitri Nowicki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to conference ICANN'2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.02313v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.02313v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.05008v1</id>
    <updated>2016-04-18T06:29:01Z</updated>
    <published>2016-04-18T06:29:01Z</published>
    <title>Forecasting Volatility in Indian Stock Market using Artificial Neural
  Network with Multiple Inputs and Outputs</title>
    <summary>  Volatility in stock markets has been extensively studied in the applied
finance literature. In this paper, Artificial Neural Network models based on
various back propagation algorithms have been constructed to predict volatility
in the Indian stock market through volatility of NIFTY returns and volatility
of gold returns. This model considers India VIX, CBOE VIX, volatility of crude
oil returns (CRUDESDR), volatility of DJIA returns (DJIASDR), volatility of DAX
returns (DAXSDR), volatility of Hang Seng returns (HANGSDR) and volatility of
Nikkei returns (NIKKEISDR) as predictor variables. Three sets of experiments
have been performed over three time periods to judge the effectiveness of the
approach.
</summary>
    <author>
      <name>Tamal Datta Chaudhuri</name>
    </author>
    <author>
      <name>Indranil Ghosh</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5120/21245-4034</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5120/21245-4034" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Applications, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1604.05008v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.05008v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.06187v1</id>
    <updated>2016-04-21T05:47:05Z</updated>
    <published>2016-04-21T05:47:05Z</published>
    <title>Evolutionary Image Transition Based on Theoretical Insights of Random
  Processes</title>
    <summary>  Evolutionary algorithms have been widely studied from a theoretical
perspective. In particular, the area of runtime analysis has contributed
significantly to a theoretical understanding and provided insights into the
working behaviour of these algorithms. We study how these insights into
evolutionary processes can be used for evolutionary art. We introduce the
notion of evolutionary image transition which transfers a given starting image
into a target image through an evolutionary process. Combining standard
mutation effects known from the optimization of the classical benchmark
function OneMax and different variants of random walks, we present ways of
performing evolutionary image transition with different artistic effects.
</summary>
    <author>
      <name>Aneta Neumann</name>
    </author>
    <author>
      <name>Bradley Alexander</name>
    </author>
    <author>
      <name>Frank Neumann</name>
    </author>
    <link href="http://arxiv.org/abs/1604.06187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.06187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07108v1</id>
    <updated>2016-04-25T02:18:45Z</updated>
    <published>2016-04-25T02:18:45Z</published>
    <title>Modeling the Evolution of Gene-Culture Divergence</title>
    <summary>  We present a model for evolving agents using both genetic and cultural
inheritance mechanisms. Within each agent our model maintains two distinct
information stores we call the genome and the memome. Processes of adaptation
are modeled as evolutionary processes at each level of adaptation
(phylogenetic, ontogenetic, sociogenetic). We review relevant competing models
and we show how our model improves on previous attempts to model genetic and
cultural evolutionary processes. In particular we argue our model can achieve
divergent gene-culture co-evolution.
</summary>
    <author>
      <name>Chris Marriott</name>
    </author>
    <author>
      <name>Jobran Chebib</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, ALIFE 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07269v1</id>
    <updated>2016-04-25T14:17:08Z</updated>
    <published>2016-04-25T14:17:08Z</published>
    <title>CMA-ES for Hyperparameter Optimization of Deep Neural Networks</title>
    <summary>  Hyperparameters of deep neural networks are often optimized by grid search,
random search or Bayesian optimization. As an alternative, we propose to use
the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), which is known
for its state-of-the-art performance in derivative-free optimization. CMA-ES
has some useful invariance properties and is friendly to parallel evaluations
of solutions. We provide a toy example comparing CMA-ES and state-of-the-art
Bayesian optimization algorithms for tuning the hyperparameters of a
convolutional neural network for the MNIST dataset on 30 GPUs in parallel.
</summary>
    <author>
      <name>Ilya Loshchilov</name>
    </author>
    <author>
      <name>Frank Hutter</name>
    </author>
    <link href="http://arxiv.org/abs/1604.07269v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07269v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1604.07796v1</id>
    <updated>2016-04-26T19:04:59Z</updated>
    <published>2016-04-26T19:04:59Z</published>
    <title>Scale Normalization</title>
    <summary>  One of the difficulties of training deep neural networks is caused by
improper scaling between layers. Scaling issues introduce exploding / gradient
problems, and have typically been addressed by careful scale-preserving
initialization. We investigate the value of preserving scale, or isometry,
beyond the initial weights. We propose two methods of maintaing isometry, one
exact and one stochastic. Preliminary experiments show that for both
determinant and scale-normalization effectively speeds up learning. Results
suggest that isometry is important in the beginning of learning, and
maintaining it leads to faster learning.
</summary>
    <author>
      <name>Henry Z. Lo</name>
    </author>
    <author>
      <name>Kevin Amaral</name>
    </author>
    <author>
      <name>Wei Ding</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preliminary version submitted to ICLR workshop 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1604.07796v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1604.07796v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01514v1</id>
    <updated>2016-05-05T07:30:02Z</updated>
    <published>2016-05-05T07:30:02Z</published>
    <title>Fitness-based Adaptive Control of Parameters in Genetic Programming:
  Adaptive Value Setting of Mutation Rate and Flood Mechanisms</title>
    <summary>  This paper concerns applications of genetic algorithms and genetic
programming to tasks for which it is difficult to find a representation that
does not map to a highly complex and discontinuous fitness landscape. In such
cases the standard algorithm is prone to getting trapped in local extremes. The
paper proposes several adaptive mechanisms that are useful in preventing the
search from getting trapped.
</summary>
    <author>
      <name>Michal Gregor</name>
    </author>
    <author>
      <name>Juraj Spalek</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in 2011 IEEE International Conference on Intelligent Computing and
  Intelligent Systems (ICIS 2011)</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01746v1</id>
    <updated>2016-05-05T20:15:47Z</updated>
    <published>2016-05-05T20:15:47Z</published>
    <title>Biobjective Performance Assessment with the COCO Platform</title>
    <summary>  This document details the rationales behind assessing the performance of
numerical black-box optimizers on multi-objective problems within the COCO
platform and in particular on the biobjective test suite bbob-biobj. The
evaluation is based on a hypervolume of all non-dominated solutions in the
archive of candidate solutions and measures the runtime until the hypervolume
value succeeds prescribed target values.
</summary>
    <author>
      <name>Dimo Brockhoff</name>
    </author>
    <author>
      <name>Tea Tušar</name>
    </author>
    <author>
      <name>Dejan Tušar</name>
    </author>
    <author>
      <name>Tobias Wagner</name>
    </author>
    <author>
      <name>Nikolaus Hansen</name>
    </author>
    <author>
      <name>Anne Auger</name>
    </author>
    <link href="http://arxiv.org/abs/1605.01746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.01988v3</id>
    <updated>2017-03-30T18:24:55Z</updated>
    <published>2016-05-06T16:11:45Z</published>
    <title>LSTM with Working Memory</title>
    <summary>  Previous RNN architectures have largely been superseded by LSTM, or "Long
Short-Term Memory". Since its introduction, there have been many variations on
this simple design. However, it is still widely used and we are not aware of a
gated-RNN architecture that outperforms LSTM in a broad sense while still being
as simple and efficient. In this paper we propose a modified LSTM-like
architecture. Our architecture is still simple and achieves better performance
on the tasks that we tested on. We also introduce a new RNN performance
benchmark that uses the handwritten digits and stresses several important
network capabilities.
</summary>
    <author>
      <name>Andrew Pulver</name>
    </author>
    <author>
      <name>Siwei Lyu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at IJCNN 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.01988v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.01988v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02486v1</id>
    <updated>2016-05-09T09:12:11Z</updated>
    <published>2016-05-09T09:12:11Z</published>
    <title>Efficiency Evaluation of Character-level RNN Training Schedules</title>
    <summary>  We present four training and prediction schedules from the same
character-level recurrent neural network. The efficiency of these schedules is
tested in terms of model effectiveness as a function of training time and
amount of training data seen. We show that the choice of training and
prediction schedule potentially has a considerable impact on the prediction
effectiveness for a given training budget.
</summary>
    <author>
      <name>Cedric De Boom</name>
    </author>
    <author>
      <name>Sam Leroux</name>
    </author>
    <author>
      <name>Steven Bohez</name>
    </author>
    <author>
      <name>Pieter Simoens</name>
    </author>
    <author>
      <name>Thomas Demeester</name>
    </author>
    <author>
      <name>Bart Dhoedt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02486v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02486v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.02720v1</id>
    <updated>2016-05-09T19:58:29Z</updated>
    <published>2016-05-09T19:58:29Z</published>
    <title>Anytime Bi-Objective Optimization with a Hybrid Multi-Objective CMA-ES
  (HMO-CMA-ES)</title>
    <summary>  We propose a multi-objective optimization algorithm aimed at achieving good
anytime performance over a wide range of problems. Performance is assessed in
terms of the hypervolume metric. The algorithm called HMO-CMA-ES represents a
hybrid of several old and new variants of CMA-ES, complemented by BOBYQA as a
warm start. We benchmark HMO-CMA-ES on the recently introduced bi-objective
problem suite of the COCO framework (COmparing Continuous Optimizers),
consisting of 55 scalable continuous optimization problems, which is used by
the Black-Box Optimization Benchmarking (BBOB) Workshop 2016.
</summary>
    <author>
      <name>Ilya Loshchilov</name>
    </author>
    <author>
      <name>Tobias Glasmachers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">BBOB workshop of GECCO'2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.02720v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.02720v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03560v1</id>
    <updated>2016-05-11T19:49:43Z</updated>
    <published>2016-05-11T19:49:43Z</published>
    <title>COCO: Performance Assessment</title>
    <summary>  We present an any-time performance assessment for benchmarking numerical
optimization algorithms in a black-box scenario, applied within the COCO
benchmarking platform. The performance assessment is based on runtimes measured
in number of objective function evaluations to reach one or several quality
indicator target values. We argue that runtime is the only available measure
with a generic, meaningful, and quantitative interpretation. We discuss the
choice of the target values, runlength-based targets, and the aggregation of
results by using simulated restarts, averages, and empirical distribution
functions.
</summary>
    <author>
      <name>Nikolaus Hansen</name>
    </author>
    <author>
      <name>Anne Auger</name>
    </author>
    <author>
      <name>Dimo Brockhoff</name>
    </author>
    <author>
      <name>Dejan Tušar</name>
    </author>
    <author>
      <name>Tea Tušar</name>
    </author>
    <link href="http://arxiv.org/abs/1605.03560v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03560v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.03764v1</id>
    <updated>2016-05-12T11:39:14Z</updated>
    <published>2016-05-12T11:39:14Z</published>
    <title>Direct Method for Training Feed-forward Neural Networks using Batch
  Extended Kalman Filter for Multi-Step-Ahead Predictions</title>
    <summary>  This paper is dedicated to the long-term, or multi-step-ahead, time series
prediction problem. We propose a novel method for training feed-forward neural
networks, such as multilayer perceptrons, with tapped delay lines. Special
batch calculation of derivatives called Forecasted Propagation Through Time and
batch modification of the Extended Kalman Filter are introduced. Experiments
were carried out on well-known time series benchmarks, the Mackey-Glass chaotic
process and the Santa Fe Laser Data Series. Recurrent and feed-forward neural
networks were evaluated.
</summary>
    <author>
      <name>Artem Chernodub</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of ICANN'2013-LCNS Series, Volume 8131.
  Springer-Verlag New York, Inc., 2013, P. 138-145</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1605.03764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.03764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05296v2</id>
    <updated>2018-06-21T02:24:27Z</updated>
    <published>2016-05-17T19:29:37Z</published>
    <title>Dataflow matrix machines as programmable, dynamically expandable,
  self-referential generalized recurrent neural networks</title>
    <summary>  Dataflow matrix machines are a powerful generalization of recurrent neural
networks. They work with multiple types of linear streams and multiple types of
neurons, including higher-order neurons which dynamically update the matrix
describing weights and topology of the network in question while the network is
running. It seems that the power of dataflow matrix machines is sufficient for
them to be a convenient general purpose programming platform. This paper
explores a number of useful programming idioms and constructions arising in
this context.
</summary>
    <author>
      <name>Michael Bukatin</name>
    </author>
    <author>
      <name>Steve Matthews</name>
    </author>
    <author>
      <name>Andrey Radul</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages (v2 - update references)</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.05296v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05296v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.05448v1</id>
    <updated>2016-05-18T05:53:44Z</updated>
    <published>2016-05-18T05:53:44Z</published>
    <title>The Bees Algorithm for the Vehicle Routing Problem</title>
    <summary>  In this thesis we present a new algorithm for the Vehicle Routing Problem
called the Enhanced Bees Algorithm. It is adapted from a fairly recent
algorithm, the Bees Algorithm, which was developed for continuous optimisation
problems. We show that the results obtained by the Enhanced Bees Algorithm are
competitive with the best meta-heuristics available for the Vehicle Routing
Problem (within 0.5% of the optimal solution for common benchmark problems). We
show that the algorithm has good runtime performance, producing results within
2% of the optimal solution within 60 seconds, making it suitable for use within
real world dispatch scenarios.
</summary>
    <author>
      <name>Aish Fenton</name>
    </author>
    <link href="http://arxiv.org/abs/1605.05448v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.05448v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.06710v1</id>
    <updated>2016-05-21T23:45:38Z</updated>
    <published>2016-05-21T23:45:38Z</published>
    <title>Chess Player by Co-Evolutionary Algorithm</title>
    <summary>  A co-evolutionary algorithm (CA) based chess player is presented.
Implementation details of the algorithms, namely coding, population, variation
operators are described. The alpha-beta or mini-max like behaviour of the
player is achieved through two competitive or cooperative populations. Special
attention is given to the fitness function evaluation (the heart of the
solution). Test results on algorithms vs. algorithms or human player is
provided.
</summary>
    <author>
      <name>Nuno Ramos</name>
    </author>
    <author>
      <name>Sergio Salgado</name>
    </author>
    <author>
      <name>Agostinho C Rosa</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 11 figures and 12 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.06710v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.06710v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.00540v1</id>
    <updated>2016-06-02T05:39:54Z</updated>
    <published>2016-06-02T05:39:54Z</published>
    <title>Multi-pretrained Deep Neural Network</title>
    <summary>  Pretraining is widely used in deep neutral network and one of the most famous
pretraining models is Deep Belief Network (DBN). The optimization formulas are
different during the pretraining process for different pretraining models. In
this paper, we pretrained deep neutral network by different pretraining models
and hence investigated the difference between DBN and Stacked Denoising
Autoencoder (SDA) when used as pretraining model. The experimental results show
that DBN get a better initial model. However the model converges to a
relatively worse model after the finetuning process. Yet after pretrained by
SDA for the second time the model converges to a better model if finetuned.
</summary>
    <author>
      <name>Zhen Hu</name>
    </author>
    <author>
      <name>Zhuyin Xue</name>
    </author>
    <author>
      <name>Tong Cui</name>
    </author>
    <author>
      <name>Shiqiang Zong</name>
    </author>
    <author>
      <name>Chenglong He</name>
    </author>
    <link href="http://arxiv.org/abs/1606.00540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.00540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.02407v1</id>
    <updated>2016-06-08T05:31:43Z</updated>
    <published>2016-06-08T05:31:43Z</published>
    <title>Structured Convolution Matrices for Energy-efficient Deep learning</title>
    <summary>  We derive a relationship between network representation in energy-efficient
neuromorphic architectures and block Toplitz convolutional matrices. Inspired
by this connection, we develop deep convolutional networks using a family of
structured convolutional matrices and achieve state-of-the-art trade-off
between energy efficiency and classification accuracy for well-known image
recognition tasks. We also put forward a novel method to train binary
convolutional networks by utilising an existing connection between
noisy-rectified linear units and binary activations.
</summary>
    <author>
      <name>Rathinakumar Appuswamy</name>
    </author>
    <author>
      <name>Tapan Nayak</name>
    </author>
    <author>
      <name>John Arthur</name>
    </author>
    <author>
      <name>Steven Esser</name>
    </author>
    <author>
      <name>Paul Merolla</name>
    </author>
    <author>
      <name>Jeffrey Mckinstry</name>
    </author>
    <author>
      <name>Timothy Melano</name>
    </author>
    <author>
      <name>Myron Flickner</name>
    </author>
    <author>
      <name>Dharmendra Modha</name>
    </author>
    <link href="http://arxiv.org/abs/1606.02407v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.02407v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.03674v2</id>
    <updated>2017-03-06T13:34:29Z</updated>
    <published>2016-06-12T07:22:58Z</published>
    <title>Critical Echo State Networks that Anticipate Input using Morphable
  Transfer Functions</title>
    <summary>  The paper investigates a new type of truly critical echo state networks where
individual transfer functions for every neuron can be modified to anticipate
the expected next input. Deviations from expected input are only forgotten
slowly in power law fashion. The paper outlines the theory, numerically
analyzes a one neuron model network and finally discusses technical and also
biological implications of this type of approach.
</summary>
    <author>
      <name>Norbert Michael Mayer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14th International Symposium on Neural Networks (ISNN), Sapporo,
  Hakodate, Japan, June 21st - 26th 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.03674v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.03674v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.04217v1</id>
    <updated>2016-06-14T07:04:37Z</updated>
    <published>2016-06-14T07:04:37Z</published>
    <title>Word Representation Models for Morphologically Rich Languages in Neural
  Machine Translation</title>
    <summary>  Dealing with the complex word forms in morphologically rich languages is an
open problem in language processing, and is particularly important in
translation. In contrast to most modern neural systems of translation, which
discard the identity for rare words, in this paper we propose several
architectures for learning word representations from character and morpheme
level word decompositions. We incorporate these representations in a novel
machine translation model which jointly learns word alignments and translations
via a hard attention mechanism. Evaluating on translating from several
morphologically rich languages into English, we show consistent improvements
over strong baseline methods, of between 1 and 1.5 BLEU points.
</summary>
    <author>
      <name>Ekaterina Vylomova</name>
    </author>
    <author>
      <name>Trevor Cohn</name>
    </author>
    <author>
      <name>Xuanli He</name>
    </author>
    <author>
      <name>Gholamreza Haffari</name>
    </author>
    <link href="http://arxiv.org/abs/1606.04217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.04217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05784v2</id>
    <updated>2016-06-30T17:29:55Z</updated>
    <published>2016-06-18T17:36:28Z</published>
    <title>Hitting times of local and global optima in genetic algorithms with very
  high selection pressure</title>
    <summary>  The paper is devoted to upper bounds on the expected first hitting times of
the sets of local or global optima for non-elitist genetic algorithms with very
high selection pressure. The results of this paper extend the range of
situations where the upper bounds on the expected runtime are known for genetic
algorithms and apply, in particular, to the Canonical Genetic Algorithm. The
obtained bounds do not require the probability of fitness-decreasing mutation
to be bounded by a constant less than one.
</summary>
    <author>
      <name>Anton Eremeev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Yugoslav Journal of Operations Research. arXiv admin
  note: text overlap with arXiv:1512.02047</arxiv:comment>
    <link href="http://arxiv.org/abs/1606.05784v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05784v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1606.05990v2</id>
    <updated>2018-08-11T19:52:59Z</updated>
    <published>2016-06-20T07:05:14Z</published>
    <title>A New Training Method for Feedforward Neural Networks Based on Geometric
  Contraction Property of Activation Functions</title>
    <summary>  We propose a new training method for a feedforward neural network having the
activation functions with the geometric contraction property. The method
consists of constructing a new functional that is less nonlinear in comparison
with the classical functional by removing the nonlinearity of the activation
function from the output layer. We validate this new method by a series of
experiments that show an improved learning speed and better classification
error.
</summary>
    <author>
      <name>Petre Birtea</name>
    </author>
    <author>
      <name>Cosmin Cernazanu-Glavan</name>
    </author>
    <author>
      <name>Alexandru Sisu</name>
    </author>
    <link href="http://arxiv.org/abs/1606.05990v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1606.05990v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20, 68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.02028v1</id>
    <updated>2016-07-06T12:23:47Z</updated>
    <published>2016-07-06T12:23:47Z</published>
    <title>Artificial neural networks and fuzzy logic for recognizing alphabet
  characters and mathematical symbols</title>
    <summary>  Optical Character Recognition software (OCR) are important tools for
obtaining accessible texts. We propose the use of artificial neural networks
(ANN) in order to develop pattern recognition algorithms capable of recognizing
both normal texts and formulae. We present an original improvement of the
backpropagation algorithm. Moreover, we describe a novel image segmentation
algorithm that exploits fuzzy logic for separating touching characters.
</summary>
    <author>
      <name>Giuseppe Airò Farulla</name>
    </author>
    <author>
      <name>Tiziana Armano</name>
    </author>
    <author>
      <name>Anna Capietto</name>
    </author>
    <author>
      <name>Nadir Murru</name>
    </author>
    <author>
      <name>Rosaria Rossini</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-41264-1_1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-41264-1_1" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Computer Science, Volume 9759 2016, Computers
  Helping People with Special Needs, p. 7-14</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1607.02028v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.02028v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.04267v1</id>
    <updated>2016-07-11T16:22:47Z</updated>
    <published>2016-07-11T16:22:47Z</published>
    <title>Enhanced Boolean Correlation Matrix Memory</title>
    <summary>  This paper introduces an Enhanced Boolean version of the Correlation Matrix
Memory (CMM), which is useful to work with binary memories. A novel Boolean
Orthonormalization Process (BOP) is presented to convert a non-orthonormal
Boolean basis, i.e., a set of non-orthonormal binary vectors (in a Boolean
sense) to an orthonormal Boolean basis, i.e., a set of orthonormal binary
vectors (in a Boolean sense). This work shows that it is possible to improve
the performance of Boolean CMM thanks BOP algorithm. Besides, the BOP algorithm
has a lot of additional fields of applications, e.g.: Steganography, Hopfield
Networks, Bi-level image processing, etc. Finally, it is important to mention
that the BOP is an extremely stable and fast algorithm.
</summary>
    <author>
      <name>Mario Mastriani</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.04267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.04267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.05108v1</id>
    <updated>2016-07-18T14:44:26Z</updated>
    <published>2016-07-18T14:44:26Z</published>
    <title>Neural Machine Translation with Recurrent Attention Modeling</title>
    <summary>  Knowing which words have been attended to in previous time steps while
generating a translation is a rich source of information for predicting what
words will be attended to in the future. We improve upon the attention model of
Bahdanau et al. (2014) by explicitly modeling the relationship between previous
and subsequent attention levels for each word using one recurrent network per
input word. This architecture easily captures informative features, such as
fertility and regularities in relative distortion. In experiments, we show our
parameterization of attention improves translation quality.
</summary>
    <author>
      <name>Zichao Yang</name>
    </author>
    <author>
      <name>Zhiting Hu</name>
    </author>
    <author>
      <name>Yuntian Deng</name>
    </author>
    <author>
      <name>Chris Dyer</name>
    </author>
    <author>
      <name>Alex Smola</name>
    </author>
    <link href="http://arxiv.org/abs/1607.05108v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.05108v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1607.07078v1</id>
    <updated>2016-07-24T18:44:54Z</updated>
    <published>2016-07-24T18:44:54Z</published>
    <title>Effective Connectivity-Based Neural Decoding: A Causal
  Interaction-Driven Approach</title>
    <summary>  We propose a geometric model-free causality measurebased on multivariate
delay embedding that can efficiently detect linear and nonlinear causal
interactions between time series with no prior information. We then exploit the
proposed causal interaction measure in real MEG data analysis. The results are
used to construct effective connectivity maps of brain activity to decode
different categories of visual stimuli. Moreover, we discovered that the
MEG-based effective connectivity maps as a response to structured images
exhibit more geometric patterns, as disclosed by analyzing the evolution of
toplogical structures of the underlying networks using persistent homology.
Extensive simulation and experimental result have been carried out to
substantiate the capabilities of the proposed approach.
</summary>
    <author>
      <name>Saba Emrani</name>
    </author>
    <author>
      <name>Hamid Krim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 13 figures, 2 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1607.07078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1607.07078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01783v1</id>
    <updated>2016-08-05T07:15:38Z</updated>
    <published>2016-08-05T07:15:38Z</published>
    <title>The Evolutionary Process of Image Transition in Conjunction with Box and
  Strip Mutation</title>
    <summary>  Evolutionary algorithms have been used in many ways to generate digital art.
We study how evolutionary processes are used for evolutionary art and present a
new approach to the transition of images. Our main idea is to define
evolutionary processes for digital image transition, combining different
variants of mutation and evolutionary mechanisms. We introduce box and strip
mutation operators which are specifically designed for image transition. Our
experimental results show that the process of an evolutionary algorithm in
combination with these mutation operators can be used as a valuable way to
produce unique generative art.
</summary>
    <author>
      <name>Aneta Neumann</name>
    </author>
    <author>
      <name>Bradley Alexander</name>
    </author>
    <author>
      <name>Frank Neumann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conference version appears at ICONIP 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01783v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01783v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.01818v2</id>
    <updated>2018-01-19T12:48:57Z</updated>
    <published>2016-08-05T09:55:59Z</published>
    <title>The BioDynaMo Project: a platform for computer simulations of biological
  dynamics</title>
    <summary>  This paper is a brief update on developments in the BioDynaMo project, a new
platform for computer simulations for biological research. We will discuss the
new capabilities of the simulator, important new concepts simulation
methodology as well as its numerous applications to the computational biology
and nanoscience communities.
</summary>
    <author>
      <name>Leonard Johard</name>
    </author>
    <author>
      <name>Lukas Breitwieser</name>
    </author>
    <author>
      <name>Alberto Di Meglio</name>
    </author>
    <author>
      <name>Marco Manca</name>
    </author>
    <author>
      <name>Manuel Mazzara</name>
    </author>
    <author>
      <name>Max Talanov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The paper contains inaccurate content and claims that need to be
  verified</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.01818v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.01818v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.05578v3</id>
    <updated>2017-03-15T15:48:37Z</updated>
    <published>2016-08-19T12:17:59Z</published>
    <title>Haploid-Diploid Evolutionary Algorithms</title>
    <summary>  This paper uses the recent idea that the fundamental haploid-diploid
lifecycle of eukaryotic organisms implements a rudimentary form of learning
within evolution. A general approach for evolutionary computation is here
derived that differs from all previous known work using diploid
representations. The primary role of recombination is also changed from that
previously considered in both natural and artificial evolution under the new
view. Using well-known abstract tuneable models it is shown that varying
fitness landscape ruggedness varies the benefit of the new approach.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1607.00318</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.05578v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.05578v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1608.08265v1</id>
    <updated>2016-08-29T22:02:39Z</updated>
    <published>2016-08-29T22:02:39Z</published>
    <title>About Learning in Recurrent Bistable Gradient Networks</title>
    <summary>  Recurrent Bistable Gradient Networks are attractor based neural networks
characterized by bistable dynamics of each single neuron. Coupled together
using linear interaction determined by the interconnection weights, these
networks do not suffer from spurious states or very limited capacity anymore.
Vladimir Chinarov and Michael Menzinger, who invented these networks, trained
them using Hebb's learning rule. We show, that this way of computing the
weights leads to unwanted behaviour and limitations of the networks
capabilities. Furthermore we evince, that using the first order of Hintons
Contrastive Divergence algorithm leads to a quite promising recurrent neural
network. These findings are tested by learning images of the MNIST database for
handwritten numbers.
</summary>
    <author>
      <name>J. Fischer</name>
    </author>
    <author>
      <name>S. Lackner</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1608.08265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1608.08265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.04468v3</id>
    <updated>2016-12-06T14:39:05Z</updated>
    <published>2016-09-14T22:42:23Z</published>
    <title>Sampling Generative Networks</title>
    <summary>  We introduce several techniques for sampling and visualizing the latent
spaces of generative models. Replacing linear interpolation with spherical
linear interpolation prevents diverging from a model's prior distribution and
produces sharper samples. J-Diagrams and MINE grids are introduced as
visualizations of manifolds created by analogies and nearest neighbors. We
demonstrate two new techniques for deriving attribute vectors: bias-corrected
vectors with data replication and synthetic vectors with data augmentation.
Binary classification using attribute vectors is presented as a technique
supporting quantitative analysis of the latent space. Most techniques are
intended to be independent of model type and examples are shown on both
Variational Autoencoders and Generative Adversarial Networks.
</summary>
    <author>
      <name>Tom White</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.04468v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.04468v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.07724v1</id>
    <updated>2016-09-25T10:18:19Z</updated>
    <published>2016-09-25T10:18:19Z</published>
    <title>The RNN-ELM Classifier</title>
    <summary>  In this paper we examine learning methods combining the Random Neural
Network, a biologically inspired neural network and the Extreme Learning
Machine that achieve state of the art classification performance while
requiring much shorter training time. The Random Neural Network is a integrate
and fire computational model of a neural network whose mathematical structure
permits the efficient analysis of large ensembles of neurons. An activation
function is derived from the RNN and used in an Extreme Learning Machine. We
compare the performance of this combination against the ELM with various
activation functions, we reduce the input dimensionality via PCA and compare
its performance vs. autoencoder based versions of the RNN-ELM.
</summary>
    <author>
      <name>Athanasios Vlontzos</name>
    </author>
    <link href="http://arxiv.org/abs/1609.07724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.07724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1609.08663v1</id>
    <updated>2016-09-27T20:53:16Z</updated>
    <published>2016-09-27T20:53:16Z</published>
    <title>Learning Genomic Representations to Predict Clinical Outcomes in Cancer</title>
    <summary>  Genomics are rapidly transforming medical practice and basic biomedical
research, providing insights into disease mechanisms and improving therapeutic
strategies, particularly in cancer. The ability to predict the future course of
a patient's disease from high-dimensional genomic profiling will be essential
in realizing the promise of genomic medicine, but presents significant
challenges for state-of-the-art survival analysis methods. In this abstract we
present an investigation in learning genomic representations with neural
networks to predict patient survival in cancer. We demonstrate the advantages
of this approach over existing survival analysis methods using brain tumor
data.
</summary>
    <author>
      <name>Safoora Yousefi</name>
    </author>
    <author>
      <name>Congzheng Song</name>
    </author>
    <author>
      <name>Nelson Nauata</name>
    </author>
    <author>
      <name>Lee Cooper</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICLR 2016 Workshop Track- May 2nd 2016 International Conference on
  Learning Representations</arxiv:comment>
    <link href="http://arxiv.org/abs/1609.08663v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1609.08663v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.01439v1</id>
    <updated>2016-10-05T14:26:27Z</updated>
    <published>2016-10-05T14:26:27Z</published>
    <title>Nonlinear Systems Identification Using Deep Dynamic Neural Networks</title>
    <summary>  Neural networks are known to be effective function approximators. Recently,
deep neural networks have proven to be very effective in pattern recognition,
classification tasks and human-level control to model highly nonlinear
realworld systems. This paper investigates the effectiveness of deep neural
networks in the modeling of dynamical systems with complex behavior. Three deep
neural network structures are trained on sequential data, and we investigate
the effectiveness of these networks in modeling associated characteristics of
the underlying dynamical systems. We carry out similar evaluations on select
publicly available system identification datasets. We demonstrate that deep
neural networks are effective model estimators from input-output data
</summary>
    <author>
      <name>Olalekan Ogunmolu</name>
    </author>
    <author>
      <name>Xuejun Gu</name>
    </author>
    <author>
      <name>Steve Jiang</name>
    </author>
    <author>
      <name>Nicholas Gans</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">American Control Conference, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1610.01439v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.01439v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1610.02732v1</id>
    <updated>2016-10-09T22:16:32Z</updated>
    <published>2016-10-09T22:16:32Z</published>
    <title>Investigating the effects Diversity Mechanisms have on Evolutionary
  Algorithms in Dynamic Environments</title>
    <summary>  Evolutionary algorithms have been successfully applied to a variety of
optimisation problems in stationary environments. However, many real world
optimisation problems are set in dynamic environments where the success
criteria shifts regularly. Population diversity affects algorithmic
performance, particularly on multiobjective and dynamic problems. Diversity
mechanisms are methods of altering evolutionary algorithms in a way that
promotes the maintenance of population diversity. This project intends to
measure and compare the performance effect a variety of diversity mechanisms
have on an evolutionary algorithm when facing an assortment of dynamic
problems.
</summary>
    <author>
      <name>Matthew Hughes</name>
    </author>
    <link href="http://arxiv.org/abs/1610.02732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1610.02732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.00577v1</id>
    <updated>2016-01-06T16:32:47Z</updated>
    <published>2016-01-06T16:32:47Z</published>
    <title>The new hybrid COAW method for solving multi-objective problems</title>
    <summary>  In this article using Cuckoo Optimization Algorithm and simple additive
weighting method the hybrid COAW algorithm is presented to solve
multi-objective problems. Cuckoo algorithm is an efficient and structured
method for solving nonlinear continuous problems. The created Pareto frontiers
of the COAW proposed algorithm are exact and have good dispersion. This method
has a high speed in finding the Pareto frontiers and identifies the beginning
and end points of Pareto frontiers properly. In order to validation the
proposed algorithm, several experimental problems were analyzed. The results of
which indicate the proper effectiveness of COAW algorithm for solving
multi-objective problems.
</summary>
    <author>
      <name>Zeinab Borhanifar</name>
    </author>
    <author>
      <name>Elham Shadkam</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.5121/ijfcst.2015.5602</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.5121/ijfcst.2015.5602" rel="related"/>
    <link href="http://arxiv.org/abs/1611.00577v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.00577v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1611.06245v1</id>
    <updated>2016-11-18T21:09:16Z</updated>
    <published>2016-11-18T21:09:16Z</published>
    <title>Spikes as regularizers</title>
    <summary>  We present a confidence-based single-layer feed-forward learning algorithm
SPIRAL (Spike Regularized Adaptive Learning) relying on an encoding of
activation spikes. We adaptively update a weight vector relying on confidence
estimates and activation offsets relative to previous activity. We regularize
updates proportionally to item-level confidence and weight-specific support,
loosely inspired by the observation from neurophysiology that high spike rates
are sometimes accompanied by low temporal precision. Our experiments suggest
that the new learning algorithm SPIRAL is more robust and less prone to
overfitting than both the averaged perceptron and AROW.
</summary>
    <author>
      <name>Anders Søgaard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Computing with Spikes at NIPS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1611.06245v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1611.06245v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.00712v1</id>
    <updated>2016-12-02T15:46:09Z</updated>
    <published>2016-12-02T15:46:09Z</published>
    <title>Probabilistic Neural Programs</title>
    <summary>  We present probabilistic neural programs, a framework for program induction
that permits flexible specification of both a computational model and inference
algorithm while simultaneously enabling the use of deep neural networks.
Probabilistic neural programs combine a computation graph for specifying a
neural network with an operator for weighted nondeterministic choice. Thus, a
program describes both a collection of decisions as well as the neural network
architecture used to make each one. We evaluate our approach on a challenging
diagram question answering task where probabilistic neural programs correctly
execute nearly twice as many programs as a baseline model.
</summary>
    <author>
      <name>Kenton W. Murray</name>
    </author>
    <author>
      <name>Jayant Krishnamurthy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Appears in NAMPI workshop at NIPS 2016</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.00712v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.00712v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.02522v1</id>
    <updated>2016-12-08T03:28:10Z</updated>
    <published>2016-12-08T03:28:10Z</published>
    <title>Geometric Decomposition of Feed Forward Neural Networks</title>
    <summary>  There have been several attempts to mathematically understand neural networks
and many more from biological and computational perspectives. The field has
exploded in the last decade, yet neural networks are still treated much like a
black box. In this work we describe a structure that is inherent to a feed
forward neural network. This will provide a framework for future work on neural
networks to improve training algorithms, compute the homology of the network,
and other applications. Our approach takes a more geometric point of view and
is unlike other attempts to mathematically understand neural networks that rely
on a functional perspective.
</summary>
    <author>
      <name>Sven Cattell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.02522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.02522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.03707v1</id>
    <updated>2016-12-12T14:36:22Z</updated>
    <published>2016-12-12T14:36:22Z</published>
    <title>Empirical Evaluation of A New Approach to Simplifying Long Short-term
  Memory (LSTM)</title>
    <summary>  The standard LSTM, although it succeeds in the modeling long-range
dependences, suffers from a highly complex structure that can be simplified
through modifications to its gate units. This paper was to perform an empirical
comparison between the standard LSTM and three new simplified variants that
were obtained by eliminating input signal, bias and hidden unit signal from
individual gates, on the tasks of modeling two sequence datasets. The
experiments show that the three variants, with reduced parameters, can achieve
comparable performance with the standard LSTM. Due attention should be paid to
turning the learning rate to achieve high accuracies
</summary>
    <author>
      <name>Yuzhen Lu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1612.03707v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.03707v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1612.05054v1</id>
    <updated>2016-12-15T13:24:41Z</updated>
    <published>2016-12-15T13:24:41Z</published>
    <title>Graphical RNN Models</title>
    <summary>  Many time series are generated by a set of entities that interact with one
another over time. This paper introduces a broad, flexible framework to learn
from multiple inter-dependent time series generated by such entities. Our
framework explicitly models the entities and their interactions through time.
It achieves this by building on the capabilities of Recurrent Neural Networks,
while also offering several ways to incorporate domain knowledge/constraints
into the model architecture. The capabilities of our approach are showcased
through an application to weather prediction, which shows gains over strong
baselines.
</summary>
    <author>
      <name>Ashish Bora</name>
    </author>
    <author>
      <name>Sugato Basu</name>
    </author>
    <author>
      <name>Joydeep Ghosh</name>
    </author>
    <link href="http://arxiv.org/abs/1612.05054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1612.05054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.03441v1</id>
    <updated>2017-01-12T18:12:05Z</updated>
    <published>2017-01-12T18:12:05Z</published>
    <title>Simplified Gating in Long Short-term Memory (LSTM) Recurrent Neural
  Networks</title>
    <summary>  The standard LSTM recurrent neural networks while very powerful in long-range
dependency sequence applications have highly complex structure and relatively
large (adaptive) parameters. In this work, we present empirical comparison
between the standard LSTM recurrent neural network architecture and three new
parameter-reduced variants obtained by eliminating combinations of the input
signal, bias, and hidden unit signals from individual gating signals. The
experiments on two sequence datasets show that the three new variants, called
simply as LSTM1, LSTM2, and LSTM3, can achieve comparable performance to the
standard LSTM model with less (adaptive) parameters.
</summary>
    <author>
      <name>Yuzhen Lu</name>
    </author>
    <author>
      <name>Fathi M. Salem</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 4 Figures, 3 Tables. arXiv admin note: substantial text
  overlap with arXiv:1612.03707</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.03441v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.03441v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05159v1</id>
    <updated>2017-01-18T17:37:35Z</updated>
    <published>2017-01-18T17:37:35Z</published>
    <title>Temporal Overdrive Recurrent Neural Network</title>
    <summary>  In this work we present a novel recurrent neural network architecture
designed to model systems characterized by multiple characteristic timescales
in their dynamics. The proposed network is composed by several recurrent groups
of neurons that are trained to separately adapt to each timescale, in order to
improve the system identification process. We test our framework on time series
prediction tasks and we show some promising, preliminary results achieved on
synthetic data. To evaluate the capabilities of our network, we compare the
performance with several state-of-the-art recurrent architectures.
</summary>
    <author>
      <name>Filippo Maria Bianchi</name>
    </author>
    <author>
      <name>Michael Kampffmeyer</name>
    </author>
    <author>
      <name>Enrico Maiorino</name>
    </author>
    <author>
      <name>Robert Jenssen</name>
    </author>
    <link href="http://arxiv.org/abs/1701.05159v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05159v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05730v1</id>
    <updated>2017-01-20T09:17:52Z</updated>
    <published>2017-01-20T09:17:52Z</published>
    <title>Using LLVM-based JIT Compilation in Genetic Programming</title>
    <summary>  The paper describes an approach to implementing genetic programming, which
uses the LLVM library to just-in-time compile/interpret the evolved abstract
syntax trees. The solution is described in some detail, including a parser
(based on FlexC++ and BisonC++) that can construct the trees from a simple toy
language with C-like syntax. The approach is compared with a previous
implementation (based on direct execution of trees using polymorphic functors)
in terms of execution speed.
</summary>
    <author>
      <name>Michal Gregor</name>
    </author>
    <author>
      <name>Juraj Spalek</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ELEKTRO.2016.7512108</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ELEKTRO.2016.7512108" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Link to the IEEE published version:
  http://ieeexplore.ieee.org/document/7512108/</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.05730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1701.05818v1</id>
    <updated>2017-01-20T15:10:09Z</updated>
    <published>2017-01-20T15:10:09Z</published>
    <title>Fusion of Heterogeneous Data in Convolutional Networks for Urban
  Semantic Labeling (Invited Paper)</title>
    <summary>  In this work, we present a novel module to perform fusion of heterogeneous
data using fully convolutional networks for semantic labeling. We introduce
residual correction as a way to learn how to fuse predictions coming out of a
dual stream architecture. Especially, we perform fusion of DSM and IRRG optical
data on the ISPRS Vaihingen dataset over a urban area and obtain new
state-of-the-art results.
</summary>
    <author>
      <name>Nicolas Audebert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Palaiseau, OBELIX</arxiv:affiliation>
    </author>
    <author>
      <name>Bertrand Le Saux</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Palaiseau</arxiv:affiliation>
    </author>
    <author>
      <name>Sébastien Lefèvre</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">OBELIX</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Joint Urban Remote Sensing Event (JURSE), Mar 2017, Dubai, United
  Arab Emirates. Joint Urban Remote Sensing Event 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1701.05818v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1701.05818v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.00993v2</id>
    <updated>2018-02-12T15:15:35Z</updated>
    <published>2017-02-03T13:07:35Z</published>
    <title>Robust Particle Swarm Optimizer based on Chemomimicry</title>
    <summary>  A particle swarm optimizer (PSO) loosely based on the phenomena of
crystallization and a chaos factor which follows the complimentary error
function is described. The method features three phases: diffusion, directed
motion, and nucleation. During the diffusion phase random walk is the only
contributor to particle motion. As the algorithm progresses the contribution
from chaos decreases and movement toward global best locations is pursued until
convergence has occurred. The algorithm was found to be more robust to local
minima in multimodal test functions than a standard PSO algorithm and is
designed for problems which feature experimental precision.
</summary>
    <author>
      <name>Casey Kneale</name>
    </author>
    <author>
      <name>Karl S. Booksh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Revision # 2. Included pseudo code, more references, changed
  description</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.00993v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.00993v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.02277v1</id>
    <updated>2017-02-08T04:27:11Z</updated>
    <published>2017-02-08T04:27:11Z</published>
    <title>A Historical Review of Forty Years of Research on CMAC</title>
    <summary>  The Cerebellar Model Articulation Controller (CMAC) is an influential
brain-inspired computing model in many relevant fields. Since its inception in
the 1970s, the model has been intensively studied and many variants of the
prototype, such as Kernel-CMAC, Self-Organizing Map CMAC, and Linguistic CMAC,
have been proposed. This review article focus on how the CMAC model is
gradually developed and refined to meet the demand of fast, adaptive, and
robust control. Two perspective, CMAC as a neural network and CMAC as a table
look-up technique are presented. Three aspects of the model: the architecture,
learning algorithms and applications are discussed. In the end, some potential
future research directions on this model are suggested.
</summary>
    <author>
      <name>Frank Z. Xing</name>
    </author>
    <link href="http://arxiv.org/abs/1702.02277v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.02277v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.03389v2</id>
    <updated>2017-03-30T12:53:54Z</updated>
    <published>2017-02-11T06:39:38Z</published>
    <title>Whale swarm algorithm for function optimization</title>
    <summary>  Increasing nature-inspired metaheuristic algorithms are applied to solving
the real-world optimization problems, as they have some advantages over the
classical methods of numerical optimization. This paper has proposed a new
nature-inspired metaheuristic called Whale Swarm Algorithm for function
optimization, which is inspired by the whales behavior of communicating with
each other via ultrasound for hunting. The proposed Whale Swarm Algorithm has
been compared with several popular metaheuristic algorithms on comprehensive
performance metrics. According to the experimental results, Whale Swarm
Algorithm has a quite competitive performance when compared with other
algorithms.
</summary>
    <author>
      <name>Bing Zeng</name>
    </author>
    <author>
      <name>Liang Gao</name>
    </author>
    <author>
      <name>Xinyu Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-63309-1_55</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-63309-1_55" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LNCS. volume 10361. ICIC 2017: pp 624-639</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1702.03389v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.03389v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1702.08231v1</id>
    <updated>2017-02-27T11:10:54Z</updated>
    <published>2017-02-27T11:10:54Z</published>
    <title>Low-Precision Batch-Normalized Activations</title>
    <summary>  Artificial neural networks can be trained with relatively low-precision
floating-point and fixed-point arithmetic, using between one and 16 bits.
Previous works have focused on relatively wide-but-shallow, feed-forward
networks. We introduce a quantization scheme that is compatible with training
very deep neural networks. Quantizing the network activations in the middle of
each batch-normalization module can greatly reduce the amount of memory and
computational power needed, with little loss in accuracy.
</summary>
    <author>
      <name>Benjamin Graham</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1702.08231v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1702.08231v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.05955v1</id>
    <updated>2017-03-17T10:46:23Z</updated>
    <published>2017-03-17T10:46:23Z</published>
    <title>Implicit Gradient Neural Networks with a Positive-Definite Mass Matrix
  for Online Linear Equations Solving</title>
    <summary>  Motivated by the advantages achieved by implicit analogue net for solving
online linear equations, a novel implicit neural model is designed based on
conventional explicit gradient neural networks in this letter by introducing a
positive-definite mass matrix. In addition to taking the advantages of the
implicit neural dynamics, the proposed implicit gradient neural networks can
still achieve globally exponential convergence to the unique theoretical
solution of linear equations and also global stability even under no-solution
and multi-solution situations. Simulative results verify theoretical
convergence analysis on the proposed neural dynamics.
</summary>
    <author>
      <name>Ke Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Information Processing Letters</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.05955v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.05955v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.07841v1</id>
    <updated>2017-03-22T20:31:47Z</updated>
    <published>2017-03-22T20:31:47Z</published>
    <title>Classification-based RNN machine translation using GRUs</title>
    <summary>  We report the results of our classification-based machine translation model,
built upon the framework of a recurrent neural network using gated recurrent
units. Unlike other RNN models that attempt to maximize the overall conditional
log probability of sentences against sentences, our model focuses a
classification approach of estimating the conditional probability of the next
word given the input sequence. This simpler approach using GRUs was hoped to be
comparable with more complicated RNN models, but achievements in this
implementation were modest and there remains a lot of room for improving this
classification approach.
</summary>
    <author>
      <name>Ri Wang</name>
    </author>
    <author>
      <name>Maysum Panju</name>
    </author>
    <author>
      <name>Mahmood Gohari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 1 figure; graduate course research project</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.07841v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.07841v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.08481v1</id>
    <updated>2017-03-24T15:53:03Z</updated>
    <published>2017-03-24T15:53:03Z</published>
    <title>Long-Term Evolution of Genetic Programming Populations</title>
    <summary>  We evolve binary mux-6 trees for up to 100000 generations evolving some
programs with more than a hundred million nodes. Our unbounded Long-Term
Evolution Experiment LTEE GP appears not to evolve building blocks but does
suggests a limit to bloat. We do see periods of tens even hundreds of
generations where the population is 100 percent functionally converged. The
distribution of tree sizes is not as predicted by theory.
</summary>
    <author>
      <name>W. B. Langdon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Longer version of Langdon:2017:GECCO, July 2017, ACM, Berlin,
  RN/17/05
  http://www.cs.ucl.ac.uk/fileadmin/UCL-CS/research/Research_Notes/RN_17_05.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/1703.08481v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.08481v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1703.09926v1</id>
    <updated>2017-03-29T08:10:26Z</updated>
    <published>2017-03-29T08:10:26Z</published>
    <title>Hierarchical Surrogate Modeling for Illumination Algorithms</title>
    <summary>  Evolutionary illumination is a recent technique that allows producing many
diverse, optimal solutions in a map of manually defined features. To support
the large amount of objective function evaluations, surrogate model assistance
was recently introduced. Illumination models need to represent many more,
diverse optimal regions than classical surrogate models. In this PhD thesis, we
propose to decompose the sample set, decreasing model complexity, by
hierarchically segmenting the training set according to their coordinates in
feature space. An ensemble of diverse models can then be trained to serve as a
surrogate to illumination.
</summary>
    <author>
      <name>Alexander Hagg</name>
    </author>
    <link href="http://arxiv.org/abs/1703.09926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1703.09926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.03477v4</id>
    <updated>2017-05-19T16:40:16Z</updated>
    <published>2017-04-11T18:09:01Z</published>
    <title>A Neural Representation of Sketch Drawings</title>
    <summary>  We present sketch-rnn, a recurrent neural network (RNN) able to construct
stroke-based drawings of common objects. The model is trained on thousands of
crude human-drawn images representing hundreds of classes. We outline a
framework for conditional and unconditional sketch generation, and describe new
robust training methods for generating coherent sketch drawings in a vector
format.
</summary>
    <author>
      <name>David Ha</name>
    </author>
    <author>
      <name>Douglas Eck</name>
    </author>
    <link href="http://arxiv.org/abs/1704.03477v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.03477v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.04879v1</id>
    <updated>2017-04-17T06:04:02Z</updated>
    <published>2017-04-17T06:04:02Z</published>
    <title>A Sport Tournament Scheduling by Genetic Algorithm with Swapping Method</title>
    <summary>  A sport tournament problem is considered the Traveling Tournament Problem
(TTP). One interesting type is the mirrored Traveling Tournament Problem
(mTTP). The objective of the problem is to minimize either the total number of
traveling or the total distances of traveling or both. This research aims to
find an optimized solution of the mirrored Traveling Tournament Problem with
minimum total number of traveling. The solutions consisting of traveling and
scheduling tables are solved by using genetic algorithm (GA) with swapping
method. The number of traveling of all teams from obtained solutions are close
to the lower bound theory of number of traveling. Moreover, this algorithm
generates better solutions than known results for most cases.
</summary>
    <author>
      <name>Tinnaluk Rutjanisarakul</name>
    </author>
    <author>
      <name>Thiradet Jiarasuksakun</name>
    </author>
    <link href="http://arxiv.org/abs/1704.04879v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.04879v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05132v1</id>
    <updated>2017-04-17T21:31:14Z</updated>
    <published>2017-04-17T21:31:14Z</published>
    <title>A hybrid CPU-GPU parallelization scheme of variable neighborhood search
  for inventory optimization problems</title>
    <summary>  In this paper, we study various parallelization schemes for the Variable
Neighborhood Search (VNS) metaheuristic on a CPU-GPU system via OpenMP and
OpenACC. A hybrid parallel VNS method is applied to recent benchmark problem
instances for the multi-product dynamic lot sizing problem with product returns
and recovery, which appears in reverse logistics and is known to be NP-hard. We
report our findings regarding these parallelization approaches and present
promising computational results.
</summary>
    <author>
      <name>Nikolaos Antoniadis</name>
    </author>
    <author>
      <name>Angelo Sifaleras</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.endm.2017.03.007</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.endm.2017.03.007" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Electronic Notes in Discrete Mathematics, Volume 58, April 2017,
  Pages 47-54, ISSN 1571-0653</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1704.05132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R99" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; I.2.8; D.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05396v1</id>
    <updated>2017-04-18T15:33:10Z</updated>
    <published>2017-04-18T15:33:10Z</published>
    <title>A Study of Deep Learning Robustness Against Computation Failures</title>
    <summary>  For many types of integrated circuits, accepting larger failure rates in
computations can be used to improve energy efficiency. We study the performance
of faulty implementations of certain deep neural networks based on pessimistic
and optimistic models of the effect of hardware faults. After identifying the
impact of hyperparameters such as the number of layers on robustness, we study
the ability of the network to compensate for computational failures through an
increase of the network size. We show that some networks can achieve equivalent
performance under faulty implementations, and quantify the required increase in
computational complexity.
</summary>
    <author>
      <name>Jean-Charles Vialatte</name>
    </author>
    <author>
      <name>François Leduc-Primeau</name>
    </author>
    <link href="http://arxiv.org/abs/1704.05396v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05396v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.05420v2</id>
    <updated>2017-04-19T23:36:18Z</updated>
    <published>2017-04-18T16:47:38Z</published>
    <title>Diagonal RNNs in Symbolic Music Modeling</title>
    <summary>  In this paper, we propose a new Recurrent Neural Network (RNN) architecture.
The novelty is simple: We use diagonal recurrent matrices instead of full. This
results in better test likelihood and faster convergence compared to regular
full RNNs in most of our experiments. We show the benefits of using diagonal
recurrent matrices with popularly used LSTM and GRU architectures as well as
with the vanilla RNN architecture, on four standard symbolic music datasets.
</summary>
    <author>
      <name>Y. Cem Subakan</name>
    </author>
    <author>
      <name>Paris Smaragdis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to Waspaa 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.05420v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.05420v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.06016v1</id>
    <updated>2017-04-20T05:09:39Z</updated>
    <published>2017-04-20T05:09:39Z</published>
    <title>Genetic Algorithm Based Floor Planning System</title>
    <summary>  Genetic Algorithms are widely used in many different optimization problems
including layout design. The layout of the shelves play an important role in
the total sales metrics for superstores since this affects the customers'
shopping behaviour. This paper employed a genetic algorithm based approach to
design shelf layout of superstores. The layout design problem was tackled by
using a novel chromosome representation which takes many different parameters
to prevent dead-ends and improve shelf visibility into consideration. Results
show that the approach can produce reasonably good layout designs in very short
amounts of time.
</summary>
    <author>
      <name>Hamide Ozlem Dalgic</name>
    </author>
    <author>
      <name>Erkan Bostanci</name>
    </author>
    <author>
      <name>Mehmet Serdar Guzel</name>
    </author>
    <link href="http://arxiv.org/abs/1704.06016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.06016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1704.08774v1</id>
    <updated>2017-04-27T23:38:36Z</updated>
    <published>2017-04-27T23:38:36Z</published>
    <title>Genealogical Distance as a Diversity Estimate in Evolutionary Algorithms</title>
    <summary>  The evolutionary edit distance between two individuals in a population, i.e.,
the amount of applications of any genetic operator it would take the
evolutionary process to generate one individual starting from the other, seems
like a promising estimate for the diversity between said individuals. We
introduce genealogical diversity, i.e., estimating two individuals' degree of
relatedness by analyzing large, unused parts of their genome, as a
computationally efficient method to approximate that measure for diversity.
</summary>
    <author>
      <name>Thomas Gabor</name>
    </author>
    <author>
      <name>Lenz Belzner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3067695.3082529</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3067695.3082529" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Measuring and Promoting Diversity in Evolutionary Algorithms @ GECCO
  2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1704.08774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1704.08774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00504v1</id>
    <updated>2017-06-01T21:57:32Z</updated>
    <published>2017-06-01T21:57:32Z</published>
    <title>Dynamic Stripes: Exploiting the Dynamic Precision Requirements of
  Activation Values in Neural Networks</title>
    <summary>  Stripes is a Deep Neural Network (DNN) accelerator that uses bit-serial
computation to offer performance that is proportional to the fixed-point
precision of the activation values. The fixed-point precisions are determined a
priori using profiling and are selected at a per layer granularity. This paper
presents Dynamic Stripes, an extension to Stripes that detects precision
variance at runtime and at a finer granularity. This extra level of precision
reduction increases performance by 41% over Stripes.
</summary>
    <author>
      <name>Alberto Delmas</name>
    </author>
    <author>
      <name>Patrick Judd</name>
    </author>
    <author>
      <name>Sayeh Sharify</name>
    </author>
    <author>
      <name>Andreas Moshovos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.00504v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00504v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.00648v1</id>
    <updated>2017-05-03T13:46:05Z</updated>
    <published>2017-05-03T13:46:05Z</published>
    <title>Dataflow Matrix Machines as a Model of Computations with Linear Streams</title>
    <summary>  We overview dataflow matrix machines as a Turing complete generalization of
recurrent neural networks and as a programming platform. We describe vector
space of finite prefix trees with numerical leaves which allows us to combine
expressive power of dataflow matrix machines with simplicity of traditional
recurrent neural networks.
</summary>
    <author>
      <name>Michael Bukatin</name>
    </author>
    <author>
      <name>Jon Anthony</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, accepted for presentation at LearnAut 2017: Learning and
  Automata workshop at LICS (Logic in Computer Science) 2017 conference.
  Preprint original version: April 9, 2017; minor correction: May 1, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.00648v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.00648v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.02766v1</id>
    <updated>2017-06-08T20:49:34Z</updated>
    <published>2017-06-08T20:49:34Z</published>
    <title>Evolutionary Multitasking for Multiobjective Continuous Optimization:
  Benchmark Problems, Performance Metrics and Baseline Results</title>
    <summary>  In this report, we suggest nine test problems for multi-task multi-objective
optimization (MTMOO), each of which consists of two multiobjective optimization
tasks that need to be solved simultaneously. The relationship between tasks
varies between different test problems, which would be helpful to have a
comprehensive evaluation of the MO-MFO algorithms. It is expected that the
proposed test problems will germinate progress the field of the MTMOO research.
</summary>
    <author>
      <name>Yuan Yuan</name>
    </author>
    <author>
      <name>Yew-Soon Ong</name>
    </author>
    <author>
      <name>Liang Feng</name>
    </author>
    <author>
      <name>A. K. Qin</name>
    </author>
    <author>
      <name>Abhishek Gupta</name>
    </author>
    <author>
      <name>Bingshui Da</name>
    </author>
    <author>
      <name>Qingfu Zhang</name>
    </author>
    <author>
      <name>Kay Chen Tan</name>
    </author>
    <author>
      <name>Yaochu Jin</name>
    </author>
    <author>
      <name>Hisao Ishibuchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 pages, technical report</arxiv:comment>
    <link href="http://arxiv.org/abs/1706.02766v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.02766v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.03470v1</id>
    <updated>2017-06-12T05:25:40Z</updated>
    <published>2017-06-12T05:25:40Z</published>
    <title>Evolutionary Multitasking for Single-objective Continuous Optimization:
  Benchmark Problems, Performance Metric, and Baseline Results</title>
    <summary>  In this report, we suggest nine test problems for multi-task single-objective
optimization (MTSOO), each of which consists of two single-objective
optimization tasks that need to be solved simultaneously. The relationship
between tasks varies between different test problems, which would be helpful to
have a comprehensive evaluation of the MFO algorithms. It is expected that the
proposed test problems will germinate progress the field of the MTSOO research.
</summary>
    <author>
      <name>Bingshui Da</name>
    </author>
    <author>
      <name>Yew-Soon Ong</name>
    </author>
    <author>
      <name>Liang Feng</name>
    </author>
    <author>
      <name>A. K. Qin</name>
    </author>
    <author>
      <name>Abhishek Gupta</name>
    </author>
    <author>
      <name>Zexuan Zhu</name>
    </author>
    <author>
      <name>Chuan-Kang Ting</name>
    </author>
    <author>
      <name>Ke Tang</name>
    </author>
    <author>
      <name>Xin Yao</name>
    </author>
    <link href="http://arxiv.org/abs/1706.03470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.03470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.04119v3</id>
    <updated>2017-10-10T15:35:12Z</updated>
    <published>2017-06-13T15:22:38Z</published>
    <title>Investigating the Parameter Space of Evolutionary Algorithms</title>
    <summary>  The practice of evolutionary algorithms involves the tuning of many
parameters. How big should the population be? How many generations should the
algorithm run? What is the (tournament selection) tournament size? What
probabilities should one assign to crossover and mutation? Through an extensive
series of experiments over multiple evolutionary algorithm implementations and
problems we show that parameter space tends to be rife with viable parameters,
at least for 25 the problems studied herein. We discuss the implications of
this finding in practice.
</summary>
    <author>
      <name>Moshe Sipper</name>
    </author>
    <author>
      <name>Weixuan Fu</name>
    </author>
    <author>
      <name>Karuna Ahuja</name>
    </author>
    <author>
      <name>Jason H. Moore</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1186/s13040-018-0164-x</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1186/s13040-018-0164-x" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">BioData Mining, 2018, 11:2</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.04119v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.04119v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00081v1</id>
    <updated>2017-07-01T01:30:21Z</updated>
    <published>2017-07-01T01:30:21Z</published>
    <title>Synthesizing Deep Neural Network Architectures using Biological Synaptic
  Strength Distributions</title>
    <summary>  In this work, we perform an exploratory study on synthesizing deep neural
networks using biological synaptic strength distributions, and the potential
influence of different distributions on modelling performance particularly for
the scenario associated with small data sets. Surprisingly, a CNN with
convolutional layer synaptic strengths drawn from biologically-inspired
distributions such as log-normal or correlated center-surround distributions
performed relatively well suggesting a possibility for designing deep neural
network architectures that do not require many data samples to learn, and can
sidestep current training procedures while maintaining or boosting modelling
performance.
</summary>
    <author>
      <name>A. H. Karimi</name>
    </author>
    <author>
      <name>M. J. Shafiee</name>
    </author>
    <author>
      <name>A. Ghodsi</name>
    </author>
    <author>
      <name>A. Wong</name>
    </author>
    <link href="http://arxiv.org/abs/1707.00081v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00081v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00451v1</id>
    <updated>2017-07-03T09:04:52Z</updated>
    <published>2017-07-03T09:04:52Z</published>
    <title>A Distance Between Populations for n-Points Crossover in Genetic
  Algorithms</title>
    <summary>  Genetic algorithms (GAs) are an optimization technique that has been
successfully used on many real-world problems. There exist different approaches
to their theoretical study. In this paper we complete a recently presented
approach to model one-point crossover using pretopologies (or Cech topologies)
in two ways. First, we extend it to the case of n-points crossover. Then, we
experimentally study how the distance distribution changes when the number of
crossover points increases.
</summary>
    <author>
      <name>Mauro Castelli</name>
    </author>
    <author>
      <name>Gianpiero Cattaneo</name>
    </author>
    <author>
      <name>Luca Manzoni</name>
    </author>
    <author>
      <name>Leonardo Vanneschi</name>
    </author>
    <link href="http://arxiv.org/abs/1707.00451v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00451v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.00884v1</id>
    <updated>2017-07-04T09:56:24Z</updated>
    <published>2017-07-04T09:56:24Z</published>
    <title>Identification of non-linear behavior models with restricted or
  redundant data</title>
    <summary>  This study presents a new strategy for the identification of material
parameters in the case of restricted or redundant data, based on a hybrid
approach combining a genetic algorithm and the Levenberg-Marquardt method. The
proposed methodology consists essentially in a statistically based topological
analysis of the search domain, after this one has been reduced by the analysis
of the parameters ranges. This is used to identify the parameters of a model
representing the behavior of damaged elastic, visco-elastic, plastic and
visco-plastic composite laminates. Optimization of the experimental tests on
tubular samples leads to the selective identification of these parameters.
</summary>
    <author>
      <name>S. Carbillet</name>
    </author>
    <author>
      <name>V. Guicheret-Retel</name>
    </author>
    <author>
      <name>F. Trivaudey</name>
    </author>
    <author>
      <name>F. Richard</name>
    </author>
    <author>
      <name>M. L. Boubakar</name>
    </author>
    <link href="http://arxiv.org/abs/1707.00884v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.00884v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.04853v2</id>
    <updated>2017-07-20T21:37:54Z</updated>
    <published>2017-07-16T10:12:13Z</published>
    <title>Overcoming Catastrophic Interference by Conceptors</title>
    <summary>  Catastrophic interference has been a major roadblock in the research of
continual learning. Here we propose a variant of the back-propagation
algorithm, "conceptor-aided back-prop" (CAB), in which gradients are shielded
by conceptors against degradation of previously learned tasks. Conceptors have
their origin in reservoir computing, where they have been previously shown to
overcome catastrophic forgetting. CAB extends these results to deep feedforward
networks. On the disjoint MNIST task CAB outperforms two other methods for
coping with catastrophic interference that have recently been proposed in the
deep learning field.
</summary>
    <author>
      <name>Xu He</name>
    </author>
    <author>
      <name>Herbert Jaeger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">14 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.04853v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.04853v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.00587v1</id>
    <updated>2017-08-02T03:23:17Z</updated>
    <published>2017-08-02T03:23:17Z</published>
    <title>Geometric Convolutional Neural Network for Analyzing Surface-Based
  Neuroimaging Data</title>
    <summary>  The conventional CNN, widely used for two-dimensional images, however, is not
directly applicable to non-regular geometric surface, such as a cortical
thickness. We propose Geometric CNN (gCNN) that deals with data representation
over a spherical surface and renders pattern recognition in a multi-shell mesh
structure. The classification accuracy for sex was significantly higher than
that of SVM and image based CNN. It only uses MRI thickness data to classify
gender but this method can expand to classify disease from other MRI or fMRI
data
</summary>
    <author>
      <name>Si-Baek Seong</name>
    </author>
    <author>
      <name>Chongwon Pae</name>
    </author>
    <author>
      <name>Hae-Jeong Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.00587v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.00587v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.01368v1</id>
    <updated>2017-08-04T03:21:23Z</updated>
    <published>2017-08-04T03:21:23Z</published>
    <title>A novel metaheuristic method for solving constrained engineering
  optimization problems: Drone Squadron Optimization</title>
    <summary>  Several constrained optimization problems have been adequately solved over
the years thanks to advances in the metaheuristics area. In this paper, we
evaluate a novel self-adaptive and auto-constructive metaheuristic called Drone
Squadron Optimization (DSO) in solving constrained engineering design problems.
This paper evaluates DSO with death penalty on three widely tested engineering
design problems. Results show that the proposed approach is competitive with
some very popular metaheuristics.
</summary>
    <author>
      <name>Vinícius Veloso de Melo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.01368v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.01368v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.03417v1</id>
    <updated>2017-08-11T00:41:56Z</updated>
    <published>2017-08-11T00:41:56Z</published>
    <title>GlobeNet: Convolutional Neural Networks for Typhoon Eye Tracking from
  Remote Sensing Imagery</title>
    <summary>  Advances in remote sensing technologies have made it possible to use
high-resolution visual data for weather observation and forecasting tasks. We
propose the use of multi-layer neural networks for understanding complex
atmospheric dynamics based on multichannel satellite images. The capability of
our model was evaluated by using a linear regression task for single typhoon
coordinates prediction. A specific combination of models and different
activation policies enabled us to obtain an interesting prediction result in
the northeastern hemisphere (ENH).
</summary>
    <author>
      <name>Seungkyun Hong</name>
    </author>
    <author>
      <name>Seongchan Kim</name>
    </author>
    <author>
      <name>Minsu Joh</name>
    </author>
    <author>
      <name>Sa-kwang Song</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review as a workshop paper at CI 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.03417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.03417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.06008v2</id>
    <updated>2019-01-18T10:35:56Z</updated>
    <published>2017-08-20T19:29:44Z</published>
    <title>Boltzmann machines and energy-based models</title>
    <summary>  We review Boltzmann machines and energy-based models. A Boltzmann machine
defines a probability distribution over binary-valued patterns. One can learn
parameters of a Boltzmann machine via gradient based approaches in a way that
log likelihood of data is increased. The gradient and Hessian of a Boltzmann
machine admit beautiful mathematical representations, although computing them
is in general intractable. This intractability motivates approximate methods,
including Gibbs sampler and contrastive divergence, and tractable alternatives,
namely energy-based models.
</summary>
    <author>
      <name>Takayuki Osogami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages. The topics covered in this paper are presented in Part I of
  IJCAI-17 tutorial on energy-based machine learning.
  https://researcher.watson.ibm.com/researcher/view_group.php?id=7834</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.06008v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.06008v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1708.08557v2</id>
    <updated>2017-09-11T19:04:57Z</updated>
    <published>2017-08-28T23:08:21Z</published>
    <title>A parameterized activation function for learning fuzzy logic operations
  in deep neural networks</title>
    <summary>  We present a deep learning architecture for learning fuzzy logic expressions.
Our model uses an innovative, parameterized, differentiable activation function
that can learn a number of logical operations by gradient descent. This
activation function allows a neural network to determine the relationships
between its input variables and provides insight into the logical significance
of learned network parameters. We provide a theoretical basis for this
parameterization and demonstrate its effectiveness and utility by successfully
applying our model to five classification problems from the UCI Machine
Learning Repository.
</summary>
    <author>
      <name>Luke B. Godfrey</name>
    </author>
    <author>
      <name>Michael S. Gashler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, IEEE SMC 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1708.08557v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1708.08557v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.03247v1</id>
    <updated>2017-09-11T05:37:53Z</updated>
    <published>2017-09-11T05:37:53Z</published>
    <title>Evolution of Convolutional Highway Networks</title>
    <summary>  Convolutional highways are deep networks based on multiple stacked
convolutional layers for feature preprocessing. We introduce an evolutionary
algorithm (EA) for optimization of the structure and hyperparameters of
convolutional highways and demonstrate the potential of this optimization
setting on the well-known MNIST data set. The (1+1)-EA employs Rechenberg's
mutation rate control and a niching mechanism to overcome local optima adapts
the optimization approach. An experimental study shows that the EA is capable
of improving the state-of-the-art network contribution and of evolving highway
networks from scratch.
</summary>
    <author>
      <name>Oliver Kramer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.03247v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.03247v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.07432v2</id>
    <updated>2017-10-25T22:31:34Z</updated>
    <published>2017-09-21T17:50:04Z</published>
    <title>Dynamic Evaluation of Neural Sequence Models</title>
    <summary>  We present methodology for using dynamic evaluation to improve neural
sequence models. Models are adapted to recent history via a gradient descent
based mechanism, causing them to assign higher probabilities to re-occurring
sequential patterns. Dynamic evaluation outperforms existing adaptation
approaches in our comparisons. Dynamic evaluation improves the state-of-the-art
word-level perplexities on the Penn Treebank and WikiText-2 datasets to 51.1
and 44.3 respectively, and the state-of-the-art character-level cross-entropies
on the text8 and Hutter Prize datasets to 1.19 bits/char and 1.08 bits/char
respectively.
</summary>
    <author>
      <name>Ben Krause</name>
    </author>
    <author>
      <name>Emmanuel Kahembwe</name>
    </author>
    <author>
      <name>Iain Murray</name>
    </author>
    <author>
      <name>Steve Renals</name>
    </author>
    <link href="http://arxiv.org/abs/1709.07432v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.07432v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09227v1</id>
    <updated>2017-09-26T19:20:31Z</updated>
    <published>2017-09-26T19:20:31Z</published>
    <title>Optimizing PID parameters with machine learning</title>
    <summary>  This paper examines the Evolutionary programming (EP) method for optimizing
PID parameters. PID is the most common type of regulator within control theory,
partly because it's relatively simple and yields stable results for most
applications. The p, i and d parameters vary for each application; therefore,
choosing the right parameters is crucial for obtaining good results but also
somewhat difficult. EP is a derivative-free optimization algorithm which makes
it suitable for PID optimization. The experiments in this paper demonstrate the
power of EP to solve the problem of optimizing PID parameters without getting
stuck in local minimums.
</summary>
    <author>
      <name>Adam Nyberg</name>
    </author>
    <link href="http://arxiv.org/abs/1709.09227v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09227v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1709.09840v2</id>
    <updated>2021-01-24T04:39:44Z</updated>
    <published>2017-09-28T08:02:16Z</published>
    <title>PSA: A novel optimization algorithm based on survival rules of porcellio
  scaber</title>
    <summary>  Bio-inspired algorithms such as neural network algorithms and genetic
algorithms have received a significant amount of attention in both academic and
engineering societies. In this paper, based on the observation of two major
survival rules of a species of woodlice, i.e., porcellio scaber, we present an
algorithm called the porcellio scaber algorithm (PSA) for solving general
unconstrained optimization problems, including differentiable and
non-differential ones as well as the case with local optima. Numerical results
based on benchmark problems are presented to validate the efficacy of PSA.
</summary>
    <author>
      <name>Yinyan Zhang</name>
    </author>
    <author>
      <name>Pei Zhang</name>
    </author>
    <author>
      <name>Shuai Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1709.09840v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1709.09840v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.01013v1</id>
    <updated>2017-10-03T07:21:03Z</updated>
    <published>2017-10-03T07:21:03Z</published>
    <title>Training Feedforward Neural Networks with Standard Logistic Activations
  is Feasible</title>
    <summary>  Training feedforward neural networks with standard logistic activations is
considered difficult because of the intrinsic properties of these sigmoidal
functions. This work aims at showing that these networks can be trained to
achieve generalization performance comparable to those based on hyperbolic
tangent activations. The solution consists on applying a set of conditions in
parameter initialization, which have been derived from the study of the
properties of a single neuron from an information-theoretic perspective. The
proposed initialization is validated through an extensive experimental
analysis.
</summary>
    <author>
      <name>Emanuele Sansone</name>
    </author>
    <author>
      <name>Francesco G. B. De Natale</name>
    </author>
    <link href="http://arxiv.org/abs/1710.01013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.01013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03414v1</id>
    <updated>2017-10-10T06:14:58Z</updated>
    <published>2017-10-10T06:14:58Z</published>
    <title>Network of Recurrent Neural Networks</title>
    <summary>  We describe a class of systems theory based neural networks called "Network
Of Recurrent neural networks" (NOR), which introduces a new structure level to
RNN related models. In NOR, RNNs are viewed as the high-level neurons and are
used to build the high-level layers. More specifically, we propose several
methodologies to design different NOR topologies according to the theory of
system evolution. Then we carry experiments on three different tasks to
evaluate our implementations. Experimental results show our models outperform
simple RNN remarkably under the same number of parameters, and sometimes
achieve even better results than GRU and LSTM.
</summary>
    <author>
      <name>Chao-Ming Wang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Under review as a conference paper at AAAI 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.03414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.03996v3</id>
    <updated>2017-10-22T07:49:54Z</updated>
    <published>2017-10-11T10:49:59Z</published>
    <title>A Simple Yet Efficient Rank One Update for Covariance Matrix Adaptation</title>
    <summary>  In this paper, we propose an efficient approximated rank one update for
covariance matrix adaptation evolution strategy (CMA-ES). It makes use of two
evolution paths as simple as that of CMA-ES, while avoiding the computational
matrix decomposition. We analyze the algorithms' properties and behaviors. We
experimentally study the proposed algorithm's performances. It generally
outperforms or performs competitively to the Cholesky CMA-ES.
</summary>
    <author>
      <name>Zhenhua Li</name>
    </author>
    <author>
      <name>Qingfu Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.03996v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.03996v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1710.10724v1</id>
    <updated>2017-10-30T00:05:03Z</updated>
    <published>2017-10-30T00:05:03Z</published>
    <title>BAS: Beetle Antennae Search Algorithm for Optimization Problems</title>
    <summary>  Meta-heuristic algorithms have become very popular because of powerful
performance on the optimization problem. A new algorithm called beetle antennae
search algorithm (BAS) is proposed in the paper inspired by the searching
behavior of longhorn beetles. The BAS algorithm imitates the function of
antennae and the random walking mechanism of beetles in nature, and then two
main steps of detecting and searching are implemented. Finally, the algorithm
is benchmarked on 2 well-known test functions, in which the numerical results
validate the efficacy of the proposed BAS algorithm.
</summary>
    <author>
      <name>Xiangyuan Jiang</name>
    </author>
    <author>
      <name>Shuai Li</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1710.10724v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1710.10724v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.02316v1</id>
    <updated>2017-12-06T18:33:55Z</updated>
    <published>2017-12-06T18:33:55Z</published>
    <title>Named Entity Sequence Classification</title>
    <summary>  Named Entity Recognition (NER) aims at locating and classifying named
entities in text. In some use cases of NER, including cases where detected
named entities are used in creating content recommendations, it is crucial to
have a reliable confidence level for the detected named entities. In this work
we study the problem of finding confidence levels for detected named entities.
We refer to this problem as Named Entity Sequence Classification (NESC). We
frame NESC as a binary classification problem and we use NER as well as
recurrent neural networks to find the probability of candidate named entity is
a real named entity. We apply this approach to Tweet texts and we show how we
could find named entities with high confidence levels from Tweets.
</summary>
    <author>
      <name>Mahdi Namazifar</name>
    </author>
    <link href="http://arxiv.org/abs/1712.02316v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.02316v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.06070v1</id>
    <updated>2017-12-17T07:53:36Z</updated>
    <published>2017-12-17T07:53:36Z</published>
    <title>Self-adaptation of Genetic Operators Through Genetic Programming
  Techniques</title>
    <summary>  Here we propose an evolutionary algorithm that self modifies its operators at
the same time that candidate solutions are evolved. This tackles convergence
and lack of diversity issues, leading to better solutions. Operators are
represented as trees and are evolved using genetic programming (GP) techniques.
The proposed approach is tested with real benchmark functions and an analysis
of operator evolution is provided.
</summary>
    <author>
      <name>Andres Felipe Cruz Salinas</name>
    </author>
    <author>
      <name>Jonatan Gomez Perdomo</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3071178.3071214</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3071178.3071214" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in GECCO 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.06070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.06070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.08319v1</id>
    <updated>2017-12-22T07:06:34Z</updated>
    <published>2017-12-22T07:06:34Z</published>
    <title>Virtual Sensor Modelling using Neural Networks with Coefficient-based
  Adaptive Weights and Biases Search Algorithm for Diesel Engines</title>
    <summary>  With the explosion in the field of Big Data and introduction of more
stringent emission norms every three to five years, automotive companies must
not only continue to enhance the fuel economy ratings of their products, but
also provide valued services to their customers such as delivering engine
performance and health reports at regular intervals. A reasonable solution to
both issues is installing a variety of sensors on the engine. Sensor data can
be used to develop fuel economy features and will directly indicate engine
performance. However, mounting a plethora of sensors is impractical in a very
cost-sensitive industry. Thus, virtual sensors can replace physical sensors by
reducing cost while capturing essential engine data.
</summary>
    <author>
      <name>Kushagra Rastogi</name>
    </author>
    <author>
      <name>Navreet Saini</name>
    </author>
    <link href="http://arxiv.org/abs/1712.08319v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.08319v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1712.09331v1</id>
    <updated>2017-12-22T19:30:51Z</updated>
    <published>2017-12-22T19:30:51Z</published>
    <title>Learning Based on CC1 and CC4 Neural Networks</title>
    <summary>  We propose that a general learning system should have three kinds of agents
corresponding to sensory, short-term, and long-term memory that implicitly will
facilitate context-free and context-sensitive aspects of learning. These three
agents perform mututally complementary functions that capture aspects of the
human cognition system. We investigate the use of CC1 and CC4 networks for use
as models of short-term and sensory memory.
</summary>
    <author>
      <name>Subhash Kak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages.arXiv admin note: text overlap with arXiv:0809.5087; text
  overlap with arXiv:cs/0601129 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/1712.09331v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1712.09331v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.06601v1</id>
    <updated>2018-01-19T23:39:15Z</updated>
    <published>2018-01-19T23:39:15Z</published>
    <title>CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs</title>
    <summary>  Deep Neural Networks are becoming increasingly popular in always-on IoT edge
devices performing data analytics right at the source, reducing latency as well
as energy consumption for data communication. This paper presents CMSIS-NN,
efficient kernels developed to maximize the performance and minimize the memory
footprint of neural network (NN) applications on Arm Cortex-M processors
targeted for intelligent IoT edge devices. Neural network inference based on
CMSIS-NN kernels achieves 4.6X improvement in runtime/throughput and 4.9X
improvement in energy efficiency.
</summary>
    <author>
      <name>Liangzhen Lai</name>
    </author>
    <author>
      <name>Naveen Suda</name>
    </author>
    <author>
      <name>Vikas Chandra</name>
    </author>
    <link href="http://arxiv.org/abs/1801.06601v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.06601v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.00180v2</id>
    <updated>2018-09-28T09:18:09Z</updated>
    <published>2018-02-01T07:34:50Z</published>
    <title>Machine learning and evolutionary techniques in interplanetary
  trajectory design</title>
    <summary>  After providing a brief historical overview on the synergies between
artificial intelligence research, in the areas of evolutionary computations and
machine learning, and the optimal design of interplanetary trajectories, we
propose and study the use of deep artificial neural networks to represent,
on-board, the optimal guidance profile of an interplanetary mission. The
results, limited to the chosen test case of an Earth-Mars orbital transfer,
extend the findings made previously for landing scenarios and quadcopter
dynamics, opening a new research area in interplanetary trajectory planning.
</summary>
    <author>
      <name>Dario Izzo</name>
    </author>
    <author>
      <name>Christopher Sprague</name>
    </author>
    <author>
      <name>Dharmesh Tailor</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to as a book chapter for a Springer book on "Optimization
  in Space Engineering"</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.00180v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.00180v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02006v1</id>
    <updated>2017-11-01T20:46:27Z</updated>
    <published>2017-11-01T20:46:27Z</published>
    <title>An Adaptive Genetic Algorithm for Solving N-Queens Problem</title>
    <summary>  In this paper a Metaheuristic approach for solving the N-Queens Problem is
introduced to find the best possible solution in a reasonable amount of time.
Genetic Algorithm is used with a novel fitness function as the Metaheuristic.
The aim of N-Queens Problem is to place N queens on an N x N chessboard, in a
way so that no queen is in conflict with the others. Chromosome representation
and genetic operations like Mutation and Crossover are described in detail.
Results show that this approach yields promising and satisfactory results in
less time compared to that obtained from the previous approaches for several
large values of N.
</summary>
    <author>
      <name>Uddalok Sarkar</name>
    </author>
    <author>
      <name>Sayan Nag</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.02006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.02987v1</id>
    <updated>2018-02-08T17:57:58Z</updated>
    <published>2018-02-08T17:57:58Z</published>
    <title>A Generalization Method of Partitioned Activation Function for Complex
  Number</title>
    <summary>  A method to convert real number partitioned activation function into complex
number one is provided. The method has 4em variations; 1 has potential to get
holomorphic activation, 2 has potential to conserve complex angle, and the last
1 guarantees interaction between real and imaginary parts. The method has been
applied to LReLU and SELU as examples. The complex number activation function
is an building block of complex number ANN, which has potential to properly
deal with complex number problems. But the complex activation is not well
established yet. Therefore, we propose a way to extend the partitioned real
activation to complex number.
</summary>
    <author>
      <name>HyeonSeok Lee</name>
    </author>
    <author>
      <name>Hyo Seon Park</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Complex Activation Function, Holomorphic, Phase-preserving,
  real-complex interaction</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.02987v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.02987v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.05991v2</id>
    <updated>2018-05-08T10:32:58Z</updated>
    <published>2018-02-16T15:49:10Z</published>
    <title>The N-Tuple Bandit Evolutionary Algorithm for Game Agent Optimisation</title>
    <summary>  This paper describes the N-Tuple Bandit Evolutionary Algorithm (NTBEA), an
optimisation algorithm developed for noisy and expensive discrete
(combinatorial) optimisation problems. The algorithm is applied to two
game-based hyper-parameter optimisation problems. The N-Tuple system directly
models the statistics, approximating the fitness and number of evaluations of
each modelled combination of parameters. The model is simple, efficient and
informative. Results show that the NTBEA significantly outperforms grid search
and an estimation of distribution algorithm.
</summary>
    <author>
      <name>Simon M Lucas</name>
    </author>
    <author>
      <name>Jialin Liu</name>
    </author>
    <author>
      <name>Diego Perez-Liebana</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures, 3 table. This is the final version of the article
  accepted by WCCI2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.05991v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.05991v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.06288v1</id>
    <updated>2018-02-17T20:42:04Z</updated>
    <published>2018-02-17T20:42:04Z</published>
    <title>Implementation of Neural Network and feature extraction to classify ECG
  signals</title>
    <summary>  This paper presents a suitable and efficient implementation of a feature
extraction algorithm (Pan Tompkins algorithm) on electrocardiography (ECG)
signals, for detection and classification of four cardiac diseases: Sleep
Apnea, Arrhythmia, Supraventricular Arrhythmia and Long Term Atrial
Fibrillation (AF) and differentiating them from the normal heart beat by using
pan Tompkins RR detection followed by feature extraction for classification
purpose .The paper also presents a new approach towards signal classification
using the existing neural networks classifiers.
</summary>
    <author>
      <name>R Karthik</name>
    </author>
    <author>
      <name>Dhruv Tyagi</name>
    </author>
    <author>
      <name>Amogh Raut</name>
    </author>
    <author>
      <name>Soumya Saxena</name>
    </author>
    <author>
      <name>Rajesh Kumar M</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">SPRINGER LNEE</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.06288v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.06288v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.07133v1</id>
    <updated>2018-02-20T14:42:35Z</updated>
    <published>2018-02-20T14:42:35Z</published>
    <title>Towards Deep Representation Learning with Genetic Programming</title>
    <summary>  Genetic Programming (GP) is an evolutionary algorithm commonly used for
machine learning tasks. In this paper we present a method that allows GP to
transform the representation of a large-scale machine learning dataset into a
more compact representation, by means of processing features from the original
representation at individual level. We develop as a proof of concept of this
method an autoencoder. We tested a preliminary version of our approach in a
variety of well-known machine learning image datasets. We speculate that this
method, used in an iterative manner, can produce results competitive with
state-of-art deep neural networks.
</summary>
    <author>
      <name>Lino Rodriguez-Coayahuitl</name>
    </author>
    <author>
      <name>Alicia Morales-Reyes</name>
    </author>
    <author>
      <name>Hugo Jair Escalante</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EuroGP preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.07133v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.07133v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08217v1</id>
    <updated>2018-02-22T18:15:53Z</updated>
    <published>2018-02-22T18:15:53Z</published>
    <title>A new model for Cerebellar computation</title>
    <summary>  The standard state space model is widely believed to account for the
cerebellar computation in motor adaptation tasks [1]. Here we show that several
recent experiments [2-4] where the visual feedback is irrelevant to the motor
response challenge the standard model. Furthermore, we propose a new model that
accounts for the the results presented in [2-4]. According to this new model,
learning and forgetting are coupled and are error size dependent. We also show
that under reasonable assumptions, our proposed model is the only model that
accounts for both the classical adaptation paradigm as well as the recent
experiments [2-4].
</summary>
    <author>
      <name>Reza Moazzezi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.08217v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08217v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08535v1</id>
    <updated>2018-02-23T14:04:30Z</updated>
    <published>2018-02-23T14:04:30Z</published>
    <title>Can Neural Networks Understand Logical Entailment?</title>
    <summary>  We introduce a new dataset of logical entailments for the purpose of
measuring models' ability to capture and exploit the structure of logical
expressions against an entailment prediction task. We use this task to compare
a series of architectures which are ubiquitous in the sequence-processing
literature, in addition to a new model class---PossibleWorldNets---which
computes entailment as a "convolution over possible worlds". Results show that
convolutional networks present the wrong inductive bias for this class of
problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM
RNNs due to their enhanced ability to exploit the syntax of logic, and
PossibleWorldNets outperform all benchmarks.
</summary>
    <author>
      <name>Richard Evans</name>
    </author>
    <author>
      <name>David Saxton</name>
    </author>
    <author>
      <name>David Amos</name>
    </author>
    <author>
      <name>Pushmeet Kohli</name>
    </author>
    <author>
      <name>Edward Grefenstette</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at ICLR 2018 (main conference)</arxiv:comment>
    <link href="http://arxiv.org/abs/1802.08535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.08590v1</id>
    <updated>2018-02-23T15:20:45Z</updated>
    <published>2018-02-23T15:20:45Z</published>
    <title>Reservoir computing with simple oscillators: Virtual and real networks</title>
    <summary>  The reservoir computing scheme is a machine learning mechanism which utilizes
the naturally occuring computational capabilities of dynamical systems. One
important subset of systems that has proven powerful both in experiments and
theory are delay-systems. In this work, we investigate the reservoir computing
performance of hybrid network-delay systems systematically by evaluating the
NARMA10 and the Sante Fe task.. We construct 'multiplexed networks' that can be
seen as intermediate steps on the scale from classical networks to the 'virtual
networks' of delay systems. We find that the delay approach can be extended to
the network case without loss of computational power, enabling the construction
of faster reservoir computing substrates.
</summary>
    <author>
      <name>André Röhm</name>
    </author>
    <author>
      <name>Kathy Lüdge</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/2399-6528/aad56d</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/2399-6528/aad56d" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Phys. Commun. 2 (2018)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1802.08590v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.08590v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1802.10393v1</id>
    <updated>2018-02-28T12:59:43Z</updated>
    <published>2018-02-28T12:59:43Z</published>
    <title>A Bayesian Model for Activities Recommendation and Event Structure
  Optimization Using Visitors Tracking</title>
    <summary>  In events that are composed by many activities, there is a problem that
involves retrieve and management the information of visitors that are visiting
the activities. This management is crucial to find some activities that are
drawing attention of visitors; identify an ideal positioning for activities;
which path is more frequented by visitors. In this work, these features are
studied using Complex Network theory. For the beginning, an artificial database
was generated to study the mentioned features. Secondly, this work shows a
method to optimize the event structure that is better than a random method and
a recommendation system that achieves ~95% of accuracy.
</summary>
    <author>
      <name>Henrique X. Goulart</name>
    </author>
    <author>
      <name>Guilherme A. Wachs-Lopes</name>
    </author>
    <link href="http://arxiv.org/abs/1802.10393v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1802.10393v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.06127v1</id>
    <updated>2018-03-16T09:43:47Z</updated>
    <published>2018-03-16T09:43:47Z</published>
    <title>Towards Advanced Phenotypic Mutations in Cartesian Genetic Programming</title>
    <summary>  Cartesian Genetic Programming is often used with a point mutation as the sole
genetic operator. In this paper, we propose two phenotypic mutation techniques
and take a step towards advanced phenotypic mutations in Cartesian Genetic
Programming. The functionality of the proposed mutations is inspired by
biological evolution which mutates DNA sequences by inserting and deleting
nucleotides. Experiments with symbolic regression and boolean functions
problems show a better search performance when the proposed mutations are in
use. The results of our experiments indicate that the use of phenotypic
mutations could be beneficial for the use of Cartesian Genetic Programming.
</summary>
    <author>
      <name>Roman Kalkreuth</name>
    </author>
    <link href="http://arxiv.org/abs/1803.06127v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.06127v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.07488v3</id>
    <updated>2020-02-12T14:19:04Z</updated>
    <published>2018-03-20T15:38:40Z</published>
    <title>Dynamic Variational Autoencoders for Visual Process Modeling</title>
    <summary>  This work studies the problem of modeling visual processes by leveraging deep
generative architectures for learning linear, Gaussian representations from
observed sequences. We propose a joint learning framework, combining a vector
autoregressive model and Variational Autoencoders. This results in an
architecture that allows Variational Autoencoders to simultaneously learn a
non-linear observation as well as a linear state model from sequences of
frames. We validate our approach on artificial sequences and dynamic textures.
</summary>
    <author>
      <name>Alexander Sagel</name>
    </author>
    <author>
      <name>Hao Shen</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ICASSP40776.2020.9053660</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ICASSP40776.2020.9053660" rel="related"/>
    <link href="http://arxiv.org/abs/1803.07488v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.07488v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.08165v1</id>
    <updated>2018-03-21T22:59:53Z</updated>
    <published>2018-03-21T22:59:53Z</published>
    <title>Comparing Fixed and Adaptive Computation Time for Recurrent Neural
  Networks</title>
    <summary>  Adaptive Computation Time for Recurrent Neural Networks (ACT) is one of the
most promising architectures for variable computation. ACT adapts to the input
sequence by being able to look at each sample more than once, and learn how
many times it should do it. In this paper, we compare ACT to Repeat-RNN, a
novel architecture based on repeating each sample a fixed number of times. We
found surprising results, where Repeat-RNN performs as good as ACT in the
selected tasks. Source code in TensorFlow and PyTorch is publicly available at
https://imatge-upc.github.io/danifojo-2018-repeatrnn/
</summary>
    <author>
      <name>Daniel Fojo</name>
    </author>
    <author>
      <name>Víctor Campos</name>
    </author>
    <author>
      <name>Xavier Giro-i-Nieto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as workshop paper at ICLR 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1803.08165v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.08165v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1803.09356v1</id>
    <updated>2018-03-25T22:01:32Z</updated>
    <published>2018-03-25T22:01:32Z</published>
    <title>Neural Nets via Forward State Transformation and Backward Loss
  Transformation</title>
    <summary>  This article studies (multilayer perceptron) neural networks with an emphasis
on the transformations involved --- both forward and backward --- in order to
develop a semantical/logical perspective that is in line with standard program
semantics. The common two-pass neural network training algorithms make this
viewpoint particularly fitting. In the forward direction, neural networks act
as state transformers. In the reverse direction, however, neural networks
change losses of outputs to losses of inputs, thereby acting like a
(real-valued) predicate transformer. In this way, backpropagation is functorial
by construction, as shown earlier in recent other work. We illustrate this
perspective by training a simple instance of a neural network.
</summary>
    <author>
      <name>Bart Jacobs</name>
    </author>
    <author>
      <name>David Sprunger</name>
    </author>
    <link href="http://arxiv.org/abs/1803.09356v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1803.09356v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20 (Primary) 18C50 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; F.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.06511v1</id>
    <updated>2018-04-18T00:20:28Z</updated>
    <published>2018-04-18T00:20:28Z</published>
    <title>Fast Weight Long Short-Term Memory</title>
    <summary>  Associative memory using fast weights is a short-term memory mechanism that
substantially improves the memory capacity and time scale of recurrent neural
networks (RNNs). As recent studies introduced fast weights only to regular
RNNs, it is unknown whether fast weight memory is beneficial to gated RNNs. In
this work, we report a significant synergy between long short-term memory
(LSTM) networks and fast weight associative memories. We show that this
combination, in learning associative retrieval tasks, results in much faster
training and lower test error, a performance boost most prominent at high
memory task difficulties.
</summary>
    <author>
      <name>T. Anderson Keller</name>
    </author>
    <author>
      <name>Sharath Nittur Sridhar</name>
    </author>
    <author>
      <name>Xin Wang</name>
    </author>
    <link href="http://arxiv.org/abs/1804.06511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.06511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.07999v1</id>
    <updated>2018-04-21T17:15:11Z</updated>
    <published>2018-04-21T17:15:11Z</published>
    <title>Swarm Intelligence: Past, Present and Future</title>
    <summary>  Many optimization problems in science and engineering are challenging to
solve, and the current trend is to use swarm intelligence (SI) and SI-based
algorithms to tackle such challenging problems. Some significant developments
have been made in recent years, though there are still many open problems in
this area. This paper provides a short but timely analysis about SI-based
algorithms and their links with self-organization. Different characteristics
and properties are analyzed here from both mathematical and qualitative
perspectives. Future research directions are outlined and open questions are
also highlighted.
</summary>
    <author>
      <name>Xin-She Yang</name>
    </author>
    <author>
      <name>Suash Deb</name>
    </author>
    <author>
      <name>Yuxin Zhao</name>
    </author>
    <author>
      <name>Simon Fong</name>
    </author>
    <author>
      <name>Xingshi He</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00500-017-2810-5</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00500-017-2810-5" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Soft Computing, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1804.07999v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.07999v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T99, 90C30, 90C59" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1804.10727v2</id>
    <updated>2019-05-20T21:06:44Z</updated>
    <published>2018-04-28T03:03:45Z</published>
    <title>Low-memory convolutional neural networks through incremental depth-first
  processing</title>
    <summary>  We introduce an incremental processing scheme for convolutional neural
network (CNN) inference, targeted at embedded applications with limited memory
budgets. Instead of processing layers one by one, individual input pixels are
propagated through all parts of the network they can influence under the given
structural constraints. This depth-first updating scheme comes with hard bounds
on the memory footprint: the memory required is constant in the case of 1D
input and proportional to the square root of the input dimension in the case of
2D input.
</summary>
    <author>
      <name>Jonathan Binas</name>
    </author>
    <author>
      <name>Yoshua Bengio</name>
    </author>
    <link href="http://arxiv.org/abs/1804.10727v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1804.10727v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00149v2</id>
    <updated>2018-06-14T03:47:47Z</updated>
    <published>2018-06-01T00:46:29Z</published>
    <title>q-Neurons: Neuron Activations based on Stochastic Jackson's Derivative
  Operators</title>
    <summary>  We propose a new generic type of stochastic neurons, called $q$-neurons, that
considers activation functions based on Jackson's $q$-derivatives with
stochastic parameters $q$. Our generalization of neural network architectures
with $q$-neurons is shown to be both scalable and very easy to implement. We
demonstrate experimentally consistently improved performances over
state-of-the-art standard activation functions, both on training and testing
loss functions.
</summary>
    <author>
      <name>Frank Nielsen</name>
    </author>
    <author>
      <name>Ke Sun</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TNNLS.2020.3005167</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TNNLS.2020.3005167" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 5 figures, 1 table</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Neural Networks and Learning Systems (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.00149v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00149v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.00797v2</id>
    <updated>2018-08-26T11:17:29Z</updated>
    <published>2018-06-03T13:38:41Z</published>
    <title>Echo state networks are universal</title>
    <summary>  This paper shows that echo state networks are universal uniform approximants
in the context of discrete-time fading memory filters with uniformly bounded
inputs defined on negative infinite times. This result guarantees that any
fading memory input/output system in discrete time can be realized as a simple
finite-dimensional neural network-type state-space model with a static linear
readout map. This approximation is valid for infinite time intervals. The proof
of this statement is based on fundamental results, also presented in this work,
about the topological nature of the fading memory property and about reservoir
computing systems generated by continuous reservoir maps.
</summary>
    <author>
      <name>Lyudmila Grigoryeva</name>
    </author>
    <author>
      <name>Juan-Pablo Ortega</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.00797v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.00797v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.01631v1</id>
    <updated>2018-04-22T10:27:54Z</updated>
    <published>2018-04-22T10:27:54Z</published>
    <title>Cuckoo Search: State-of-the-Art and Opportunities</title>
    <summary>  Since the development of cuckoo search (CS) by Yang and Deb in 2009, CS has
been applied in a diverse range of applications. This paper first outlines the
key features of the algorithm and its variants, and then briefly summarizes the
state-of-the-art developments in many applications. The opportunities for
further research are also identified.
</summary>
    <author>
      <name>Xin-She Yang</name>
    </author>
    <author>
      <name>Suash Deb</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISCMI.2017.8279597</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISCMI.2017.8279597" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4th IEEE Conference on Soft Computing and Machine Learning
  (ISCMI2017), Mauritius, 23-24 Nov 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.01631v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.01631v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C30, 68W20, 68T99" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.05695v1</id>
    <updated>2018-06-14T18:10:46Z</updated>
    <published>2018-06-14T18:10:46Z</published>
    <title>Evolving simple programs for playing Atari games</title>
    <summary>  Cartesian Genetic Programming (CGP) has previously shown capabilities in
image processing tasks by evolving programs with a function set specialized for
computer vision. A similar approach can be applied to Atari playing. Programs
are evolved using mixed type CGP with a function set suited for matrix
operations, including image processing, but allowing for controller behavior to
emerge. While the programs are relatively small, many controllers are
competitive with state of the art methods for the Atari benchmark set and
require less training time. By evaluating the programs of the best evolved
individuals, simple but effective strategies can be found.
</summary>
    <author>
      <name>Dennis G Wilson</name>
    </author>
    <author>
      <name>Sylvain Cussat-Blanc</name>
    </author>
    <author>
      <name>Hervé Luga</name>
    </author>
    <author>
      <name>Julian F Miller</name>
    </author>
    <link href="http://arxiv.org/abs/1806.05695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.05695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.06010v1</id>
    <updated>2018-05-18T03:57:41Z</updated>
    <published>2018-05-18T03:57:41Z</published>
    <title>A Self-Replication Basis for Designing Complex Agents</title>
    <summary>  In this work, we describe a self-replication-based mechanism for designing
agents of increasing complexity. We demonstrate the validity of this approach
by solving simple, standard evolutionary computation problems in simulation. In
the context of these simulation results, we describe the fundamental
differences of this approach when compared to traditional approaches. Further,
we highlight the possible advantages of applying this approach to the problem
of designing complex artificial agents, along with the potential drawbacks and
issues to be addressed in the future.
</summary>
    <author>
      <name>Thommen George Karimpanal</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3205651.3208762</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3205651.3208762" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Genetic and Evolutionary Computation Conference
  Companion, Pages 45-46, Kyoto, Japan, July 15 - 19, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.06010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.06010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08099v2</id>
    <updated>2018-12-19T16:44:25Z</updated>
    <published>2018-06-21T07:55:22Z</published>
    <title>Lamarckian Evolution of Convolutional Neural Networks</title>
    <summary>  Convolutional neural networks belong to the most successul image classifiers,
but the adaptation of their network architecture to a particular problem is
computationally expensive. We show that an evolutionary algorithm saves
training time during the network architecture optimization, if learned network
weights are inherited over generations by Lamarckian evolution. Experiments on
typical image datasets show similar or significantly better test accuracies and
improved convergence speeds compared to two different baselines without weight
inheritance. On CIFAR-10 and CIFAR-100 a 75 % improvement in data efficiency is
observed.
</summary>
    <author>
      <name>Jonas Prellberg</name>
    </author>
    <author>
      <name>Oliver Kramer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at PPSN 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.08099v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08099v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.09731v1</id>
    <updated>2018-06-26T00:03:23Z</updated>
    <published>2018-06-26T00:03:23Z</published>
    <title>Evotype: Towards the Evolution of Type Stencils</title>
    <summary>  Typefaces are an essential resource employed by graphic designers. The
increasing demand for innovative type design work increases the need for good
technological means to assist the designer in the creation of a typeface. We
present an evolutionary computation approach for the generation of type
stencils to draw coherent glyphs for different characters. The proposed system
employs a Genetic Algorithm to evolve populations of type stencils. The
evaluation of each candidate stencil uses a hill climbing algorithm to search
the best configurations to draw the target glyphs. We study the interplay
between legibility, coherence and expressiveness, and show how our framework
can be used in practice.
</summary>
    <author>
      <name>Tiago Martins</name>
    </author>
    <author>
      <name>João Correia</name>
    </author>
    <author>
      <name>Ernesto Costa</name>
    </author>
    <author>
      <name>Penousal Machado</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-319-77583-8_20</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-319-77583-8_20" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EvoMUSART 2018 Best paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.09731v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.09731v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.00188v2</id>
    <updated>2018-07-11T09:26:40Z</updated>
    <published>2018-06-30T15:08:48Z</published>
    <title>Benchmarking the Hill-Valley Evolutionary Algorithm for the GECCO 2018
  Competition on Niching Methods Multimodal Optimization</title>
    <summary>  This report presents benchmarking results of the latest version of the
Hill-Valley Evolutionary Algorithm (HillVallEA) on the CEC2013 niching
benchmark suite. The benchmarking follows restrictions required by the GECCO
2018 competition on Niching methods for Multimodal Optimization. In particular,
no problem dependent parameter tuning is performed. A number of adjustments
have been made to original publication of HillVallEA that are discussed in this
report.
</summary>
    <author>
      <name>S. C. Maree</name>
    </author>
    <author>
      <name>T. Alderliesten</name>
    </author>
    <author>
      <name>D. Thierens</name>
    </author>
    <author>
      <name>P. A. N. Bosman</name>
    </author>
    <link href="http://arxiv.org/abs/1807.00188v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.00188v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.05076v1</id>
    <updated>2018-07-12T14:40:06Z</updated>
    <published>2018-07-12T14:40:06Z</published>
    <title>Metalearning with Hebbian Fast Weights</title>
    <summary>  We unify recent neural approaches to one-shot learning with older ideas of
associative memory in a model for metalearning. Our model learns jointly to
represent data and to bind class labels to representations in a single shot. It
builds representations via slow weights, learned across tasks through SGD,
while fast weights constructed by a Hebbian learning rule implement one-shot
binding for each new task. On the Omniglot, Mini-ImageNet, and Penn Treebank
one-shot learning benchmarks, our model achieves state-of-the-art results.
</summary>
    <author>
      <name>Tsendsuren Munkhdalai</name>
    </author>
    <author>
      <name>Adam Trischler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 3 figures, 4 tables. arXiv admin note: text overlap with
  arXiv:1712.09926</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.05076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.05076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.07839v1</id>
    <updated>2018-07-20T13:50:55Z</updated>
    <published>2018-07-20T13:50:55Z</published>
    <title>Distance-based Kernels for Surrogate Model-based Neuroevolution</title>
    <summary>  The topology optimization of artificial neural networks can be particularly
difficult if the fitness evaluations require expensive experiments or
simulations. For that reason, the optimization methods may need to be supported
by surrogate models. We propose different distances for a suitable surrogate
model, and compare them in a simple numerical test scenario.
</summary>
    <author>
      <name>Jörg Stork</name>
    </author>
    <author>
      <name>Martin Zaefferer</name>
    </author>
    <author>
      <name>Thomas Bartz-Beielstein</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure. This publication was accepted to the Developmental
  Neural Networks Workshop of the Parallel Problem Solving from Nature 2018
  (PPSN XV) conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.07839v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.07839v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.08655v1</id>
    <updated>2018-06-29T21:06:06Z</updated>
    <published>2018-06-29T21:06:06Z</published>
    <title>Training Humans and Machines</title>
    <summary>  For many years, researchers in psychology, education, statistics, and machine
learning have been developing practical methods to improve learning speed,
retention, and generalizability, and this work has been successful. Many of
these methods are rooted in common underlying principles that seem to drive
learning and overlearning in both humans and machines. I present a review of a
small part of this work to point to potentially novel applications in both
machine and human learning that may be worth exploring.
</summary>
    <author>
      <name>Aki Nikolaidis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Computational Cognitive Neuroscience</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.08655v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.08655v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09488v1</id>
    <updated>2018-07-25T08:55:45Z</updated>
    <published>2018-07-25T08:55:45Z</published>
    <title>Prototype Discovery using Quality-Diversity</title>
    <summary>  An iterative computer-aided ideation procedure is introduced, building on
recent quality-diversity algorithms, which search for diverse as well as
high-performing solutions. Dimensionality reduction is used to define a
similarity space, in which solutions are clustered into classes. These classes
are represented by prototypes, which are presented to the user for selection.
In the next iteration, quality-diversity focuses on searching within the
selected class. A quantitative analysis is performed on a 2D airfoil, and a
more complex 3D side view mirror domain shows how computer-aided ideation can
help to enhance engineers' intuition while allowing their design decisions to
influence the design process.
</summary>
    <author>
      <name>Alexander Hagg</name>
    </author>
    <author>
      <name>Alexander Asteroth</name>
    </author>
    <author>
      <name>Thomas Bäck</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Parallel Problem Solving using Nature 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09488v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09488v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.09760v1</id>
    <updated>2018-07-24T19:13:22Z</updated>
    <published>2018-07-24T19:13:22Z</published>
    <title>Method for Hybrid Precision Convolutional Neural Network Representation</title>
    <summary>  This invention addresses fixed-point representations of convolutional neural
networks (CNN) in integrated circuits. When quantizing a CNN for a practical
implementation there is a trade-off between the precision used for operations
between coefficients and data and the accuracy of the system. A homogenous
representation may not be sufficient to achieve the best level of performance
at a reasonable cost in implementation complexity or power consumption.
Parsimonious ways of representing data and coefficients are needed to improve
power efficiency and throughput while maintaining accuracy of a CNN.
</summary>
    <author>
      <name>Mo'taz Al-Hami</name>
    </author>
    <author>
      <name>Marcin Pietron</name>
    </author>
    <author>
      <name>Rishi Kumar</name>
    </author>
    <author>
      <name>Raul A. Casas</name>
    </author>
    <author>
      <name>Samer L. Hijazi</name>
    </author>
    <author>
      <name>Chris Rowen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Cadence Design Systems</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.09760v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.09760v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.10068v1</id>
    <updated>2018-07-26T11:17:12Z</updated>
    <published>2018-07-26T11:17:12Z</published>
    <title>A Linear Constrained Optimization Benchmark For Probabilistic Search
  Algorithms: The Rotated Klee-Minty Problem</title>
    <summary>  The development, assessment, and comparison of randomized search algorithms
heavily rely on benchmarking. Regarding the domain of constrained optimization,
the number of currently available benchmark environments bears no relation to
the number of distinct problem features. The present paper advances a proposal
of a scalable linear constrained optimization problem that is suitable for
benchmarking Evolutionary Algorithms. By comparing two recent EA variants, the
linear benchmarking environment is demonstrated.
</summary>
    <author>
      <name>Michael Hellwig</name>
    </author>
    <author>
      <name>Hans-Georg Beyer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This preprint consists of 12 pages including 3 figures and 4 tables.
  The final authenticated publication will be referred to as soon as possible.
  Current status: submitted</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.10068v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.10068v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00206v2</id>
    <updated>2020-07-08T11:38:27Z</updated>
    <published>2018-08-01T07:34:46Z</published>
    <title>Beetle Swarm Optimization Algorithm:Theory and Application</title>
    <summary>  In this paper, a new meta-heuristic algorithm, called beetle swarm
optimization algorithm, is proposed by enhancing the performance of swarm
optimization through beetle foraging principles. The performance of 23
benchmark functions is tested and compared with widely used algorithms,
including particle swarm optimization algorithm, genetic algorithm (GA) and
grasshopper optimization algorithm . Numerical experiments show that the beetle
swarm optimization algorithm outperforms its counterparts. Besides, to
demonstrate the practical impact of the proposed algorithm, two classic
engineering design problems, namely, pressure vessel design problem and
himmelblaus optimization problem, are also considered and the proposed beetle
swarm optimization algorithm is shown to be competitive in those applications.
</summary>
    <author>
      <name>Tiantian Wang</name>
    </author>
    <author>
      <name>Long Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1808.00206v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00206v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.00260v1</id>
    <updated>2018-08-01T10:53:01Z</updated>
    <published>2018-08-01T10:53:01Z</published>
    <title>A Review on the Application of Natural Computing in Environmental
  Informatics</title>
    <summary>  Natural computing offers new opportunities to understand, model and analyze
the complexity of the physical and human-created environment. This paper
examines the application of natural computing in environmental informatics, by
investigating related work in this research field. Various nature-inspired
techniques are presented, which have been employed to solve different relevant
problems. Advantages and disadvantages of these techniques are discussed,
together with analysis of how natural computing is generally used in
environmental research.
</summary>
    <author>
      <name>Andreas Kamilaris</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. of EnviroInfo 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.00260v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.00260v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.02822v1</id>
    <updated>2018-08-08T15:23:14Z</updated>
    <published>2018-08-08T15:23:14Z</published>
    <title>Backprop Evolution</title>
    <summary>  The back-propagation algorithm is the cornerstone of deep learning. Despite
its importance, few variations of the algorithm have been attempted. This work
presents an approach to discover new variations of the back-propagation
equation. We use a domain specific lan- guage to describe update equations as a
list of primitive functions. An evolution-based method is used to discover new
propagation rules that maximize the generalization per- formance after a few
epochs of training. We find several update equations that can train faster with
short training times than standard back-propagation, and perform similar as
standard back-propagation at convergence.
</summary>
    <author>
      <name>Maximilian Alber</name>
    </author>
    <author>
      <name>Irwan Bello</name>
    </author>
    <author>
      <name>Barret Zoph</name>
    </author>
    <author>
      <name>Pieter-Jan Kindermans</name>
    </author>
    <author>
      <name>Prajit Ramachandran</name>
    </author>
    <author>
      <name>Quoc Le</name>
    </author>
    <link href="http://arxiv.org/abs/1808.02822v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.02822v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.07530v1</id>
    <updated>2018-08-22T19:20:45Z</updated>
    <published>2018-08-22T19:20:45Z</published>
    <title>An Overview of Datatype Quantization Techniques for Convolutional Neural
  Networks</title>
    <summary>  Convolutional Neural Networks (CNNs) are becoming increasingly popular due to
their superior performance in the domain of computer vision, in applications
such as objection detection and recognition. However, they demand complex,
power-consuming hardware which makes them unsuitable for implementation on
low-power mobile and embedded devices. In this paper, a description and
comparison of various techniques is presented which aim to mitigate this
problem. This is primarily achieved by quantizing the floating-point weights
and activations to reduce the hardware requirements, and adapting the training
and inference algorithms to maintain the network's performance.
</summary>
    <author>
      <name>Ali Athar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1808.07530v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.07530v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Computer Vision: I.5.4, Data compaction and compression: E.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1808.10866v1</id>
    <updated>2018-08-31T17:48:53Z</updated>
    <published>2018-08-31T17:48:53Z</published>
    <title>Algoritmos Genéticos Aplicado ao Problema de Roteamento de Veículos</title>
    <summary>  Routing problems are often faced by companies who serve costumers through
vehicles. Such problems have a challenging structure to optimize, despite the
recent advances in combinatorial optimization. The goal of this project is to
study and propose optimization algorithms to the vehicle routing problems
(VRP). Focus will be on the problem variant in which the length of the route is
restricted by a constant. A real problem will be tackled: optimization of
postmen routes. Such problem was modeled as {multi-objective} in a roadmap with
25 vehicles and {30,000 deliveries} per day.
</summary>
    <author>
      <name>Felipe F. Müller</name>
    </author>
    <author>
      <name>Luis A. A. Meira</name>
    </author>
    <link href="http://arxiv.org/abs/1808.10866v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1808.10866v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.01125v1</id>
    <updated>2018-10-02T09:10:42Z</updated>
    <published>2018-10-02T09:10:42Z</published>
    <title>Robust Optimization through Neuroevolution</title>
    <summary>  We propose a method for evolving solutions that are robust with respect to
variations of the environmental conditions (i.e. that can operate effectively
in new conditions immediately, without the need to adapt to variations). The
obtained results show how the method proposed is effective and computational
tractable. It permits to improve performance on an extended version of the
double-pole balancing problem, to outperform the best available human-designed
controllers on a car racing problem, and to generate rather effective solutions
for a swarm robotic problem. The comparison of different algorithms indicates
that the CMA-ES and xNES methods, that operate by optimizing a distribution of
parameters, represent the best options for the evolution of robust neural
network controllers.
</summary>
    <author>
      <name>Paolo Pagliuca</name>
    </author>
    <author>
      <name>Stefano Nolfi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1371/journal.pone.0213193</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1371/journal.pone.0213193" rel="related"/>
    <link href="http://arxiv.org/abs/1810.01125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.01125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.02225v1</id>
    <updated>2018-09-14T18:47:34Z</updated>
    <published>2018-09-14T18:47:34Z</published>
    <title>Memristor-based Deep Convolution Neural Network: A Case Study</title>
    <summary>  In this paper, we firstly introduce a method to efficiently implement
large-scale high-dimensional convolution with realistic memristor-based circuit
components. An experiment verified simulator is adapted for accurate prediction
of analog crossbar behavior. An improved conversion algorithm is developed to
convert convolution kernels to memristor-based circuits, which minimizes the
error with consideration of the data and kernel patterns in CNNs. With circuit
simulation for all convolution layers in ResNet-20, we found that 8-bit ADC/DAC
is necessary to preserve software level classification accuracy.
</summary>
    <author>
      <name>Fan Zhang</name>
    </author>
    <author>
      <name>Miao Hu</name>
    </author>
    <link href="http://arxiv.org/abs/1810.02225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.02225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.03974v1</id>
    <updated>2018-10-09T13:42:11Z</updated>
    <published>2018-10-09T13:42:11Z</published>
    <title>Collective evolution of weights in wide neural networks</title>
    <summary>  We derive a nonlinear integro-differential transport equation describing
collective evolution of weights under gradient descent in large-width
neural-network-like models. We characterize stationary points of the evolution
and analyze several scenarios where the transport equation can be solved
approximately. We test our general method in the special case of linear
free-knot splines, and find good agreement between theory and experiment in
observations of global optima, stability of stationary points, and convergence
rates.
</summary>
    <author>
      <name>Dmitry Yarotsky</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.03974v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.03974v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.04119v1</id>
    <updated>2018-10-09T16:49:04Z</updated>
    <published>2018-10-09T16:49:04Z</published>
    <title>Positional Cartesian Genetic Programming</title>
    <summary>  Cartesian Genetic Programming (CGP) has many modifications across a variety
of implementations, such as recursive connections and node weights. Alternative
genetic operators have also been proposed for CGP, but have not been fully
studied. In this work, we present a new form of genetic programming based on a
floating point representation. In this new form of CGP, called Positional CGP,
node positions are evolved. This allows for the evaluation of many different
genetic operators while allowing for previous CGP improvements like recurrency.
Using nine benchmark problems from three different classes, we evaluate the
optimal parameters for CGP and PCGP, including novel genetic operators.
</summary>
    <author>
      <name>DG Wilson</name>
    </author>
    <author>
      <name>Julian F. Miller</name>
    </author>
    <author>
      <name>Sylvain Cussat-Blanc</name>
    </author>
    <author>
      <name>Hervé Luga</name>
    </author>
    <link href="http://arxiv.org/abs/1810.04119v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.04119v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.05474v1</id>
    <updated>2018-10-12T12:19:56Z</updated>
    <published>2018-10-12T12:19:56Z</published>
    <title>Pre-gen metrics: Predicting caption quality metrics without generating
  captions</title>
    <summary>  Image caption generation systems are typically evaluated against reference
outputs. We show that it is possible to predict output quality without
generating the captions, based on the probability assigned by the neural model
to the reference captions. Such pre-gen metrics are strongly correlated to
standard evaluation metrics.
</summary>
    <author>
      <name>Marc Tanti</name>
    </author>
    <author>
      <name>Albert Gatt</name>
    </author>
    <author>
      <name>Adrian Muscat</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-11018-5_11</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-11018-5_11" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 6 figures This publication will appear in the Proceedings
  of the First Workshop on Shortcomings in Vision and Language (2018). DOI to
  be inserted later</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.05474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.05474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.05475v1</id>
    <updated>2018-10-12T12:23:08Z</updated>
    <published>2018-10-12T12:23:08Z</published>
    <title>Quantifying the amount of visual information used by neural caption
  generators</title>
    <summary>  This paper addresses the sensitivity of neural image caption generators to
their visual input. A sensitivity analysis and omission analysis based on image
foils is reported, showing that the extent to which image captioning
architectures retain and are sensitive to visual information varies depending
on the type of word being generated and the position in the caption as a whole.
We motivate this work in the context of broader goals in the field to achieve
more explainability in AI.
</summary>
    <author>
      <name>Marc Tanti</name>
    </author>
    <author>
      <name>Albert Gatt</name>
    </author>
    <author>
      <name>Kenneth P. Camilleri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-11018-5_11</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-11018-5_11" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 4 figures This publication will appear in the Proceedings
  of the First Workshop on Shortcomings in Vision and Language (2018). DOI to
  be inserted later</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.05475v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.05475v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.08653v1</id>
    <updated>2018-10-08T14:47:51Z</updated>
    <published>2018-10-08T14:47:51Z</published>
    <title>Deep Learning with the Random Neural Network and its Applications</title>
    <summary>  The random neural network (RNN) is a mathematical model for an "integrate and
fire" spiking network that closely resembles the stochastic behaviour of
neurons in mammalian brains. Since its proposal in 1989, there have been
numerous investigations into the RNN's applications and learning algorithms.
Deep learning (DL) has achieved great success in machine learning. Recently,
the properties of the RNN for DL have been investigated, in order to combine
their power. Recent results demonstrate that the gap between RNNs and DL can be
bridged and the DL tools based on the RNN are faster and can potentially be
used with less energy expenditure than existing methods.
</summary>
    <author>
      <name>Yonghua Yin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 19 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.08653v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.08653v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.08886v1</id>
    <updated>2018-10-21T03:16:22Z</updated>
    <published>2018-10-21T03:16:22Z</published>
    <title>Electricity consumption forecasting method based on MPSO-BP neural
  network model</title>
    <summary>  This paper deals with the problem of the electricity consumption forecasting
method. An MPSO-BP (modified particle swarm optimization-back propagation)
neural network model is constructed based on the history data of a mineral
company of Anshan in China. The simulation showed that the convergence of the
algorithm and forecasting accuracy using the obtained model are better than
those of other traditional ones, such as BP, PSO, fuzzy neural network and so
on. Then we predict the electricity consumption of each month in 2017 based on
the MPSO-BP neural network model.
</summary>
    <author>
      <name>Youshan Zhang</name>
    </author>
    <author>
      <name>Liangdong Guo</name>
    </author>
    <author>
      <name>Qi Li</name>
    </author>
    <author>
      <name>Junhui Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.2991/iceeecs-16.2016.133</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.2991/iceeecs-16.2016.133" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2016 4th International Conference On Electrical
  Electronics Engineering And Computer Science (ICEEECS 2016), 50:674-678, 2016</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1810.08886v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.08886v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.10453v2</id>
    <updated>2020-02-03T17:01:08Z</updated>
    <published>2018-10-24T15:32:05Z</published>
    <title>Evolving Graphs with Semantic Neutral Drift</title>
    <summary>  We introduce the concept of Semantic Neutral Drift (SND) for genetic
programming (GP), where we exploit equivalence laws to design semantics
preserving mutations guaranteed to preserve individuals' fitness scores. A
number of digital circuit benchmark problems have been implemented with
rule-based graph programs and empirically evaluated, demonstrating quantitative
improvements in evolutionary performance. Analysis reveals that the benefits of
the designed SND reside in more complex processes than simple growth of
individuals, and that there are circumstances where it is beneficial to choose
otherwise detrimental parameters for a GP system if that facilitates the
inclusion of SND.
</summary>
    <author>
      <name>Timothy Atkinson</name>
    </author>
    <author>
      <name>Detlef Plump</name>
    </author>
    <author>
      <name>Susan Stepney</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11047-019-09772-4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11047-019-09772-4" rel="related"/>
    <link href="http://arxiv.org/abs/1810.10453v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.10453v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1810.12470v1</id>
    <updated>2018-10-30T00:51:02Z</updated>
    <published>2018-10-30T00:51:02Z</published>
    <title>Inheritance-Based Diversity Measures for Explicit Convergence Control in
  Evolutionary Algorithms</title>
    <summary>  Diversity is an important factor in evolutionary algorithms to prevent
premature convergence towards a single local optimum. In order to maintain
diversity throughout the process of evolution, various means exist in
literature. We analyze approaches to diversity that (a) have an explicit and
quantifiable influence on fitness at the individual level and (b) require no
(or very little) additional domain knowledge such as domain-specific distance
functions. We also introduce the concept of genealogical diversity in a broader
study. We show that employing these approaches can help evolutionary algorithms
for global optimization in many cases.
</summary>
    <author>
      <name>Thomas Gabor</name>
    </author>
    <author>
      <name>Lenz Belzner</name>
    </author>
    <author>
      <name>Claudia Linnhoff-Popien</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3205455.3205630</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3205455.3205630" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">GECCO '18: Genetic and Evolutionary Computation Conference, 2018,
  Kyoto, Japan</arxiv:comment>
    <link href="http://arxiv.org/abs/1810.12470v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1810.12470v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.00323v1</id>
    <updated>2018-11-01T11:39:49Z</updated>
    <published>2018-11-01T11:39:49Z</published>
    <title>Taylor-based Optimized Recursive Extended Exponential Smoothed Neural
  Networks Forecasting Method</title>
    <summary>  A newly introduced method called Taylor-based Optimized Recursive Extended
Exponential Smoothed Neural Networks Forecasting method is applied and extended
in this study to forecast numerical values. Unlike traditional forecasting
techniques which forecast only future values, our proposed method provides a
new extension to correct the predicted values which is done by forecasting the
estimated error. Experimental results demonstrated that the proposed method has
a high accuracy both in training and testing data and outperform the
state-of-the-art RNN models on Mackey-Glass, NARMA, Lorenz and Henon map
datasets.
</summary>
    <author>
      <name>Emna Krichene</name>
    </author>
    <author>
      <name>Wael Ouarda</name>
    </author>
    <author>
      <name>Habib Chabchoub</name>
    </author>
    <author>
      <name>Adel M. Alimi</name>
    </author>
    <link href="http://arxiv.org/abs/1811.00323v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.00323v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.02010v1</id>
    <updated>2018-11-05T19:57:53Z</updated>
    <published>2018-11-05T19:57:53Z</published>
    <title>A Unified Perspective of Evolutionary Game Dynamics Using Generalized
  Growth Transforms</title>
    <summary>  In this paper, we show that different types of evolutionary game dynamics
are, in principle, special cases of a dynamical system model based on our
previously reported framework of generalized growth transforms. The framework
shows that different dynamics arise as a result of minimizing a population
energy such that the population as a whole evolves to reach the most stable
state. By introducing a population dependent time-constant in the generalized
growth transform model, the proposed framework can be used to explain a vast
repertoire of evolutionary dynamics, including some novel forms of game
dynamics with non-linear payoffs.
</summary>
    <author>
      <name>Oindrila Chatterjee</name>
    </author>
    <author>
      <name>Shantanu Chakrabartty</name>
    </author>
    <link href="http://arxiv.org/abs/1811.02010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.02010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.06804v1</id>
    <updated>2018-11-16T13:58:21Z</updated>
    <published>2018-11-16T13:58:21Z</published>
    <title>Evolutionary Diversity Optimization Using Multi-Objective Indicators</title>
    <summary>  Evolutionary diversity optimization aims to compute a diverse set of
solutions where all solutions meet a given quality criterion. With this paper,
we bridge the areas of evolutionary diversity optimization and evolutionary
multi-objective optimization. We show how popular indicators frequently used in
the area of multi-objective optimization can be used for evolutionary diversity
optimization. Our experimental investigations for evolving diverse sets of TSP
instances and images according to various features show that two of the most
prominent multi-objective indicators, namely the hypervolume indicator and the
inverted generational distance, provide excellent results in terms of
visualization and various diversity indicators.
</summary>
    <author>
      <name>Aneta Neumann</name>
    </author>
    <author>
      <name>Wanru Gao</name>
    </author>
    <author>
      <name>Markus Wagner</name>
    </author>
    <author>
      <name>Frank Neumann</name>
    </author>
    <link href="http://arxiv.org/abs/1811.06804v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.06804v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1811.09196v1</id>
    <updated>2018-11-22T14:40:55Z</updated>
    <published>2018-11-22T14:40:55Z</published>
    <title>Using External Archive for Improved Performance in Multi-Objective
  Optimization</title>
    <summary>  It is shown that the use of an external archive, purely for storage purposes,
can bring substantial benefits in multi-objective optimization. A new scheme
for archive management for the above purpose is described. The new scheme is
combined with the NSGA-II algorithm for solving two multi-objective
optimization problems, and it is demonstrated that this combination gives
significantly improved sets of Pareto-optimal solutions. The additional
computational effort because of the external archive is found to be
insignificant when the objective functions are expensive to evaluate.
</summary>
    <author>
      <name>Mahesh B. Patil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 figures, submitted to IEEE Transactions on Evolutionary
  Computation</arxiv:comment>
    <link href="http://arxiv.org/abs/1811.09196v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1811.09196v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.03775v2</id>
    <updated>2020-02-22T23:15:46Z</updated>
    <published>2019-01-12T00:26:13Z</published>
    <title>Creative AI Through Evolutionary Computation</title>
    <summary>  The main power of artificial intelligence is not in modeling what we already
know, but in creating solutions that are new. Such solutions exist in extremely
large, high-dimensional, and complex search spaces. Population-based search
techniques, i.e. variants of evolutionary computation, are well suited to
finding them. These techniques are also well positioned to take advantage of
large-scale parallel computing resources, making creative AI through
evolutionary computation the likely "next deep learning".
</summary>
    <author>
      <name>Risto Miikkulainen</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In Banzhaf et al. (editors), Evolution in Action---Past, Present
  and Future. New York: Springer. 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1901.03775v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.03775v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.05737v1</id>
    <updated>2019-01-17T11:35:13Z</updated>
    <published>2019-01-17T11:35:13Z</published>
    <title>Genetic Algorithms and the Traveling Salesman Problem a historical
  Review</title>
    <summary>  In this paper a highly abstracted view on the historical development of
Genetic Algorithms for the Traveling Salesman Problem is given. In a meta-data
analysis three phases in the development can be distinguished. First
exponential growth in interest till 1996 can be observed, growth stays linear
till 2011 and after that publications deteriorate. These three phases are
examined and the major milestones are presented. Lastly an outlook to future
work in this field is infered.
</summary>
    <author>
      <name>Jan Scholz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.22632.78088/1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.22632.78088/1" rel="related"/>
    <link href="http://arxiv.org/abs/1901.05737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.05737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.09614v1</id>
    <updated>2019-01-28T11:43:25Z</updated>
    <published>2019-01-28T11:43:25Z</published>
    <title>A Simple Method to Reduce Off-chip Memory Accesses on Convolutional
  Neural Networks</title>
    <summary>  For convolutional neural networks, a simple algorithm to reduce off-chip
memory accesses is proposed by maximally utilizing on-chip memory in a neural
process unit. Especially, the algorithm provides an effective way to process a
module which consists of multiple branches and a merge layer. For Inception-V3
on Samsung's NPU in Exynos, our evaluation shows that the proposed algorithm
makes off-chip memory accesses reduced by 1/50, and accordingly achieves 97.59
% reduction in the amount of feature-map data to be transferred from/to
off-chip memory.
</summary>
    <author>
      <name>Doyun Kim</name>
    </author>
    <author>
      <name>Kyoung-Young Kim</name>
    </author>
    <author>
      <name>Sangsoo Ko</name>
    </author>
    <author>
      <name>Sanghyuck Ha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 10 figures, under review (by ICML2019)</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.09614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.09614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.11090v1</id>
    <updated>2019-01-30T20:37:40Z</updated>
    <published>2019-01-30T20:37:40Z</published>
    <title>Neuroevolution with Perceptron Turing Machines</title>
    <summary>  We introduce the perceptron Turing machine and show how it can be used to
create a system of neuroevolution. Advantages of this approach include
automatic scaling of solutions to larger problem sizes, the ability to
experiment with hand-coded solutions, and an enhanced potential for
understanding evolved solutions. Hand-coded solutions may be implemented in the
low-level language of Turing machines, which is the genotype used in
neuroevolution, but a high-level language called Lopro is introduced to make
the job easier.
</summary>
    <author>
      <name>David Landaeta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Patent pending</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.11090v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.11090v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.11115v2</id>
    <updated>2019-02-17T01:49:22Z</updated>
    <published>2019-01-30T21:46:42Z</published>
    <title>Code Farming: A Process for Creating Generic Computational Building
  Blocks</title>
    <summary>  Motivated by a desire to improve on the current state of the art in genetic
programming, and aided by recent progress in understanding the computational
aspects of evolutionary systems, we describe a process that creates a set of
generic computational building blocks for the purpose of seeding initial
populations of programs in any genetic programming system. This provides an
advantage over the standard approach of initializing the population purely
randomly in that it avoids the need to constantly rediscover such building
blocks. It is also better than seeding the initial population with hand-coded
building blocks, since it lessens the amount of human intervention required by
the system.
</summary>
    <author>
      <name>David Landaeta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Patent pending</arxiv:comment>
    <link href="http://arxiv.org/abs/1901.11115v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.11115v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1901.11271v1</id>
    <updated>2019-01-31T09:00:33Z</updated>
    <published>2019-01-31T09:00:33Z</published>
    <title>Improving Evolutionary Strategies with Generative Neural Networks</title>
    <summary>  Evolutionary Strategies (ES) are a popular family of black-box zeroth-order
optimization algorithms which rely on search distributions to efficiently
optimize a large variety of objective functions. This paper investigates the
potential benefits of using highly flexible search distributions in classical
ES algorithms, in contrast to standard ones (typically Gaussians). We model
such distributions with Generative Neural Networks (GNNs) and introduce a new
training algorithm that leverages their expressiveness to accelerate the ES
procedure. We show that this tailored algorithm can readily incorporate
existing ES algorithms, and outperforms the state-of-the-art on diverse
objective functions.
</summary>
    <author>
      <name>Louis Faury</name>
    </author>
    <author>
      <name>Clement Calauzenes</name>
    </author>
    <author>
      <name>Olivier Fercoq</name>
    </author>
    <author>
      <name>Syrine Krichen</name>
    </author>
    <link href="http://arxiv.org/abs/1901.11271v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1901.11271v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.01429v2</id>
    <updated>2019-02-18T18:13:17Z</updated>
    <published>2019-02-04T19:16:08Z</published>
    <title>A Spiking Neural Network with Local Learning Rules Derived From
  Nonnegative Similarity Matching</title>
    <summary>  The design and analysis of spiking neural network algorithms will be
accelerated by the advent of new theoretical approaches. In an attempt at such
approach, we provide a principled derivation of a spiking algorithm for
unsupervised learning, starting from the nonnegative similarity matching cost
function. The resulting network consists of integrate-and-fire units and
exhibits local learning rules, making it biologically plausible and also
suitable for neuromorphic hardware. We show in simulations that the algorithm
can perform sparse feature extraction and manifold learning, two tasks which
can be formulated as nonnegative similarity matching problems.
</summary>
    <author>
      <name>Cengiz Pehlevan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ICASSP 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.01429v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.01429v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.03011v1</id>
    <updated>2019-02-08T10:36:30Z</updated>
    <published>2019-02-08T10:36:30Z</published>
    <title>Fourier Neural Networks: A Comparative Study</title>
    <summary>  We review neural network architectures which were motivated by Fourier series
and integrals and which are referred to as Fourier neural networks. These
networks are empirically evaluated in synthetic and real-world tasks. Neither
of them outperforms the standard neural network with sigmoid activation
function in the real-world tasks. All neural networks, both Fourier and the
standard one, empirically demonstrate lower approximation error than the
truncated Fourier series when it comes to an approximation of a known function
of multiple variables.
</summary>
    <author>
      <name>Abylay Zhumekenov</name>
    </author>
    <author>
      <name>Malika Uteuliyeva</name>
    </author>
    <author>
      <name>Olzhas Kabdolov</name>
    </author>
    <author>
      <name>Rustem Takhanov</name>
    </author>
    <author>
      <name>Zhenisbek Assylbekov</name>
    </author>
    <author>
      <name>Alejandro J. Castro</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3233/IDA-195050</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3233/IDA-195050" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Intell. Data Anal. 24 (2020), 1107-1120</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1902.03011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.03011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.09215v1</id>
    <updated>2019-02-25T12:01:18Z</updated>
    <published>2019-02-25T12:01:18Z</published>
    <title>Faster Genetic Programming GPquick via multicore and Advanced Vector
  Extensions</title>
    <summary>  We evolve floating point Sextic polynomial populations of genetic programming
binary trees for up to a million generations. Programs with almost four hundred
million instructions are created by crossover. To support unbounded Long-Term
Evolution Experiment LTEE GP we use both SIMD parallel AVX 512 bit instructions
and 48 threads to yield performance of up to 139 billion GP operations per
second, 139 giga GPops, on a single Intel Xeon Gold 6126 2.60GHz server.
</summary>
    <author>
      <name>W. B. Langdon</name>
    </author>
    <author>
      <name>W. Banzhaf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 17 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.09215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.09215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.09864v1</id>
    <updated>2019-02-26T11:21:28Z</updated>
    <published>2019-02-26T11:21:28Z</published>
    <title>Spiking Neural Network based Region Proposal Networks for Neuromorphic
  Vision Sensors</title>
    <summary>  This paper presents a three layer spiking neural network based region
proposal network operating on data generated by neuromorphic vision sensors.
The proposed architecture consists of refractory, convolution and clustering
layers designed with bio-realistic leaky integrate and fire (LIF) neurons and
synapses. The proposed algorithm is tested on traffic scene recordings from a
DAVIS sensor setup. The performance of the region proposal network has been
compared with event based mean shift algorithm and is found to be far superior
(~50% better) in recall for similar precision (~85%). Computational and memory
complexity of the proposed method are also shown to be similar to that of event
based mean shift
</summary>
    <author>
      <name>Jyotibdha Acharya</name>
    </author>
    <author>
      <name>Vandana Padala</name>
    </author>
    <author>
      <name>Arindam Basu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in IEEE ISCAS, 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.09864v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.09864v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1902.10834v2</id>
    <updated>2020-08-24T10:26:30Z</updated>
    <published>2019-02-27T23:41:35Z</published>
    <title>An evolutionary model that satisfies detailed balance</title>
    <summary>  We propose a class of evolutionary models that involves an arbitrary
exchangeable process as the breeding process and different selection schemes.
In those models, a new genome is born according to the breeding process, and
then a genome is removed according to the selection scheme that involves
fitness. Thus the population size remains constant. The process evolves
according to a Markov chain, and, unlike in many other existing models, the
stationary distribution -- so called mutation-selection equilibrium -- can be
easily found and studied. The behaviour of the stationary distribution when the
population size increases is our main object of interest. Several
phase-transition theorems are proved.
</summary>
    <author>
      <name>Jüri Lember</name>
    </author>
    <author>
      <name>Chris Watkins</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">38 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1902.10834v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1902.10834v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.01893v1</id>
    <updated>2018-12-13T07:27:39Z</updated>
    <published>2018-12-13T07:27:39Z</published>
    <title>Algorithms Inspired by Nature: A Survey</title>
    <summary>  Nature is known to be the best optimizer. Natural processes most often than
not reach an optimal equilibrium. Scientists have always strived to understand
and model such processes.Thus, many algorithms exist today that are inspired by
nature. Many of these algorithms and heuristics can be used to solve problems
for which no polynomial time algorithms exist,such as Job Shop Scheduling and
many other Combinatorial Optimization problems. We will discuss some of these
algorithms and heuristics and how they help us solve complex problems of
practical importance.
</summary>
    <author>
      <name>Pranshu Gupta</name>
    </author>
    <link href="http://arxiv.org/abs/1903.01893v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.01893v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.10574v1</id>
    <updated>2019-03-25T19:57:02Z</updated>
    <published>2019-03-25T19:57:02Z</published>
    <title>Spike-based primitives for graph algorithms</title>
    <summary>  In this paper we consider graph algorithms and graphical analysis as a new
application for neuromorphic computing platforms. We demonstrate how the
nonlinear dynamics of spiking neurons can be used to implement low-level graph
operations. Our results are hardware agnostic, and we present multiple versions
of routines that can utilize static synapses or require synapse plasticity.
</summary>
    <author>
      <name>Kathleen E. Hamilton</name>
    </author>
    <author>
      <name>Tiffany M. Mintz</name>
    </author>
    <author>
      <name>Catherine D. Schuman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.10574v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.10574v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.01947v1</id>
    <updated>2019-04-03T12:12:03Z</updated>
    <published>2019-04-03T12:12:03Z</published>
    <title>Extracting Tables from Documents using Conditional Generative
  Adversarial Networks and Genetic Algorithms</title>
    <summary>  Extracting information from tables in documents presents a significant
challenge in many industries and in academic research. Existing methods which
take a bottom-up approach of integrating lines into cells and rows or columns
neglect the available prior information relating to table structure. Our
proposed method takes a top-down approach, first using a generative adversarial
network to map a table image into a standardised `skeleton' table form denoting
the approximate row and column borders without table content, then fitting
renderings of candidate latent table structures to the skeleton structure using
a distance measure optimised by a genetic algorithm.
</summary>
    <author>
      <name>Nataliya Le Vine</name>
    </author>
    <author>
      <name>Matthew Zeigenfuse</name>
    </author>
    <author>
      <name>Mark Rowan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures. Published at IJCNN 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.01947v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.01947v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.02397v1</id>
    <updated>2019-04-04T07:52:56Z</updated>
    <published>2019-04-04T07:52:56Z</published>
    <title>Convergence analysis of beetle antennae search algorithm and its
  applications</title>
    <summary>  The beetle antennae search algorithm was recently proposed and investigated
for solving global optimization problems. Although the performance of the
algorithm and its variants were shown to be better than some existing
meta-heuristic algorithms, there is still a lack of convergence analysis. In
this paper, we provide theoretical analysis on the convergence of the beetle
antennae search algorithm. We test the performance of the BAS algorithm via
some representative benchmark functions. Meanwhile, some applications of the
BAS algorithm are also presented.
</summary>
    <author>
      <name>Yinyan Zhang</name>
    </author>
    <author>
      <name>Shuai Li</name>
    </author>
    <author>
      <name>Bin Xu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">no</arxiv:comment>
    <link href="http://arxiv.org/abs/1904.02397v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.02397v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.03819v1</id>
    <updated>2019-04-08T03:35:07Z</updated>
    <published>2019-04-08T03:35:07Z</published>
    <title>WeNet: Weighted Networks for Recurrent Network Architecture Search</title>
    <summary>  In recent years, there has been increasing demand for automatic architecture
search in deep learning. Numerous approaches have been proposed and led to
state-of-the-art results in various applications, including image
classification and language modeling. In this paper, we propose a novel way of
architecture search by means of weighted networks (WeNet), which consist of a
number of networks, with each assigned a weight. These weights are updated with
back-propagation to reflect the importance of different networks. Such weighted
networks bear similarity to mixture of experts. We conduct experiments on Penn
Treebank and WikiText-2. We show that the proposed WeNet can find recurrent
architectures which result in state-of-the-art performance.
</summary>
    <author>
      <name>Zhiheng Huang</name>
    </author>
    <author>
      <name>Bing Xiang</name>
    </author>
    <link href="http://arxiv.org/abs/1904.03819v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.03819v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.05061v2</id>
    <updated>2020-11-15T10:15:53Z</updated>
    <published>2019-04-10T08:36:46Z</published>
    <title>A review on Neural Turing Machine</title>
    <summary>  One of the major objectives of Artificial Intelligence is to design learning
algorithms that are executed on a general purposes computational machines such
as human brain. Neural Turing Machine (NTM) is a step towards realizing such a
computational machine. The attempt is made here to run a systematic review on
Neural Turing Machine. First, the mind-map and taxonomy of machine learning,
neural networks, and Turing machine are introduced. Next, NTM is inspected in
terms of concepts, structure, variety of versions, implemented tasks,
comparisons, etc. Finally, the paper discusses on issues and ends up with
several future works.
</summary>
    <author>
      <name>Soroor Malekmohammadi Faradonbeh</name>
    </author>
    <author>
      <name>Faramarz Safi-Esfahani</name>
    </author>
    <link href="http://arxiv.org/abs/1904.05061v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.05061v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T01" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.08839v1</id>
    <updated>2019-04-18T15:21:45Z</updated>
    <published>2019-04-18T15:21:45Z</published>
    <title>On the validity of memristor modeling in the neural network literature</title>
    <summary>  An analysis of the literature shows that there are two types of
non-memristive models that have been widely used in the modeling of so-called
"memristive" neural networks. Here, we demonstrate that such models have
nothing in common with the concept of memristive elements: they describe either
non-linear resistors or certain bi-state systems, which all are devices without
memory. Therefore, the results presented in a significant number of
publications are at least questionable, if not completely irrelevant to the
actual field of memristive neural networks.
</summary>
    <author>
      <name>Y. V. Pershin</name>
    </author>
    <author>
      <name>M. Di Ventra</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neunet.2019.08.026</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neunet.2019.08.026" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks 121, 52 (2020)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1904.08839v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.08839v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1904.13310v2</id>
    <updated>2019-10-25T04:36:14Z</updated>
    <published>2019-04-25T21:21:52Z</published>
    <title>Survey of Dropout Methods for Deep Neural Networks</title>
    <summary>  Dropout methods are a family of stochastic techniques used in neural network
training or inference that have generated significant research interest and are
widely used in practice. They have been successfully applied in neural network
regularization, model compression, and in measuring the uncertainty of neural
network outputs. While original formulated for dense neural network layers,
recent advances have made dropout methods also applicable to convolutional and
recurrent neural network layers. This paper summarizes the history of dropout
methods, their various applications, and current areas of research interest.
Important proposed methods are described in additional detail.
</summary>
    <author>
      <name>Alex Labach</name>
    </author>
    <author>
      <name>Hojjat Salehinejad</name>
    </author>
    <author>
      <name>Shahrokh Valaee</name>
    </author>
    <link href="http://arxiv.org/abs/1904.13310v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1904.13310v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.03726v1</id>
    <updated>2019-05-09T16:07:37Z</updated>
    <published>2019-05-09T16:07:37Z</published>
    <title>A Reinforcement Learning Perspective on the Optimal Control of Mutation
  Probabilities for the (1+1) Evolutionary Algorithm: First Results on the
  OneMax Problem</title>
    <summary>  We study how Reinforcement Learning can be employed to optimally control
parameters in evolutionary algorithms. We control the mutation probability of a
(1+1) evolutionary algorithm on the OneMax function. This problem is modeled as
a Markov Decision Process and solved with Value Iteration via the known
transition probabilities. It is then solved via Q-Learning, a Reinforcement
Learning algorithm, where the exact transition probabilities are not needed.
This approach also allows previous expert or empirical knowledge to be included
into learning. It opens new perspectives, both formally and computationally,
for the problem of parameter control in optimization.
</summary>
    <author>
      <name>Luca Mossina</name>
    </author>
    <author>
      <name>Emmanuel Rachelson</name>
    </author>
    <author>
      <name>Daniel Delahaye</name>
    </author>
    <link href="http://arxiv.org/abs/1905.03726v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.03726v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.04100v1</id>
    <updated>2019-02-19T20:11:08Z</updated>
    <published>2019-02-19T20:11:08Z</published>
    <title>Deep Reinforcement Learning using Genetic Algorithm for Parameter
  Optimization</title>
    <summary>  Reinforcement learning (RL) enables agents to take decision based on a reward
function. However, in the process of learning, the choice of values for
learning algorithm parameters can significantly impact the overall learning
process. In this paper, we use a genetic algorithm (GA) to find the values of
parameters used in Deep Deterministic Policy Gradient (DDPG) combined with
Hindsight Experience Replay (HER), to help speed up the learning agent. We used
this method on fetch-reach, slide, push, pick and place, and door opening in
robotic manipulation tasks. Our experimental evaluation shows that our method
leads to better performance, faster than the original algorithm.
</summary>
    <author>
      <name>Adarsh Sehgal</name>
    </author>
    <author>
      <name>Hung Manh La</name>
    </author>
    <author>
      <name>Sushil J. Louis</name>
    </author>
    <author>
      <name>Hai Nguyen</name>
    </author>
    <link href="http://arxiv.org/abs/1905.04100v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.04100v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.06379v1</id>
    <updated>2019-05-15T18:40:05Z</updated>
    <published>2019-05-15T18:40:05Z</published>
    <title>ELIMINATION from Design to Analysis</title>
    <summary>  Elimination is a word puzzle game for browsers and mobile devices, where all
levels are generated by a constrained evolutionary algorithm with no human
intervention. This paper describes the design of the game and its level
generation methods, and analysis of playtraces from almost a thousand users who
played the game since its release. The analysis corroborates that the level
generator creates a sawtooth-shaped difficulty curve, as intended. The analysis
also offers insights into player behavior in this game.
</summary>
    <author>
      <name>Ahmed Khalifa</name>
    </author>
    <author>
      <name>Dan Gopstein</name>
    </author>
    <author>
      <name>Julian Togelius</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures, submitted to CoG as a short paper</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.06379v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.06379v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.06636v1</id>
    <updated>2019-05-16T10:20:12Z</updated>
    <published>2019-05-16T10:20:12Z</published>
    <title>Heterogeneous Parallel Genetic Algorithm Paradigm</title>
    <summary>  The encoding representation of the genetic algorithm can boost or hinder its
performance albeit the care one can devote to operator design. Unfortunately, a
representation-theory foundation that helps to find the suitable encoding for
any problem has not yet become mature. Furthermore, we argue that such a
best-performing encoding scheme can differ even for instances of the same
problem. In this contribution, we present the basic principles of the
heterogeneous parallel genetic algorithm that federates the efforts of many
encoding representations in order to efficiently solve the problem in hand
without prior knowledge of the best encoding.
</summary>
    <author>
      <name>Menouar Boulif</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 5 figures, accepted at The 2nd Conference on Informatics and
  Applied Mathematics (IAM'19), Guelma, Algeria</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.06636v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.06636v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1905.08105v1</id>
    <updated>2019-05-20T13:33:37Z</updated>
    <published>2019-05-20T13:33:37Z</published>
    <title>Water Distribution System Design Using Multi-Objective Genetic Algorithm
  with External Archive and Local Search</title>
    <summary>  Hybridisation of the multi-objective optimisation algorithm NSGA-II and local
search is proposed for water distribution system design. Results obtained with
the proposed algorithm are presented for four medium-size water networks taken
from the literature. Local search is found to be beneficial for one of the
networks in terms of finding new solutions not reported earlier. It is also
shown that simply using an external archive to save all non-dominated solutions
visited by the population, even without local search, leads to substantial
improvement in the non-dominated set produced by the algorithm.
</summary>
    <author>
      <name>Mahesh Patil</name>
    </author>
    <author>
      <name>M. Naveen Naidu</name>
    </author>
    <author>
      <name>A. Vasan</name>
    </author>
    <author>
      <name>Murari R. R. Varma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1905.08105v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1905.08105v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.02871v1</id>
    <updated>2019-07-05T14:50:00Z</updated>
    <published>2019-07-05T14:50:00Z</published>
    <title>Genetic Network Architecture Search</title>
    <summary>  We propose a method for learning the neural network architecture that based
on Genetic Algorithm (GA). Our approach uses a genetic algorithm integrated
with standard Stochastic Gradient Descent(SGD) which allows the sharing of
weights across all architecture solutions. The method uses GA to design a
sub-graph of Convolution cell which maximizes the accuracy on the
validation-set. Through experiments, we demonstrate this methods performance on
both CIFAR10 and CIFAR100 dataset with an accuracy of 96% and 80.1%. The code
and result of this work available in
GitHub:https://github.com/haihabi/GeneticNAS.
</summary>
    <author>
      <name>Hai Victor Habi</name>
    </author>
    <author>
      <name>Gil Rafalovich</name>
    </author>
    <link href="http://arxiv.org/abs/1907.02871v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.02871v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.03743v1</id>
    <updated>2019-06-12T11:30:38Z</updated>
    <published>2019-06-12T11:30:38Z</published>
    <title>A K-means-based Multi-subpopulation Particle Swarm Optimization for
  Neural Network Ensemble</title>
    <summary>  This paper presents a k-means-based multi-subpopulation particle swarm
optimization, denoted as KMPSO, for training the neural network ensemble. In
the proposed KMPSO, particles are dynamically partitioned into clusters via the
k-means clustering algorithm at every iteration, and each of the resulting
clusters is responsible for training a component neural network. The
performance of the KMPSO has been evaluated on several benchmark problems. Our
results show that the proposed method can effectively control the trade-off
between the diversity and accuracy in the ensemble, thus achieving competitive
results in comparison with related algorithms.
</summary>
    <author>
      <name>Hui Yu</name>
    </author>
    <link href="http://arxiv.org/abs/1907.03743v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.03743v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.04060v1</id>
    <updated>2019-07-09T09:51:04Z</updated>
    <published>2019-07-09T09:51:04Z</published>
    <title>Event-based attention and tracking on neuromorphic hardware</title>
    <summary>  We present a fully event-driven vision and processing system for selective
attention and tracking, realized on a neuromorphic processor Loihi interfaced
to an event-based Dynamic Vision Sensor DAVIS. The attention mechanism is
realized as a recurrent spiking neural network that implements
attractor-dynamics of dynamic neural fields. We demonstrate capability of the
system to create sustained activation that supports object tracking when
distractors are present or when the object slows down or stops, reducing the
number of generated events.
</summary>
    <author>
      <name>Alpha Renner</name>
    </author>
    <author>
      <name>Matthew Evanusa</name>
    </author>
    <author>
      <name>Yulia Sandamirskaya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Conference on Computer Vision and Pattern Recognition Workshops
  (CVPRW), 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.04060v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04060v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.04160v1</id>
    <updated>2019-07-09T13:40:14Z</updated>
    <published>2019-07-09T13:40:14Z</published>
    <title>Learning in Competitive Network with Haeusslers Equation adapted using
  FIREFLY algorithm</title>
    <summary>  Many of the competitive neural network consists of spatially arranged
neurons. The weigh matrix that connects cells represents local excitation and
long-range inhibition. They are known as soft-winner-take-all networks and
shown to exhibit desirable information-processing. The local excitatory
connections are many times predefined hand-wired based depending on spatial
arrangement which is chosen using the previous knowledge of data. Here we
present learning in recurrent network through Haeusslers equation and modified
wiring scheme based on biologically based Firefly algorithm. Following results
show learning in such network from input patterns without hand-wiring with
fixed topology.
</summary>
    <author>
      <name>N. Joshi</name>
    </author>
    <link href="http://arxiv.org/abs/1907.04160v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.04160v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.07255v1</id>
    <updated>2019-07-15T06:00:41Z</updated>
    <published>2019-07-15T06:00:41Z</published>
    <title>Iterative temporal differencing with random synaptic feedback weights
  support error backpropagation for deep learning</title>
    <summary>  This work shows that a differentiable activation function is not necessary
any more for error backpropagation. The derivative of the activation function
can be replaced by an iterative temporal differencing using fixed random
feedback alignment. Using fixed random synaptic feedback alignment with an
iterative temporal differencing is transforming the traditional error
backpropagation into a more biologically plausible approach for learning deep
neural network architectures. This can be a big step toward the integration of
STDP-based error backpropagation in deep learning.
</summary>
    <author>
      <name>Aras R. Dargazany</name>
    </author>
    <link href="http://arxiv.org/abs/1907.07255v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.07255v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.09248v3</id>
    <updated>2019-09-05T08:05:22Z</updated>
    <published>2019-07-22T11:51:47Z</published>
    <title>A Simple Yet Effective Approach to Robust Optimization Over Time</title>
    <summary>  Robust optimization over time (ROOT) refers to an optimization problem where
its performance is evaluated over a period of future time. Most of the existing
algorithms use particle swarm optimization combined with another method which
predicts future solutions to the optimization problem. We argue that this
approach may perform subpar and suggest instead a method based on a random
sampling of the search space. We prove its theoretical guarantees and show that
it significantly outperforms the state-of-the-art methods for ROOT.
</summary>
    <author>
      <name>Lukáš Adam</name>
    </author>
    <author>
      <name>Xin Yao</name>
    </author>
    <link href="http://arxiv.org/abs/1907.09248v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.09248v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.11910v1</id>
    <updated>2019-07-27T13:48:40Z</updated>
    <published>2019-07-27T13:48:40Z</published>
    <title>SOM-Guided Evolutionary Search for Solving MinMax Multiple-TSP</title>
    <summary>  Multiple-TSP, also abbreviated in the literature as mTSP, is an extension of
the Traveling Salesman Problem that lies at the core of many variants of the
Vehicle Routing problem of great practical importance. The current paper
develops and experiments with Self Organizing Maps, Evolutionary Algorithms and
Ant Colony Systems to tackle the MinMax formulation of the Single-Depot
Multiple-TSP. Hybridization between the neural network approach and the two
meta-heuristics shows to bring significant improvements, outperforming results
reported in the literature on a set of problem instances taken from TSPLIB.
</summary>
    <author>
      <name>Vlad-Ioan Lupoaie</name>
    </author>
    <author>
      <name>Ivona-Alexandra Chili</name>
    </author>
    <author>
      <name>Mihaela Elena Breaban</name>
    </author>
    <author>
      <name>Madalina Raschip</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 12 figures, 2 tables, CEC 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.11910v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.11910v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.12698v1</id>
    <updated>2019-07-30T01:42:42Z</updated>
    <published>2019-07-30T01:42:42Z</published>
    <title>EVO* 2019 -- Late-Breaking Abstracts Volume</title>
    <summary>  This volume contains the Late-Breaking Abstracts submitted to the EVO* 2019
Conference, that took place in Leipzig, from 24 to 26 of April. These papers
where presented as short talks and also at the poster session of the conference
together with other regular submissions. All of them present ongoing research
and preliminary results investigating on the application of different
approaches of Evolutionary Computation to different problems, most of them real
world ones.
</summary>
    <author>
      <name>A. M. Mora</name>
    </author>
    <author>
      <name>A. I. Esparcia-Alcázar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LBAs accepted in EVO* 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1907.12698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.12698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1908.08020v1</id>
    <updated>2019-07-23T03:08:08Z</updated>
    <published>2019-07-23T03:08:08Z</published>
    <title>Comparing reliability of grid-based Quality-Diversity algorithms using
  artificial landscapes</title>
    <summary>  Quality-Diversity (QD) algorithms are a recent type of optimisation methods
that search for a collection of both diverse and high performing solutions.
They can be used to effectively explore a target problem according to features
defined by the user. However, the field of QD still does not possess extensive
methodologies and reference benchmarks to compare these algorithms. We propose
a simple benchmark to compare the reliability of QD algorithms by optimising
the Rastrigin function, an artificial landscape function often used to test
global optimisation methods.
</summary>
    <author>
      <name>Leo Cazenille</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3319619.3321895</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3319619.3321895" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Genetic and Evolutionary Computation Conference Companion (GECCO
  '19 Companion), July 13--17, 2019, Prague, Czech Republic</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1908.08020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1908.08020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.02776v1</id>
    <updated>2019-10-07T13:22:13Z</updated>
    <published>2019-10-07T13:22:13Z</published>
    <title>Biologically-Inspired Spatial Neural Networks</title>
    <summary>  We introduce bio-inspired artificial neural networks consisting of neurons
that are additionally characterized by spatial positions. To simulate
properties of biological systems we add the costs penalizing long connections
and the proximity of neurons in a two-dimensional space. Our experiments show
that in the case where the network performs two different tasks, the neurons
naturally split into clusters, where each cluster is responsible for processing
a different task. This behavior not only corresponds to the biological systems,
but also allows for further insight into interpretability or continual
learning.
</summary>
    <author>
      <name>Maciej Wołczyk</name>
    </author>
    <author>
      <name>Jacek Tabor</name>
    </author>
    <author>
      <name>Marek Śmieja</name>
    </author>
    <author>
      <name>Szymon Maszke</name>
    </author>
    <link href="http://arxiv.org/abs/1910.02776v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.02776v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.03354v1</id>
    <updated>2019-10-08T12:15:46Z</updated>
    <published>2019-10-08T12:15:46Z</published>
    <title>Research on the Concept of Liquid State Machine</title>
    <summary>  Liquid State Machine (LSM) is a neural model with real time computations
which transforms the time varying inputs stream to a higher dimensional space.
The concept of LSM is a novel field of research in biological inspired
computation with most research effort on training the model as well as finding
the optimum learning method. In this review, the performance of LSM model was
investigated using two learning method, online learning and offline (batch)
learning methods. The review revealed that optimal performance of LSM was
recorded through online method as computational space and other complexities
associated with batch learning is eliminated.
</summary>
    <author>
      <name>Gideon Gbenga Oladipupo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 7 figures and 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.03354v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.03354v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.06854v1</id>
    <updated>2019-10-15T15:15:39Z</updated>
    <published>2019-10-15T15:15:39Z</published>
    <title>Optimizing Convolutional Neural Networks for Embedded Systems by Means
  of Neuroevolution</title>
    <summary>  Automated design methods for convolutional neural networks (CNNs) have
recently been developed in order to increase the design productivity. We
propose a neuroevolution method capable of evolving and optimizing CNNs with
respect to the classification error and CNN complexity (expressed as the number
of tunable CNN parameters), in which the inference phase can partly be executed
using fixed point operations to further reduce power consumption. Experimental
results are obtained with TinyDNN framework and presented using two common
image classification benchmark problems -- MNIST and CIFAR-10.
</summary>
    <author>
      <name>Filip Badan</name>
    </author>
    <author>
      <name>Lukas Sekanina</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-34500-6_7</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-34500-6_7" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">TPNC 2019, LNCS 11934, pp. 1-13, 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1910.06854v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.06854v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1910.07225v1</id>
    <updated>2019-10-16T09:08:42Z</updated>
    <published>2019-10-16T09:08:42Z</published>
    <title>Structural Analysis of Sparse Neural Networks</title>
    <summary>  Sparse Neural Networks regained attention due to their potential for
mathematical and computational advantages. We give motivation to study
Artificial Neural Networks (ANNs) from a network science perspective, provide a
technique to embed arbitrary Directed Acyclic Graphs into ANNs and report study
results on predicting the performance of image classifiers based on the
structural properties of the networks' underlying graph. Results could further
progress neuroevolution and add explanations for the success of distinct
architectures from a structural perspective.
</summary>
    <author>
      <name>Julian Stier</name>
    </author>
    <author>
      <name>Michael Granitzer</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.procs.2019.09.165</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.procs.2019.09.165" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23rd International Conference on Knowledge-Based and Intelligent
  Information &amp; Engineering Systems</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Procedia Computer Science, 2018</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1910.07225v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1910.07225v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.06322v4</id>
    <updated>2019-12-26T02:17:47Z</updated>
    <published>2019-11-14T22:41:21Z</published>
    <title>Auto-encoding a Knowledge Graph Using a Deep Belief Network: A Random
  Fields Perspective</title>
    <summary>  We started with a knowledge graph of connected entities and descriptive
properties of those entities, from which, a hierarchical representation of the
knowledge graph is derived. Using a graphical, energy-based neural network, we
are able to show that the structure of the hierarchy can be internally captured
by the neural network, which allows for efficient output of the underlying
equilibrium distribution from which the data are drawn.
</summary>
    <author>
      <name>Robert A. Murphy</name>
    </author>
    <link href="http://arxiv.org/abs/1911.06322v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.06322v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.07112v1</id>
    <updated>2019-11-16T23:08:13Z</updated>
    <published>2019-11-16T23:08:13Z</published>
    <title>Particle Swarm and EDAs</title>
    <summary>  The Particle Swarm Optimization (PSO) algorithm is developed for solving the
Schaffer F6 function in fewer than 4000 function evaluations on a total of 30
runs. Four variations of the Full Model of Particle Swarm Optimization (PSO)
algorithms are presented which consist of combinations of Ring and Star
topologies with Synchronous and Asynchronous updates. The Full Model with
combinations of Ring and Star topologies in combination with Synchronous and
Asynchronous Particle Updates is explored.
</summary>
    <author>
      <name>Alison Jenkins</name>
    </author>
    <author>
      <name>Vinika Gupta</name>
    </author>
    <author>
      <name>Alexis Myrick</name>
    </author>
    <author>
      <name>Mary Lenoir</name>
    </author>
    <link href="http://arxiv.org/abs/1911.07112v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.07112v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="91E40, 47N70, 34H05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.07302v1</id>
    <updated>2019-11-13T11:39:05Z</updated>
    <published>2019-11-13T11:39:05Z</published>
    <title>Haploid-Diploid Evolution: Nature's Memetic Algorithm</title>
    <summary>  This paper uses a recent explanation for the fundamental haploid-diploid
lifecycle of eukaryotic organisms to present a new memetic algorithm that
differs from all previous known work using diploid representations. A form of
the Baldwin effect has been identified as inherent to the evolutionary
mechanisms of eukaryotes and a simplified version is presented here which
maintains such behaviour. Using a well-known abstract tuneable model, it is
shown that varying fitness landscape ruggedness varies the benefit of
haploid-diploid algorithms. Moreover, the methodology is applied to optimise
the targeted delivery of a therapeutic compound utilizing nano-particles to
cancerous tumour cells with the multicellular simulator PhysiCell.
</summary>
    <author>
      <name>Michail-Antisthenis Tsompanas</name>
    </author>
    <author>
      <name>Larry Bull</name>
    </author>
    <author>
      <name>Andrew Adamatzky</name>
    </author>
    <author>
      <name>Igor Balaz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:1903.11598</arxiv:comment>
    <link href="http://arxiv.org/abs/1911.07302v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.07302v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1911.09230v1</id>
    <updated>2019-11-21T00:54:55Z</updated>
    <published>2019-11-21T00:54:55Z</published>
    <title>Predictive Coding as Stimulus Avoidance in Spiking Neural Networks</title>
    <summary>  Predictive coding can be regarded as a function which reduces the error
between an input signal and a top-down prediction. If reducing the error is
equivalent to reducing the influence of stimuli from the environment,
predictive coding can be regarded as stimulation avoidance by prediction. Our
previous studies showed that action and selection for stimulation avoidance
emerge in spiking neural networks through spike-timing dependent plasticity
(STDP). In this study, we demonstrate that spiking neural networks with random
structure spontaneously learn to predict temporal sequences of stimuli based
solely on STDP.
</summary>
    <author>
      <name>Atsushi Masumori</name>
    </author>
    <author>
      <name>Lana Sinapayen</name>
    </author>
    <author>
      <name>Takashi Ikegami</name>
    </author>
    <link href="http://arxiv.org/abs/1911.09230v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1911.09230v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.01088v1</id>
    <updated>2019-12-02T21:41:15Z</updated>
    <published>2019-12-02T21:41:15Z</published>
    <title>Simulation of neural function in an artificial Hebbian network</title>
    <summary>  Artificial neural networks have diverged far from their early inspiration in
neurology. In spite of their technological and commercial success, they have
several shortcomings, most notably the need for a large number of training
examples and the resulting computation resources required for iterative
learning. Here we describe an approach to neurological network simulation, both
architectural and algorithmic, that adheres more closely to established
biological principles and overcomes some of the shortcomings of conventional
networks.
</summary>
    <author>
      <name>J. Campbell Scott</name>
    </author>
    <author>
      <name>Thomas F. Hayes</name>
    </author>
    <author>
      <name>Ahmet S. Ozcan</name>
    </author>
    <author>
      <name>Winfried W. Wilcke</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">20 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.01088v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.01088v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.02535v1</id>
    <updated>2019-12-05T12:33:40Z</updated>
    <published>2019-12-05T12:33:40Z</published>
    <title>Is perturbation an effective restart strategy?</title>
    <summary>  Premature convergence can be detrimental to the performance of search
methods, which is why many search algorithms include restart strategies to deal
with it. While it is common to perturb the incumbent solution with
diversification steps of various sizes with the hope that the search method
will find a new basin of attraction leading to a better local optimum, it is
usually not clear how big the perturbation step should be. We introduce a new
property of fitness landscapes termed "Neighbours with Similar Fitness" and we
demonstrate that the effectiveness of a restart strategy depends on this
property.
</summary>
    <author>
      <name>Aldeida Aleti</name>
    </author>
    <author>
      <name>Mark Wallace</name>
    </author>
    <author>
      <name>Markus Wagner</name>
    </author>
    <link href="http://arxiv.org/abs/1912.02535v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.02535v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.03341v1</id>
    <updated>2019-12-06T20:58:09Z</updated>
    <published>2019-12-06T20:58:09Z</published>
    <title>Deep Reinforcement Learning for Routing a Heterogeneous Fleet of
  Vehicles</title>
    <summary>  Motivated by the promising advances of deep-reinforcement learning (DRL)
applied to cooperative multi-agent systems we propose a model and learning
procedure to solve the Capacitated Multi-Vehicle Routing Problem (CMVRP) with
fixed fleet size. Our learning procedure follows a centralized-training and
decentralized-execution paradigm. We empirically test our model and showed its
capability for producing near-optimal solutions through cooperative actions. In
large instances, our model generates better solutions than other commonly used
heuristics. Additionally, our model can solve arbitrary instances of the CMVRP
without requiring re-training.
</summary>
    <author>
      <name>Jose Manuel Vera</name>
    </author>
    <author>
      <name>Andres G. Abad</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6th Latin American Conference on Computational Intelligence</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.03341v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.03341v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1912.07423v3</id>
    <updated>2020-05-24T17:42:31Z</updated>
    <published>2019-12-16T14:49:37Z</published>
    <title>Faster and Simpler SNN Simulation with Work Queues</title>
    <summary>  We present a clock-driven Spiking Neural Network simulator which is up to 3x
faster than the state of the art while, at the same time, being more general
and requiring less programming effort on both the user's and maintainer's side.
This is made possible by designing our pipeline around "work queues" which act
as interfaces between stages and greatly reduce implementation complexity. We
evaluate our work using three well-established SNN models on a series of
benchmarks.
</summary>
    <author>
      <name>Dennis Bautembach</name>
    </author>
    <author>
      <name>Iason Oikonomidis</name>
    </author>
    <author>
      <name>Nikolaos Kyriazis</name>
    </author>
    <author>
      <name>Antonis Argyros</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IJCNN48605.2020.9206752</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IJCNN48605.2020.9206752" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Camera-ready version, as accepted by IJCNN 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/1912.07423v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1912.07423v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.09706v1</id>
    <updated>2020-02-22T14:17:52Z</updated>
    <published>2020-02-22T14:17:52Z</published>
    <title>Structural Combinatorial of Network Information System of Systems based
  on Evolutionary Optimization Method</title>
    <summary>  The network information system is a military information network system with
evolution characteristics. Evolution is a process of replacement between
disorder and order, chaos and equilibrium. Given that the concept of evolution
originates from biological systems, in this article, the evolution of network
information architecture is analyzed by genetic algorithms, and the network
information architecture is represented by chromosomes. Besides, the genetic
algorithm is also applied to find the optimal chromosome in the architecture
space. The evolutionary simulation is used to predict the optimal scheme of the
network information architecture and provide a reference for system
construction.
</summary>
    <author>
      <name>Tingting Zhang</name>
    </author>
    <author>
      <name>Yushi Lan</name>
    </author>
    <author>
      <name>Aiguo Song</name>
    </author>
    <author>
      <name>Kun Liu</name>
    </author>
    <author>
      <name>Nan Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2002.09706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.09706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.11656v1</id>
    <updated>2020-02-26T17:36:27Z</updated>
    <published>2020-02-26T17:36:27Z</published>
    <title>Inceptive Event Time-Surfaces for Object Classification Using
  Neuromorphic Cameras</title>
    <summary>  This paper presents a novel fusion of low-level approaches for dimensionality
reduction into an effective approach for high-level objects in neuromorphic
camera data called Inceptive Event Time-Surfaces (IETS). IETSs overcome several
limitations of conventional time-surfaces by increasing robustness to noise,
promoting spatial consistency, and improving the temporal localization of
(moving) edges. Combining IETS with transfer learning improves state-of-the-art
performance on the challenging problem of object classification utilizing event
camera data.
</summary>
    <author>
      <name>R Wes Baldwin</name>
    </author>
    <author>
      <name>Mohammed Almatrafi</name>
    </author>
    <author>
      <name>Jason R Kaufman</name>
    </author>
    <author>
      <name>Vijayan Asari</name>
    </author>
    <author>
      <name>Keigo Hirakawa</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-27272-2_35</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-27272-2_35" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Image Analysis and Recognition. ICIAR 2019. Lecture Notes in
  Computer Science, vol 11663. Springer, Cham</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2002.11656v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.11656v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.09415v2</id>
    <updated>2020-04-09T13:45:34Z</updated>
    <published>2020-03-20T17:54:08Z</published>
    <title>Comments on Sejnowski's "The unreasonable effectiveness of deep learning
  in artificial intelligence" [arXiv:2002.04806]</title>
    <summary>  Terry Sejnowski's 2020 paper [arXiv:2002.04806] is entitled "The unreasonable
effectiveness of deep learning in artificial intelligence". However, the paper
doesn't attempt to answer the implied question of why Deep Convolutional Neural
Networks (DCNNs) can approximate so many of the mappings that they have been
trained to model. While there are detailed mathematical analyses, this short
paper attempts to look at the issue differently, considering the way that these
networks are used, the subset of these functions that can be achieved by
training (starting from some location in the original function space), as well
as the functions to which these networks will actually be applied.
</summary>
    <author>
      <name>Leslie S. Smith</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2003.09415v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.09415v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.10847v1</id>
    <updated>2020-03-24T13:47:07Z</updated>
    <published>2020-03-24T13:47:07Z</published>
    <title>Re-Training StyleGAN -- A First Step Towards Building Large, Scalable
  Synthetic Facial Datasets</title>
    <summary>  StyleGAN is a state-of-art generative adversarial network architecture that
generates random 2D high-quality synthetic facial data samples. In this paper,
we recap the StyleGAN architecture and training methodology and present our
experiences of retraining it on a number of alternative public datasets.
Practical issues and challenges arising from the retraining process are
discussed. Tests and validation results are presented and a comparative
analysis of several different re-trained StyleGAN weightings is provided 1. The
role of this tool in building large, scalable datasets of synthetic facial data
is also discussed.
</summary>
    <author>
      <name>Viktor Varkarakis</name>
    </author>
    <author>
      <name>Shabab Bazrafkan</name>
    </author>
    <author>
      <name>Peter Corcoran</name>
    </author>
    <link href="http://arxiv.org/abs/2003.10847v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.10847v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.10948v1</id>
    <updated>2020-03-24T16:25:31Z</updated>
    <published>2020-03-24T16:25:31Z</published>
    <title>Reservoir Computing with Planar Nanomagnet Arrays</title>
    <summary>  Reservoir computing is an emerging methodology for neuromorphic computing
that is especially well-suited for hardware implementations in size, weight,
and power (SWaP) constrained environments. This work proposes a novel hardware
implementation of a reservoir computer using a planar nanomagnet array. A small
nanomagnet reservoir is demonstrated via micromagnetic simulations to be able
to identify simple waveforms with 100% accuracy. Planar nanomagnet reservoirs
are a promising new solution to the growing need for dedicated neuromorphic
hardware.
</summary>
    <author>
      <name>Peng Zhou</name>
    </author>
    <author>
      <name>Nathan R. McDonald</name>
    </author>
    <author>
      <name>Alexander J. Edwards</name>
    </author>
    <author>
      <name>Lisa Loomis</name>
    </author>
    <author>
      <name>Clare D. Thiem</name>
    </author>
    <author>
      <name>Joseph S. Friedman</name>
    </author>
    <link href="http://arxiv.org/abs/2003.10948v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.10948v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.11120v1</id>
    <updated>2020-03-24T21:25:53Z</updated>
    <published>2020-03-24T21:25:53Z</published>
    <title>Unsupervised Competitive Hardware Learning Rule for Spintronic
  Clustering Architecture</title>
    <summary>  We propose a hardware learning rule for unsupervised clustering within a
novel spintronic computing architecture. The proposed approach leverages the
three-terminal structure of domain-wall magnetic tunnel junction devices to
establish a feedback loop that serves to train such devices when they are used
as synapses in a neuromorphic computing architecture.
</summary>
    <author>
      <name>Alvaro Velasquez</name>
    </author>
    <author>
      <name>Christopher H. Bennett</name>
    </author>
    <author>
      <name>Naimul Hassan</name>
    </author>
    <author>
      <name>Wesley H. Brigner</name>
    </author>
    <author>
      <name>Otitoaleke G. Akinola</name>
    </author>
    <author>
      <name>Jean Anne C. Incorvia</name>
    </author>
    <author>
      <name>Matthew J. Marinella</name>
    </author>
    <author>
      <name>Joseph S. Friedman</name>
    </author>
    <link href="http://arxiv.org/abs/2003.11120v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.11120v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.11708v1</id>
    <updated>2020-03-26T02:31:50Z</updated>
    <published>2020-03-26T02:31:50Z</published>
    <title>Multi-User Remote lab: Timetable Scheduling Using Simplex Nondominated
  Sorting Genetic Algorithm</title>
    <summary>  The scheduling of multi-user remote laboratories is modeled as a multimodal
function for the proposed optimization algorithm. The hybrid optimization
algorithm, hybridization of the Nelder-Mead Simplex algorithm and Non-dominated
Sorting Genetic Algorithm (NSGA), is proposed to optimize the timetable problem
for the remote laboratories to coordinate shared access. The proposed algorithm
utilizes the Simplex algorithm in terms of exploration, and NSGA for sorting
local optimum points with consideration of potential areas. The proposed
algorithm is applied to difficult nonlinear continuous multimodal functions,
and its performance is compared with hybrid Simplex Particle Swarm
Optimization, Simplex Genetic Algorithm, and other heuristic algorithms.
</summary>
    <author>
      <name>Seid Miad Zandavi</name>
    </author>
    <author>
      <name>Vera Chung</name>
    </author>
    <author>
      <name>Ali Anaissi</name>
    </author>
    <link href="http://arxiv.org/abs/2003.11708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.11708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2003.13750v1</id>
    <updated>2020-03-30T18:58:55Z</updated>
    <published>2020-03-30T18:58:55Z</published>
    <title>Extending BrainScaleS OS for BrainScaleS-2</title>
    <summary>  BrainScaleS-2 is a mixed-signal accelerated neuromorphic system targeted for
research in the fields of computational neuroscience and beyond-von-Neumann
computing. To augment its flexibility, the analog neural network core is
accompanied by an embedded SIMD microprocessor. The BrainScaleS Operating
System (BrainScaleS OS) is a software stack designed for the user-friendly
operation of the BrainScaleS architectures. We present and walk through the
software-architectural enhancements that were introduced for the BrainScaleS-2
architecture. Finally, using a second-version BrainScaleS-2 prototype we
demonstrate its application in an example experiment based on spike-based
expectation maximization.
</summary>
    <author>
      <name>Eric Müller</name>
    </author>
    <author>
      <name>Christian Mauch</name>
    </author>
    <author>
      <name>Philipp Spilger</name>
    </author>
    <author>
      <name>Oliver Julien Breitwieser</name>
    </author>
    <author>
      <name>Johann Klähn</name>
    </author>
    <author>
      <name>David Stöckel</name>
    </author>
    <author>
      <name>Timo Wunderlich</name>
    </author>
    <author>
      <name>Johannes Schemmel</name>
    </author>
    <link href="http://arxiv.org/abs/2003.13750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2003.13750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.01902v2</id>
    <updated>2020-09-30T09:16:55Z</updated>
    <published>2020-04-04T10:36:11Z</published>
    <title>Rational neural networks</title>
    <summary>  We consider neural networks with rational activation functions. The choice of
the nonlinear activation function in deep learning architectures is crucial and
heavily impacts the performance of a neural network. We establish optimal
bounds in terms of network complexity and prove that rational neural networks
approximate smooth functions more efficiently than ReLU networks with
exponentially smaller depth. The flexibility and smoothness of rational
activation functions make them an attractive alternative to ReLU, as we
demonstrate with numerical experiments.
</summary>
    <author>
      <name>Nicolas Boullé</name>
    </author>
    <author>
      <name>Yuji Nakatsukasa</name>
    </author>
    <author>
      <name>Alex Townsend</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.01902v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.01902v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.06632v1</id>
    <updated>2020-03-18T18:17:56Z</updated>
    <published>2020-03-18T18:17:56Z</published>
    <title>A Survey on Activation Functions and their relation with Xavier and He
  Normal Initialization</title>
    <summary>  In artificial neural network, the activation function and the weight
initialization method play important roles in training and performance of a
neural network. The question arises is what properties of a function are
important/necessary for being a well-performing activation function. Also, the
most widely used weight initialization methods - Xavier and He normal
initialization have fundamental connection with activation function. This
survey discusses the important/necessary properties of activation function and
the most widely used activation functions (sigmoid, tanh, ReLU, LReLU and
PReLU). This survey also explores the relationship between these activation
functions and the two weight initialization methods - Xavier and He normal
initialization.
</summary>
    <author>
      <name>Leonid Datta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 Pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.06632v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.06632v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.07607v1</id>
    <updated>2020-04-16T11:31:18Z</updated>
    <published>2020-04-16T11:31:18Z</published>
    <title>Distributed Evolution of Deep Autoencoders</title>
    <summary>  Autoencoders have seen wide success in domains ranging from feature selection
to information retrieval. Despite this success, designing an autoencoder for a
given task remains a challenging undertaking due to the lack of firm intuition
on how the backing neural network architectures of the encoder and decoder
impact the overall performance of the autoencoder. In this work we present a
distributed system that uses an efficient evolutionary algorithm to design a
modular autoencoder. We demonstrate the effectiveness of this system on the
tasks of manifold learning and image denoising. The system beats random search
by nearly an order of magnitude on both tasks while achieving near linear
horizontal scaling as additional worker nodes are added to the system.
</summary>
    <author>
      <name>Jeff Hajewski</name>
    </author>
    <author>
      <name>Suely Oliveira</name>
    </author>
    <author>
      <name>Xiaoyu Xing</name>
    </author>
    <link href="http://arxiv.org/abs/2004.07607v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.07607v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.10061v1</id>
    <updated>2020-04-20T15:01:24Z</updated>
    <published>2020-04-20T15:01:24Z</published>
    <title>Exploring Distributed Control with the NK Model</title>
    <summary>  The NK model has been used widely to explore aspects of natural evolution and
complex systems. This paper introduces a modified form of the NK model for
exploring distributed control in complex systems such as organisations, social
networks, collective robotics, etc. Initial results show how varying the size
and underlying functional structure of a given system affects the performance
of different distributed control structures and decision making, including
within dynamically formed structures and those with differing numbers of
control nodes.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:1903.07429,
  arXiv:1808.03471, arXiv:1811.04073</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.10061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.10061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.11687v1</id>
    <updated>2020-04-24T12:25:48Z</updated>
    <published>2020-04-24T12:25:48Z</published>
    <title>Variance Reduction for Better Sampling in Continuous Domains</title>
    <summary>  Design of experiments, random search, initialization of population-based
methods, or sampling inside an epoch of an evolutionary algorithm use a sample
drawn according to some probability distribution for approximating the location
of an optimum. Recent papers have shown that the optimal search distribution,
used for the sampling, might be more peaked around the center of the
distribution than the prior distribution modelling our uncertainty about the
location of the optimum. We confirm this statement, provide explicit values for
this reshaping of the search distribution depending on the population size
$\lambda$ and the dimension $d$, and validate our results experimentally.
</summary>
    <author>
      <name>Laurent Meunier</name>
    </author>
    <author>
      <name>Carola Doerr</name>
    </author>
    <author>
      <name>Jeremy Rapin</name>
    </author>
    <author>
      <name>Olivier Teytaud</name>
    </author>
    <link href="http://arxiv.org/abs/2004.11687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.11687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.14827v2</id>
    <updated>2020-05-26T07:14:01Z</updated>
    <published>2020-04-29T10:45:41Z</published>
    <title>On the Baldwin Effect under Coevolution</title>
    <summary>  The potentially beneficial interaction between learning and evolution, the
Baldwin effect, has long been established. This paper considers their
interaction within a coevolutionary scenario, ie, where the adaptations of one
species typically affects the fitness of others. Using the NKCS model, which
allows the systematic exploration of the effects of fitness landscape size,
ruggedness, and degree of coupling, it is shown how the amount of learning and
the relative rate of evolution can alter behaviour.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2004.10061,
  arXiv:1903.07429, arXiv:1808.03471, arXiv:1811.04073</arxiv:comment>
    <link href="http://arxiv.org/abs/2004.14827v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.14827v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.03476v2</id>
    <updated>2021-04-16T13:22:54Z</updated>
    <published>2020-05-06T11:20:21Z</published>
    <title>Brain-like approaches to unsupervised learning of hidden representations
  -- a comparative study</title>
    <summary>  Unsupervised learning of hidden representations has been one of the most
vibrant research directions in machine learning in recent years. In this work
we study the brain-like Bayesian Confidence Propagating Neural Network (BCPNN)
model, recently extended to extract sparse distributed high-dimensional
representations. The usefulness and class-dependent separability of the hidden
representations when trained on MNIST and Fashion-MNIST datasets is studied
using an external linear classifier and compared with other unsupervised
learning methods that include restricted Boltzmann machines and autoencoders.
</summary>
    <author>
      <name>Naresh Balaji Ravichandran</name>
    </author>
    <author>
      <name>Anders Lansner</name>
    </author>
    <author>
      <name>Pawel Herman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2003.12415</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.03476v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.03476v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.05151v1</id>
    <updated>2020-05-11T14:43:55Z</updated>
    <published>2020-05-11T14:43:55Z</published>
    <title>Autonomous learning and chaining of motor primitives using the Free
  Energy Principle</title>
    <summary>  In this article, we apply the Free-Energy Principle to the question of motor
primitives learning. An echo-state network is used to generate motor
trajectories. We combine this network with a perception module and a controller
that can influence its dynamics. This new compound network permits the
autonomous learning of a repertoire of motor trajectories. To evaluate the
repertoires built with our method, we exploit them in a handwriting task where
primitives are chained to produce long-range sequences.
</summary>
    <author>
      <name>Louis Annabi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ETIS</arxiv:affiliation>
    </author>
    <author>
      <name>Alexandre Pitti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ETIS</arxiv:affiliation>
    </author>
    <author>
      <name>Mathias Quoy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ETIS</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2005.05151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.05151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.05744v3</id>
    <updated>2022-12-28T11:44:17Z</updated>
    <published>2020-05-12T13:16:30Z</published>
    <title>Deep Learning: Our Miraculous Year 1990-1991</title>
    <summary>  In 2020-2021, we celebrated that many of the basic ideas behind the deep
learning revolution were published three decades ago within fewer than 12
months in our "Annus Mirabilis" or "Miraculous Year" 1990-1991 at TU Munich.
Back then, few people were interested, but a quarter century later, neural
networks based on these ideas were on over 3 billion devices such as
smartphones, and used many billions of times per day, consuming a significant
fraction of the world's compute.
</summary>
    <author>
      <name>Juergen Schmidhuber</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, 279 references, 16 illustrations, based on work of 4 Oct
  2019</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.05744v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.05744v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.06142v1</id>
    <updated>2020-05-13T04:07:43Z</updated>
    <published>2020-05-13T04:07:43Z</published>
    <title>Using Genetic Algorithm To Evolve Cellular Automata In Performing Edge
  Detection</title>
    <summary>  Cellular automata are discrete and computational models thatcan be shown as
general models of complexity. They are used in varied applications to derive
the generalized behavior of the presented model. In this paper we have took one
such application. We have made an effort to perform edge detection on an image
using genetic algorithm. The purpose and the intention here is to analyze the
capability and performance of the suggested genetic algorithm. Genetic
algorithms are used to depict or obtain a general solution of given problem.
Using this feature of GA we have tried to evolve the cellular automata and
shown that how with time it converges to the desired results.
</summary>
    <author>
      <name>Karan Nayak</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.06142v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.06142v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.07235v1</id>
    <updated>2020-05-14T19:37:34Z</updated>
    <published>2020-05-14T19:37:34Z</published>
    <title>Evo* 2020 -- Late-Breaking Abstracts Volume</title>
    <summary>  This volume contains the Late-Breaking Abstracts submitted to the Evo* 2020
Conference, that took place online, from 15 to 17 of April 2020. These papers
where presented as short talks and also at the poster session of the conference
together with other regular submissions. All of them present ongoing research
and preliminary results investigating on the application of different
approaches of Bioinspired Methods (mainly Evolutionary Computation) to
different problems, most of them real world ones.
</summary>
    <author>
      <name>A. M. Mora</name>
    </author>
    <author>
      <name>A. I. Esparcia-Alcázar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">LBAs accepted in Evo* 2020. Part of the Conference Proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.07235v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.07235v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05, 68W20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.0; I.2; K.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.08368v2</id>
    <updated>2020-07-22T00:03:10Z</updated>
    <published>2020-05-17T20:56:33Z</published>
    <title>Multi-Objective level generator generation with Marahel</title>
    <summary>  This paper introduces a new system to design constructive level generators by
searching the space of constructive level generators defined by Marahel
language. We use NSGA-II, a multi-objective optimization algorithm, to search
for generators for three different problems (Binary, Zelda, and Sokoban). We
restrict the representation to a subset of Marahel language to push the
evolution to find more efficient generators. The results show that the
generated generators were able to achieve good performance on most of the
fitness functions over these three problems. However, on Zelda and Sokoban,
they tend to depend on the initial state than modifying the map.
</summary>
    <author>
      <name>Ahmed Khalifa</name>
    </author>
    <author>
      <name>Julian Togelius</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at the PCGWorkshop 2020, 8pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.08368v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.08368v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.12557v1</id>
    <updated>2020-05-26T07:53:15Z</updated>
    <published>2020-05-26T07:53:15Z</published>
    <title>Spoken digit classification using a spin-wave delay-line active-ring
  reservoir computing</title>
    <summary>  As a test of general applicability, we use the recently proposed spin-wave
delay line active-ring reservoir computer to perform the spoken digit
recognition task. On this, classification accuracies of up to 93% are achieved.
The tested device prototype employs improved spin wave transducers (antennas).
Therefore, in addition, we also let the computer complete the short-term memory
(STM) task and the parity check (PC) tasks, because the fading memory and
nonlinearity are essential to reservoir computing performance. The resulting
STM and PC capacities reach maximum values of 4.77 and 1.47 respectively.
</summary>
    <author>
      <name>Stuart Watt</name>
    </author>
    <author>
      <name>Mikhail Kostylev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.12557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.12557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.12826v2</id>
    <updated>2020-06-06T09:07:15Z</updated>
    <published>2020-05-26T16:02:38Z</published>
    <title>BHN: A Brain-like Heterogeneous Network</title>
    <summary>  The human brain works in an unsupervised way, and more than one brain region
is essential for lighting up intelligence. Inspired by this, we propose a
brain-like heterogeneous network (BHN), which can cooperatively learn a lot of
distributed representations and one global attention representation. By
optimizing distributed, self-supervised, and gradient-isolated objective
functions in a minimax fashion, our model improves its representations, which
are generated from patches of pictures or frames of videos in experiments.
</summary>
    <author>
      <name>Tao Liu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Improve the readability, and add an image experiment</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.12826v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.12826v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.13105v1</id>
    <updated>2020-05-27T00:45:24Z</updated>
    <published>2020-05-27T00:45:24Z</published>
    <title>Genetic optimization algorithms applied toward mission computability
  models</title>
    <summary>  Genetic algorithms are modeled after the biological evolutionary processes
that use natural selection to select the best species to survive. They are
heuristics based and low cost to compute. Genetic algorithms use selection,
crossover, and mutation to obtain a feasible solution to computational
problems. In this paper, we describe our genetic optimization algorithms to a
mission-critical and constraints-aware computation problem.
</summary>
    <author>
      <name>Mee Seong Im</name>
    </author>
    <author>
      <name>Venkat R. Dasari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.13105v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.13105v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2005.13971v1</id>
    <updated>2020-05-17T11:38:42Z</updated>
    <published>2020-05-17T11:38:42Z</published>
    <title>Separation of Memory and Processing in Dual Recurrent Neural Networks</title>
    <summary>  We explore a neural network architecture that stacks a recurrent layer and a
feedforward layer that is also connected to the input, and compare it to
standard Elman and LSTM architectures in terms of accuracy and
interpretability. When noise is introduced into the activation function of the
recurrent units, these neurons are forced into a binary activation regime that
makes the networks behave much as finite automata. The resulting models are
simpler, easier to interpret and get higher accuracy on different sample
problems, including the recognition of regular languages, the computation of
additions in different bases and the generation of arithmetic expressions.
</summary>
    <author>
      <name>Christian Oliva</name>
    </author>
    <author>
      <name>Luis F. Lago-Fernández</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2005.13971v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2005.13971v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.FL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.02267v1</id>
    <updated>2020-06-03T13:33:35Z</updated>
    <published>2020-06-03T13:33:35Z</published>
    <title>FastONN -- Python based open-source GPU implementation for Operational
  Neural Networks</title>
    <summary>  Operational Neural Networks (ONNs) have recently been proposed as a special
class of artificial neural networks for grid structured data. They enable
heterogenous non-linear operations to generalize the widely adopted
convolution-based neuron model. This work introduces a fast GPU-enabled library
for training operational neural networks, FastONN, which is based on a novel
vectorized formulation of the operational neurons. Leveraging on automatic
reverse-mode differentiation for backpropagation, FastONN enables increased
flexibility with the incorporation of new operator sets and customized gradient
flows. Additionally, bundled auxiliary modules offer interfaces for performance
tracking and checkpointing across different data partitions and customized
metrics.
</summary>
    <author>
      <name>Junaid Malik</name>
    </author>
    <author>
      <name>Serkan Kiranyaz</name>
    </author>
    <author>
      <name>Moncef Gabbouj</name>
    </author>
    <link href="http://arxiv.org/abs/2006.02267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.02267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.04663v3</id>
    <updated>2020-10-31T10:42:28Z</updated>
    <published>2020-06-08T15:04:51Z</published>
    <title>Runtime Analysis of Evolutionary Algorithms via Symmetry Arguments</title>
    <summary>  We use an elementary argument building on group actions to prove that the
selection-free steady state genetic algorithm analyzed by Sutton and Witt
(GECCO 2019) takes an expected number of $\Omega(2^n / \sqrt n)$ iterations to
find any particular target search point. This bound is valid for all population
sizes $\mu$. Our result improves over the previous lower bound of
$\Omega(\exp(n^{\delta/2}))$ valid for population sizes $\mu = O(n^{1/2 -
\delta})$, $0 &lt; \delta &lt; 1/2$.
</summary>
    <author>
      <name>Benjamin Doerr</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.ipl.2020.106064</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.ipl.2020.106064" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Minor changes compared to the previous version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Inf. Process. Lett. 166: 106064 (2021)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2006.04663v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.04663v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.04716v1</id>
    <updated>2020-06-08T16:13:16Z</updated>
    <published>2020-06-08T16:13:16Z</published>
    <title>Energy Constraints Improve Liquid State Machine Performance</title>
    <summary>  A model of metabolic energy constraints is applied to a liquid state machine
in order to analyze its effects on network performance. It was found that, in
certain combinations of energy constraints, a significant increase in testing
accuracy emerged; an improvement of 4.25% was observed on a seizure detection
task using a digital liquid state machine while reducing overall reservoir
spiking activity by 6.9%. The accuracy improvements appear to be linked to the
energy constraints' impact on the reservoir's dynamics, as measured through
metrics such as the Lyapunov exponent and the separation of the reservoir.
</summary>
    <author>
      <name>Andrew Fountain</name>
    </author>
    <author>
      <name>Cory Merkel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures. Submitted to ICONS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.04716v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.04716v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.09980v1</id>
    <updated>2020-06-05T07:58:50Z</updated>
    <published>2020-06-05T07:58:50Z</published>
    <title>Genome as a functional program</title>
    <summary>  We discuss a model of genome as a program with functional architecture and
consider the approach to Darwinian evolution as a learning problem for
functional programming. In particular we introduce a model of learning for some
class of functional programs. This approach is related to information geometry
-- the learning model uses some kind of distance in the information space (the
reduction graph of the model), we consider statistical sum over paths in the
reduction graph and discuss relation of this sum to temperature learning.
</summary>
    <author>
      <name>S. V. Kozyrev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2006.09980v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.09980v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.10935v1</id>
    <updated>2020-06-19T02:28:57Z</updated>
    <published>2020-06-19T02:28:57Z</published>
    <title>Particle Swarm Optimization with Velocity Restriction and Evolutionary
  Parameters Selection for Scheduling Problem</title>
    <summary>  The article presents a study of the Particle Swarm optimization method for
scheduling problem. To improve the method's performance a restriction of
particles' velocity and an evolutionary meta-optimization were realized. The
approach proposed uses the Genetic algorithms for selection of the parameters
of Particle Swarm optimization. Experiments were carried out on test tasks of
the job-shop scheduling problem. This research proves the applicability of the
approach and shows the importance of tuning the behavioral parameters of the
swarm intelligence methods to achieve a high performance.
</summary>
    <author>
      <name>Pavel Matrenin</name>
    </author>
    <author>
      <name>Viktor Sekaev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/SIBCON.2015.7147143</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/SIBCON.2015.7147143" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2015 International Siberian Conference on Control and
  Communications (SIBCON), 21-23 May 2015, Omsk, Russia</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2006.10935v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.10935v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.2.1; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.03787v1</id>
    <updated>2020-07-07T21:04:10Z</updated>
    <published>2020-07-07T21:04:10Z</published>
    <title>Artificial Life in Game Mods for Intuitive Evolution Education</title>
    <summary>  The understanding and acceptance of evolution by natural selection has become
a difficult issue in many parts of the world, particularly the United States of
America. The use of games to improve intuition about evolution via natural
selection is promising but can be challenging. We propose the use of
modifications to commercial games using artificial life techniques to 'stealth
teach' about evolution via natural selection, provide a proof-of-concept mod of
the game Stardew Valley, and report on its initial reception.
</summary>
    <author>
      <name>Anya E. Vostinar</name>
    </author>
    <author>
      <name>Barbara Z. Johnson</name>
    </author>
    <author>
      <name>Kevin Connors</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2007.03787v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.03787v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.06463v1</id>
    <updated>2020-07-13T15:54:36Z</updated>
    <published>2020-07-13T15:54:36Z</published>
    <title>Semi-steady-state Jaya Algorithm</title>
    <summary>  The Jaya algorithm is arguably one of the fastest-emerging metaheuristics
amongst the newest members of the evolutionary computation family. The present
paper proposes a new, improved Jaya algorithm by modifying the update
strategies of the best and the worst members in the population. Simulation
results on a twelve-function benchmark test-suite as well as a real-world
problem of practical importance show that the proposed strategy produces
results that are better and faster in the majority of cases. Statistical tests
of significance are used to validate the performance improvement.
</summary>
    <author>
      <name>Uday K. Chakraborty</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.3390/app10155388</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.3390/app10155388" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Applied Sciences 10(15), 5388, 2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2007.06463v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.06463v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2007.13941v2</id>
    <updated>2021-08-06T17:30:26Z</updated>
    <published>2020-07-28T01:47:39Z</published>
    <title>A Generalized Strong-Inversion CMOS Circuitry for Neuromorphic
  Applications</title>
    <summary>  It has always been a challenge in the neuromorphic field to systematically
translate biological models into analog electronic circuitry. In this paper, a
generalized circuit design platform is introduced where biological models can
be conveniently implemented using CMOS circuitry operating in strong-inversion.
The application of the method is demonstrated by synthesizing a relatively
complex two-dimensional (2-D) nonlinear neuron model. The validity of our
approach is verified by nominal simulated results with realistic process
parameters from the commercially available AMS 0.35 um technology. The circuit
simulation results exhibit regular spiking responses in good agreement with
their mathematical counterpart.
</summary>
    <author>
      <name>Hamid Soleimani</name>
    </author>
    <author>
      <name>Emmanuel. M. Drakakis</name>
    </author>
    <link href="http://arxiv.org/abs/2007.13941v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2007.13941v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.06400v2</id>
    <updated>2021-08-09T11:53:13Z</updated>
    <published>2020-12-11T14:58:43Z</published>
    <title>Differential Evolution for Neural Architecture Search</title>
    <summary>  Neural architecture search (NAS) methods rely on a search strategy for
deciding which architectures to evaluate next and a performance estimation
strategy for assessing their performance (e.g., using full evaluations,
multi-fidelity evaluations, or the one-shot model). In this paper, we focus on
the search strategy. We introduce the simple yet powerful evolutionary
algorithm of differential evolution to the NAS community. Using the simplest
performance evaluation strategy of full evaluations, we comprehensively compare
this search strategy to regularized evolution and Bayesian optimization and
demonstrate that it yields improved and more robust results for 13 tabular NAS
benchmarks based on NAS-Bench-101, NAS-Bench-1Shot1, NAS-Bench-201 and NAS-HPO
bench.
</summary>
    <author>
      <name>Noor Awad</name>
    </author>
    <author>
      <name>Neeratyoy Mallik</name>
    </author>
    <author>
      <name>Frank Hutter</name>
    </author>
    <link href="http://arxiv.org/abs/2012.06400v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.06400v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.06453v1</id>
    <updated>2020-12-11T16:19:59Z</updated>
    <published>2020-12-11T16:19:59Z</published>
    <title>Better call Surrogates: A hybrid Evolutionary Algorithm for
  Hyperparameter optimization</title>
    <summary>  In this paper, we propose a surrogate-assisted evolutionary algorithm (EA)
for hyperparameter optimization of machine learning (ML) models. The proposed
STEADE model initially estimates the objective function landscape using
RadialBasis Function interpolation, and then transfers the knowledge to an EA
technique called Differential Evolution that is used to evolve new solutions
guided by a Bayesian optimization framework. We empirically evaluate our model
on the hyperparameter optimization problems as a part of the black box
optimization challenge at NeurIPS 2020 and demonstrate the improvement brought
about by STEADE over the vanilla EA.
</summary>
    <author>
      <name>Subhodip Biswas</name>
    </author>
    <author>
      <name>Adam D Cobb</name>
    </author>
    <author>
      <name>Andreea Sistrunk</name>
    </author>
    <author>
      <name>Naren Ramakrishnan</name>
    </author>
    <author>
      <name>Brian Jalaian</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted at the black box optimization challenge at NeurIPS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.06453v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.06453v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.08063v1</id>
    <updated>2020-12-15T03:22:06Z</updated>
    <published>2020-12-15T03:22:06Z</published>
    <title>A New Many-Objective Evolutionary Algorithm Based on Determinantal Point
  Processes</title>
    <summary>  To handle different types of Many-Objective Optimization Problems (MaOPs),
Many-Objective Evolutionary Algorithms (MaOEAs) need to simultaneously maintain
convergence and population diversity in the high-dimensional objective space.
In order to balance the relationship between diversity and convergence, we
introduce a Kernel Matrix and probability model called Determinantal Point
Processes (DPPs). Our Many-Objective Evolutionary Algorithm with Determinantal
Point Processes (MaOEADPPs) is presented and compared with several
state-of-the-art algorithms on various types of MaOPs \textcolor{blue}{with
different numbers of objectives}. The experimental results demonstrate that
MaOEADPPs is competitive.
</summary>
    <author>
      <name>Peng Zhang</name>
    </author>
    <author>
      <name>Jinlong Li</name>
    </author>
    <author>
      <name>Tengfei Li</name>
    </author>
    <author>
      <name>Huanhuan Chen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.08063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.08063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="14J60 (Primary) 14F05, 14J26 (Secondary)" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.08738v1</id>
    <updated>2020-12-16T04:27:45Z</updated>
    <published>2020-12-16T04:27:45Z</published>
    <title>Pareto Optimization for Subset Selection with Dynamic Partition Matroid
  Constraints</title>
    <summary>  In this study, we consider the subset selection problems with submodular or
monotone discrete objective functions under partition matroid constraints where
the thresholds are dynamic. We focus on POMC, a simple Pareto optimization
approach that has been shown to be effective on such problems. Our analysis
departs from singular constraint problems and extends to problems of multiple
constraints. We show that previous results of POMC's performance also hold for
multiple constraints. Our experimental investigations on random undirected
maxcut problems demonstrate POMC's competitiveness against the classical GREEDY
algorithm with restart strategy.
</summary>
    <author>
      <name>Anh Viet Do</name>
    </author>
    <author>
      <name>Frank Neumann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.08738v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.08738v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.09810v1</id>
    <updated>2020-12-17T18:22:05Z</updated>
    <published>2020-12-17T18:22:05Z</published>
    <title>Deep Learning Techniques for Super-Resolution in Video Games</title>
    <summary>  The computational cost of video game graphics is increasing and hardware for
processing graphics is struggling to keep up. This means that computer
scientists need to develop creative new ways to improve the performance of
graphical processing hardware. Deep learning techniques for video
super-resolution can enable video games to have high quality graphics whilst
offsetting much of the computational cost. These emerging technologies allow
consumers to have improved performance and enjoyment from video games and have
the potential to become standard within the game development industry.
</summary>
    <author>
      <name>Alexander Watson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2012.09810v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.09810v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2012.10388v1</id>
    <updated>2020-11-25T13:01:19Z</updated>
    <published>2020-11-25T13:01:19Z</published>
    <title>aw_nas: A Modularized and Extensible NAS framework</title>
    <summary>  Neural Architecture Search (NAS) has received extensive attention due to its
capability to discover neural network architectures in an automated manner.
aw_nas is an open-source Python framework implementing various NAS algorithms
in a modularized manner. Currently, aw_nas can be used to reproduce the results
of mainstream NAS algorithms of various types. Also, due to the modularized
design, one can simply experiment with different NAS algorithms for various
applications with awnas (e.g., classification, detection, text modeling, fault
tolerance, adversarial robustness, hardware efficiency, and etc.). Codes and
documentation are available at https://github.com/walkerning/aw_nas.
</summary>
    <author>
      <name>Xuefei Ning</name>
    </author>
    <author>
      <name>Changcheng Tang</name>
    </author>
    <author>
      <name>Wenshuo Li</name>
    </author>
    <author>
      <name>Songyi Yang</name>
    </author>
    <author>
      <name>Tianchen Zhao</name>
    </author>
    <author>
      <name>Niansong Zhang</name>
    </author>
    <author>
      <name>Tianyi Lu</name>
    </author>
    <author>
      <name>Shuang Liang</name>
    </author>
    <author>
      <name>Huazhong Yang</name>
    </author>
    <author>
      <name>Yu Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2012.10388v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2012.10388v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.07592v1</id>
    <updated>2021-01-19T12:32:07Z</updated>
    <published>2021-01-19T12:32:07Z</published>
    <title>Synaptic metaplasticity in binarized neural networks</title>
    <summary>  Unlike the brain, artificial neural networks, including state-of-the-art deep
neural networks for computer vision, are subject to "catastrophic forgetting":
they rapidly forget the previous task when trained on a new one. Neuroscience
suggests that biological synapses avoid this issue through the process of
synaptic consolidation and metaplasticity: the plasticity itself changes upon
repeated synaptic events. In this work, we show that this concept of
metaplasticity can be transferred to a particular type of deep neural networks,
binarized neural networks, to reduce catastrophic forgetting.
</summary>
    <author>
      <name>Axel Laborieux</name>
    </author>
    <author>
      <name>Maxence Ernoult</name>
    </author>
    <author>
      <name>Tifenn Hirtzlin</name>
    </author>
    <author>
      <name>Damien Querlioz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1038/s41467-021-22768-y</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1038/s41467-021-22768-y" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Computational and Systems Neuroscience (Cosyne) 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2101.07592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.07592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.07627v1</id>
    <updated>2021-01-19T14:02:54Z</updated>
    <published>2021-01-19T14:02:54Z</published>
    <title>Self-Organizing Intelligent Matter: A blueprint for an AI generating
  algorithm</title>
    <summary>  We propose an artificial life framework aimed at facilitating the emergence
of intelligent organisms. In this framework there is no explicit notion of an
agent: instead there is an environment made of atomic elements. These elements
contain neural operations and interact through exchanges of information and
through physics-like rules contained in the environment. We discuss how an
evolutionary process can lead to the emergence of different organisms made of
many such atomic elements which can coexist and thrive in the environment. We
discuss how this forms the basis of a general AI generating algorithm. We
provide a simplified implementation of such system and discuss what advances
need to be made to scale it up further.
</summary>
    <author>
      <name>Karol Gregor</name>
    </author>
    <author>
      <name>Frederic Besse</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.07627v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.07627v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.10483v1</id>
    <updated>2021-01-26T00:04:00Z</updated>
    <published>2021-01-26T00:04:00Z</published>
    <title>Cyber Kittens, or Some First Steps Towards Categorical Cybernetics</title>
    <summary>  We define a categorical notion of cybernetic system as a dynamical
realisation of a generalized open game, along with a coherence condition. We
show that this notion captures a wide class of cybernetic systems in
computational neuroscience and statistical machine learning, exposes their
compositional structure, and gives an abstract justification for the
bidirectional structure empirically observed in cortical circuits. Our
construction is built on the observation that Bayesian updates compose
optically, a fact which we prove along the way, via a fibred category of
state-dependent stochastic channels.
</summary>
    <author>
      <name>Toby St Clere Smithe</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Oxford</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.4204/EPTCS.333.8</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.4204/EPTCS.333.8" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings ACT 2020, arXiv:2101.07888. Includes a summary of
  arXiv:2006.01631</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">EPTCS 333, 2021, pp. 108-124</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2101.10483v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.10483v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2101.10936v1</id>
    <updated>2021-01-25T09:34:52Z</updated>
    <published>2021-01-25T09:34:52Z</published>
    <title>Combining Particle Swarm Optimizer with SQP Local Search for Constrained
  Optimization Problems</title>
    <summary>  The combining of a General-Purpose Particle Swarm Optimizer (GP-PSO) with
Sequential Quadratic Programming (SQP) algorithm for constrained optimization
problems has been shown to be highly beneficial to the refinement, and in some
cases, the success of finding a global optimum solution. It is shown that the
likely difference between leading algorithms are in their local search ability.
A comparison with other leading optimizers on the tested benchmark suite,
indicate the hybrid GP-PSO with implemented local search to compete along side
other leading PSO algorithms.
</summary>
    <author>
      <name>Carwyn Pelley</name>
    </author>
    <author>
      <name>Mauro S. Innocente</name>
    </author>
    <author>
      <name>Johann Sienz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint submitted to the 8th ASMO UK Conference on Engineering
  Design Optimization</arxiv:comment>
    <link href="http://arxiv.org/abs/2101.10936v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2101.10936v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.04751v1</id>
    <updated>2021-02-24T15:49:11Z</updated>
    <published>2021-02-24T15:49:11Z</published>
    <title>A Memory Optimized Data Structure for Binary Chromosomes in Genetic
  Algorithm</title>
    <summary>  This paper presents a memory-optimized metadata-based data structure for
implementation of binary chromosome in Genetic Algorithm. In GA different types
of genotypes are used depending on the problem domain. Among these, binary
genotype is the most popular one for non-enumerated encoding owing to its
representational and computational simplicity. This paper proposes a
memory-optimized implementation approach of binary genotype. The approach
improves the memory utilization as well as capacity of retaining alleles.
Mathematical proof has been provided to establish the same.
</summary>
    <author>
      <name>Avijit Basak</name>
    </author>
    <link href="http://arxiv.org/abs/2103.04751v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.04751v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.06739v1</id>
    <updated>2021-03-11T15:37:52Z</updated>
    <published>2021-03-11T15:37:52Z</published>
    <title>Multi-objective discovery of PDE systems using evolutionary approach</title>
    <summary>  Usually, the systems of partial differential equations (PDEs) are discovered
from observational data in the single vector equation form. However, this
approach restricts the application to the real cases, where, for example, the
form of the external forcing is of interest. In the paper, a multi-objective
co-evolution algorithm is described. The single equations within the system and
the system itself are evolved simultaneously to obtain the system. This
approach allows discovering the systems with the form-independent equations. In
contrast to the single vector equation, a component-wise system is more
suitable for expert interpretation and, therefore, for applications. The
example of the two-dimensional Navier-Stokes equation is considered.
</summary>
    <author>
      <name>Mikhail Maslyaev</name>
    </author>
    <author>
      <name>Alexander Hvatov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CEC45853.2021.9504712</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CEC45853.2021.9504712" rel="related"/>
    <link href="http://arxiv.org/abs/2103.06739v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.06739v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2103.15550v1</id>
    <updated>2021-03-08T01:26:50Z</updated>
    <published>2021-03-08T01:26:50Z</published>
    <title>SCNN: Swarm Characteristic Neural Network</title>
    <summary>  Deep learning is a powerful approach with good performance on many different
tasks. However, these models often require massive computational resources. It
is a worrying trend that we increasingly need models that work well on more
complex problems. In this paper, we propose and verify the effectiveness and
efficiency of SCNN, an innovative neural network inspired by the swarm concept.
In addition to introducing the relevant theories, our detailed experiments
suggest that fewer parameters may perform better than models with more
parameters. Besides, our experiments show that SCNN needs less data than
traditional models. That could be an essential hint for problems where there is
not much data.
</summary>
    <author>
      <name>Ha-Thanh Nguyen</name>
    </author>
    <author>
      <name>Le-Minh Nguyen</name>
    </author>
    <link href="http://arxiv.org/abs/2103.15550v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2103.15550v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.04121v1</id>
    <updated>2021-04-09T00:26:37Z</updated>
    <published>2021-04-09T00:26:37Z</published>
    <title>Fast, Smart Neuromorphic Sensors Based on Heterogeneous Networks and
  Mixed Encodings</title>
    <summary>  Neuromorphic architectures are ideally suited for the implementation of smart
sensors able to react, learn, and respond to a changing environment. Our work
uses the insect brain as a model to understand how heterogeneous architectures,
incorporating different types of neurons and encodings, can be leveraged to
create systems integrating input processing, evaluation, and response. Here we
show how the combination of time and rate encodings can lead to fast sensors
that are able to generate a hypothesis on the input in only a few cycles and
then use that hypothesis as secondary input for more detailed analysis.
</summary>
    <author>
      <name>Angel Yanguas-Gil</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper accepted in GOMACTech 2018</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.04121v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.04121v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.08239v1</id>
    <updated>2021-04-16T17:17:28Z</updated>
    <published>2021-04-16T17:17:28Z</published>
    <title>An exploration of asocial and social learning in the evolution of
  variable-length structures</title>
    <summary>  We wish to explore the contribution that asocial and social learning might
play as a mechanism for self-adaptation in the search for variable-length
structures by an evolutionary algorithm. An extremely challenging, yet simple
to understand problem landscape is adopted where the probability of randomly
finding a solution is approximately one in a trillion. A number of learning
mechanisms operating on variable-length structures are implemented and their
performance analysed. The social learning setup, which combines forms of both
social and asocial learning in combination with evolution is found to be most
performant, while the setups exclusively adopting evolution are incapable of
finding solutions.
</summary>
    <author>
      <name>Michael O'Neill</name>
    </author>
    <author>
      <name>Anthony Brabazon</name>
    </author>
    <link href="http://arxiv.org/abs/2104.08239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.08239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.08774v1</id>
    <updated>2021-04-18T08:46:38Z</updated>
    <published>2021-04-18T08:46:38Z</published>
    <title>ARCH-Elites: Quality-Diversity for Urban Design</title>
    <summary>  This paper introduces ARCH-Elites, a MAP-Elites implementation that can
reconfigure large-scale urban layouts at real-world locations via a pre-trained
surrogate model instead of costly simulations. In a series of experiments, we
generate novel urban designs for two real-world locations in Boston,
Massachusetts. Combining the exploration of a possibility space with real-time
performance evaluation creates a powerful new paradigm for architectural
generative design that can extract and articulate design intelligence.
</summary>
    <author>
      <name>Theodoros Galanos</name>
    </author>
    <author>
      <name>Antonios Liapis</name>
    </author>
    <author>
      <name>Georgios N. Yannakakis</name>
    </author>
    <author>
      <name>Reinhard Koenig</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3449726.3459490</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3449726.3459490" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published at Genetic and Evolutionary Computation Conference
  Companion, 2021, 2 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.08774v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.08774v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.HC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.09103v2</id>
    <updated>2021-04-20T09:28:17Z</updated>
    <published>2021-04-19T07:48:55Z</published>
    <title>Conditional Coding and Variable Bitrate for Practical Learned Video
  Coding</title>
    <summary>  This paper introduces a practical learned video codec. Conditional coding and
quantization gain vectors are used to provide flexibility to a single
encoder/decoder pair, which is able to compress video sequences at a variable
bitrate. The flexibility is leveraged at test time by choosing the rate and GOP
structure to optimize a rate-distortion cost. Using the CLIC21 video test
conditions, the proposed approach shows performance on par with HEVC.
</summary>
    <author>
      <name>Théo Ladune</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IETR</arxiv:affiliation>
    </author>
    <author>
      <name>Pierrick Philippe</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IETR</arxiv:affiliation>
    </author>
    <author>
      <name>Wassim Hamidouche</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IETR</arxiv:affiliation>
    </author>
    <author>
      <name>Lu Zhang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IETR</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Déforges</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">IETR</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">CLIC workshop, CVPR 2021, Jun 2021, Nashville, United States</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2104.09103v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.09103v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.10041v1</id>
    <updated>2021-04-09T07:32:14Z</updated>
    <published>2021-04-09T07:32:14Z</published>
    <title>Particle swarm optimization in constrained maximum likelihood estimation
  a case study</title>
    <summary>  The aim of paper is to apply two types of particle swarm optimization, global
best andlocal best PSO to a constrained maximum likelihood estimation problem
in pseudotime anal-ysis, a sub-field in bioinformatics. The results have shown
that particle swarm optimizationis extremely useful and efficient when the
optimization problem is non-differentiable and non-convex so that analytical
solution can not be derived and gradient-based methods can not beapplied.
</summary>
    <author>
      <name>Elvis Cui</name>
    </author>
    <author>
      <name>Dongyuan Song</name>
    </author>
    <author>
      <name>Weng Kee Wong</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.10041v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.10041v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.AP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.11169v1</id>
    <updated>2021-04-22T16:40:33Z</updated>
    <published>2021-04-22T16:40:33Z</published>
    <title>Noise-Robust Deep Spiking Neural Networks with Temporal Information</title>
    <summary>  Spiking neural networks (SNNs) have emerged as energy-efficient neural
networks with temporal information. SNNs have shown a superior efficiency on
neuromorphic devices, but the devices are susceptible to noise, which hinders
them from being applied in real-world applications. Several studies have
increased noise robustness, but most of them considered neither deep SNNs nor
temporal information. In this paper, we investigate the effect of noise on deep
SNNs with various neural coding methods and present a noise-robust deep SNN
with temporal information. With the proposed methods, we have achieved a deep
SNN that is efficient and robust to spike deletion and jitter.
</summary>
    <author>
      <name>Seongsik Park</name>
    </author>
    <author>
      <name>Dongjin Lee</name>
    </author>
    <author>
      <name>Sungroh Yoon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to DAC 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2104.11169v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.11169v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.00293v2</id>
    <updated>2021-05-04T08:10:55Z</updated>
    <published>2021-05-01T16:05:14Z</published>
    <title>A Single-Layer Asymmetric RNN: Potential Low Hardware Complexity Linear
  Equation Solver</title>
    <summary>  A single layer neural network for the solution of linear equations is
presented. The proposed circuit is based on the standard Hopfield model albeit
with the added flexibility that the interconnection weight matrix need not be
symmetric. This results in an asymmetric Hopfield neural network capable of
solving linear equations. PSPICE simulation results are given which verify the
theoretical predictions. Experimental results for circuits set up to solve
small problems further confirm the operation of the proposed circuit.
</summary>
    <author>
      <name>Mohammad Samar Ansari</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint submitted to Neurocomputing</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.00293v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.00293v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.03680v1</id>
    <updated>2021-05-08T11:43:26Z</updated>
    <published>2021-05-08T11:43:26Z</published>
    <title>A Crossover That Matches Diverse Parents Together in Evolutionary
  Algorithms</title>
    <summary>  Crossover and mutation are the two main operators that lead to new solutions
in evolutionary approaches. In this article, a new method of performing the
crossover phase is presented. The problem of choice is evolutionary decision
tree construction. The method aims at finding such individuals that together
complement each other. Hence we say that they are diversely specialized. We
propose the way of calculating the so-called complementary fitness. In several
empirical experiments, we evaluate the efficacy of the method proposed in four
variants and compare it to a fitness-rank-based approach. One variant emerges
clearly as the best approach, whereas the remaining ones are below the
baseline.
</summary>
    <author>
      <name>Maciej Świechowski</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3449726.3459431</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3449726.3459431" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to GECCO 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.03680v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.03680v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8; I.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.04252v1</id>
    <updated>2021-05-10T10:39:03Z</updated>
    <published>2021-05-10T10:39:03Z</published>
    <title>An Analysis of Phenotypic Diversity in Multi-Solution Optimization</title>
    <summary>  More and more, optimization methods are used to find diverse solution sets.
We compare solution diversity in multi-objective optimization, multimodal
optimization, and quality diversity in a simple domain. We show that
multiobjective optimization does not always produce much diversity, multimodal
optimization produces higher fitness solutions, and quality diversity is not
sensitive to genetic neutrality and creates the most diverse set of solutions.
An autoencoder is used to discover phenotypic features automatically, producing
an even more diverse solution set with quality diversity. Finally, we make
recommendations about when to use which approach.
</summary>
    <author>
      <name>Alexander Hagg</name>
    </author>
    <author>
      <name>Mike Preuss</name>
    </author>
    <author>
      <name>Alexander Asteroth</name>
    </author>
    <author>
      <name>Thomas Bäck</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-030-63710-1_4</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-030-63710-1_4" rel="related"/>
    <link href="http://arxiv.org/abs/2105.04252v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.04252v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.04311v1</id>
    <updated>2021-05-10T12:46:26Z</updated>
    <published>2021-05-10T12:46:26Z</published>
    <title>Overcoming Complexity Catastrophe: An Algorithm for Beneficial
  Far-Reaching Adaptation under High Complexity</title>
    <summary>  In his seminal work with NK algorithms, Kauffman noted that fitness outcomes
from algorithms navigating an NK landscape show a sharp decline at high
complexity arising from pervasive interdependence among problem dimensions.
This phenomenon - where complexity effects dominate (Darwinian) adaptation
efforts - is called complexity catastrophe. We present an algorithm -
incremental change taking turns (ICTT) - that finds distant configurations
having fitness superior to that reported in extant research, under high
complexity. Thus, complexity catastrophe is not inevitable: a series of
incremental changes can lead to excellent outcomes.
</summary>
    <author>
      <name>Sasanka Sekhar Chanda</name>
    </author>
    <author>
      <name>Sai Yayavaram</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.04311v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.04311v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2105.13971v2</id>
    <updated>2021-10-04T20:49:52Z</updated>
    <published>2021-05-27T16:54:23Z</published>
    <title>Artificial life: sustainable self-replicating systems</title>
    <summary>  Nature has found one method of organizing living matter, but maybe other
options exist -- not yet discovered -- on how to create life. To study the life
"as it could be" is the objective of an interdisciplinary field called
Artificial Life (commonly abbreviated as ALife). The word "artificial" refers
to the fact that humans are involved in the creation process. The artificial
life forms might be completely unlike natural forms of life, with different
chemical compositions, and even computer programs exhibiting life-like
behaviours.
</summary>
    <author>
      <name>Carlos Gershenson</name>
    </author>
    <author>
      <name>Jitka Cejkova</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Short review chapter, to be included in EPS Grand Challenges 2050
  book</arxiv:comment>
    <link href="http://arxiv.org/abs/2105.13971v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2105.13971v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.chem-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.01920v1</id>
    <updated>2021-06-03T15:14:46Z</updated>
    <published>2021-06-03T15:14:46Z</published>
    <title>Convolutional Neural Network(CNN/ConvNet) in Stock Price Movement
  Prediction</title>
    <summary>  With technological advancements and the exponential growth of data, we have
been unfolding different capabilities of neural networks in different sectors.
In this paper, I have tried to use a specific type of Neural Network known as
Convolutional Neural Network(CNN/ConvNet) in the stock market. In other words,
I have tried to construct and train a convolutional neural network on past
stock prices data and then tried to predict the movement of stock price i.e.
whether the stock price would rise or fall, in the coming time.
</summary>
    <author>
      <name>Kunal Bhardwaj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.01920v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.01920v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.04775v1</id>
    <updated>2021-06-09T02:37:46Z</updated>
    <published>2021-06-09T02:37:46Z</published>
    <title>A 2020 taxonomy of algorithms inspired on living beings behavior</title>
    <summary>  Taking the role of a computer naturalist, a journey is taken through bio
inspired algorithms taking account on algorithms which are inspired on living
being behaviors. A compilation of algorithms is made considering several
reviews or surveys of bio-inspired heuristics and swarm intelligence until 2020
year. A classification is made considering kingdoms as used by biologists
generating several branches for animalia, bacteria, plants, fungi and protista
to develop a taxonomy.
</summary>
    <author>
      <name>Luis Torres-Treviño</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">a collection of algorithms names, 24 pages, two figures, 9 tables, a
  recompilation</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.04775v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.04775v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8; I.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.06876v1</id>
    <updated>2021-06-12T22:41:18Z</updated>
    <published>2021-06-12T22:41:18Z</published>
    <title>Affine OneMax</title>
    <summary>  A new class of test functions for black box optimization is introduced.
Affine OneMax (AOM) functions are defined as compositions of OneMax and
invertible affine maps on bit vectors. The black box complexity of the class is
upper bounded by a polynomial of large degree in the dimension. The proof
relies on discrete Fourier analysis and the Kushilevitz-Mansour algorithm.
Tunable complexity is achieved by expressing invertible linear maps as finite
products of transvections. The black box complexity of sub-classes of AOM
functions is studied. Finally, experimental results are given to illustrate the
performance of search algorithms on AOM functions.
</summary>
    <author>
      <name>Arnaud Berny</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An extended two-page abstract of this work will appear in 2021
  Genetic and Evolutionary Computation Conference Companion (GECCO '21
  Companion)</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.06876v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.06876v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.07095v1</id>
    <updated>2021-06-13T21:28:03Z</updated>
    <published>2021-06-13T21:28:03Z</published>
    <title>Linear representation of categorical values</title>
    <summary>  We propose a binary representation of categorical values using a linear map.
This linear representation preserves the neighborhood structure of categorical
values. In the context of evolutionary algorithms, it means that every
categorical value can be reached in a single mutation. The linear
representation is embedded into standard metaheuristics, applied to the problem
of Sudoku puzzles, and compared to the more traditional direct binary encoding.
It shows promising results in fixed-budget experiments and empirical cumulative
distribution functions with high dimension instances, and also in fixed-target
experiments with small dimension instances.
</summary>
    <author>
      <name>Arnaud Berny</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">An extended two-page abstract of this work will appear in 2021
  Genetic and Evolutionary Computation Conference Companion (GECCO '21
  Companion)</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.07095v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.07095v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.08937v1</id>
    <updated>2021-06-16T16:48:14Z</updated>
    <published>2021-06-16T16:48:14Z</published>
    <title>A Predictive Coding Account for Chaotic Itinerancy</title>
    <summary>  As a phenomenon in dynamical systems allowing autonomous switching between
stable behaviors, chaotic itinerancy has gained interest in neurorobotics
research. In this study, we draw a connection between this phenomenon and the
predictive coding theory by showing how a recurrent neural network implementing
predictive coding can generate neural trajectories similar to chaotic
itinerancy in the presence of input noise. We propose two scenarios generating
random and past-independent attractor switching trajectories using our model.
</summary>
    <author>
      <name>Louis Annabi</name>
    </author>
    <author>
      <name>Alexandre Pitti</name>
    </author>
    <author>
      <name>Mathias Quoy</name>
    </author>
    <link href="http://arxiv.org/abs/2106.08937v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.08937v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.11151v1</id>
    <updated>2021-06-21T14:46:18Z</updated>
    <published>2021-06-21T14:46:18Z</published>
    <title>The Role of Evolution in Machine Intelligence</title>
    <summary>  Machine intelligence can develop either directly from experience or by
inheriting experience through evolution. The bulk of current research efforts
focus on algorithms which learn directly from experience. I argue that the
alternative, evolution, is important to the development of machine intelligence
and underinvested in terms of research allocation. The primary aim of this work
is to assess where along the spectrum of evolutionary algorithms to invest in
research. My first-order suggestion is to diversify research across a broader
spectrum of evolutionary approaches. I also define meta-evolutionary algorithms
and argue that they may yield an optimal trade-off between the many factors
influencing the development of machine intelligence.
</summary>
    <author>
      <name>Awni Hannun</name>
    </author>
    <link href="http://arxiv.org/abs/2106.11151v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.11151v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.11914v1</id>
    <updated>2021-06-07T13:24:08Z</updated>
    <published>2021-06-07T13:24:08Z</published>
    <title>MONCAE: Multi-Objective Neuroevolution of Convolutional Autoencoders</title>
    <summary>  In this paper, we present a novel neuroevolutionary method to identify the
architecture and hyperparameters of convolutional autoencoders. Remarkably, we
used a hypervolume indicator in the context of neural architecture search for
autoencoders, for the first time to our current knowledge. Results show that
images were compressed by a factor of more than 10, while still retaining
enough information to achieve image classification for the majority of the
tasks. Thus, this new approach can be used to speed up the AutoML pipeline for
image compression.
</summary>
    <author>
      <name>Daniel Dimanov</name>
    </author>
    <author>
      <name>Emili Balaguer-Ballester</name>
    </author>
    <author>
      <name>Colin Singleton</name>
    </author>
    <author>
      <name>Shahin Rostami</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published as a Poster paper in ICLR 2021 Neural Architecture Search
  workshop</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.11914v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.11914v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.11916v1</id>
    <updated>2021-06-06T00:00:41Z</updated>
    <published>2021-06-06T00:00:41Z</published>
    <title>Selecting Miners within Blockchain-based Systems Using Evolutionary
  Algorithms for Energy Optimisation</title>
    <summary>  In this paper, we represent the problem of selecting miners within a
blockchain-based system as a subset selection problem. We formulate the problem
of minimising blockchain energy consumption as an optimisation problem with two
conflicting objectives: energy consumption and trust. The proposed model is
compared across different algorithms to demonstrate its performance.
</summary>
    <author>
      <name>Akram Alofi</name>
    </author>
    <author>
      <name>Mahmoud A. Bokhari</name>
    </author>
    <author>
      <name>Robert Hendley</name>
    </author>
    <author>
      <name>Rami Bahsoon</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3449726.3459558</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3449726.3459558" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in 2021 Genetic and Evolutionary Computation Conference
  Companion (GECCO '21 Companion), July 10--14, 2021, Lille, France</arxiv:comment>
    <link href="http://arxiv.org/abs/2106.11916v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.11916v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.14487v1</id>
    <updated>2021-06-28T09:04:51Z</updated>
    <published>2021-06-28T09:04:51Z</published>
    <title>A Meta-Heuristic Search Algorithm based on Infrasonic Mating Displays in
  Peafowls</title>
    <summary>  Meta-heuristic techniques are important as they are used to find solutions to
computationally intractable problems. Simplistic methods such as exhaustive
search become computationally expensive and unreliable as the solution space
for search algorithms increase. As no method is guaranteed to perform better
than all others in all classes of optimization search problems, there is a need
to constantly find new and/or adapt old search algorithms. This research
proposes an Infrasonic Search Algorithm, inspired from the Gravitational Search
Algorithm and the mating behaviour in peafowls. The Infrasonic Search Algorithm
identified competitive solutions to 23 benchmark unimodal and multimodal test
functions compared to the Genetic Algorithm, Particle Swarm Optimization
Algorithm and the Gravitational Search Algorithm.
</summary>
    <author>
      <name>Patrick Kenekayoro</name>
    </author>
    <link href="http://arxiv.org/abs/2106.14487v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.14487v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2106.14773v1</id>
    <updated>2021-06-28T14:47:15Z</updated>
    <published>2021-06-28T14:47:15Z</published>
    <title>Designing color symmetry in stigmergic art</title>
    <summary>  Color symmetry is an extension of symmetry imposed by isometric
transformations and means that the colors of geometrical objects are assigned
according to the symmetry properties of the objects. A color symmetry permutes
the coloring of the objects consistently with their symmetry group. We apply
this concept to bio-inspired generative art. Therefore, we interpret the
geometrical objects as motifs that may repeat themselves with a
symmetry-consistent coloring. The motifs are obtained by design principles from
stigmergy. We discuss a design procedure and present visual results.
</summary>
    <author>
      <name>Hendrik Richter</name>
    </author>
    <link href="http://arxiv.org/abs/2106.14773v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2106.14773v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.03156v1</id>
    <updated>2021-08-06T15:10:36Z</updated>
    <published>2021-08-06T15:10:36Z</published>
    <title>Substitution of the Fittest: A Novel Approach for Mitigating
  Disengagement in Coevolutionary Genetic Algorithms</title>
    <summary>  We propose substitution of the fittest (SF), a novel technique designed to
counteract the problem of disengagement in two-population competitive
coevolutionary genetic algorithms. The approach presented is domain-independent
and requires no calibration. In a minimal domain, we perform a controlled
evaluation of the ability to maintain engagement and the capacity to discover
optimal solutions. Results demonstrate that the solution discovery performance
of SF is comparable with other techniques in the literature, while SF also
offers benefits including a greater ability to maintain engagement and a much
simpler mechanism.
</summary>
    <author>
      <name>Hugo Alcaraz-Herrera</name>
    </author>
    <author>
      <name>John Cartlidge</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, authors' manuscript accepted for publication in 13th
  International Conference on Evolutionary Computation Theory and Algorithms
  (ECTA 2021)</arxiv:comment>
    <link href="http://arxiv.org/abs/2108.03156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.03156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2108.08691v1</id>
    <updated>2021-08-19T13:58:06Z</updated>
    <published>2021-08-19T13:58:06Z</published>
    <title>A Cuckoo Quantum Evolutionary Algorithm for the Graph Coloring Problem</title>
    <summary>  Based on the framework of the quantum-inspired evolutionary algorithm, a
cuckoo quantum evolutionary algorithm (CQEA) is proposed for solving the graph
coloring problem (GCP). To reduce iterations for the search of the chromatic
number, the initial quantum population is generated by random initialization
assisted by inheritance. Moreover, improvement of global exploration is
achieved by incorporating the cuckoo search strategy, and a local search
operation, as well as a perturbance strategy, is developed to enhance its
performance on GCPs. Numerical results demonstrate that CQEA operates with
strong exploration and exploitation abilities, and is competitive to the
compared state-of-the-art heuristic algorithms.
</summary>
    <author>
      <name>Yongjian Xu</name>
    </author>
    <author>
      <name>Yu Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2108.08691v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2108.08691v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.08234v1</id>
    <updated>2021-09-16T21:42:28Z</updated>
    <published>2021-09-16T21:42:28Z</published>
    <title>Deep Spiking Neural Networks with Resonate-and-Fire Neurons</title>
    <summary>  In this work, we explore a new Spiking Neural Network (SNN) formulation with
Resonate-and-Fire (RAF) neurons (Izhikevich, 2001) trained with gradient
descent via back-propagation. The RAF-SNN, while more biologically plausible,
achieves performance comparable to or higher than conventional models in the
Machine Learning literature across different network configurations, using
similar or fewer parameters. Strikingly, the RAF-SNN proves robust against
noise induced at testing/training time, under both static and dynamic
conditions. Against CNN on MNIST, we show 25% higher absolute accuracy with
N(0, 0.2) induced noise at testing time. Against LSTM on N-MNIST, we show 70%
higher absolute accuracy with 20% induced noise at training time.
</summary>
    <author>
      <name>Badr AlKhamissi</name>
    </author>
    <author>
      <name>Muhammad ElNokrashy</name>
    </author>
    <author>
      <name>David Bernal-Casas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Preprint</arxiv:comment>
    <link href="http://arxiv.org/abs/2109.08234v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.08234v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.12786v2</id>
    <updated>2021-10-05T03:13:31Z</updated>
    <published>2021-09-27T04:08:23Z</published>
    <title>Self-Replicating Neural Programs</title>
    <summary>  In this work, a neural network is trained to replicate the code that trains
it using only its own output as input. A paradigm for evolutionary
self-replication in neural programs is introduced, where program parameters are
mutated, and the ability for the program to more efficiently train itself leads
to greater reproductive success. This evolutionary paradigm is demonstrated to
produce more efficient learning in organisms from a setting without any
explicit guidance, solely based on natural selection favoring organisms with
faster reproductive maturity.
</summary>
    <author>
      <name>Samuel Schmidgall</name>
    </author>
    <link href="http://arxiv.org/abs/2109.12786v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.12786v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.13078v1</id>
    <updated>2021-09-22T11:18:57Z</updated>
    <published>2021-09-22T11:18:57Z</published>
    <title>Analysis of chaotic dynamical systems with autoencoders</title>
    <summary>  We focus on chaotic dynamical systems and analyze their time series with the
use of autoencoders, i.e., configurations of neural networks that map identical
output to input. This analysis results in the determination of the latent space
dimension of each system and thus determines the minimal number of nodes
necessary to capture the essential information contained in the chaotic time
series. The constructed chaotic autoencoders generate similar maximal Lyapunov
exponents as the original chaotic systems and thus encompass their essential
dynamical information.
</summary>
    <author>
      <name>N. Almazova</name>
    </author>
    <author>
      <name>G. D. Barmparis</name>
    </author>
    <author>
      <name>G. P. Tsironis</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1063/5.0055673</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1063/5.0055673" rel="related"/>
    <link href="http://arxiv.org/abs/2109.13078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.13078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.13102v2</id>
    <updated>2021-10-05T04:50:49Z</updated>
    <published>2021-09-07T13:04:55Z</published>
    <title>A Biologically Plausible Learning Rule for Perceptual Systems of
  organisms that Maximize Mutual Information</title>
    <summary>  It is widely believed that the perceptual system of an organism is optimized
for the properties of the environment to which it is exposed. A specific
instance of this principle known as the Infomax principle holds that the
purpose of early perceptual processing is to maximize the mutual information
between the neural coding and the incoming sensory signal. In this article, we
present a method to implement this principle accurately with a local,
spike-based, and continuous-time learning rule.
</summary>
    <author>
      <name>Tao Liu</name>
    </author>
    <link href="http://arxiv.org/abs/2109.13102v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.13102v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.13107v1</id>
    <updated>2021-08-21T15:48:50Z</updated>
    <published>2021-08-21T15:48:50Z</published>
    <title>Evolving Digital Circuits for the Knapsack Problem</title>
    <summary>  Multi Expression Programming (MEP) is a Genetic Programming variant that uses
linear chromosomes for solution encoding. A unique feature of MEP is its
ability of encoding multiple solutions of a problem in a single chromosome. In
this paper we use Multi Expression Programming for evolving digital circuits
for a well-known NP-Complete problem: the knapsack (subset sum) problem.
Numerical experiments show that Multi Expression Programming performs well on
the considered test problems.
</summary>
    <author>
      <name>Mihai Oltean</name>
    </author>
    <author>
      <name>Crina Groşan</name>
    </author>
    <author>
      <name>Mihaela Oltean</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/b97989</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/b97989" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">E-HARD Workshop, Edited by M. Bubak, G. D. van Albada, P. Sloot,
  and J. Dongarra, Vol. III, pp. 1257-1264, 6-9 June, Krakow, Poland,
  Springer-Verlag, Berlin, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2109.13107v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.13107v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.13110v1</id>
    <updated>2021-08-21T19:15:07Z</updated>
    <published>2021-08-21T19:15:07Z</published>
    <title>Evolving Evolutionary Algorithms using Linear Genetic Programming</title>
    <summary>  A new model for evolving Evolutionary Algorithms is proposed in this paper.
The model is based on the Linear Genetic Programming (LGP) technique. Every LGP
chromosome encodes an EA which is used for solving a particular problem.
Several Evolutionary Algorithms for function optimization, the Traveling
Salesman Problem, and the Quadratic Assignment Problem are evolved by using the
considered model. Numerical experiments show that the evolved Evolutionary
Algorithms perform similarly and sometimes even better than standard approaches
for several well-known benchmarking problems.
</summary>
    <author>
      <name>Mihai Oltean</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/1063656054794815</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/1063656054794815" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, 3 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Evolutionary Computation, MIT Press, Vol. 13, Issue 3, pp.
  387-410, 2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2109.13110v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.13110v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2109.13737v1</id>
    <updated>2021-08-22T09:30:57Z</updated>
    <published>2021-08-22T09:30:57Z</published>
    <title>Evolving Evolutionary Algorithms using Multi Expression Programming</title>
    <summary>  Finding the optimal parameter setting (i.e. the optimal population size, the
optimal mutation probability, the optimal evolutionary model etc) for an
Evolutionary Algorithm (EA) is a difficult task. Instead of evolving only the
parameters of the algorithm we will evolve an entire EA capable of solving a
particular problem. For this purpose the Multi Expression Programming (MEP)
technique is used. Each MEP chromosome will encode multiple EAs. An
nongenerational EA for function optimization is evolved in this paper.
Numerical experiments show the effectiveness of this approach.
</summary>
    <author>
      <name>Mihai Oltean</name>
    </author>
    <author>
      <name>Crina Groşan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-540-39432-7_70</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-540-39432-7_70" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:2109.13110</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">European Conference on Artificial Life, LNCS 2801, pp. 651-658,
  2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2109.13737v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2109.13737v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.02014v1</id>
    <updated>2021-10-04T13:23:32Z</updated>
    <published>2021-10-04T13:23:32Z</published>
    <title>Solving even-parity problems using traceless genetic programming</title>
    <summary>  A genetic programming (GP) variant called traceless genetic programming (TGP)
is proposed in this paper. TGP is a hybrid method combining a technique for
building individuals and a technique for representing individuals. The main
difference between TGP and other GP techniques is that TGP does not explicitly
store the evolved computer programs. Two genetic operators are used in
conjunction with TGP: crossover and insertion. TGP is applied for evolving
digital circuits for the even-parity problem. Numerical experiments show that
TGP outperforms standard GP with several orders of magnitude.
</summary>
    <author>
      <name>Mihai Oltean</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CEC.2004.1331116</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CEC.2004.1331116" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Figures, 3 Tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Congress on Evolutionary Computation, IEEE Press, Vol.2, pp.
  1813-1819, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2110.02014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.02014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.02775v1</id>
    <updated>2021-10-05T15:45:56Z</updated>
    <published>2021-10-05T15:45:56Z</published>
    <title>NEWRON: A New Generalization of the Artificial Neuron to Enhance the
  Interpretability of Neural Networks</title>
    <summary>  In this work, we formulate NEWRON: a generalization of the McCulloch-Pitts
neuron structure. This new framework aims to explore additional desirable
properties of artificial neurons. We show that some specializations of NEWRON
allow the network to be interpretable with no change in their expressiveness.
By just inspecting the models produced by our NEWRON-based networks, we can
understand the rules governing the task. Extensive experiments show that the
quality of the generated models is better than traditional interpretable models
and in line or better than standard neural networks.
</summary>
    <author>
      <name>Federico Siciliano</name>
    </author>
    <author>
      <name>Maria Sofia Bucarelli</name>
    </author>
    <author>
      <name>Gabriele Tolomei</name>
    </author>
    <author>
      <name>Fabrizio Silvestri</name>
    </author>
    <link href="http://arxiv.org/abs/2110.02775v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.02775v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.05951v1</id>
    <updated>2021-10-10T16:26:20Z</updated>
    <published>2021-10-10T16:26:20Z</published>
    <title>Evolving Evolutionary Algorithms with Patterns</title>
    <summary>  A new model for evolving Evolutionary Algorithms (EAs) is proposed in this
paper. The model is based on the Multi Expression Programming (MEP) technique.
Each MEP chromosome encodes an evolutionary pattern that is repeatedly used for
generating the individuals of a new generation. The evolved pattern is embedded
into a standard evolutionary scheme that is used for solving a particular
problem. Several evolutionary algorithms for function optimization are evolved
by using the considered model. The evolved evolutionary algorithms are compared
with a human-designed Genetic Algorithm. Numerical experiments show that the
evolved evolutionary algorithms can compete with standard approaches for
several well-known benchmarking problems.
</summary>
    <author>
      <name>Mihai Oltean</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00500-006-0079-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00500-006-0079-1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2109.13110</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Soft Computing, Springer-Verlag Vol. 11, Issue 6, pp. 503-518,
  2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2110.05951v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.05951v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.10320v1</id>
    <updated>2021-10-20T00:11:39Z</updated>
    <published>2021-10-20T00:11:39Z</published>
    <title>Frontiers in Evolutionary Computation: A Workshop Report</title>
    <summary>  In July of 2021, the Santa Fe Institute hosted a workshop on evolutionary
computation as part of its Foundations of Intelligence in Natural and
Artificial Systems project. This project seeks to advance the field of
artificial intelligence by promoting interdisciplinary research on the nature
of intelligence. The workshop brought together computer scientists and
biologists to share their insights about the nature of evolution and the future
of evolutionary computation. In this report, we summarize each of the talks and
the subsequent discussions. We also draw out a number of key themes and
identify important frontiers for future research.
</summary>
    <author>
      <name>Tyler Millhouse</name>
    </author>
    <author>
      <name>Melanie Moses</name>
    </author>
    <author>
      <name>Melanie Mitchell</name>
    </author>
    <link href="http://arxiv.org/abs/2110.10320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.10320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.11239v1</id>
    <updated>2021-10-13T09:38:50Z</updated>
    <published>2021-10-13T09:38:50Z</published>
    <title>Improving the Search by Encoding Multiple Solutions in a Chromosome</title>
    <summary>  We investigate the possibility of encoding multiple solutions of a problem in
a single chromosome. The best solution encoded in an individual will represent
(will provide the fitness of) that individual. In order to obtain some benefits
the chromosome decoding process must have the same complexity as in the case of
a single solution in a chromosome. Three Genetic Programming techniques are
analyzed for this purpose: Multi Expression Programming, Linear Genetic
Programming, and Infix Form Genetic Programming. Numerical experiments show
that encoding multiple solutions in a chromosome greatly improves the search
process.
</summary>
    <author>
      <name>Mihai Oltean</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Evolutionary Machine Design, Nova Science Publisher, New-York,
  edited by Nadia Nedjah (et al.), pp. 81-107, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2110.11239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.11239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2110.13614v1</id>
    <updated>2021-10-19T12:49:24Z</updated>
    <published>2021-10-19T12:49:24Z</published>
    <title>Model-Free Prediction of Chaotic Systems Using High Efficient
  Next-generation Reservoir Computing</title>
    <summary>  To predict the future evolution of dynamical systems purely from observations
of the past data is of great potential application. In this work, a new
formulated paradigm of reservoir computing is proposed for achieving model-free
predication for both low-dimensional and very large spatiotemporal chaotic
systems. Compared with traditional reservoir computing models, it is more
efficient in terms of predication length, training data set required and
computational expense. By taking the Lorenz and Kuramoto-Sivashinsky equations
as two classical examples of dynamical systems, numerical simulations are
conducted, and the results show our model excels at predication tasks than the
latest reservoir computing methods.
</summary>
    <author>
      <name>Zhuo Liu</name>
    </author>
    <author>
      <name>Leisheng Jin</name>
    </author>
    <link href="http://arxiv.org/abs/2110.13614v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2110.13614v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.06174v2</id>
    <updated>2023-05-27T17:47:01Z</updated>
    <published>2021-11-11T12:35:13Z</published>
    <title>Does the Brain Infer Invariance Transformations from Graph Symmetries?</title>
    <summary>  The invariance of natural objects under perceptual changes is possibly
encoded in the brain by symmetries in the graph of synaptic connections. The
graph can be established via unsupervised learning in a biologically plausible
process across different perceptual modalities. This hypothetical encoding
scheme is supported by the correlation structure of naturalistic audio and
image data and it predicts a neural connectivity architecture which is
consistent with many empirical observations about primary sensory cortex.
</summary>
    <author>
      <name>Helmut Linde</name>
    </author>
    <link href="http://arxiv.org/abs/2111.06174v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.06174v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.14790v1</id>
    <updated>2021-10-07T06:13:07Z</updated>
    <published>2021-10-07T06:13:07Z</published>
    <title>Solving classification problems using Traceless Genetic Programming</title>
    <summary>  Traceless Genetic Programming (TGP) is a new Genetic Programming (GP) that
may be used for solving difficult real-world problems. The main difference
between TGP and other GP techniques is that TGP does not explicitly store the
evolved computer programs. In this paper, TGP is used for solving real-world
classification problems taken from PROBEN1. Numerical experiments show that TGP
performs similar and sometimes even better than other GP techniques for the
considered test problems.
</summary>
    <author>
      <name>Mihai Oltean</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2110.02014,
  arXiv:2110.13608, arXiv:2110.00468</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">World Computer Congress, The Symposium on Professional Practice in
  AI, 26-29 August, Toulouse, France, edited by E. Mercier-Laurent, J.
  Debenham, pp. 403-412, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2111.14790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.14790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2111.15250v1</id>
    <updated>2021-11-30T10:09:18Z</updated>
    <published>2021-11-30T10:09:18Z</published>
    <title>2D-Motion Detection using SNNs with Graphene-Insulator-Graphene
  Memristive Synapses</title>
    <summary>  The event-driven nature of spiking neural networks makes them biologically
plausible and more energy-efficient than artificial neural networks. In this
work, we demonstrate motion detection of an object in a two-dimensional visual
field. The network architecture presented here is biologically plausible and
uses CMOS analog leaky integrate-and-fire neurons and ultra-low power
multi-layer RRAM synapses. Detailed transistorlevel SPICE simulations show that
the proposed structure can accurately and reliably detect complex motions of an
object in a two-dimensional visual field.
</summary>
    <author>
      <name>Shubham Pande</name>
    </author>
    <author>
      <name>Karthi Srinivasan</name>
    </author>
    <author>
      <name>Suresh Balanethiram</name>
    </author>
    <author>
      <name>Bhaswar Chakrabarti</name>
    </author>
    <author>
      <name>Anjan Chakravorty</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ISCAS 2022, 5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2111.15250v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2111.15250v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.04749v1</id>
    <updated>2021-12-09T08:11:47Z</updated>
    <published>2021-12-09T08:11:47Z</published>
    <title>Experimental Demonstration of Neuromorphic Network with STT MTJ Synapses</title>
    <summary>  We present the first experimental demonstration of a neuromorphic network
with magnetic tunnel junction (MTJ) synapses, which performs image recognition
via vector-matrix multiplication. We also simulate a large MTJ network
performing MNIST handwritten digit recognition, demonstrating that MTJ
crossbars can match memristor accuracy while providing increased precision,
stability, and endurance.
</summary>
    <author>
      <name>Peng Zhou</name>
    </author>
    <author>
      <name>Alexander J. Edwards</name>
    </author>
    <author>
      <name>Fred B. Mancoff</name>
    </author>
    <author>
      <name>Dimitri Houssameddine</name>
    </author>
    <author>
      <name>Sanjeev Aggarwal</name>
    </author>
    <author>
      <name>Joseph S. Friedman</name>
    </author>
    <link href="http://arxiv.org/abs/2112.04749v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.04749v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.mes-hall" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.app-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.11157v1</id>
    <updated>2021-12-20T15:18:11Z</updated>
    <published>2021-12-20T15:18:11Z</published>
    <title>Integral representations of shallow neural network with Rectified Power
  Unit activation function</title>
    <summary>  In this effort, we derive a formula for the integral representation of a
shallow neural network with the Rectified Power Unit activation function.
Mainly, our first result deals with the univariate case of representation
capability of RePU shallow networks. The multidimensional result in this paper
characterizes the set of functions that can be represented with bounded norm
and possibly unbounded width.
</summary>
    <author>
      <name>Ahmed Abdeljawad</name>
    </author>
    <author>
      <name>Philipp Grohs</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, This is the first version. Some revisions in the near
  future is expected to be performed. arXiv admin note: text overlap with
  arXiv:1910.01635 by other authors</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.11157v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.11157v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.FA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T07, 26B40, 46E30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.14853v1</id>
    <updated>2021-12-29T22:30:25Z</updated>
    <published>2021-12-29T22:30:25Z</published>
    <title>Effects of Plasticity Functions on Neural Assemblies</title>
    <summary>  We explore the effects of various plasticity functions on assemblies of
neurons. To bridge the gap between experimental and computational theories we
make use of a conceptual framework, the Assembly Calculus, which is a formal
system for the description of brain function based on assemblies of neurons.
The Assembly Calculus includes operations for projecting, associating, and
merging assemblies of neurons. Our research is focused on simulating different
plasticity functions with Assembly Calculus. Our main contribution is the
modification and evaluation of the projection operation. We experiment with
Oja's and Spike Time-Dependent Plasticity (STDP) rules and test the effect of
various hyper-parameters.
</summary>
    <author>
      <name>Christodoulos Constantinides</name>
    </author>
    <author>
      <name>Kareem Nassar</name>
    </author>
    <link href="http://arxiv.org/abs/2112.14853v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.14853v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.07006v1</id>
    <updated>2022-03-14T11:20:07Z</updated>
    <published>2022-03-14T11:20:07Z</published>
    <title>Spiking Neural Network Integrated Circuits: A Review of Trends and
  Future Directions</title>
    <summary>  In this paper, we reviewed Spiking neural network (SNN) integrated circuit
designs and analyzed the trends among mixed-signal cores, fully digital cores
and large-scale, multi-core designs. Recently reported SNN integrated circuits
are compared under three broad categories: (a) Large-scale multi-core designs
that have dedicated NOC for spike routing, (b) digital single-core designs and
(c) mixed-signal single-core designs. Finally, we finish the paper with some
directions for future progress.
</summary>
    <author>
      <name>Arindam Basu</name>
    </author>
    <author>
      <name>Charlotte Frenkel</name>
    </author>
    <author>
      <name>Lei Deng</name>
    </author>
    <author>
      <name>Xueyong Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2203.07006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.07006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.13190v1</id>
    <updated>2022-02-17T10:49:01Z</updated>
    <published>2022-02-17T10:49:01Z</published>
    <title>GEMA: An open-source Python library for self-organizing-maps</title>
    <summary>  Organizations have realized the importance of data analysis and its benefits.
This in combination with Machine Learning algorithms has allowed to solve
problems more easily, making these processes less time-consuming. Neural
networks are the Machine Learning technique that is recently obtaining very
good best results. This paper describes an open-source Python library called
GEMA developed to work with a type of neural network model called
Self-Organizing-Maps. GEMA is freely available under GNU General Public License
at GitHub (https://github.com/ufvceiec/GEMA). The library has been evaluated in
different a particular use case obtaining accurate results.
</summary>
    <author>
      <name>Alvaro J. Garcia-Tejedor</name>
    </author>
    <author>
      <name>Alberto Nogales</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.simpa.2022.100280</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.simpa.2022.100280" rel="related"/>
    <link href="http://arxiv.org/abs/2203.13190v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.13190v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2203.13194v1</id>
    <updated>2022-02-02T22:48:56Z</updated>
    <published>2022-02-02T22:48:56Z</published>
    <title>Temporal Heterogeneity Improves Speed and Convergence in Genetic
  Algorithms</title>
    <summary>  Genetic algorithms have been used in recent decades to solve a broad variety
of search problems. These algorithms simulate natural selection to explore a
parameter space in search of solutions for a broad variety of problems. In this
paper, we explore the effects of introducing temporal heterogeneity in genetic
algorithms. In particular, we set the crossover probability to be inversely
proportional to the individual's fitness, i.e., better solutions change slower
than those with a lower fitness. As case studies, we apply heterogeneity to
solve the $N$-Queens and Traveling Salesperson problems. We find that temporal
heterogeneity consistently improves search without having prior knowledge of
the parameter space.
</summary>
    <author>
      <name>Yoshio Martinez</name>
    </author>
    <author>
      <name>Katya Rodriguez</name>
    </author>
    <author>
      <name>Carlos Gershenson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Draft from November 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2203.13194v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2203.13194v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.02522v1</id>
    <updated>2022-04-05T23:59:33Z</updated>
    <published>2022-04-05T23:59:33Z</published>
    <title>Solving integer multi-objective optimization problems using TOPSIS,
  Differential Evolution and Tabu Search</title>
    <summary>  This paper presents a method to solve non-linear integer multiobjective
optimization problems. First the problem is formulated using the Technique for
Order Preference by Similarity to Ideal Solution (TOPSIS). Next, the
Differential Evolution (DE) algorithm in its three versions (standard DE, DE
best and DEGL) are used as optimizer. Since the solutions found by the DE
algorithms are continuous, the Tabu Search (TS) algorithm is employed to find
integer solutions during the optimization process. Experimental results show
the effectiveness of the proposed method.
</summary>
    <author>
      <name>Renato A. Krohling</name>
    </author>
    <author>
      <name>Erick R. F. A. Schneider</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.02522v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.02522v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.05312v1</id>
    <updated>2022-04-11T15:30:52Z</updated>
    <published>2022-04-11T15:30:52Z</published>
    <title>Position-wise optimizer: A nature-inspired optimization algorithm</title>
    <summary>  The human nervous system utilizes synaptic plasticity to solve optimization
problems. Previous studies have tried to add the plasticity factor to the
training process of artificial neural networks, but most of those models
require complex external control over the network or complex novel rules. In
this manuscript, a novel nature-inspired optimization algorithm is introduced
that imitates biological neural plasticity. Furthermore, the model is tested on
three datasets and the results are compared with gradient descent optimization.
</summary>
    <author>
      <name>Amir Valizadeh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.05312v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.05312v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.06461v1</id>
    <updated>2022-04-13T15:33:51Z</updated>
    <published>2022-04-13T15:33:51Z</published>
    <title>Population Diversity Leads to Short Running Times of Lexicase Selection</title>
    <summary>  In this paper we investigate why the running time of lexicase parent
selection is empirically much lower than its worst-case bound of O(N*C). We
define a measure of population diversity and prove that high diversity leads to
low running times O(N + C) of lexicase selection. We then show empirically that
genetic programming populations evolved under lexicase selection are diverse
for several program synthesis problems, and explore the resulting differences
in running time bounds.
</summary>
    <author>
      <name>Thomas Helmuth</name>
    </author>
    <author>
      <name>Johannes Lengler</name>
    </author>
    <author>
      <name>William La Cava</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.06461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.06461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.11674v1</id>
    <updated>2022-04-25T14:08:50Z</updated>
    <published>2022-04-25T14:08:50Z</published>
    <title>HyperNCA: Growing Developmental Networks with Neural Cellular Automata</title>
    <summary>  In contrast to deep reinforcement learning agents, biological neural networks
are grown through a self-organized developmental process. Here we propose a new
hypernetwork approach to grow artificial neural networks based on neural
cellular automata (NCA). Inspired by self-organising systems and
information-theoretic approaches to developmental biology, we show that our
HyperNCA method can grow neural networks capable of solving common
reinforcement learning tasks. Finally, we explore how the same approach can be
used to build developmental metamorphosis networks capable of transforming
their weights to solve variations of the initial RL task.
</summary>
    <author>
      <name>Elias Najarro</name>
    </author>
    <author>
      <name>Shyam Sudhakaran</name>
    </author>
    <author>
      <name>Claire Glanois</name>
    </author>
    <author>
      <name>Sebastian Risi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper accepted as a conference paper at ICLR 'From Cells to
  Societies' workshop 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.11674v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.11674v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.13284v1</id>
    <updated>2022-04-28T04:42:11Z</updated>
    <published>2022-04-28T04:42:11Z</published>
    <title>Benchmarking the Hooke-Jeeves Method, MTS-LS1, and BSrr on the
  Large-scale BBOB Function Set</title>
    <summary>  This paper investigates the performance of three black-box optimizers
exploiting separability on the 24 large-scale BBOB functions, including the
Hooke-Jeeves method, MTS-LS1, and BSrr. Although BSrr was not specially
designed for large-scale optimization, the results show that BSrr has a
state-of-the-art performance on the five separable large-scale BBOB functions.
The results show that the asymmetry significantly influences the performance of
MTS-LS1. The results also show that the Hooke-Jeeves method performs better
than MTS-LS1 on unimodal separable BBOB functions.
</summary>
    <author>
      <name>Ryoji Tanabe</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3520304.3533951</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3520304.3533951" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an accepted version of a paper to the Workshop on Black-Box
  Optimization Benchmarking (BBOB 2022) published in the companion volume of
  GECCO2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2204.13284v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.13284v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2204.13990v2</id>
    <updated>2022-07-10T14:23:03Z</updated>
    <published>2022-04-02T00:26:23Z</published>
    <title>Particle Swarm Optimization Based Demand Response Using Artificial
  Neural Network Based Load Prediction</title>
    <summary>  In the present study, a Particle Swarm Optimization (PSO) based Demand
Response (DR) model, using Artificial Neural Network (ANN) to predict load is
proposed. The electrical load and climatological data of a residential area in
Austin city in Texas are used as the inputs of the ANN. Then, the outcomes with
the day-ahead prices data are used to solve the load shifting and cost
reduction problem. According to the results, the proposed model has the ability
to decrease payment costs and peak load.
</summary>
    <author>
      <name>Nasrin Bayat</name>
    </author>
    <link href="http://arxiv.org/abs/2204.13990v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2204.13990v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0705.0199v2</id>
    <updated>2007-05-08T01:06:10Z</updated>
    <published>2007-05-02T04:04:51Z</published>
    <title>The Parameter-Less Self-Organizing Map algorithm</title>
    <summary>  The Parameter-Less Self-Organizing Map (PLSOM) is a new neural network
algorithm based on the Self-Organizing Map (SOM). It eliminates the need for a
learning rate and annealing schemes for learning rate and neighbourhood size.
We discuss the relative performance of the PLSOM and the SOM and demonstrate
some tasks in which the SOM fails but the PLSOM performs satisfactory. Finally
we discuss some example applications of the PLSOM and present a proof of
ordering under certain limited conditions.
</summary>
    <author>
      <name>Erik Berglund</name>
    </author>
    <author>
      <name>Joaquin Sitte</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages, 27 figures. Based on publication in IEEE Trans. on Neural
  Networks</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Neural Networks, 2006 v.17, n.2, pp.305-316</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0705.0199v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0705.0199v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.1568v1</id>
    <updated>2008-03-11T12:39:01Z</updated>
    <published>2008-03-11T12:39:01Z</published>
    <title>Dempster-Shafer for Anomaly Detection</title>
    <summary>  In this paper, we implement an anomaly detection system using the
Dempster-Shafer method. Using two standard benchmark problems we show that by
combining multiple signals it is possible to achieve better results than by
using a single signal. We further show that by applying this approach to a
real-world email dataset the algorithm works for email worm detection.
Dempster-Shafer can be a promising method for anomaly detection problems with
multiple features (data sources), and two or more classes.
</summary>
    <author>
      <name>Qi Chen</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the International Conference on Data Mining (DMIN
  2006), pp 232-238, Las Vegas, USA 2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.1568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.1568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.1596v1</id>
    <updated>2008-03-11T14:19:33Z</updated>
    <published>2008-03-11T14:19:33Z</published>
    <title>Using Intelligent Agents to understand organisational behaviour</title>
    <summary>  This paper introduces two ongoing research projects which seek to apply
computer modelling techniques in order to simulate human behaviour within
organisations. Previous research in other disciplines has suggested that
complex social behaviours are governed by relatively simple rules which, when
identified, can be used to accurately model such processes using computer
technology. The broad objective of our research is to develop a similar
capability within organisational psychology.
</summary>
    <author>
      <name>Helen Celia</name>
    </author>
    <author>
      <name>Christopher Clegg</name>
    </author>
    <author>
      <name>Mark Robinson</name>
    </author>
    <author>
      <name>Peer-Olaf Siebers</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Christine Sprigg</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the British Psychology Society Annual Conference,
  Occupational Psychology Division (BPS 2007), Bristol, UK 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.1596v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.1596v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.1926v1</id>
    <updated>2008-03-13T11:43:25Z</updated>
    <published>2008-03-13T11:43:25Z</published>
    <title>Improved evolutionary generation of XSLT stylesheets</title>
    <summary>  This paper introduces a procedure based on genetic programming to evolve XSLT
programs (usually called stylesheets or logicsheets). XSLT is a general
purpose, document-oriented functional language, generally used to transform XML
documents (or, in general, solve any problem that can be coded as an XML
document). The proposed solution uses a tree representation for the stylesheets
as well as diverse specific operators in order to obtain, in the studied cases
and a reasonable time, a XSLT stylesheet that performs the transformation.
Several types of representation have been compared, resulting in different
performance and degree of success.
</summary>
    <author>
      <name>Pablo Garcia-Sanchez</name>
    </author>
    <author>
      <name>J. L. J. Laredo</name>
    </author>
    <author>
      <name>J. P. Sevilla</name>
    </author>
    <author>
      <name>Pedro Castillo</name>
    </author>
    <author>
      <name>J. J. Merelo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as poster at the GECCO-2008 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/0803.1926v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.1926v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.2695v1</id>
    <updated>2008-03-18T18:27:14Z</updated>
    <published>2008-03-18T18:27:14Z</published>
    <title>KohonAnts: A Self-Organizing Ant Algorithm for Clustering and Pattern
  Classification</title>
    <summary>  In this paper we introduce a new ant-based method that takes advantage of the
cooperative self-organization of Ant Colony Systems to create a naturally
inspired clustering and pattern recognition method. The approach considers each
data item as an ant, which moves inside a grid changing the cells it goes
through, in a fashion similar to Kohonen's Self-Organizing Maps. The resulting
algorithm is conceptually more simple, takes less free parameters than other
ant-based clustering algorithms, and, after some parameter tuning, yields very
good results on some benchmark problems.
</summary>
    <author>
      <name>C. Fernandes</name>
    </author>
    <author>
      <name>A. M. Mora</name>
    </author>
    <author>
      <name>J. J. Merelo</name>
    </author>
    <author>
      <name>V. Ramos</name>
    </author>
    <author>
      <name>J. L. J. Laredo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to ALIFE XI</arxiv:comment>
    <link href="http://arxiv.org/abs/0803.2695v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.2695v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.2925v1</id>
    <updated>2008-03-20T03:50:53Z</updated>
    <published>2008-03-20T03:50:53Z</published>
    <title>Equivalence of Probabilistic Tournament and Polynomial Ranking Selection</title>
    <summary>  Crucial to an Evolutionary Algorithm's performance is its selection scheme.
We mathematically investigate the relation between polynomial rank and
probabilistic tournament methods which are (respectively) generalisations of
the popular linear ranking and tournament selection schemes. We show that every
probabilistic tournament is equivalent to a unique polynomial rank scheme. In
fact, we derived explicit operators for translating between these two types of
selection. Of particular importance is that most linear and most practical
quadratic rank schemes are probabilistic tournaments.
</summary>
    <author>
      <name>Kassel Hingee</name>
    </author>
    <author>
      <name>Marcus Hutter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 double-column pages, 5 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. 2008 Congress on Evolutionary Computation (CEC 2008) pages
  564-571</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.2925v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.2925v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.2957v1</id>
    <updated>2008-03-20T10:19:01Z</updated>
    <published>2008-03-20T10:19:01Z</published>
    <title>Enhanced Direct and Indirect Genetic Algorithm Approaches for a Mall
  Layout and Tenant Selection Problem</title>
    <summary>  During our earlier research, it was recognised that in order to be successful
with an indirect genetic algorithm approach using a decoder, the decoder has to
strike a balance between being an optimiser in its own right and finding
feasible solutions. Previously this balance was achieved manually. Here we
extend this by presenting an automated approach where the genetic algorithm
itself, simultaneously to solving the problem, sets weights to balance the
components out. Subsequently we were able to solve a complex and non-linear
scheduling problem better than with a standard direct genetic algorithm
implementation.
</summary>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Kathryn Dowsland</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1023/A:1016536623961,</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1023/A:1016536623961," rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Heuristics, 8(5), pp 503-514, 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.2957v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.2957v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.3746v1</id>
    <updated>2008-03-26T15:14:33Z</updated>
    <published>2008-03-26T15:14:33Z</published>
    <title>Cluster Approach to the Domains Formation</title>
    <summary>  As a rule, a quadratic functional depending on a great number of binary
variables has a lot of local minima. One of approaches allowing one to find in
averaged deeper local minima is aggregation of binary variables into larger
blocks/domains. To minimize the functional one has to change the states of
aggregated variables (domains). In the present publication we discuss methods
of domains formation. It is shown that the best results are obtained when
domains are formed by variables that are strongly connected with each other.
</summary>
    <author>
      <name>Leonid B. Litinskii</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 5 figures, PDF-file</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Optical Memory &amp; Neural Networks (Information Optics), 2007,
  v.16(3) pp.144-153</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.3746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.3746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0803.4240v1</id>
    <updated>2008-03-29T07:50:24Z</updated>
    <published>2008-03-29T07:50:24Z</published>
    <title>Neutral Fitness Landscape in the Cellular Automata Majority Problem</title>
    <summary>  We study in detail the fitness landscape of a difficult cellular automata
computational task: the majority problem. Our results show why this problem
landscape is so hard to search, and we quantify the large degree of neutrality
found in various ways. We show that a particular subspace of the solution
space, called the "Olympus", is where good solutions concentrate, and give
measures to quantitatively characterize this subspace.
</summary>
    <author>
      <name>Sébastien Verel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Collard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Marco Tomassini</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISI</arxiv:affiliation>
    </author>
    <author>
      <name>Leonardo Vanneschi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISI</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans ACRI 2006 - 7th International Conference on Cellular Automata
  For Research and Industry - ACRI 2006, France (2006)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0803.4240v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0803.4240v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1105.3351v2</id>
    <updated>2017-02-23T10:03:27Z</updated>
    <published>2011-05-17T12:24:23Z</published>
    <title>Splitting method for spatio-temporal search efforts planning</title>
    <summary>  This article deals with the spatio-temporal sensors deployment in order to
maximize detection probability of an intelligent and randomly moving target in
an area under surveillance. Our work is based on the rare events simulation
framework. More precisely, we derive a novel stochastic optimization algorithm
based on the generalized splitting method. This new approach offers promising
results without any state-space discretization and can handle various types of
constraints.
</summary>
    <author>
      <name>Chouchane Mathieu</name>
    </author>
    <author>
      <name>Paris Sébastien</name>
    </author>
    <author>
      <name>Le Gland François</name>
    </author>
    <author>
      <name>Ouladsine Mustapha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Article rejected</arxiv:comment>
    <link href="http://arxiv.org/abs/1105.3351v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1105.3351v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.4502v1</id>
    <updated>2012-10-10T11:08:40Z</updated>
    <published>2012-10-10T11:08:40Z</published>
    <title>Comparing several heuristics for a packing problem</title>
    <summary>  Packing problems are in general NP-hard, even for simple cases. Since now
there are no highly efficient algorithms available for solving packing
problems. The two-dimensional bin packing problem is about packing all given
rectangular items, into a minimum size rectangular bin, without overlapping.
The restriction is that the items cannot be rotated. The current paper is
comparing a greedy algorithm with a hybrid genetic algorithm in order to see
which technique is better for the given problem. The algorithms are tested on
different sizes data.
</summary>
    <author>
      <name>Camelia-M. Pintea</name>
    </author>
    <author>
      <name>Cristian Pascan</name>
    </author>
    <author>
      <name>Mara Hajdu-Macelaru</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1504/IJAIP.2012.052071</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1504/IJAIP.2012.052071" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 figures, 2 tables; accepted: International Journal of Advanced
  Intelligence Paradigms</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Int J Advanced Intelligence Paradigms 4(3/4):268-277 (2012)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.4502v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.4502v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.5500v1</id>
    <updated>2012-10-19T19:03:11Z</updated>
    <published>2012-10-19T19:03:11Z</published>
    <title>Modeling with Copulas and Vines in Estimation of Distribution Algorithms</title>
    <summary>  The aim of this work is studying the use of copulas and vines in the
optimization with Estimation of Distribution Algorithms (EDAs). Two EDAs are
built around the multivariate product and normal copulas, and other two are
based on pair-copula decomposition of vine models. Empirically we study the
effect of both marginal distributions and dependence structure separately, and
show that both aspects play a crucial role in the success of the optimization.
The results show that the use of copulas and vines opens new opportunities to a
more appropriate modeling of search distributions in EDAs.
</summary>
    <author>
      <name>Marta Soto</name>
    </author>
    <author>
      <name>Yasser González-Fernández</name>
    </author>
    <author>
      <name>Alberto Ochoa</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Investigaci\'on Operacional, 36(1), 1-23</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.5500v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.5500v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ME" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.6511v1</id>
    <updated>2012-10-24T12:37:53Z</updated>
    <published>2012-10-24T12:37:53Z</published>
    <title>Neural Networks for Complex Data</title>
    <summary>  Artificial neural networks are simple and efficient machine learning tools.
Defined originally in the traditional setting of simple vector data, neural
network models have evolved to address more and more difficulties of complex
real world problems, ranging from time evolving data to sophisticated data
structures such as graphs and functions. This paper summarizes advances on
those themes from the last decade, with a focus on results obtained by members
of the SAMM team of Universit\'e Paris 1
</summary>
    <author>
      <name>Marie Cottrell</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAMM</arxiv:affiliation>
    </author>
    <author>
      <name>Madalina Olteanu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAMM</arxiv:affiliation>
    </author>
    <author>
      <name>Fabrice Rossi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAMM</arxiv:affiliation>
    </author>
    <author>
      <name>Joseph Rynkiewicz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAMM</arxiv:affiliation>
    </author>
    <author>
      <name>Nathalie Villa-Vialaneix</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">SAMM</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s13218-012-0207-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s13218-012-0207-2" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">K\"unstliche Intelligenz 26, 4 (2012) 373-380</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.6511v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.6511v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1210.8124v1</id>
    <updated>2012-10-30T19:11:06Z</updated>
    <published>2012-10-30T19:11:06Z</published>
    <title>Hierarchical Learning Algorithm for the Beta Basis Function Neural
  Network</title>
    <summary>  The paper presents a two-level learning method for the design of the Beta
Basis Function Neural Network BBFNN. A Genetic Algorithm is employed at the
upper level to construct BBFNN, while the key learning parameters :the width,
the centers and the Beta form are optimised using the gradient algorithm at the
lower level. In order to demonstrate the effectiveness of this hierarchical
learning algorithm HLABBFNN, we need to validate our algorithm for the
approximation of non-linear function.
</summary>
    <author>
      <name>Habib Dhahri</name>
    </author>
    <author>
      <name>Mohamed Adel Alimi</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Third International Conference on Systems, Signals &amp; Device, March
  21-24, 2005 , Sousse, Tunisia</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1210.8124v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1210.8124v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.0442v1</id>
    <updated>2013-06-03T14:47:11Z</updated>
    <published>2013-06-03T14:47:11Z</published>
    <title>Evolutionary Approach for the Containers Bin-Packing Problem</title>
    <summary>  This paper deals with the resolution of combinatorial optimization problems,
particularly those concerning the maritime transport scheduling. We are
interested in the management platforms in a river port and more specifically in
container organisation operations with a view to minimizing the number of
container rehandlings. Subsequently, we rmeet customers delivery deadlines and
we reduce ship stoppage time In this paper, we propose a genetic algorithm to
solve this problem and we present some experiments and results.
</summary>
    <author>
      <name>R. Kammarti</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">: LACS, Ecole Nationale des Ingenieurs de Tunis, Tunis - Belvedere. TUNISIE</arxiv:affiliation>
    </author>
    <author>
      <name>I. Ayachi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">: LAGIS, Ecole Centrale de Lille, Villeneuve dAscq, France</arxiv:affiliation>
    </author>
    <author>
      <name>M. Ksouri</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">: LACS, Ecole Nationale des Ingenieurs de Tunis, Tunis - Belvedere. TUNISIE</arxiv:affiliation>
    </author>
    <author>
      <name>P. Borne</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">: LAGIS, Ecole Centrale de Lille, Villeneuve dAscq, France</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Studies in Informatics and Control, Vol. 18, No. 4/2009, pages
  315-324</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1306.0442v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.0442v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.4622v1</id>
    <updated>2013-06-19T17:31:47Z</updated>
    <published>2013-06-19T17:31:47Z</published>
    <title>Solution to Quadratic Equation Using Genetic Algorithm</title>
    <summary>  Solving Quadratic equation is one of the intrinsic interests as it is the
simplest nonlinear equations. A novel approach for solving Quadratic Equation
based on Genetic Algorithms (GAs) is presented. Genetic Algorithms (GAs) are a
technique to solve problems which need optimization. Generation of trial
solutions have been formed by this method. Many examples have been worked out,
and in most cases we find out the exact solution. We have discussed the effect
of different parameters on the performance of the developed algorithm. The
results are concluded after rigorous testing on different equations.
</summary>
    <author>
      <name>Tanistha Nayak</name>
    </author>
    <author>
      <name>Tirtharaj Dash</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">appeared in: Conf. Proceedings of National Conference on Artificial
  Intelligence, Robotics and Embedded Systems (AIRES-2012), Andhra University,
  Vishakhapatnam, India (29-30 June, 2012), pp. 10-13</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.4622v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.4622v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1306.4793v1</id>
    <updated>2013-06-20T09:00:40Z</updated>
    <published>2013-06-20T09:00:40Z</published>
    <title>Evolving Boolean Regulatory Networks with Epigenetic Control</title>
    <summary>  The significant role of epigenetic mechanisms within natural systems has
become increasingly clear. This paper uses a recently presented abstract,
tunable Boolean genetic regulatory network model to explore aspects of
epigenetics. It is shown how dynamically controlling transcription via a DNA
methylation-inspired mechanism can be selected for by simulated evolution under
various single and multiple cell scenarios. Further, it is shown that the
effects of such control can be inherited without detriment to fitness.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 10 figures. arXiv admin note: substantial text overlap with
  arXiv:1303.7220</arxiv:comment>
    <link href="http://arxiv.org/abs/1306.4793v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1306.4793v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.MN" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06004v1</id>
    <updated>2015-03-20T06:48:14Z</updated>
    <published>2015-03-20T06:48:14Z</published>
    <title>Feeder Load Balancing using Neural Network</title>
    <summary>  The distribution system problems, such as planning, loss minimization, and
energy restoration, usually involve the phase balancing or network
reconfiguration procedures. The determination of an optimal phase balance is,
in general, a combinatorial optimization problem. This paper proposes optimal
reconfiguration of the phase balancing using the neural network, to switch on
and off the different switches, allowing the three phases supply by the
transformer to the end-users to be balanced. This paper presents the
application examples of the proposed method using the real and simulated test
data.
</summary>
    <author>
      <name>A. Ukil</name>
    </author>
    <author>
      <name>W. Siti</name>
    </author>
    <author>
      <name>J. Jordaan</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/11760023_190</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/11760023_190" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages in final published version</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture Notes in Computer Science, Springer, vol. 3972, pp.
  1311-1316, 2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.06004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.06866v4</id>
    <updated>2016-02-20T09:57:40Z</updated>
    <published>2015-03-23T22:30:39Z</published>
    <title>Study of all the periods of a Neuronal Recurrence Equation</title>
    <summary>  We characterize the structure of the periods of a neuronal recurrence
equation. Firstly, we give a characterization of k-chains in 0-1 periodic
sequences. Secondly, we characterize the periods of all cycles of some neuronal
recurrence equation. Thirdly, we explain how these results can be used to
deduce the existence of the generalized period-halving bifurcation.
</summary>
    <author>
      <name>Serge Alain Ebélé</name>
    </author>
    <author>
      <name>Renè Ndoundam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Complex Systems, 24, 2015</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1503.06866v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.06866v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="92B20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.02395v1</id>
    <updated>2017-11-07T11:07:05Z</updated>
    <published>2017-11-07T11:07:05Z</published>
    <title>Beetle Antennae Search without Parameter Tuning (BAS-WPT) for
  Multi-objective Optimization</title>
    <summary>  Beetle antennae search (BAS) is an efficient meta-heuristic algorithm
inspired by foraging behaviors of beetles. This algorithm includes several
parameters for tuning and the existing results are limited to solve single
objective optimization. This work pushes forward the research on BAS by
providing one variant that releases the tuning parameters and is able to handle
multi-objective optimization. This new approach applies normalization to
simplify the original algorithm and uses a penalty function to exploit
infeasible solutions with low constraint violation to solve the constraint
optimization problem. Extensive experimental studies are carried out and the
results reveal efficacy of the proposed approach to constraint handling.
</summary>
    <author>
      <name>Xiangyuan Jiang</name>
    </author>
    <author>
      <name>Shuai Li</name>
    </author>
    <link href="http://arxiv.org/abs/1711.02395v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.02395v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.04759v1</id>
    <updated>2017-11-13T18:50:04Z</updated>
    <published>2017-11-13T18:50:04Z</published>
    <title>Neural Networks Architecture Evaluation in a Quantum Computer</title>
    <summary>  In this work, we propose a quantum algorithm to evaluate neural networks
architectures named Quantum Neural Network Architecture Evaluation (QNNAE). The
proposed algorithm is based on a quantum associative memory and the learning
algorithm for artificial neural networks. Unlike conventional algorithms for
evaluating neural network architectures, QNNAE does not depend on
initialization of weights. The proposed algorithm has a binary output and
results in 0 with probability proportional to the performance of the network.
And its computational cost is equal to the computational cost to train a neural
network.
</summary>
    <author>
      <name>Adenilton José da Silva</name>
    </author>
    <author>
      <name>Rodolfo Luan F. de Oliveira</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/BRACIS.2017.33</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/BRACIS.2017.33" rel="related"/>
    <link href="http://arxiv.org/abs/1711.04759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.04759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.05993v1</id>
    <updated>2017-11-16T09:13:16Z</updated>
    <published>2017-11-16T09:13:16Z</published>
    <title>On evolutionary selection of blackjack strategies</title>
    <summary>  We apply the approach of evolutionary programming to the problem of
optimization of the blackjack basic strategy. We demonstrate that the
population of initially random blackjack strategies evolves and saturates to a
profitable performance in about one hundred generations. The resulting strategy
resembles the known blackjack basic strategies in the specifics of its
prescriptions, and has a similar performance. We also study evolution of the
population of strategies initialized to the Thorp's basic strategy.
</summary>
    <author>
      <name>Mikhail Goykhman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Code is available here:
  https://github.com/Goykhman/Blackjack_evolution</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.05993v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.05993v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.07655v1</id>
    <updated>2017-11-21T07:23:32Z</updated>
    <published>2017-11-21T07:23:32Z</published>
    <title>Genetic Algorithms for Evolving Deep Neural Networks</title>
    <summary>  In recent years, deep learning methods applying unsupervised learning to
train deep layers of neural networks have achieved remarkable results in
numerous fields. In the past, many genetic algorithms based methods have been
successfully applied to training neural networks. In this paper, we extend
previous work and propose a GA-assisted method for deep learning. Our
experimental results indicate that this GA-assisted approach improves the
performance of a deep autoencoder, producing a sparser neural network.
</summary>
    <author>
      <name>Eli David</name>
    </author>
    <author>
      <name>Iddo Greental</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/2598394.2602287</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/2598394.2602287" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ACM Genetic and Evolutionary Computation Conference (GECCO), pages
  1451-1452, Vancouver, Canada, July 2014</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1711.07655v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.07655v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.10204v1</id>
    <updated>2017-11-28T09:47:51Z</updated>
    <published>2017-11-28T09:47:51Z</published>
    <title>Block Neural Network Avoids Catastrophic Forgetting When Learning
  Multiple Task</title>
    <summary>  In the present work we propose a Deep Feed Forward network architecture which
can be trained according to a sequential learning paradigm, where tasks of
increasing difficulty are learned sequentially, yet avoiding catastrophic
forgetting. The proposed architecture can re-use the features learned on
previous tasks in a new task when the old tasks and the new one are related.
The architecture needs fewer computational resources (neurons and connections)
and less data for learning the new task than a network trained from scratch
</summary>
    <author>
      <name>Guglielmo Montone</name>
    </author>
    <author>
      <name>J. Kevin O'Regan</name>
    </author>
    <author>
      <name>Alexander V. Terekhov</name>
    </author>
    <link href="http://arxiv.org/abs/1711.10204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.10204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00092v1</id>
    <updated>2018-04-30T20:51:20Z</updated>
    <published>2018-04-30T20:51:20Z</published>
    <title>New Methods of Studying Valley Fitness Landscapes</title>
    <summary>  The word "valley" is a popular term used in intuitively describing fitness
landscapes. What is a valley on a fitness landscape? How to identify the
direction and location of a valley if it exists? However, such questions are
seldom rigorously studied in evolutionary optimization especially when the
search space is a high dimensional continuous space. This paper presents two
methods of studying valleys on a fitness landscape. The first method is based
on the topological homeomorphism. It establishes a rigorous definition of a
valley. A valley is regarded as a one-dimensional manifold. The second method
takes a different viewpoint from statistics. It provides an algorithm of
identifying the valley direction and location using principle component
analysis.
</summary>
    <author>
      <name>Jun He</name>
    </author>
    <author>
      <name>Tao Xu</name>
    </author>
    <link href="http://arxiv.org/abs/1805.00092v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00092v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.00509v1</id>
    <updated>2018-05-01T18:31:40Z</updated>
    <published>2018-05-01T18:31:40Z</published>
    <title>Spiking Neural Algorithms for Markov Process Random Walk</title>
    <summary>  The random walk is a fundamental stochastic process that underlies many
numerical tasks in scientific computing applications. We consider here two
neural algorithms that can be used to efficiently implement random walks on
spiking neuromorphic hardware. The first method tracks the positions of
individual walkers independently by using a modular code inspired by the grid
cell spatial representation in the brain. The second method tracks the
densities of random walkers at each spatial location directly. We analyze the
scaling complexity of each of these methods and illustrate their ability to
model random walkers under different probabilistic conditions.
</summary>
    <author>
      <name>William Severa</name>
    </author>
    <author>
      <name>Rich Lehoucq</name>
    </author>
    <author>
      <name>Ojas Parekh</name>
    </author>
    <author>
      <name>James B. Aimone</name>
    </author>
    <link href="http://arxiv.org/abs/1805.00509v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.00509v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01385v2</id>
    <updated>2018-05-31T13:01:02Z</updated>
    <published>2018-05-03T15:51:51Z</published>
    <title>Research on the Brain-inspired Cross-modal Neural Cognitive Computing
  Framework</title>
    <summary>  To address modeling problems of brain-inspired intelligence, this thesis is
focused on researching in the semantic-oriented framework design for multimedia
and multimodal information. The Multimedia Neural Cognitive Computing (MNCC)
model was designed based on the nervous mechanism and cognitive architecture.
Furthermore, the semantic-oriented hierarchical Cross-modal Neural Cognitive
Computing (CNCC) framework was proposed based on MNCC model, and formal
description and analysis for CNCC framework was given. It would effectively
improve the performance of semantic processing for multimedia and cross-modal
information, and has far-reaching significance for exploration and realization
brain-inspired computing.
</summary>
    <author>
      <name>Yang Liu</name>
    </author>
    <link href="http://arxiv.org/abs/1805.01385v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01385v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.01623v1</id>
    <updated>2018-05-04T06:37:43Z</updated>
    <published>2018-05-04T06:37:43Z</published>
    <title>Recent Progress on Graph Partitioning Problems Using Evolutionary
  Computation</title>
    <summary>  The graph partitioning problem (GPP) is a representative combinatorial
optimization problem which is NP-hard. Currently, various approaches to solve
GPP have been introduced. Among these, the GPP solution using evolutionary
computation (EC) is an effective approach. There has not been any survey on the
research applying EC to GPP since 2011. In this survey, we introduce various
attempts to apply EC to GPP made in the recent seven years.
</summary>
    <author>
      <name>Hye-Jin Kim</name>
    </author>
    <author>
      <name>Yong-Hyuk Kim</name>
    </author>
    <link href="http://arxiv.org/abs/1805.01623v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.01623v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.05855v1</id>
    <updated>2018-04-22T13:44:18Z</updated>
    <published>2018-04-22T13:44:18Z</published>
    <title>Social Algorithms</title>
    <summary>  This article concerns the review of a special class of swarm intelligence
based algorithms for solving optimization problems and these algorithms can be
referred to as social algorithms. Social algorithms use multiple agents and the
social interactions to design rules for algorithms so as to mimic certain
successful characteristics of the social/biological systems such as ants, bees,
bats, birds and animals.
</summary>
    <author>
      <name>Xin-She Yang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-642-27737-5_678-1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-642-27737-5_678-1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Encyclopedia of Complexity and Systems Science, 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.05855v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.05855v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T99, 68W20, 90C30" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.06366v1</id>
    <updated>2018-05-16T15:20:30Z</updated>
    <published>2018-05-16T15:20:30Z</published>
    <title>Towards Complex Artificial Life</title>
    <summary>  An object-oriented combinator chemistry was used to construct an artificial
organism with a system architecture possessing characteristics necessary for
organisms to evolve into more complex forms. This architecture supports
modularity by providing a mechanism for the construction of executable modules
called $methods$ that can be duplicated and specialized to increase complexity.
At the same time, its support for concurrency provides the flexibility in
execution order necessary for redundancy, degeneracy and parallelism to
mitigate increased replication costs. The organism is a moving,
self-replicating, spatially distributed assembly of elemental combinators
called a $roving \: pile.$ The pile hosts an asynchronous message passing
computation implemented by parallel subprocesses encoded by genes distributed
through out the pile like the plasmids of a bacterial cell.
</summary>
    <author>
      <name>Lance R. Williams</name>
    </author>
    <link href="http://arxiv.org/abs/1805.06366v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.06366v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09244v1</id>
    <updated>2018-05-23T15:58:57Z</updated>
    <published>2018-05-23T15:58:57Z</published>
    <title>Concentric ESN: Assessing the Effect of Modularity in Cycle Reservoirs</title>
    <summary>  The paper introduces concentric Echo State Network, an approach to design
reservoir topologies that tries to bridge the gap between deterministically
constructed simple cycle models and deep reservoir computing approaches. We
show how to modularize the reservoir into simple unidirectional and concentric
cycles with pairwise bidirectional jump connections between adjacent loops. We
provide a preliminary experimental assessment showing how concentric reservoirs
yield to superior predictive accuracy and memory capacity with respect to
single cycle reservoirs and deep reservoir models.
</summary>
    <author>
      <name>Davide Bacciu</name>
    </author>
    <author>
      <name>Andrea Bongiorno</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09244v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09244v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11201v1</id>
    <updated>2018-05-28T23:44:44Z</updated>
    <published>2018-05-28T23:44:44Z</published>
    <title>A parallel implementation of the covariance matrix adaptation evolution
  strategy</title>
    <summary>  In many practical optimization problems, the derivatives of the functions to
be optimized are unavailable or unreliable. Such optimization problems are
solved using derivative-free optimization techniques. One of the
state-of-the-art techniques for derivative-free optimization is the covariance
matrix adaptation evolution strategy (CMA-ES) algorithm. However, the
complexity of CMA-ES algorithm makes it undesirable for tasks where fast
optimization is needed. To reduce the execution time of CMA-ES, a parallel
implementation is proposed, and its performance is analyzed using the benchmark
problems in PythOPT optimization environment.
</summary>
    <author>
      <name>Najeeb Khan</name>
    </author>
    <link href="http://arxiv.org/abs/1805.11201v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11201v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.11236v1</id>
    <updated>2018-05-29T03:54:05Z</updated>
    <published>2018-05-29T03:54:05Z</published>
    <title>Review of Applications of Generalized Regression Neural Networks in
  Identification and Control of Dynamic Systems</title>
    <summary>  This paper depicts a brief revision of Generalized Regression Neural Networks
(GRNN) applications in system identification and control of dynamic systems. In
addition, a comparison study between the performance of back-propagation neural
networks and GRNN is presented for system identification problems. The results
of the comparison confirm that GRNN has shorter training time and higher
accuracy than the counterpart back-propagation neural networks.
</summary>
    <author>
      <name>Ahmad Jobran Al-Mahasneh</name>
    </author>
    <author>
      <name>Sreenatha G. Anavatti</name>
    </author>
    <author>
      <name>Matthew A. Garratt</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1805.11236v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.11236v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.01942v1</id>
    <updated>2018-09-06T12:24:44Z</updated>
    <published>2018-09-06T12:24:44Z</published>
    <title>A tutorial on Particle Swarm Optimization Clustering</title>
    <summary>  This paper proposes a tutorial on the Data Clustering technique using the
Particle Swarm Optimization approach. Following the work proposed by Merwe et
al. here we present an in-deep analysis of the algorithm together with a Matlab
implementation and a short tutorial that explains how to modify the proposed
implementation and the effect of the parameters of the original algorithm.
Moreover, we provide a comparison against the results obtained using the well
known K-Means approach. All the source code presented in this paper is publicly
available under the GPL-v2 license.
</summary>
    <author>
      <name>Augusto Luis Ballardini</name>
    </author>
    <link href="http://arxiv.org/abs/1809.01942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.01942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.02440v1</id>
    <updated>2018-09-07T12:37:50Z</updated>
    <published>2018-09-07T12:37:50Z</published>
    <title>Optimizing deep video representation to match brain activity</title>
    <summary>  The comparison of observed brain activity with the statistics generated by
artificial intelligence systems is useful to probe brain functional
organization under ecological conditions. Here we study fMRI activity in ten
subjects watching color natural movies and compute deep representations of
these movies with an architecture that relies on optical flow and image
content. The association of activity in visual areas with the different layers
of the deep architecture displays complexity-related contrasts across visual
areas and reveals a striking foveal/peripheral dichotomy.
</summary>
    <author>
      <name>Hugo Richard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PARIETAL</arxiv:affiliation>
    </author>
    <author>
      <name>Ana Pinho</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">NEUROSPIN</arxiv:affiliation>
    </author>
    <author>
      <name>Bertrand Thirion</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">PARIETAL</arxiv:affiliation>
    </author>
    <author>
      <name>Guillaume Charpiat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">TAU</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2018 Conference on Cognitive Computational Neuroscience, Sep 2018,
  Philadelphia, United States</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1809.02440v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.02440v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1809.04166v2</id>
    <updated>2018-09-19T23:17:17Z</updated>
    <published>2018-09-11T21:09:25Z</published>
    <title>Leabra7: a Python package for modeling recurrent, biologically-realistic
  neural networks</title>
    <summary>  Emergent is a software package that uses the AdEx neural dynamics model and
LEABRA learning algorithm to simulate and train arbitrary recurrent neural
network architectures in a biologically-realistic manner. We present Leabra7, a
complementary Python library that implements these same algorithms. Leabra7 is
developed and distributed using modern software development principles, and
integrates tightly with Python's scientific stack. We demonstrate recurrent
Leabra7 networks using traditional pattern-association tasks and a standard
machine learning task, classifying the IRIS dataset.
</summary>
    <author>
      <name>C. Daniel Greenidge</name>
    </author>
    <author>
      <name>Noam Miller</name>
    </author>
    <author>
      <name>Kenneth A. Norman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Fix minor typos</arxiv:comment>
    <link href="http://arxiv.org/abs/1809.04166v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1809.04166v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.05687v2</id>
    <updated>2019-02-01T07:57:22Z</updated>
    <published>2018-12-13T20:58:37Z</published>
    <title>Ablation of a Robot's Brain: Neural Networks Under a Knife</title>
    <summary>  It is still not fully understood exactly how neural networks are able to
solve the complex tasks that have recently pushed AI research forward. We
present a novel method for determining how information is structured inside a
neural network. Using ablation (a neuroscience technique for cutting away parts
of a brain to determine their function), we approach several neural network
architectures from a biological perspective. Through an analysis of this
method's results, we examine important similarities between biological and
artificial neural networks to search for the implicit knowledge locked away in
the network's weights.
</summary>
    <author>
      <name>Peter E. Lillian</name>
    </author>
    <author>
      <name>Richard Meyes</name>
    </author>
    <author>
      <name>Tobias Meisen</name>
    </author>
    <link href="http://arxiv.org/abs/1812.05687v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.05687v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1812.05815v2</id>
    <updated>2019-03-21T10:37:16Z</updated>
    <published>2018-12-14T08:15:15Z</published>
    <title>Unsupervised Change Detection in Satellite Images Using Convolutional
  Neural Networks</title>
    <summary>  This paper proposes an efficient unsupervised method for detecting relevant
changes between two temporally different images of the same scene. A
convolutional neural network (CNN) for semantic segmentation is implemented to
extract compressed image features, as well as to classify the detected changes
into the correct semantic classes. A difference image is created using the
feature map information generated by the CNN, without explicitly training on
target difference images. Thus, the proposed change detection method is
unsupervised, and can be performed using any CNN model pre-trained for semantic
segmentation.
</summary>
    <author>
      <name>Kevin Louis de Jong</name>
    </author>
    <author>
      <name>Anna Sergeevna Bosman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Paper accepted to IJCNN 2019</arxiv:comment>
    <link href="http://arxiv.org/abs/1812.05815v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1812.05815v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.01563v3</id>
    <updated>2019-09-05T04:20:28Z</updated>
    <published>2019-06-04T16:27:55Z</published>
    <title>Hamiltonian Neural Networks</title>
    <summary>  Even though neural networks enjoy widespread use, they still struggle to
learn the basic laws of physics. How might we endow them with better inductive
biases? In this paper, we draw inspiration from Hamiltonian mechanics to train
models that learn and respect exact conservation laws in an unsupervised
manner. We evaluate our models on problems where conservation of energy is
important, including the two-body problem and pixel observations of a pendulum.
Our model trains faster and generalizes better than a regular neural network.
An interesting side effect is that our model is perfectly reversible in time.
</summary>
    <author>
      <name>Sam Greydanus</name>
    </author>
    <author>
      <name>Misko Dzamba</name>
    </author>
    <author>
      <name>Jason Yosinski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conference paper at NeurIPS 2019. Main paper has 8 pages and 5
  figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.01563v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.01563v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.01892v1</id>
    <updated>2019-06-05T09:08:58Z</updated>
    <published>2019-06-05T09:08:58Z</published>
    <title>Genetic Random Weight Change Algorithm for the Learning of Multilayer
  Neural Networks</title>
    <summary>  A new method to improve the performance of Random weight change (RWC)
algorithm based on a simple genetic algorithm, namely, Genetic random weight
change (GRWC) is proposed. It is to find the optimal values of global minima
via learning. In contrast to Random Weight Change (RWC), GRWC contains an
effective optimization procedure which are good at exploring a large and
complex space in an intellectual strategies influenced by the GA/RWC synergy.
By implementing our simple GA in RWC we achieve an astounding accuracy of
finding global minima.
</summary>
    <author>
      <name>Mohammad Ibraim Sarker</name>
    </author>
    <author>
      <name>Yali Nie</name>
    </author>
    <author>
      <name>Hong Yongki</name>
    </author>
    <author>
      <name>Hyongsuk Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, Published in ISITC 2016 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.01892v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.01892v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.02010v1</id>
    <updated>2019-05-26T10:45:58Z</updated>
    <published>2019-05-26T10:45:58Z</published>
    <title>A Hybrid Algorithm for Metaheuristic Optimization</title>
    <summary>  We propose a novel, flexible algorithm for combining together
metaheuristicoptimizers for non-convex optimization problems. Our approach
treatsthe constituent optimizers as a team of complex agents that
communicateinformation amongst each other at various intervals during the
simulationprocess. The information produced by each individual agent can be
combinedin various ways via higher-level operators. In our experiments on
keybenchmark functions, we investigate how the performance of our
algorithmvaries with respect to several of its key modifiable properties.
Finally,we apply our proposed algorithm to classification problems involving
theoptimization of support-vector machine classifiers.
</summary>
    <author>
      <name>Sujit Pramod Khanna</name>
    </author>
    <author>
      <name>Alexander Ororbia II</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.02010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.02010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.03959v1</id>
    <updated>2019-06-10T13:03:16Z</updated>
    <published>2019-06-10T13:03:16Z</published>
    <title>Exploration and Exploitation in Symbolic Regression using
  Quality-Diversity and Evolutionary Strategies Algorithms</title>
    <summary>  By combining Genetic Programming, MAP-Elites and Covariance Matrix Adaptation
Evolution Strategy, we demonstrate very high success rates in Symbolic
Regression problems. MAP-Elites is used to improve exploration while preserving
diversity and avoiding premature convergence and bloat. Then, a Covariance
Matrix Adaptation-Evolution Strategy is used to evaluate free scalars through a
non-gradient-based black-box optimizer. Although this evaluation approach is
not computationally scalable to high dimensional problems, our algorithm is
able to find exactly most of the $31$ targets extracted from the literature on
which we evaluate it.
</summary>
    <author>
      <name>J. -P. Bruneton</name>
    </author>
    <author>
      <name>L. Cazenille</name>
    </author>
    <author>
      <name>A. Douin</name>
    </author>
    <author>
      <name>V. Reverdy</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 7 figures, 5 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.03959v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.03959v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68W30, 68T20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.10189v2</id>
    <updated>2019-10-04T17:49:07Z</updated>
    <published>2019-06-24T19:26:10Z</published>
    <title>Evolutionary Computation and AI Safety: Research Problems Impeding
  Routine and Safe Real-world Application of Evolution</title>
    <summary>  Recent developments in artificial intelligence and machine learning have
spurred interest in the growing field of AI safety, which studies how to
prevent human-harming accidents when deploying AI systems. This paper thus
explores the intersection of AI safety with evolutionary computation, to show
how safety issues arise in evolutionary computation and how understanding from
evolutionary computational and biological evolution can inform the broader
study of AI safety.
</summary>
    <author>
      <name>Joel Lehman</name>
    </author>
    <link href="http://arxiv.org/abs/1906.10189v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.10189v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1909.09586v1</id>
    <updated>2019-09-12T15:44:51Z</updated>
    <published>2019-09-12T15:44:51Z</published>
    <title>Understanding LSTM -- a tutorial into Long Short-Term Memory Recurrent
  Neural Networks</title>
    <summary>  Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN) are one of the
most powerful dynamic classifiers publicly known. The network itself and the
related learning algorithms are reasonably well documented to get an idea how
it works. This paper will shed more light into understanding how LSTM-RNNs
evolved and why they work impressively well, focusing on the early,
ground-breaking publications. We significantly improved documentation and fixed
a number of errors and inconsistencies that accumulated in previous
publications. To support understanding we as well revised and unified the
notation used.
</summary>
    <author>
      <name>Ralf C. Staudemeyer</name>
    </author>
    <author>
      <name>Eric Rothstein Morris</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">42 pages, 11 figures, tutorial</arxiv:comment>
    <link href="http://arxiv.org/abs/1909.09586v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1909.09586v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.01647v1</id>
    <updated>2019-12-04T22:21:16Z</updated>
    <published>2019-12-04T22:21:16Z</published>
    <title>Are skip connections necessary for biologically plausible learning
  rules?</title>
    <summary>  Backpropagation is the workhorse of deep learning, however, several other
biologically-motivated learning rules have been introduced, such as random
feedback alignment and difference target propagation. None of these methods
have produced a competitive performance against backpropagation. In this paper,
we show that biologically-motivated learning rules with skip connections
between intermediate layers can perform as well as backpropagation on the MNIST
dataset and are robust to various sets of hyper-parameters.
</summary>
    <author>
      <name>Daniel Jiwoong Im</name>
    </author>
    <author>
      <name>Rutuja Patil</name>
    </author>
    <author>
      <name>Kristin Branson</name>
    </author>
    <link href="http://arxiv.org/abs/2001.01647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.01686v1</id>
    <updated>2019-12-22T03:28:05Z</updated>
    <published>2019-12-22T03:28:05Z</published>
    <title>A Deep Neuro-Fuzzy Network for Image Classification</title>
    <summary>  The combination of neural network and fuzzy systems into neuro-fuzzy systems
integrates fuzzy reasoning rules into the connectionist networks. However, the
existing neuro-fuzzy systems are developed under shallow structures having
lower generalization capacity. We propose the first end-to-end deep neuro-fuzzy
network and investigate its application for image classification. Two new
operations are developed based on definitions of Takagi-Sugeno-Kang (TSK) fuzzy
model namely fuzzy inference operation and fuzzy pooling operations; stacks of
these operations comprise the layers in this network. We evaluate the network
on MNIST, CIFAR-10 and CIFAR-100 datasets, finding that the network has a
reasonable accuracy in these benchmarks.
</summary>
    <author>
      <name>Omolbanin Yazdanbakhsh</name>
    </author>
    <author>
      <name>Scott Dick</name>
    </author>
    <link href="http://arxiv.org/abs/2001.01686v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01686v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.01687v1</id>
    <updated>2019-12-11T03:12:50Z</updated>
    <published>2019-12-11T03:12:50Z</published>
    <title>A Supervised Modified Hebbian Learning Method On Feed-forward Neural
  Networks</title>
    <summary>  In this paper, we present a new supervised learning algorithm that is based
on the Hebbian learning algorithm in an attempt to offer a substitute for back
propagation along with the gradient descent for a more biologically plausible
method. The best performance for the algorithm was achieved when it was run on
a feed-forward neural network with the MNIST handwritten digits data set
reaching an accuracy of 70.4% on the test data set and 71.48% on the validation
data set.
</summary>
    <author>
      <name>Rafi Qumsieh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.01687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.01687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.05381v1</id>
    <updated>2020-01-15T15:40:01Z</updated>
    <published>2020-01-15T15:40:01Z</published>
    <title>Analysis of Genetic Algorithm on Bearings-Only Target Motion Analysis</title>
    <summary>  Target motion analysis using only bearing angles is an important study for
tracking targets in water. Several methods including Kalman-like filters and
evolutionary strategies are used to get a good predictor. Kalman-like filters
couldn't get the expected results thus evolutionary strategies have been using
in this area for a long time. Target Motion Analysis with Genetic Algorithm is
the most successful method for Bearings-Only Target Motion Analysis and we
investigated it. We found that Covariance Matrix Adaptation Evolutionary
Strategies does the similar work with Target Motion Analysis with Genetic
Algorithm and tried it; but it has statistical feedback mechanism and converges
faster than other methods. In this study, we compared and criticize the
methods.
</summary>
    <author>
      <name>Erdem Kose</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.05381v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.05381v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.09725v1</id>
    <updated>2020-01-07T06:55:57Z</updated>
    <published>2020-01-07T06:55:57Z</published>
    <title>Probabilistic spike propagation for FPGA implementation of spiking
  neural networks</title>
    <summary>  Evaluation of spiking neural networks requires fetching a large number of
synaptic weights to update postsynaptic neurons. This limits parallelism and
becomes a bottleneck for hardware.
  We present an approach for spike propagation based on a probabilistic
interpretation of weights, thus reducing memory accesses and updates. We study
the effects of introducing randomness into the spike processing, and show on
benchmark networks that this can be done with minimal impact on the recognition
accuracy.
  We present an architecture and the trade-offs in accuracy on fully connected
and convolutional networks for the MNIST and CIFAR10 datasets on the Xilinx
Zynq platform.
</summary>
    <author>
      <name>Abinand Nallathambi</name>
    </author>
    <author>
      <name>Nitin Chandrachoodan</name>
    </author>
    <link href="http://arxiv.org/abs/2001.09725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.09725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.04212v3</id>
    <updated>2021-02-15T02:10:27Z</updated>
    <published>2020-08-04T19:53:39Z</published>
    <title>Creative AI Through Evolutionary Computation: Principles and Examples</title>
    <summary>  The main power of artificial intelligence is not in modeling what we already
know, but in creating solutions that are new. Such solutions exist in extremely
large, high-dimensional, and complex search spaces. Population-based search
techniques, i.e. variants of evolutionary computation, are well suited to
finding them. These techniques make it possible to find creative solutions to
practical problems in the real world, making creative AI through evolutionary
computation the likely "next deep learning."
</summary>
    <author>
      <name>Risto Miikkulainen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an extended version of arXiv:1901.03775</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.04212v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.04212v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.04214v1</id>
    <updated>2020-07-28T21:14:42Z</updated>
    <published>2020-07-28T21:14:42Z</published>
    <title>Mastering high-dimensional dynamics with Hamiltonian neural networks</title>
    <summary>  We detail how incorporating physics into neural network design can
significantly improve the learning and forecasting of dynamical systems, even
nonlinear systems of many dimensions. A map building perspective elucidates the
superiority of Hamiltonian neural networks over conventional neural networks.
The results clarify the critical relation between data, dimension, and neural
network learning performance.
</summary>
    <author>
      <name>Scott T. Miller</name>
    </author>
    <author>
      <name>John F. Lindner</name>
    </author>
    <author>
      <name>Anshul Choudhary</name>
    </author>
    <author>
      <name>Sudeshna Sinha</name>
    </author>
    <author>
      <name>William L. Ditto</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 9 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.04214v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.04214v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2008.12082v2</id>
    <updated>2020-11-05T10:56:59Z</updated>
    <published>2020-08-27T12:28:07Z</published>
    <title>Training robust anomaly detection using ML-Enhanced simulations</title>
    <summary>  This paper describes the use of neural networks to enhance simulations for
subsequent training of anomaly-detection systems. Simulations can provide edge
conditions for anomaly detection which may be sparse or non-existent in
real-world data. Simulations suffer, however, by producing data that is "too
clean" resulting in anomaly detection systems that cannot transition from
simulated data to actual conditions. Our approach enhances simulations using
neural networks trained on real-world data to create outputs that are more
realistic and variable than traditional simulations.
</summary>
    <author>
      <name>Philip Feldman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 13 figures. Presented at GVSETS 2020</arxiv:comment>
    <link href="http://arxiv.org/abs/2008.12082v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2008.12082v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.3; I.6.5; I.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.08928v1</id>
    <updated>2020-09-15T02:24:16Z</updated>
    <published>2020-09-15T02:24:16Z</published>
    <title>A Study of Genetic Algorithms for Hyperparameter Optimization of Neural
  Networks in Machine Translation</title>
    <summary>  With neural networks having demonstrated their versatility and benefits, the
need for their optimal performance is as prevalent as ever. A defining
characteristic, hyperparameters, can greatly affect its performance. Thus
engineers go through a process, tuning, to identify and implement optimal
hyperparameters. That being said, excess amounts of manual effort are required
for tuning network architectures, training configurations, and preprocessing
settings such as Byte Pair Encoding (BPE). In this study, we propose an
automatic tuning method modeled after Darwin's Survival of the Fittest Theory
via a Genetic Algorithm (GA). Research results show that the proposed method, a
GA, outperforms a random selection of hyperparameters.
</summary>
    <author>
      <name>Keshav Ganapathy</name>
    </author>
    <link href="http://arxiv.org/abs/2009.08928v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.08928v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.08931v1</id>
    <updated>2020-09-06T23:08:25Z</updated>
    <published>2020-09-06T23:08:25Z</published>
    <title>Spatio-Temporal Activation Function To Map Complex Dynamical Systems</title>
    <summary>  Most of the real world is governed by complex and chaotic dynamical systems.
All of these dynamical systems pose a challenge in modelling them using neural
networks. Currently, reservoir computing, which is a subset of recurrent neural
networks, is actively used to simulate complex dynamical systems. In this work,
a two dimensional activation function is proposed which includes an additional
temporal term to impart dynamic behaviour on its output. The inclusion of a
temporal term alters the fundamental nature of an activation function, it
provides capability to capture the complex dynamics of time series data without
relying on recurrent neural networks.
</summary>
    <author>
      <name>Parth Mahendra</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.08931v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.08931v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.08932v2</id>
    <updated>2020-09-23T20:54:20Z</updated>
    <published>2020-09-06T13:17:33Z</published>
    <title>Multi-Activation Hidden Units for Neural Networks with Random Weights</title>
    <summary>  Single layer feedforward networks with random weights are successful in a
variety of classification and regression problems. These networks are known for
their non-iterative and fast training algorithms. A major drawback of these
networks is that they require a large number of hidden units. In this paper, we
propose the use of multi-activation hidden units. Such units increase the
number of tunable parameters and enable formation of complex decision surfaces,
without increasing the number of hidden units. We experimentally show that
multi-activation hidden units can be used either to improve the classification
accuracy, or to reduce computations.
</summary>
    <author>
      <name>Ajay M. Patrikar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 4 figures. arXiv admin note: substantial text overlap with
  arXiv:2008.10425</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.08932v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.08932v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.09579v1</id>
    <updated>2020-09-21T02:18:58Z</updated>
    <published>2020-09-21T02:18:58Z</published>
    <title>On the Performance of Generative Adversarial Network (GAN) Variants: A
  Clinical Data Study</title>
    <summary>  Generative Adversarial Network (GAN) is a useful type of Neural Networks in
various types of applications including generative models and feature
extraction. Various types of GANs are being researched with different insights,
resulting in a diverse family of GANs with a better performance in each
generation. This review focuses on various GANs categorized by their common
traits.
</summary>
    <author>
      <name>Jaesung Yoo</name>
    </author>
    <author>
      <name>Jeman Park</name>
    </author>
    <author>
      <name>An Wang</name>
    </author>
    <author>
      <name>David Mohaisen</name>
    </author>
    <author>
      <name>Joongheon Kim</name>
    </author>
    <link href="http://arxiv.org/abs/2009.09579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.13207v1</id>
    <updated>2020-09-28T10:43:13Z</updated>
    <published>2020-09-28T10:43:13Z</published>
    <title>A thermodynamically consistent chemical spiking neuron capable of
  autonomous Hebbian learning</title>
    <summary>  We propose a fully autonomous, thermodynamically consistent set of chemical
reactions that implements a spiking neuron. This chemical neuron is able to
learn input patterns in a Hebbian fashion. The system is scalable to
arbitrarily many input channels. We demonstrate its performance in learning
frequency biases in the input as well as correlations between different input
channels. Efficient computation of time-correlations requires a highly
non-linear activation function. The resource requirements of a non-linear
activation function are discussed. In addition to the thermodynamically
consistent model of the CN, we also propose a biologically plausible version
that could be engineered in a synthetic biology context.
</summary>
    <author>
      <name>Jakub Fil</name>
    </author>
    <author>
      <name>Dominique Chu</name>
    </author>
    <link href="http://arxiv.org/abs/2009.13207v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.13207v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.13347v1</id>
    <updated>2020-09-28T14:14:36Z</updated>
    <published>2020-09-28T14:14:36Z</published>
    <title>A Review of Evolutionary Multi-modal Multi-objective Optimization</title>
    <summary>  Multi-modal multi-objective optimization aims to find all Pareto optimal
solutions including overlapping solutions in the objective space. Multi-modal
multi-objective optimization has been investigated in the evolutionary
computation community since 2005. However, it is difficult to survey existing
studies in this field because they have been independently conducted and do not
explicitly use the term "multi-modal multi-objective optimization". To address
this issue, this paper reviews existing studies of evolutionary multi-modal
multi-objective optimization, including studies published under names that are
different from "multi-modal multi-objective optimization". Our review also
clarifies open issues in this research area.
</summary>
    <author>
      <name>Ryoji Tanabe</name>
    </author>
    <author>
      <name>Hisao Ishibuchi</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TEVC.2019.2909744</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TEVC.2019.2909744" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is an accepted version of a paper published in the IEEE
  Transactions on Evolutionary Computation</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.13347v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.13347v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.01200v1</id>
    <updated>2020-10-02T21:02:35Z</updated>
    <published>2020-10-02T21:02:35Z</published>
    <title>FPGA Implementation of Simplified Spiking Neural Network</title>
    <summary>  Spiking Neural Networks (SNN) are third-generation Artificial Neural Networks
(ANN) which are close to the biological neural system. In recent years SNN has
become popular in the area of robotics and embedded applications, therefore, it
has become imperative to explore its real-time and energy-efficient
implementations. SNNs are more powerful than their predecessors because they
encode temporal information and use biologically plausible plasticity rules. In
this paper, a simpler and computationally efficient SNN model using FPGA
architecture is described. The proposed model is validated on a Xilinx Virtex 6
FPGA and analyzes a fully connected network which consists of 800 neurons and
12,544 synapses in real-time.
</summary>
    <author>
      <name>Shikhar Gupta</name>
    </author>
    <author>
      <name>Arpan Vyas</name>
    </author>
    <author>
      <name>Gaurav Trivedi</name>
    </author>
    <link href="http://arxiv.org/abs/2010.01200v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.01200v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.05943v1</id>
    <updated>2020-10-12T18:05:04Z</updated>
    <published>2020-10-12T18:05:04Z</published>
    <title>Activation function impact on Sparse Neural Networks</title>
    <summary>  While the concept of a Sparse Neural Network has been researched for some
time, researchers have only recently made notable progress in the matter.
Techniques like Sparse Evolutionary Training allow for significantly lower
computational complexity when compared to fully connected models by reducing
redundant connections. That typically takes place in an iterative process of
weight creation and removal during network training. Although there have been
numerous approaches to optimize the redistribution of the removed weights,
there seems to be little or no study on the effect of activation functions on
the performance of the Sparse Networks. This research provides insights into
the relationship between the activation function used and the network
performance at various sparsity levels.
</summary>
    <author>
      <name>Adam Dubowski</name>
    </author>
    <link href="http://arxiv.org/abs/2010.05943v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.05943v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.12047v1</id>
    <updated>2020-10-22T20:42:50Z</updated>
    <published>2020-10-22T20:42:50Z</published>
    <title>Fading memory echo state networks are universal</title>
    <summary>  Echo state networks (ESNs) have been recently proved to be universal
approximants for input/output systems with respect to various $L ^p$-type
criteria. When $1\leq p&lt; \infty$, only $p$-integrability hypotheses need to be
imposed, while in the case $p=\infty$ a uniform boundedness hypotheses on the
inputs is required. This note shows that, in the last case, a universal family
of ESNs can be constructed that contains exclusively elements that have the
echo state and the fading memory properties. This conclusion could not be drawn
with the results and methods available so far in the literature.
</summary>
    <author>
      <name>Lukas Gonon</name>
    </author>
    <author>
      <name>Juan-Pablo Ortega</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages letter</arxiv:comment>
    <link href="http://arxiv.org/abs/2010.12047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.12047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2010.14618v1</id>
    <updated>2020-10-11T00:31:27Z</updated>
    <published>2020-10-11T00:31:27Z</published>
    <title>A computationally and cognitively plausible model of supervised and
  unsupervised learning</title>
    <summary>  Both empirical and mathematical demonstrations of the importance of
chance-corrected measures are discussed, and a new model of learning is
proposed based on empirical psychological results on association learning. Two
forms of this model are developed, the Informatron as a chance-corrected
Perceptron, and AdaBook as a chance-corrected AdaBoost procedure. Computational
results presented show chance correction facilitates learning.
</summary>
    <author>
      <name>David M W Powers</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 figures, 24 references. Amended version of paper
  presented at BICS 2013</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Conference on Brain Inspired Cognitive Systems 2013,
  pp. 145-156</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2010.14618v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2010.14618v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.05281v2</id>
    <updated>2020-11-15T21:47:37Z</updated>
    <published>2020-10-27T20:41:30Z</published>
    <title>A Genetic Algorithm Based Approach for Satellite Autonomy</title>
    <summary>  Autonomous spacecraft maneuver planning using an evolutionary algorithmic
approach is investigated. Simulated spacecraft were placed into four different
initial orbits. Each was allowed a string of thirty delta-v impulse maneuvers
in six cartesian directions, the positive and negative x, y and z directions.
The goal of the spacecraft maneuver string was to, starting from some non-polar
starting orbit, place the spacecraft into a polar, low eccentricity orbit. A
genetic algorithm was implemented, using a mating, fitness, mutation and
crossover scheme for impulse strings. The genetic algorithm was successfully
able to produce this result for all the starting orbits. Performance and future
work is also discussed.
</summary>
    <author>
      <name>Sidhdharth Sikka</name>
    </author>
    <author>
      <name>Harshvardhan Sikka</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.13140/RG.2.2.17934.18247/2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.13140/RG.2.2.17934.18247/2" rel="related"/>
    <link href="http://arxiv.org/abs/2011.05281v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.05281v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.05588v1</id>
    <updated>2020-11-11T06:21:08Z</updated>
    <published>2020-11-11T06:21:08Z</published>
    <title>Deep Neural Networks and Neuro-Fuzzy Networks for Intellectual Analysis
  of Economic Systems</title>
    <summary>  In tis paper we consider approaches for time series forecasting based on deep
neural networks and neuro-fuzzy nets. Also, we make short review of researches
in forecasting based on various models of ANFIS models. Deep Learning has
proven to be an effective method for making highly accurate predictions from
complex data sources. Also, we propose our models of DL and Neuro-Fuzzy
Networks for this task. Finally, we show possibility of using these models for
data science tasks. This paper presents also an overview of approaches for
incorporating rule-based methodology into deep learning neural networks.
</summary>
    <author>
      <name>Alexey Averkin</name>
    </author>
    <author>
      <name>Sergey Yarushev</name>
    </author>
    <link href="http://arxiv.org/abs/2011.05588v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.05588v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="econ.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.EC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.06913v2</id>
    <updated>2021-03-01T17:49:23Z</updated>
    <published>2020-11-13T13:56:13Z</published>
    <title>Finding optimal Pulse Repetion Intervals with Many-objective
  Evolutionary Algorithms</title>
    <summary>  In this paper we consider the problem of finding Pulse Repetition Intervals
allowing the best compromises mitigating range and Doppler ambiguities in a
Pulsed-Doppler radar system. We revisit a problem that was proposed to the
Evolutionary Computation community as a real-world case to test Many-objective
Optimization algorithms. We use it as a baseline to compare several
Evolutionary Algorithms for black-box optimization with different metrics.
Resulting data is aggregated to build a reference set of Pareto optimal points
and is the starting point for further analysis and operational use by the radar
designer.
</summary>
    <author>
      <name>Paul Dufossé</name>
    </author>
    <author>
      <name>Cyrille Enderli</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 tables, 2 figures, submitted to EUSIPCO 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.06913v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.06913v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.07464v2</id>
    <updated>2021-10-23T20:15:41Z</updated>
    <published>2020-11-15T07:15:18Z</published>
    <title>Predictive Coding, Variational Autoencoders, and Biological Connections</title>
    <summary>  This paper reviews predictive coding, from theoretical neuroscience, and
variational autoencoders, from machine learning, identifying the common origin
and mathematical framework underlying both areas. As each area is prominent
within its respective field, more firmly connecting these areas could prove
useful in the dialogue between neuroscience and machine learning. After
reviewing each area, we discuss two possible correspondences implied by this
perspective: cortical pyramidal dendrites as analogous to (non-linear) deep
networks and lateral inhibition as analogous to normalizing flows. These
connections may provide new directions for further investigations in each
field.
</summary>
    <author>
      <name>Joseph Marino</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">NeurIPS NeuroAI Workshop, NAISys, Neural Computation</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.07464v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.07464v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.09534v1</id>
    <updated>2020-11-18T20:34:16Z</updated>
    <published>2020-11-18T20:34:16Z</published>
    <title>Randomized Self Organizing Map</title>
    <summary>  We propose a variation of the self organizing map algorithm by considering
the random placement of neurons on a two-dimensional manifold, following a blue
noise distribution from which various topologies can be derived. These
topologies possess random (but controllable) discontinuities that allow for a
more flexible self-organization, especially with high-dimensional data. The
proposed algorithm is tested on one-, two- and three-dimensions tasks as well
as on the MNIST handwritten digits dataset and validated using spectral
analysis and topological data analysis tools. We also demonstrate the ability
of the randomized self-organizing map to gracefully reorganize itself in case
of neural lesion and/or neurogenesis.
</summary>
    <author>
      <name>Nicolas P. Rougier</name>
    </author>
    <author>
      <name>Georgios Is. Detorakis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages, 19 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.09534v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.09534v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.09969v1</id>
    <updated>2020-11-16T02:18:52Z</updated>
    <published>2020-11-16T02:18:52Z</published>
    <title>Neural network algorithm and its application in reactive distillation</title>
    <summary>  Reactive distillation is a special distillation technology based on the
coupling of chemical reaction and distillation. It has the characteristics of
low energy consumption and high separation efficiency. However, because the
combination of reaction and separation produces highly nonlinear robust
behavior, the control and optimization of the reactive distillation process
cannot use conventional methods, but must rely on neural network algorithms.
This paper briefly describes the characteristics and research progress of
reactive distillation technology and neural network algorithms, and summarizes
the application of neural network algorithms in reactive distillation, aiming
to provide reference for the development and innovation of industry technology.
</summary>
    <author>
      <name>Huihui Wang</name>
    </author>
    <author>
      <name>Ruyang Mo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2011.09969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.09969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2011.12338v1</id>
    <updated>2020-11-24T19:33:08Z</updated>
    <published>2020-11-24T19:33:08Z</published>
    <title>PeleNet: A Reservoir Computing Framework for Loihi</title>
    <summary>  High-level frameworks for spiking neural networks are a key factor for fast
prototyping and efficient development of complex algorithms. Such frameworks
have emerged in the last years for traditional computers, but programming
neuromorphic hardware is still a challenge. Often low level programming with
knowledge about the hardware of the neuromorphic chip is required. The PeleNet
framework aims to simplify reservoir computing for the neuromorphic hardware
Loihi. It is build on top of the NxSDK from Intel and is written in Python. The
framework manages weight matrices, parameters and probes. In particular, it
provides an automatic and efficient distribution of networks over several cores
and chips. With this, the user is not confronted with technical details and can
concentrate on experiments.
</summary>
    <author>
      <name>Carlo Michaelis</name>
    </author>
    <link href="http://arxiv.org/abs/2011.12338v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2011.12338v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.00879v1</id>
    <updated>2021-02-01T14:38:14Z</updated>
    <published>2021-02-01T14:38:14Z</published>
    <title>Evolutionary computational platform for the automatic discovery of
  nanocarriers for cancer treatment</title>
    <summary>  We present the EVONANO platform for the evolution of nanomedicines with
application to anti-cancer treatments. EVONANO includes a simulator to grow
tumours, extract representative scenarios, and then simulate nanoparticle
transport through these scenarios to predict nanoparticle distribution. The
nanoparticle designs are optimised using machine learning to efficiently find
the most effective anti-cancer treatments. We demonstrate our platform with two
examples optimising the properties of nanoparticles and treatment to
selectively kill cancer cells over a range of tumour environments.
</summary>
    <author>
      <name>Namid Stillman</name>
    </author>
    <author>
      <name>Igor Balaz</name>
    </author>
    <author>
      <name>Antisthenis Tsompanas</name>
    </author>
    <author>
      <name>Marina Kovacevic</name>
    </author>
    <author>
      <name>Sepinoud Azimi</name>
    </author>
    <author>
      <name>Sebastien Lafond</name>
    </author>
    <author>
      <name>Andrew Adamatzky</name>
    </author>
    <author>
      <name>Sabine Hauert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">30 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.00879v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.00879v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.med-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.01011v1</id>
    <updated>2020-12-28T03:15:46Z</updated>
    <published>2020-12-28T03:15:46Z</published>
    <title>Deep Evolutionary Learning for Molecular Design</title>
    <summary>  In this paper, we propose a deep evolutionary learning (DEL) process that
integrates fragment-based deep generative model and multi-objective
evolutionary computation for molecular design. Our approach enables (1)
evolutionary operations in the latent space of the generative model, rather
than the structural space, to generate novel promising molecular structures for
the next evolutionary generation, and (2) generative model fine-tuning using
newly generated high-quality samples. Thus, DEL implements a data-model
co-evolution concept which improves both sample population and generative model
learning. Experiments on two public datasets indicate that sample population
obtained by DEL exhibits improved property distributions, and dominates samples
generated by multi-objective Bayesian optimization algorithms.
</summary>
    <author>
      <name>Yifeng Li</name>
    </author>
    <author>
      <name>Hsu Kiang Ooi</name>
    </author>
    <author>
      <name>Alain Tchagang</name>
    </author>
    <link href="http://arxiv.org/abs/2102.01011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.01011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.05437v1</id>
    <updated>2021-02-10T14:00:39Z</updated>
    <published>2021-02-10T14:00:39Z</published>
    <title>Pruning of Convolutional Neural Networks Using Ising Energy Model</title>
    <summary>  Pruning is one of the major methods to compress deep neural networks. In this
paper, we propose an Ising energy model within an optimization framework for
pruning convolutional kernels and hidden units. This model is designed to
reduce redundancy between weight kernels and detect inactive kernels/hidden
units. Our experiments using ResNets, AlexNet, and SqueezeNet on CIFAR-10 and
CIFAR-100 datasets show that the proposed method on average can achieve a
pruning rate of more than $50\%$ of the trainable parameters with approximately
$&lt;10\%$ and $&lt;5\%$ drop of Top-1 and Top-5 classification accuracy,
respectively.
</summary>
    <author>
      <name>Hojjat Salehinejad</name>
    </author>
    <author>
      <name>Shahrokh Valaee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper is accepted for presentation at IEEE International
  Conference on Acoustics, Speech and Signal Processing (IEEE ICASSP), 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.05437v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.05437v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.08849v1</id>
    <updated>2021-02-17T16:19:17Z</updated>
    <published>2021-02-17T16:19:17Z</published>
    <title>Automated Curriculum Learning for Embodied Agents: A Neuroevolutionary
  Approach</title>
    <summary>  We demonstrate how an evolutionary algorithm can be extended with a
curriculum learning process that selects automatically the environmental
conditions in which the evolving agents are evaluated. The environmental
conditions are selected so to adjust the level of difficulty to the ability
level of the current evolving agents and so to challenge the weaknesses of the
evolving agents. The method does not require domain knowledge and does not
introduce additional hyperparameters. The results collected on two benchmark
problems, that require to solve a task in significantly varying environmental
conditions, demonstrate that the method proposed outperforms conventional
algorithms and generates solutions that are robust to variations
</summary>
    <author>
      <name>Nicola Milano</name>
    </author>
    <author>
      <name>Stefano Nolfi</name>
    </author>
    <link href="http://arxiv.org/abs/2102.08849v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.08849v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.10911v1</id>
    <updated>2021-02-22T11:29:09Z</updated>
    <published>2021-02-22T11:29:09Z</published>
    <title>Elementary superexpressive activations</title>
    <summary>  We call a finite family of activation functions superexpressive if any
multivariate continuous function can be approximated by a neural network that
uses these activations and has a fixed architecture only depending on the
number of input variables (i.e., to achieve any accuracy we only need to adjust
the weights, without increasing the number of neurons). Previously, it was
known that superexpressive activations exist, but their form was quite complex.
We give examples of very simple superexpressive families: for example, we prove
that the family {sin, arcsin} is superexpressive. We also show that most
practical activations (not involving periodic functions) are not
superexpressive.
</summary>
    <author>
      <name>Dmitry Yarotsky</name>
    </author>
    <link href="http://arxiv.org/abs/2102.10911v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.10911v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.04964v1</id>
    <updated>2021-07-11T04:31:10Z</updated>
    <published>2021-07-11T04:31:10Z</published>
    <title>Self-Referential Quality Diversity Through Differential Map-Elites</title>
    <summary>  Differential MAP-Elites is a novel algorithm that combines the illumination
capacity of CVT-MAP-Elites with the continuous-space optimization capacity of
Differential Evolution. The algorithm is motivated by observations that
illumination algorithms, and quality-diversity algorithms in general, offer
qualitatively new capabilities and applications for evolutionary computation
yet are in their original versions relatively unsophisticated optimizers. The
basic Differential MAP-Elites algorithm, introduced for the first time here, is
relatively simple in that it simply combines the operators from Differential
Evolution with the map structure of CVT-MAP-Elites. Experiments based on 25
numerical optimization problems suggest that Differential MAP-Elites clearly
outperforms CVT-MAP-Elites, finding better-quality and more diverse solutions.
</summary>
    <author>
      <name>Tae Jong Choi</name>
    </author>
    <author>
      <name>Julian Togelius</name>
    </author>
    <link href="http://arxiv.org/abs/2107.04964v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.04964v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.06862v1</id>
    <updated>2021-06-22T17:38:34Z</updated>
    <published>2021-06-22T17:38:34Z</published>
    <title>Differentiable Programming of Reaction-Diffusion Patterns</title>
    <summary>  Reaction-Diffusion (RD) systems provide a computational framework that
governs many pattern formation processes in nature. Current RD system design
practices boil down to trial-and-error parameter search. We propose a
differentiable optimization method for learning the RD system parameters to
perform example-based texture synthesis on a 2D plane. We do this by
representing the RD system as a variant of Neural Cellular Automata and using
task-specific differentiable loss functions. RD systems generated by our method
exhibit robust, non-trivial 'life-like' behavior.
</summary>
    <author>
      <name>Alexander Mordvintsev</name>
    </author>
    <author>
      <name>Ettore Randazzo</name>
    </author>
    <author>
      <name>Eyvind Niklasson</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ALIFE 2021</arxiv:comment>
    <link href="http://arxiv.org/abs/2107.06862v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.06862v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.07382v1</id>
    <updated>2021-07-11T16:43:11Z</updated>
    <published>2021-07-11T16:43:11Z</published>
    <title>Hybrid Ant Swarm-Based Data Clustering</title>
    <summary>  Biologically inspired computing techniques are very effective and useful in
many areas of research including data clustering. Ant clustering algorithm is a
nature-inspired clustering technique which is extensively studied for over two
decades. In this study, we extend the ant clustering algorithm (ACA) to a
hybrid ant clustering algorithm (hACA). Specifically, we include a genetic
algorithm in standard ACA to extend the hybrid algorithm for better
performance. We also introduced novel pick up and drop off rules to speed up
the clustering performance. We study the performance of the hACA algorithm and
compare with standard ACA as a benchmark.
</summary>
    <author>
      <name>Md Ali Azam</name>
    </author>
    <author>
      <name>Abir Hossen</name>
    </author>
    <author>
      <name>Md Hafizur Rahman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/AIIoT52608.2021.9454238</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/AIIoT52608.2021.9454238" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">2021 IEEE World AI IoT Congress (AIIoT)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2107.07382v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.07382v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2107.08484v2</id>
    <updated>2023-05-04T16:05:24Z</updated>
    <published>2021-07-18T16:19:53Z</published>
    <title>A Novel Evolutionary Algorithm for Hierarchical Neural Architecture
  Search</title>
    <summary>  In this work, we propose a novel evolutionary algorithm for neural
architecture search, applicable to global search spaces. The algorithm's
architectural representation organizes the topology in multiple hierarchical
modules, while the design process exploits this representation, in order to
explore the search space. We also employ a curation system, which promotes the
utilization of well performing sub-structures to subsequent generations. We
apply our method to Fashion-MNIST and NAS-Bench101, achieving accuracies of
$93.2\%$ and $94.8\%$ respectively in a relatively small number of generations.
</summary>
    <author>
      <name>Aristeidis Christoforidis</name>
    </author>
    <author>
      <name>George Kyriakides</name>
    </author>
    <author>
      <name>Konstantinos Margaritis</name>
    </author>
    <link href="http://arxiv.org/abs/2107.08484v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2107.08484v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.04815v2</id>
    <updated>2022-01-19T12:01:07Z</updated>
    <published>2022-01-13T07:19:28Z</published>
    <title>Direct Mutation and Crossover in Genetic Algorithms Applied to
  Reinforcement Learning Tasks</title>
    <summary>  Neuroevolution has recently been shown to be quite competitive in
reinforcement learning (RL) settings, and is able to alleviate some of the
drawbacks of gradient-based approaches. This paper will focus on applying
neuroevolution using a simple genetic algorithm (GA) to find the weights of a
neural network that produce optimally behaving agents. In addition, we present
two novel modifications that improve the data efficiency and speed of
convergence when compared to the initial implementation. The modifications are
evaluated on the FrozenLake environment provided by OpenAI gym and prove to be
significantly better than the baseline approach.
</summary>
    <author>
      <name>Tarek Faycal</name>
    </author>
    <author>
      <name>Claudio Zito</name>
    </author>
    <link href="http://arxiv.org/abs/2201.04815v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.04815v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.07208v1</id>
    <updated>2021-12-03T19:34:16Z</updated>
    <published>2021-12-03T19:34:16Z</published>
    <title>Enhanced Self-Organizing Map Solution for the Traveling Salesman Problem</title>
    <summary>  Using an enhanced Self-Organizing Map method, we provided suboptimal
solutions to the Traveling Salesman Problem. Besides, we employed
hyperparameter tuning to identify the most critical features in the algorithm.
All improvements in the benchmark work brought consistent results and may
inspire future efforts to improve this algorithm and apply it to different
problems.
</summary>
    <author>
      <name>Joao P. A. Dantas</name>
    </author>
    <author>
      <name>Andre N. Costa</name>
    </author>
    <author>
      <name>Marcos R. O. A. Maximo</name>
    </author>
    <author>
      <name>Takashi Yoneyama</name>
    </author>
    <link href="http://arxiv.org/abs/2201.07208v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.07208v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.10037v1</id>
    <updated>2022-01-25T01:38:53Z</updated>
    <published>2022-01-25T01:38:53Z</published>
    <title>Diversity Enhancement via Magnitude</title>
    <summary>  Promoting and maintaining diversity of candidate solutions is a key
requirement of evolutionary algorithms in general and multi-objective
evolutionary algorithms in particular. In this paper, we use the recently
developed theory of magnitude to construct a gradient flow and similar notions
that systematically manipulate finite subsets of Euclidean space to enhance
their diversity, and apply the ideas in service of multi-objective evolutionary
algorithms. We demonstrate diversity enhancement on benchmark problems using
leading algorithms, and discuss extensions of the framework.
</summary>
    <author>
      <name>Steve Huntsman</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-031-27250-9_27</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-031-27250-9_27" rel="related"/>
    <link href="http://arxiv.org/abs/2201.10037v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.10037v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90-08" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.11726v3</id>
    <updated>2022-06-30T11:04:39Z</updated>
    <published>2022-01-27T18:40:07Z</published>
    <title>Search Trajectories Networks of Multiobjective Evolutionary Algorithms</title>
    <summary>  Understanding the search dynamics of multiobjective evolutionary algorithms
(MOEAs) is still an open problem. This paper extends a recent network-based
tool, search trajectory networks (STNs), to model the behavior of MOEAs. Our
approach uses the idea of decomposition, where a multiobjective problem is
transformed into several single-objective problems. We show that STNs can be
used to model and distinguish the search behavior of two popular multiobjective
algorithms, MOEA/D and NSGA-II, using 10 continuous benchmark problems with 2
and 3 objectives. Our findings suggest that we can improve our understanding of
MOEAs using STNs for algorithm analysis.
</summary>
    <author>
      <name>Yuri Lavinas</name>
    </author>
    <author>
      <name>Claus Aranha</name>
    </author>
    <author>
      <name>Gabriela Ochoa</name>
    </author>
    <link href="http://arxiv.org/abs/2201.11726v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.11726v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.04365v3</id>
    <updated>2022-06-28T09:37:26Z</updated>
    <published>2022-02-09T10:03:12Z</published>
    <title>AIVC: Artificial Intelligence based Video Codec</title>
    <summary>  This paper introduces AIVC, an end-to-end neural video codec. It is based on
two conditional autoencoders MNet and CNet, for motion compensation and coding.
AIVC learns to compress videos using any coding configurations through a single
end-to-end rate-distortion optimization. Furthermore, it offers performance
competitive with the recent video coder HEVC under several established test
conditions. A comprehensive ablation study is performed to evaluate the
benefits of the different modules composing AIVC. The implementation is made
available at https://orange-opensource.github.io/AIVC/.
</summary>
    <author>
      <name>Théo Ladune</name>
    </author>
    <author>
      <name>Pierrick Philippe</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ICIP 2022 (IEEE International Conference on Image Processing), Oct
  2022, Bordeaux, France</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2202.04365v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.04365v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.07132v1</id>
    <updated>2022-02-15T02:09:33Z</updated>
    <published>2022-02-15T02:09:33Z</published>
    <title>Memory via Temporal Delays in weightless Spiking Neural Network</title>
    <summary>  A common view in the neuroscience community is that memory is encoded in the
connection strength between neurons. This perception led artificial neural
network models to focus on connection weights as the key variables to modulate
learning. In this paper, we present a prototype for weightless spiking neural
networks that can perform a simple classification task. The memory in this
network is stored in the timing between neurons, rather than the strength of
the connection, and is trained using a Hebbian Spike Timing Dependent
Plasticity (STDP), which modulates the delays of the connection.
</summary>
    <author>
      <name>Hananel Hazan</name>
    </author>
    <author>
      <name>Simon Caby</name>
    </author>
    <author>
      <name>Christopher Earl</name>
    </author>
    <author>
      <name>Hava Siegelmann</name>
    </author>
    <author>
      <name>Michael Levin</name>
    </author>
    <link href="http://arxiv.org/abs/2202.07132v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.07132v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2202.12829v1</id>
    <updated>2022-02-08T19:36:43Z</updated>
    <published>2022-02-08T19:36:43Z</published>
    <title>Convergence of a New Learning Algorithm</title>
    <summary>  A new learning algorithm proposed by Brandt and Lin for neural network [1],
[2] has been shown to be mathematically equivalent to the conventional
back-propagation learning algorithm, but has several advantages over the
backpropagation algorithm, including feedback-network-free implementation and
biological plausibility. In this paper, we investigate the convergence of the
new algorithm. A necessary and sufficient condition for the algorithm to
converge is derived. A convergence measure is proposed to measure the
convergence rate of the new algorithm. Simulation studies are conducted to
investigate the convergence of the algorithm with respect to the number of
neurons, the connection distance, the connection density, the ratio of
excitatory/inhibitory synapses, the membrane potentials, and the synapse
strengths.
</summary>
    <author>
      <name>Feng Lin</name>
    </author>
    <link href="http://arxiv.org/abs/2202.12829v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2202.12829v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.03251v1</id>
    <updated>2022-05-06T14:09:38Z</updated>
    <published>2022-05-06T14:09:38Z</published>
    <title>A Trillion Genetic Programming Instructions per Second</title>
    <summary>  We summarise how a 3.0 GHz 16 core AVX512 computer can interpret the
equivalent of up to on average 1103370000000 GPop/s. Citations to existing
publications are given. Implementation stress is placed on both parallel
computing, bandwidth limits and avoiding repeated calculation. Information
theory suggests in digital computing, failed disruption propagation gives huge
speed ups as FDP and incremental evaluation can be used to reduce fitness
evaluation time in phenotypically converged populations. Conversely FDP may be
responsible for evolution stagnation. So the wider Evolutionary Computing,
Artificial Life, Unconventional Computing and Software Engineering community
may need to avoid deep nesting.
</summary>
    <author>
      <name>W. B. Langdon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.03251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.03251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.04792v1</id>
    <updated>2022-05-10T10:36:31Z</updated>
    <published>2022-05-10T10:36:31Z</published>
    <title>Neural Networks with Different Initialization Methods for Depression
  Detection</title>
    <summary>  As a common mental disorder, depression is a leading cause of various
diseases worldwide. Early detection and treatment of depression can
dramatically promote remission and prevent relapse. However, conventional ways
of depression diagnosis require considerable human effort and cause economic
burden, while still being prone to misdiagnosis. On the other hand, recent
studies report that physical characteristics are major contributors to the
diagnosis of depression, which inspires us to mine the internal relationship by
neural networks instead of relying on clinical experiences. In this paper,
neural networks are constructed to predict depression from physical
characteristics. Two initialization methods are examined - Xaiver and Kaiming
initialization. Experimental results show that a 3-layers neural network with
Kaiming initialization achieves $83\%$ accuracy.
</summary>
    <author>
      <name>Tianle Yang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.04792v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.04792v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.10118v1</id>
    <updated>2022-05-16T14:49:58Z</updated>
    <published>2022-05-16T14:49:58Z</published>
    <title>An Artificial Neural Network Functionalized by Evolution</title>
    <summary>  The topology of artificial neural networks has a significant effect on their
performance. Characterizing efficient topology is a field of promising research
in Artificial Intelligence. However, it is not a trivial task and it is mainly
experimented on through convolutional neural networks. We propose a hybrid
model which combines the tensor calculus of feed-forward neural networks with
Pseudo-Darwinian mechanisms. This allows for finding topologies that are well
adapted for elaboration of strategies, control problems or pattern recognition
tasks. In particular, the model can provide adapted topologies at early
evolutionary stages, and 'structural convergence', which can found applications
in robotics, big-data and artificial life.
</summary>
    <author>
      <name>Fabien Furfaro</name>
    </author>
    <author>
      <name>Avner Bar-Hen</name>
    </author>
    <author>
      <name>Geoffroy Berthelot</name>
    </author>
    <link href="http://arxiv.org/abs/2205.10118v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.10118v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.10122v2</id>
    <updated>2022-08-23T12:02:23Z</updated>
    <published>2022-05-06T18:42:36Z</published>
    <title>Stochastic resonance neurons in artificial neural networks</title>
    <summary>  Many modern applications of the artificial neural networks ensue large number
of layers making traditional digital implementations increasingly complex.
Optical neural networks offer parallel processing at high bandwidth, but have
the challenge of noise accumulation. We propose here a new type of neural
networks using stochastic resonances as an inherent part of the architecture
and demonstrate a possibility of significant reduction of the required number
of neurons for a given performance accuracy. We also show that such a neural
network is more robust against the impact of noise.
</summary>
    <author>
      <name>Egor Manuylovich</name>
    </author>
    <author>
      <name>Diego Argüello Ron</name>
    </author>
    <author>
      <name>Morteza Kamalian-Kopae</name>
    </author>
    <author>
      <name>Sergei Turitsyn</name>
    </author>
    <link href="http://arxiv.org/abs/2205.10122v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.10122v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.15990v2</id>
    <updated>2022-07-29T18:13:05Z</updated>
    <published>2022-05-31T17:47:50Z</published>
    <title>Correlation versus RMSE Loss Functions in Symbolic Regression Tasks</title>
    <summary>  The use of correlation as a fitness function is explored in symbolic
regression tasks and the performance is compared against the typical RMSE
fitness function. Using correlation with an alignment step to conclude the
evolution led to significant performance gains over RMSE as a fitness function.
Using correlation as a fitness function led to solutions being found in fewer
generations compared to RMSE, as well it was found that fewer data points were
needed in the training set to discover the correct equations. The Feynman
Symbolic Regression Benchmark as well as several other old and recent GP
benchmark problems were used to evaluate performance.
</summary>
    <author>
      <name>Nathan Haut</name>
    </author>
    <author>
      <name>Wolfgang Banzhaf</name>
    </author>
    <author>
      <name>Bill Punch</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the GPTP conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2205.15990v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.15990v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.06098v1</id>
    <updated>2022-05-31T18:44:13Z</updated>
    <published>2022-05-31T18:44:13Z</published>
    <title>A comparative study of back propagation and its alternatives on
  multilayer perceptrons</title>
    <summary>  The de facto algorithm for training the back pass of a feedforward neural
network is backpropagation (BP). The use of almost-everywhere differentiable
activation functions made it efficient and effective to propagate the gradient
backwards through layers of deep neural networks. However, in recent years,
there has been much research in alternatives to backpropagation. This analysis
has largely focused on reaching state-of-the-art accuracy in multilayer
perceptrons (MLPs) and convolutional neural networks (CNNs). In this paper, we
analyze the stability and similarity of predictions and neurons in MLPs and
propose a new variation of one of the algorithms.
</summary>
    <author>
      <name>John Waldo</name>
    </author>
    <link href="http://arxiv.org/abs/2206.06098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.06098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.11087v1</id>
    <updated>2022-05-25T17:58:18Z</updated>
    <published>2022-05-25T17:58:18Z</published>
    <title>Federated Adaptation of Reservoirs via Intrinsic Plasticity</title>
    <summary>  We propose a novel algorithm for performing federated learning with Echo
State Networks (ESNs) in a client-server scenario. In particular, our proposal
focuses on the adaptation of reservoirs by combining Intrinsic Plasticity with
Federated Averaging. The former is a gradient-based method for adapting the
reservoir's non-linearity in a local and unsupervised manner, while the latter
provides the framework for learning in the federated scenario. We evaluate our
approach on real-world datasets from human monitoring, in comparison with the
previous approach for federated ESNs existing in literature. Results show that
adapting the reservoir with our algorithm provides a significant improvement on
the performance of the global model.
</summary>
    <author>
      <name>Valerio De Caro</name>
    </author>
    <author>
      <name>Claudio Gallicchio</name>
    </author>
    <author>
      <name>Davide Bacciu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2206.11087v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.11087v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.12082v1</id>
    <updated>2022-06-24T05:20:37Z</updated>
    <published>2022-06-24T05:20:37Z</published>
    <title>Symbolic-Regression Boosting</title>
    <summary>  Modifying standard gradient boosting by replacing the embedded weak learner
in favor of a strong(er) one, we present SyRBo: Symbolic-Regression Boosting.
Experiments over 98 regression datasets show that by adding a small number of
boosting stages -- between 2--5 -- to a symbolic regressor, statistically
significant improvements can often be attained. We note that coding SyRBo on
top of any symbolic regressor is straightforward, and the added cost is simply
a few more evolutionary rounds. SyRBo is essentially a simple add-on that can
be readily added to an extant symbolic regressor, often with beneficial
results.
</summary>
    <author>
      <name>Moshe Sipper</name>
    </author>
    <author>
      <name>Jason H Moore</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10710-021-09400-0</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10710-021-09400-0" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Genetic Programming and Evolvable Machines, 22, 357-381, 2021</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2206.12082v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.12082v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.12706v1</id>
    <updated>2022-06-25T18:38:40Z</updated>
    <published>2022-06-25T18:38:40Z</published>
    <title>Binary and Multinomial Classification through Evolutionary Symbolic
  Regression</title>
    <summary>  We present three evolutionary symbolic regression-based classification
algorithms for binary and multinomial datasets: GPLearnClf, CartesianClf, and
ClaSyCo. Tested over 162 datasets and compared to three state-of-the-art
machine learning algorithms -- XGBoost, LightGBM, and a deep neural network --
we find our algorithms to be competitive. Further, we demonstrate how to find
the best method for one's dataset automatically, through the use of a
state-of-the-art hyperparameter optimizer.
</summary>
    <author>
      <name>Moshe Sipper</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3520304.3528922</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3520304.3528922" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 2022 Genetic and Evolutionary Computation
  Conference Companion</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2206.12706v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.12706v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2206.15409v1</id>
    <updated>2022-06-30T16:55:33Z</updated>
    <published>2022-06-30T16:55:33Z</published>
    <title>Automatically Balancing Model Accuracy and Complexity using Solution and
  Fitness Evolution (SAFE)</title>
    <summary>  When seeking a predictive model in biomedical data, one often has more than a
single objective in mind, e.g., attaining both high accuracy and low complexity
(to promote interpretability). We investigate herein whether multiple
objectives can be dynamically tuned by our recently proposed coevolutionary
algorithm, SAFE (Solution And Fitness Evolution). We find that SAFE is able to
automatically tune accuracy and complexity with no performance loss, as
compared with a standard evolutionary algorithm, over complex simulated
genetics datasets produced by the GAMETES tool.
</summary>
    <author>
      <name>Moshe Sipper</name>
    </author>
    <author>
      <name>Jason H. Moore</name>
    </author>
    <author>
      <name>Ryan J. Urbanowicz</name>
    </author>
    <link href="http://arxiv.org/abs/2206.15409v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2206.15409v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.00708v1</id>
    <updated>2022-07-02T01:22:39Z</updated>
    <published>2022-07-02T01:22:39Z</published>
    <title>Parameter efficient dendritic-tree neurons outperform perceptrons</title>
    <summary>  Biological neurons are more powerful than artificial perceptrons, in part due
to complex dendritic input computations. Inspired to empower the perceptron
with biologically inspired features, we explore the effect of adding and tuning
input branching factors along with input dropout. This allows for parameter
efficient non-linear input architectures to be discovered and benchmarked.
Furthermore, we present a PyTorch module to replace multi-layer perceptron
layers in existing architectures. Our initial experiments on MNIST
classification demonstrate the accuracy and generalization improvement of
dendritic neurons compared to existing perceptron architectures.
</summary>
    <author>
      <name>Ziwen Han</name>
    </author>
    <author>
      <name>Evgeniya Gorobets</name>
    </author>
    <author>
      <name>Pan Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2207.00708v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.00708v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.10367v2</id>
    <updated>2023-04-19T12:05:44Z</updated>
    <published>2022-07-21T08:49:09Z</published>
    <title>EC-KitY: Evolutionary Computation Tool Kit in Python with Seamless
  Machine Learning Integration</title>
    <summary>  EC-KitY is a comprehensive Python library for doing evolutionary computation
(EC), licensed under the BSD 3-Clause License, and compatible with
scikit-learn. Designed with modern software engineering and machine learning
integration in mind, EC-KitY can support all popular EC paradigms, including
genetic algorithms, genetic programming, coevolution, evolutionary
multi-objective optimization, and more. This paper provides an overview of the
package, including the ease of setting up an EC experiment, the architecture,
the main features, and a comparison with other libraries.
</summary>
    <author>
      <name>Moshe Sipper</name>
    </author>
    <author>
      <name>Tomer Halperin</name>
    </author>
    <author>
      <name>Itai Tzruia</name>
    </author>
    <author>
      <name>Achiya Elyasaf</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.softx.2023.101381</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.softx.2023.101381" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 1 figure, 1 table. Published in Elsevier SoftwareX</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SoftwareX, 22, 2023, 101381, ISSN 2352-7110</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2207.10367v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.10367v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2207.14036v1</id>
    <updated>2022-07-28T12:02:15Z</updated>
    <published>2022-07-28T12:02:15Z</published>
    <title>Co-Evolutionary Diversity Optimisation for the Traveling Thief Problem</title>
    <summary>  Recently different evolutionary computation approaches have been developed
that generate sets of high quality diverse solutions for a given optimisation
problem. Many studies have considered diversity 1) as a mean to explore niches
in behavioural space (quality diversity) or 2) to increase the structural
differences of solutions (evolutionary diversity optimisation). In this study,
we introduce a co-evolutionary algorithm to simultaneously explore the two
spaces for the multi-component traveling thief problem. The results show the
capability of the co-evolutionary algorithm to achieve significantly higher
diversity compared to the baseline evolutionary diversity algorithms from the
the literature.
</summary>
    <author>
      <name>Adel Nikfarjam</name>
    </author>
    <author>
      <name>Aneta Neumann</name>
    </author>
    <author>
      <name>Jakob Bossek</name>
    </author>
    <author>
      <name>Frank Neumann</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at PPSN 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2207.14036v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2207.14036v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.06474v1</id>
    <updated>2022-02-28T13:44:53Z</updated>
    <published>2022-02-28T13:44:53Z</published>
    <title>Review of research on fireworks algorithm</title>
    <summary>  Fireworks algorithm is a new type of intelligent optimization algorithm.
Because of its fast convergence speed, easy implementation, explosiveness,
diversity, simplicity and randomness, it has attracted more and more attention
in many research fields recently. This paper introduces the background,
composition, improvement idea of fireworks algorithm (analysis and improvement
of operator, improvement of hybrid algorithm), and its application in
continuous optimization, discrete optimization, single-objective optimization,
multi-objective optimization and other fields. Finally, the future research
directions of fireworks algorithm are summarized, including theoretical
analysis, operator analysis and improvement, hybrid algorithm research and
algorithm application.
</summary>
    <author>
      <name>Zhao Zhigang</name>
    </author>
    <author>
      <name>Li Zhimei</name>
    </author>
    <author>
      <name>Mo Haimiao</name>
    </author>
    <author>
      <name>Zeng Min</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">in Chinese language</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.06474v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.06474v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.09759v1</id>
    <updated>2022-08-20T22:45:07Z</updated>
    <published>2022-08-20T22:45:07Z</published>
    <title>ReckOn: A 28nm Sub-mm2 Task-Agnostic Spiking Recurrent Neural Network
  Processor Enabling On-Chip Learning over Second-Long Timescales</title>
    <summary>  A robust real-world deployment of autonomous edge devices requires on-chip
adaptation to user-, environment- and task-induced variability. Due to on-chip
memory constraints, prior learning devices were limited to static stimuli with
no temporal contents. We propose a 0.45-mm$^2$ spiking RNN processor enabling
task-agnostic online learning over seconds, which we demonstrate for
navigation, gesture recognition, and keyword spotting within a 0.8-% memory
overhead and a &lt;150-$\mu$W training power budget.
</summary>
    <author>
      <name>Charlotte Frenkel</name>
    </author>
    <author>
      <name>Giacomo Indiveri</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISSCC42614.2022.9731734</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISSCC42614.2022.9731734" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in the 2022 IEEE International Solid-State Circuits
  Conference (ISSCC), 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2208.09759v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.09759v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.14619v1</id>
    <updated>2022-08-31T03:36:56Z</updated>
    <published>2022-08-31T03:36:56Z</published>
    <title>Accelerating differential evolution algorithm with Gaussian sampling
  based on estimating the convergence points</title>
    <summary>  In this paper, we propose a simple strategy for estimating the convergence
point approximately by averaging the elite sub-population. Based on this idea,
we derive two methods, which are ordinary averaging strategy, and weighted
averaging strategy. We also design a Gaussian sampling operator with the mean
of the estimated convergence point with a certain standard deviation. This
operator is combined with the traditional differential evolution algorithm (DE)
to accelerate the convergence. Numerical experiments show that our proposal can
accelerate the DE on most functions of 28 low-dimensional test functions on the
CEC2013 Suite, and our proposal can easily be extended to combine with other
population-based evolutionary algorithms with a simple modification.
</summary>
    <author>
      <name>Rui Zhong</name>
    </author>
    <author>
      <name>Masaharu Munetomo</name>
    </author>
    <link href="http://arxiv.org/abs/2208.14619v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.14619v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.04425v1</id>
    <updated>2022-09-07T23:22:57Z</updated>
    <published>2022-09-07T23:22:57Z</published>
    <title>The Role Of Biology In Deep Learning</title>
    <summary>  Artificial neural networks took a lot of inspiration from their biological
counterparts in becoming our best machine perceptual systems. This work
summarizes some of that history and incorporates modern theoretical
neuroscience into experiments with artificial neural networks from the field of
deep learning. Specifically, iterative magnitude pruning is used to train
sparsely connected networks with 33x fewer weights without loss in performance.
These are used to test and ultimately reject the hypothesis that weight
sparsity alone improves image noise robustness. Recent work mitigated
catastrophic forgetting using weight sparsity, activation sparsity, and active
dendrite modeling. This paper replicates those findings, and extends the method
to train convolutional neural networks on a more challenging continual learning
task. The code has been made publicly available.
</summary>
    <author>
      <name>Robert Bain</name>
    </author>
    <link href="http://arxiv.org/abs/2209.04425v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.04425v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2209.09280v1</id>
    <updated>2022-09-19T18:08:23Z</updated>
    <published>2022-09-19T18:08:23Z</published>
    <title>An accurate and flexible analog emulation of AdEx neuron dynamics in
  silicon</title>
    <summary>  Analog neuromorphic hardware promises fast brain emulation on the one hand
and an efficient implementation of novel, brain-inspired computing paradigms on
the other. Bridging this spectrum requires flexibly configurable circuits with
reliable and reproducible dynamics fostered by an accurate implementation of
the targeted neuron and synapse models. This manuscript presents the analog
neuron circuits of the mixed-signal accelerated neuromorphic system
BrainScaleS-2. They are capable of flexibly and accurately emulating the
adaptive exponential leaky integrate-and-fire model equations in combination
with both current- and conductance-based synapses, as demonstrated by precisely
replicating a wide range of complex neuronal dynamics and firing patterns.
</summary>
    <author>
      <name>Sebastian Billaudelle</name>
    </author>
    <author>
      <name>Johannes Weis</name>
    </author>
    <author>
      <name>Philipp Dauer</name>
    </author>
    <author>
      <name>Johannes Schemmel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for ICECS 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2209.09280v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2209.09280v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.03517v1</id>
    <updated>2022-10-06T06:53:02Z</updated>
    <published>2022-10-06T06:53:02Z</published>
    <title>Fairness in generative modeling</title>
    <summary>  We design general-purpose algorithms for addressing fairness issues and mode
collapse in generative modeling. More precisely, to design fair algorithms for
as many sensitive variables as possible, including variables we might not be
aware of, we assume no prior knowledge of sensitive variables: our algorithms
use unsupervised fairness only, meaning no information related to the sensitive
variables is used for our fairness-improving methods. All images of faces (even
generated ones) have been removed to mitigate legal risks.
</summary>
    <author>
      <name>Mariia Zameshina</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIGM, FAIR</arxiv:affiliation>
    </author>
    <author>
      <name>Olivier Teytaud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">FAIR</arxiv:affiliation>
    </author>
    <author>
      <name>Fabien Teytaud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ULCO</arxiv:affiliation>
    </author>
    <author>
      <name>Vlad Hosu</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIGM</arxiv:affiliation>
    </author>
    <author>
      <name>Nathanael Carraz</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIGM</arxiv:affiliation>
    </author>
    <author>
      <name>Laurent Najman</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIGM</arxiv:affiliation>
    </author>
    <author>
      <name>Markus Wagner</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3520304.3528992</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3520304.3528992" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">GECCO '22: Genetic and Evolutionary Computation Conference, Jul
  2022, Boston Massachusetts, France. pp.320-323</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2210.03517v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.03517v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.06052v1</id>
    <updated>2022-10-12T09:45:45Z</updated>
    <published>2022-10-12T09:45:45Z</published>
    <title>System theoretic approach of information processing in nested cellular
  automata</title>
    <summary>  The subject of this paper is the evolution of the concept of information
processing in regular structures based on multi-level processing in nested
cellular automata. The essence of the proposed model is a discrete space-time
containing nested orthogonal space-times at its points. The factorization of
the function describing the global behavior of a system is the key element of
the mathematical description. Factorization describes the relations of physical
connections, signal propagation times and signal processing to global behavior.
In the model appear expressions similar to expressions used in the Special
Relativity Theory.
</summary>
    <author>
      <name>Jerzy Szynka</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.06052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.06052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68R01" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2210.12193v1</id>
    <updated>2022-10-21T18:43:18Z</updated>
    <published>2022-10-21T18:43:18Z</published>
    <title>A Trainable Sequence Learner that Learns and Recognizes Two-Input
  Sequence Patterns</title>
    <summary>  We present two designs for an analog circuit that can learn to detect a
temporal sequence of two inputs. The training phase is done by feeding the
circuit with the desired sequence and, after the training is completed, each
time the trained sequence is encountered again the circuit will emit a signal
of correct recognition. Sequences are in the order of tens of nanoseconds. The
first design can reset the trained sequence on runtime but assumes very strict
timing of the inputs. The second design can only be trained once but is lenient
in the input's timing.
</summary>
    <author>
      <name>Jan Hohenheim</name>
    </author>
    <author>
      <name>Zhaoyu Devon Liu</name>
    </author>
    <author>
      <name>Tommaso Stecconi</name>
    </author>
    <author>
      <name>Pietro Palopoli</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TENCON55691.2022.9977663</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TENCON55691.2022.9977663" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE TENCON 2022</arxiv:comment>
    <link href="http://arxiv.org/abs/2210.12193v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2210.12193v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.04526v1</id>
    <updated>2022-11-08T19:52:13Z</updated>
    <published>2022-11-08T19:52:13Z</published>
    <title>Disclosure of a Neuromorphic Starter Kit</title>
    <summary>  This paper presents a Neuromorphic Starter Kit, which has been designed to
help a variety of research groups perform research, exploration and real-world
demonstrations of brain-based, neuromorphic processors and hardware
environments. A prototype kit has been built and tested. We explain the
motivation behind the kit, its design and composition, and a prototype physical
demonstration.
</summary>
    <author>
      <name>James S. Plank</name>
    </author>
    <author>
      <name>Bryson Gullett</name>
    </author>
    <author>
      <name>Adam Z. Foshie</name>
    </author>
    <author>
      <name>Garrett S. Rose</name>
    </author>
    <author>
      <name>Catherine D. Schuman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.04526v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.04526v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.08018v1</id>
    <updated>2022-11-15T10:03:45Z</updated>
    <published>2022-11-15T10:03:45Z</published>
    <title>Universal Time-Uniform Trajectory Approximation for Random Dynamical
  Systems with Recurrent Neural Networks</title>
    <summary>  The capability of recurrent neural networks to approximate trajectories of a
random dynamical system, with random inputs, on non-compact domains, and over
an indefinite or infinite time horizon is considered. The main result states
that certain random trajectories over an infinite time horizon may be
approximated to any desired accuracy, uniformly in time, by a certain class of
deep recurrent neural networks, with simple feedback structures. The
formulation here contrasts with related literature on this topic, much of which
is restricted to compact state spaces and finite time intervals. The model
conditions required here are natural, mild, and easy to test, and the proof is
very simple.
</summary>
    <author>
      <name>Adrian N. Bishop</name>
    </author>
    <link href="http://arxiv.org/abs/2211.08018v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.08018v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.08407v4</id>
    <updated>2023-05-10T10:28:53Z</updated>
    <published>2022-10-27T13:37:50Z</published>
    <title>Trust-Awareness to Secure Swarm Intelligence from Data Injection Attack</title>
    <summary>  Enabled by the emerging industrial agent (IA) technology, swarm intelligence
(SI) is envisaged to play an important role in future industrial Internet of
Things (IIoT) that is shaped by Sixth Generation (6G) mobile communications and
digital twin (DT). However, its fragility against data injection attack may
halt it from practical deployment. In this paper we propose an efficient trust
approach to address this security concern for SI.
</summary>
    <author>
      <name>Bin Han</name>
    </author>
    <author>
      <name>Dennis Krummacker</name>
    </author>
    <author>
      <name>Qiuheng Zhou</name>
    </author>
    <author>
      <name>Hans D. Schotten</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted and to be presented at IEEE ICC 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.08407v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.08407v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.08408v1</id>
    <updated>2022-10-27T06:08:06Z</updated>
    <published>2022-10-27T06:08:06Z</published>
    <title>On the biological plausibility of orthogonal initialisation for solving
  gradient instability in deep neural networks</title>
    <summary>  Initialising the synaptic weights of artificial neural networks (ANNs) with
orthogonal matrices is known to alleviate vanishing and exploding gradient
problems. A major objection against such initialisation schemes is that they
are deemed biologically implausible as they mandate factorization techniques
that are difficult to attribute to a neurobiological process. This paper
presents two initialisation schemes that allow a network to naturally evolve
its weights to form orthogonal matrices, provides theoretical analysis that
pre-training orthogonalisation always converges, and empirically confirms that
the proposed schemes outperform randomly initialised recurrent and feedforward
networks.
</summary>
    <author>
      <name>Nikolay Manchev</name>
    </author>
    <author>
      <name>Michael Spratling</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ISCMI56532.2022.10068489</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ISCMI56532.2022.10068489" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 3 figures, to be published in ISCMI2022 conference
  proceedings</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.08408v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.08408v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.11937v2</id>
    <updated>2023-01-19T23:21:53Z</updated>
    <published>2022-11-22T01:16:13Z</published>
    <title>Genetic Algorithm for Program Synthesis</title>
    <summary>  A deductive program synthesis tool takes a specification as input and derives
a program that satisfies the specification. The drawback of this approach is
that search spaces for such correct programs tend to be enormous, making it
difficult to derive correct programs within a realistic timeout. To speed up
such program derivation, we improve the search strategy of a deductive program
synthesis tool, SuSLik, using evolutionary computation. Our cross-validation
shows that the improvement brought by evolutionary computation generalises to
unforeseen problems.
</summary>
    <author>
      <name>Yutaka Nagashima</name>
    </author>
    <link href="http://arxiv.org/abs/2211.11937v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.11937v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.PL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.15156v1</id>
    <updated>2022-11-28T09:09:07Z</updated>
    <published>2022-11-28T09:09:07Z</published>
    <title>Matrix representations of spiking neural P systems: Revisited</title>
    <summary>  In the 2010, matrix representation of SN P system without delay was presented
while in the case of SN P systems with delay, matrix representation was
suggested in the 2017. These representations brought about series of simulation
of SN P systems using computer software and hardware technology. In this work,
we revisit these representation and provide some observations on the behavior
of the computations of SN P systems. The concept of reachability of
configuration is considered in both SN P systems with and without delays. A
better computation of next configuration is proposed in the case of SN P system
with delay.
</summary>
    <author>
      <name>Henry N. Adorna</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In: Gheorghe Paun (Ed) Proceedings of the 20th International
  Conference on Membrane Computing (CMC20), Editura Bibliostar, Ramnicu Valcea
  (2019) pp 227-247</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.15156v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.15156v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.15387v2</id>
    <updated>2023-03-21T19:47:52Z</updated>
    <published>2022-11-24T10:02:38Z</published>
    <title>AIREPAIR: A Repair Platform for Neural Networks</title>
    <summary>  We present AIREPAIR, a platform for repairing neural networks. It features
the integration of existing network repair tools. Based on AIREPAIR, one can
run different repair methods on the same model, thus enabling the fair
comparison of different repair techniques. We evaluate AIREPAIR with three
state-of-the-art repair tools on popular deep-learning datasets and models. Our
evaluation confirms the utility of AIREPAIR, by comparing and analyzing the
results from different repair techniques. A demonstration is available at
https://youtu.be/UkKw5neeWhw.
</summary>
    <author>
      <name>Xidan Song</name>
    </author>
    <author>
      <name>Youcheng Sun</name>
    </author>
    <author>
      <name>Mustafa A. Mustafa</name>
    </author>
    <author>
      <name>Lucas Cordeiro</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.48550/arXiv.2211.15387</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.48550/arXiv.2211.15387" rel="related"/>
    <link href="http://arxiv.org/abs/2211.15387v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.15387v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.15416v1</id>
    <updated>2022-11-12T17:50:39Z</updated>
    <published>2022-11-12T17:50:39Z</published>
    <title>Development of a Neural Network-Based Mathematical Operation Protocol
  for Embedded Hexadecimal Digits Using Neural Architecture Search (NAS)</title>
    <summary>  It is beneficial to develop an efficient machine-learning based method for
addition using embedded hexadecimal digits. Through a comparison between
human-developed machine learning model and models sampled through Neural
Architecture Search (NAS) we determine an efficient approach to solve this
problem with a final testing loss of 0.2937 for a human-developed model.
</summary>
    <author>
      <name>Victor Robila</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Hunter College High School</arxiv:affiliation>
    </author>
    <author>
      <name>Kexin Pei</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Columbia University</arxiv:affiliation>
    </author>
    <author>
      <name>Junfeng Yang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Columbia University</arxiv:affiliation>
    </author>
    <link href="http://arxiv.org/abs/2211.15416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.15416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; D.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.16978v1</id>
    <updated>2022-10-20T18:41:57Z</updated>
    <published>2022-10-20T18:41:57Z</published>
    <title>Combining Neuro-Evolution of Augmenting Topologies with Convolutional
  Neural Networks</title>
    <summary>  Current deep convolutional networks are fixed in their topology. We explore
the possibilites of making the convolutional topology a parameter itself by
combining NeuroEvolution of Augmenting Topologies (NEAT) with Convolutional
Neural Networks (CNNs) and propose such a system using blocks of Residual
Networks (ResNets). We then explain how our suggested system can only be built
once additional optimizations have been made, as genetic algorithms are way
more demanding than training per backpropagation. On the way there we explain
most of those buzzwords and offer a gentle and brief introduction to the most
important modern areas of machine learning
</summary>
    <author>
      <name>Jan Hohenheim</name>
    </author>
    <author>
      <name>Mathias Fischler</name>
    </author>
    <author>
      <name>Sara Zarubica</name>
    </author>
    <author>
      <name>Jeremy Stucki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Unpublished</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.16978v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.16978v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.17234v4</id>
    <updated>2023-01-25T13:32:53Z</updated>
    <published>2022-11-30T18:36:42Z</published>
    <title>Genetic Programming with Local Scoring</title>
    <summary>  We present new techniques for synthesizing programs through sequences of
mutations. Among these are (1) a method of local scoring assigning a score to
each expression in a program, allowing us to more precisely identify buggy
code, (2) suppose-expressions which act as an intermediate step to evolving
if-conditionals, and (3) cyclic evolution in which we evolve programs through
phases of expansion and reduction. To demonstrate their merits, we provide a
basic proof-of-concept implementation which we show evolves correct code for
several functions manipulating integers and lists, including some that are
intractable by means of existing Genetic Programming techniques.
</summary>
    <author>
      <name>Max Vistrup</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Minor improvements</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.17234v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.17234v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.02610v1</id>
    <updated>2023-01-06T17:14:11Z</updated>
    <published>2023-01-06T17:14:11Z</published>
    <title>Feedback-Gated Rectified Linear Units</title>
    <summary>  Feedback connections play a prominent role in the human brain but have not
received much attention in artificial neural network research. Here, a
biologically inspired feedback mechanism which gates rectified linear units is
proposed. On the MNIST dataset, autoencoders with feedback show faster
convergence, better performance, and more robustness to noise compared to their
counterparts without feedback. Some benefits, although less pronounced and less
consistent, can be observed when networks with feedback are applied on the
CIFAR-10 dataset.
</summary>
    <author>
      <name>Marco Kemmerling</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 26 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.02610v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.02610v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.02764v3</id>
    <updated>2023-02-01T01:52:40Z</updated>
    <published>2023-01-07T01:57:10Z</published>
    <title>Mathematical Models and Reinforcement Learning based Evolutionary
  Algorithm Framework for Satellite Scheduling Problem</title>
    <summary>  For complex combinatorial optimization problems, models and algorithms are at
the heart of the solution. The complexity of many types of satellite mission
planning problems is NP-hard and places high demands on the solution. In this
paper, two types of satellite scheduling problem models are introduced and a
reinforcement learning based evolutionary algorithm framework based is
proposed.
</summary>
    <author>
      <name>Yanjie Song</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages. arXiv admin note: substantial text overlap with
  arXiv:2206.05694</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.02764v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.02764v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.04126v1</id>
    <updated>2022-12-13T23:59:21Z</updated>
    <published>2022-12-13T23:59:21Z</published>
    <title>Temporal Weights</title>
    <summary>  In artificial neural networks, weights are a static representation of
synapses. However, synapses are not static, they have their own interacting
dynamics over time. To instill weights with interacting dynamics, we use a
model describing synchronization that is capable of capturing core mechanisms
of a range of neural and general biological phenomena over time. An ideal fit
for these Temporal Weights (TW) are Neural ODEs, with continuous dynamics and a
dependency on time. The resulting recurrent neural networks efficiently model
temporal dynamics by computing on the ordering of sequences, and the length and
scale of time. By adding temporal weights to a model, we demonstrate better
performance, smaller models, and data efficiency on sparse, irregularly sampled
time series datasets.
</summary>
    <author>
      <name>Adam Kohan</name>
    </author>
    <author>
      <name>Ed Rietman</name>
    </author>
    <author>
      <name>Hava Siegelmann</name>
    </author>
    <link href="http://arxiv.org/abs/2301.04126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.04126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2302.13210v1</id>
    <updated>2023-02-26T02:26:45Z</updated>
    <published>2023-02-26T02:26:45Z</published>
    <title>AutoML for neuromorphic computing and application-driven co-design:
  asynchronous, massively parallel optimization of spiking architectures</title>
    <summary>  In this work we have extended AutoML inspired approaches to the exploration
and optimization of neuromorphic architectures. Through the integration of a
parallel asynchronous model-based search approach with a simulation framework
to simulate spiking architectures, we are able to efficiently explore the
configuration space of neuromorphic architectures and identify the subset of
conditions leading to the highest performance in a targeted application. We
have demonstrated this approach on an exemplar case of real time, on-chip
learning application. Our results indicate that we can effectively use
optimization approaches to optimize complex architectures, therefore providing
a viable pathway towards application-driven codesign.
</summary>
    <author>
      <name>Angel Yanguas-Gil</name>
    </author>
    <author>
      <name>Sandeep Madireddy</name>
    </author>
    <link href="http://arxiv.org/abs/2302.13210v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2302.13210v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.00066v1</id>
    <updated>2023-02-28T20:09:12Z</updated>
    <published>2023-02-28T20:09:12Z</published>
    <title>Hyperdimensional Computing with Spiking-Phasor Neurons</title>
    <summary>  Vector Symbolic Architectures (VSAs) are a powerful framework for
representing compositional reasoning. They lend themselves to neural-network
implementations, allowing us to create neural networks that can perform
cognitive functions, like spatial reasoning, arithmetic, symbol binding, and
logic. But the vectors involved can be quite large, hence the alternative label
Hyperdimensional (HD) computing. Advances in neuromorphic hardware hold the
promise of reducing the running time and energy footprint of neural networks by
orders of magnitude. In this paper, we extend some pioneering work to run VSA
algorithms on a substrate of spiking neurons that could be run efficiently on
neuromorphic hardware.
</summary>
    <author>
      <name>Jeff Orchard</name>
    </author>
    <author>
      <name>Russell Jarvis</name>
    </author>
    <link href="http://arxiv.org/abs/2303.00066v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.00066v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.08109v2</id>
    <updated>2023-03-31T21:54:06Z</updated>
    <published>2023-03-14T17:44:23Z</published>
    <title>Vision-based route following by an embodied insect-inspired sparse
  neural network</title>
    <summary>  We compared the efficiency of the FlyHash model, an insect-inspired sparse
neural network (Dasgupta et al., 2017), to similar but non-sparse models in an
embodied navigation task. This requires a model to control steering by
comparing current visual inputs to memories stored along a training route. We
concluded the FlyHash model is more efficient than others, especially in terms
of data encoding.
</summary>
    <author>
      <name>Lu Yihe</name>
    </author>
    <author>
      <name>Rana Alkhoury Maroun</name>
    </author>
    <author>
      <name>Barbara Webb</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures; work-in-progress submission, accepted as a poster
  at ICLR 2023 Workshop on Sparsity in Neural Networks; non-archival</arxiv:comment>
    <link href="http://arxiv.org/abs/2303.08109v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.08109v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.10761v1</id>
    <updated>2023-03-19T20:27:51Z</updated>
    <published>2023-03-19T20:27:51Z</published>
    <title>Calibration of Neural Networks</title>
    <summary>  Neural networks solving real-world problems are often required not only to
make accurate predictions but also to provide a confidence level in the
forecast. The calibration of a model indicates how close the estimated
confidence is to the true probability. This paper presents a survey of
confidence calibration problems in the context of neural networks and provides
an empirical comparison of calibration methods. We analyze problem statement,
calibration definitions, and different approaches to evaluation: visualizations
and scalar measures that estimate whether the model is well-calibrated. We
review modern calibration techniques: based on post-processing or requiring
changes in training. Empirical experiments cover various datasets and models,
comparing calibration methods according to different criteria.
</summary>
    <author>
      <name>Ruslan Vasilev</name>
    </author>
    <author>
      <name>Alexander D'yakonov</name>
    </author>
    <link href="http://arxiv.org/abs/2303.10761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.10761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.10780v2</id>
    <updated>2023-03-21T16:48:53Z</updated>
    <published>2023-03-19T22:07:27Z</published>
    <title>A Comprehensive Review of Spiking Neural Networks: Interpretation,
  Optimization, Efficiency, and Best Practices</title>
    <summary>  Biological neural networks continue to inspire breakthroughs in neural
network performance. And yet, one key area of neural computation that has been
under-appreciated and under-investigated is biologically plausible,
energy-efficient spiking neural networks, whose potential is especially
attractive for low-power, mobile, or otherwise hardware-constrained settings.
We present a literature review of recent developments in the interpretation,
optimization, efficiency, and accuracy of spiking neural networks. Key
contributions include identification, discussion, and comparison of
cutting-edge methods in spiking neural network optimization, energy-efficiency,
and evaluation, starting from first principles so as to be accessible to new
practitioners.
</summary>
    <author>
      <name>Kai Malcolm</name>
    </author>
    <author>
      <name>Josue Casco-Rodriguez</name>
    </author>
    <link href="http://arxiv.org/abs/2303.10780v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.10780v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2303.14592v1</id>
    <updated>2023-03-26T00:23:29Z</updated>
    <published>2023-03-26T00:23:29Z</published>
    <title>Exploring Novel Quality Diversity Methods For Generalization in
  Reinforcement Learning</title>
    <summary>  The Reinforcement Learning field is strong on achievements and weak on
reapplication; a computer playing GO at a super-human level is still terrible
at Tic-Tac-Toe. This paper asks whether the method of training networks
improves their generalization. Specifically we explore core quality diversity
algorithms, compare against two recent algorithms, and propose a new algorithm
to deal with shortcomings in existing methods. Although results of these
methods are well below the performance hoped for, our work raises important
points about the choice of behavior criterion in quality diversity, the
interaction of differential and evolutionary training methods, and the role of
offline reinforcement learning and randomized learning in evolutionary search.
</summary>
    <author>
      <name>Brad Windsor</name>
    </author>
    <author>
      <name>Brandon O'Shea</name>
    </author>
    <author>
      <name>Mengxi Wu</name>
    </author>
    <link href="http://arxiv.org/abs/2303.14592v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2303.14592v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.04207v1</id>
    <updated>2023-04-09T10:23:30Z</updated>
    <published>2023-04-09T10:23:30Z</published>
    <title>Scalable Multiple Patterning Layout Decomposition Implemented by a
  Distribution Evolutionary Algorithm</title>
    <summary>  As the feature size of semiconductor technology shrinks to 10 nm and beyond,
the multiple patterning lithography (MPL) attracts more attention from the
industry. In this paper, we model the layout decomposition of MPL as a
generalized graph coloring problem, which is addressed by a distribution
evolutionary algorithm based on a population of probabilistic model (DEA-PPM).
DEA-PPM can strike a balance between decomposition results and running time,
being scalable for varied settings of mask number and lithography resolution.
Due to its robustness of decomposition results, this could be an alternative
technique for multiple patterning layout decomposition in next-generation
technology nodes.
</summary>
    <author>
      <name>Yu Chen</name>
    </author>
    <author>
      <name>Yongjian Xu</name>
    </author>
    <author>
      <name>Ning Xu</name>
    </author>
    <link href="http://arxiv.org/abs/2304.04207v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.04207v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.06487v1</id>
    <updated>2023-04-11T15:15:04Z</updated>
    <published>2023-04-11T15:15:04Z</published>
    <title>Recurrent Neural Networks as Electrical Networks, a formalization</title>
    <summary>  Since the 1980s, and particularly with the Hopfield model, recurrent neural
networks or RNN became a topic of great interest. The first works of neural
networks consisted of simple systems of a few neurons that were commonly
simulated through analogue electronic circuits. The passage from the equations
to the circuits was carried out directly without justification and subsequent
formalisation. The present work shows a way to formally obtain the equivalence
between an analogue circuit and a neural network and formalizes the connection
between both systems. We also show which are the properties that these
electrical networks must satisfy. We can have confidence that the
representation in terms of circuits is mathematically equivalent to the
equations that represent the network.
</summary>
    <author>
      <name>Mariano Caruso</name>
    </author>
    <author>
      <name>Cecilia Jarne</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-031-23210-7_10</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-031-23210-7_10" rel="related"/>
    <link href="http://arxiv.org/abs/2304.06487v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.06487v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.09540v2</id>
    <updated>2023-07-11T18:46:27Z</updated>
    <published>2023-04-19T10:11:29Z</published>
    <title>Learning Hierarchically-Structured Concepts II: Overlapping Concepts,
  and Networks With Feedback</title>
    <summary>  We continue our study from Lynch and Mallmann-Trenn (Neural Networks, 2021),
of how concepts that have hierarchical structure might be represented in
brain-like neural networks, how these representations might be used to
recognize the concepts, and how these representations might be learned.
  In Lynch and Mallmann-Trenn (Neural Networks, 2021), we considered simple
tree-structured concepts and feed-forward layered networks. Here we extend the
model in two ways: we allow limited overlap between children of different
concepts, and we allow networks to include feedback edges.
  For these more general cases, we describe and analyze algorithms for
recognition and algorithms for learning.
</summary>
    <author>
      <name>Nancy Lynch</name>
    </author>
    <author>
      <name>Frederik Mallmann-Trenn</name>
    </author>
    <link href="http://arxiv.org/abs/2304.09540v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.09540v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.13524v1</id>
    <updated>2023-03-31T08:56:54Z</updated>
    <published>2023-03-31T08:56:54Z</published>
    <title>Towards replicated algorithms</title>
    <summary>  The main deficiency of the algorithms running on digital computers nowadays
is their inability to change themselves during the execution. In line with
this, the paper introduces the so-called replicated algorithms, inspired by the
concept of developing a human brain. Similar to the human brain, where the
process of thinking is strongly parallel, replicated algorithms, incorporated
into a population, are also capable of replicating themselves and solving
problems in parallel. They operate as a model for mapping the known input to a
known output. In our preliminary study, these algorithms are built as sequences
of arithmetic operators, applied for calculating arithmetic expressions, while
their behavior showed that they can operate in the condition of open-ended
evolution.
</summary>
    <author>
      <name>Iztok Fister Jr.</name>
    </author>
    <author>
      <name>Iztok Fister</name>
    </author>
    <link href="http://arxiv.org/abs/2304.13524v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.13524v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.13537v1</id>
    <updated>2023-03-17T06:43:51Z</updated>
    <published>2023-03-17T06:43:51Z</published>
    <title>A Two-Step Rule for Backpropagation</title>
    <summary>  We present a simplified computational rule for the back-propagation formulas
for artificial neural networks. In this work, we provide a generic two-step
rule for the back-propagation algorithm in matrix notation. Moreover, this rule
incorporates both the forward and backward phases of the computations involved
in the learning process. Specifically, this recursive computing rule permits
the propagation of the changes to all synaptic weights in the network, layer by
layer, efficiently. In particular, we use this rule to compute both the up and
down partial derivatives of the cost function of all the connections feeding
into the output layer.
</summary>
    <author>
      <name>Ahmed Boughammoura</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.53508/ijiam.1265832</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.53508/ijiam.1265832" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Informatics and Applied Mathematics 6
  (2023), 57-69</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2304.13537v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.13537v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2304.13820v2</id>
    <updated>2023-04-30T07:35:18Z</updated>
    <published>2023-03-29T06:21:53Z</published>
    <title>Backpropagation and F-adjoint</title>
    <summary>  This paper presents a concise mathematical framework for investigating both
feed-forward and backward process, during the training to learn model weights,
of an artificial neural network (ANN). Inspired from the idea of the two-step
rule for backpropagation, we define a notion of F-adjoint which is aimed at a
better description of the backpropagation algorithm. In particular, by
introducing the notions of F-propagation and F-adjoint through a deep neural
network architecture, the backpropagation associated to a cost/loss function is
proven to be completely characterized by the F-adjoint of the corresponding
F-propagation relatively to the partial derivative, with respect to the inputs,
of the cost function.
</summary>
    <author>
      <name>Ahmed Boughammoura</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2304.13537</arxiv:comment>
    <link href="http://arxiv.org/abs/2304.13820v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2304.13820v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.01264v2</id>
    <updated>2023-07-17T17:13:24Z</updated>
    <published>2023-05-02T09:01:07Z</published>
    <title>Multi-Task Multi-Behavior MAP-Elites</title>
    <summary>  We propose Multi-Task Multi-Behavior MAP-Elites, a variant of MAP-Elites that
finds a large number of high-quality solutions for a large set of tasks
(optimization problems from a given family). It combines the original
MAP-Elites for the search for diversity and Multi-Task MAP-Elites for
leveraging similarity between tasks. It performs better than three baselines on
a humanoid fault-recovery set of tasks, solving more tasks and finding twice as
many solutions per solved task.
</summary>
    <author>
      <name> Anne</name>
    </author>
    <author>
      <name> Mouret</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3583133.3590730</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3583133.3590730" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted as Poster for GECCO 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.01264v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.01264v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.08012v2</id>
    <updated>2024-02-08T15:26:33Z</updated>
    <published>2023-05-13T21:43:09Z</published>
    <title>Quantization in Spiking Neural Networks</title>
    <summary>  In spiking neural networks (SNN), at each node, an incoming sequence of
weighted Dirac pulses is converted into an output sequence of weighted Dirac
pulses by a leaky-integrate-and-fire (LIF) neuron model based on spike
aggregation and thresholding. We show that this mapping can be understood as a
quantization operator and state a corresponding formula for the quantization
error by means of the Alexiewicz norm. This analysis has implications for
rethinking re-initialization in the LIF model, leading to the proposal of
'reset-to-mod' as a modulo-based reset variant.
</summary>
    <author>
      <name>Bernhard A. Moser</name>
    </author>
    <author>
      <name>Michael Lunglmayr</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2305.05772</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.08012v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.08012v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="82C32, 41A65" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.08753v1</id>
    <updated>2023-05-15T16:05:58Z</updated>
    <published>2023-05-15T16:05:58Z</published>
    <title>Neural Oscillators are Universal</title>
    <summary>  Coupled oscillators are being increasingly used as the basis of machine
learning (ML) architectures, for instance in sequence modeling, graph
representation learning and in physical neural networks that are used in analog
ML devices. We introduce an abstract class of neural oscillators that
encompasses these architectures and prove that neural oscillators are
universal, i.e, they can approximate any continuous and casual operator mapping
between time-varying functions, to desired accuracy. This universality result
provides theoretical justification for the use of oscillator based ML systems.
The proof builds on a fundamental result of independent interest, which shows
that a combination of forced harmonic oscillators with a nonlinear read-out
suffices to approximate the underlying operators.
</summary>
    <author>
      <name>Samuel Lanthaler</name>
    </author>
    <author>
      <name>T. Konstantin Rusch</name>
    </author>
    <author>
      <name>Siddhartha Mishra</name>
    </author>
    <link href="http://arxiv.org/abs/2305.08753v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.08753v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.09257v1</id>
    <updated>2023-05-16T08:06:02Z</updated>
    <published>2023-05-16T08:06:02Z</published>
    <title>A new node-shift encoding representation for the travelling salesman
  problem</title>
    <summary>  This paper presents a new genetic algorithm encoding representation to solve
the travelling salesman problem. To assess the performance of the proposed
chromosome structure, we compare it with state-of-the-art encoding
representations. For that purpose, we use 14 benchmarks of different sizes
taken from TSPLIB. Finally, after conducting the experimental study, we report
the obtained results and draw our conclusion.
</summary>
    <author>
      <name>Menouar Boulif</name>
    </author>
    <author>
      <name>Aghiles Gharbi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 5 figures. Accepted in ICL2022, Jeddah, Saudi Arabia
  conference (postponed to 2024)</arxiv:comment>
    <link href="http://arxiv.org/abs/2305.09257v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.09257v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90c59" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.12221v1</id>
    <updated>2023-05-20T16:04:06Z</updated>
    <published>2023-05-20T16:04:06Z</published>
    <title>Patterns of Convergence and Bound Constraint Violation in Differential
  Evolution on SBOX-COST Benchmarking Suite</title>
    <summary>  This study investigates the influence of several bound constraint handling
methods (BCHMs) on the search process specific to Differential Evolution (DE),
with a focus on identifying similarities between BCHMs and grouping patterns
with respect to the number of cases when a BCHM is activated. The empirical
analysis is conducted on the SBOX-COST benchmarking test suite, where bound
constraints are enforced on the problem domain. This analysis provides some
insights that might be useful in designing adaptive strategies for handling
such constraints.
</summary>
    <author>
      <name>Mădălina-Andreea Mitran</name>
    </author>
    <author>
      <name>Anna V. Kononova</name>
    </author>
    <author>
      <name>Fabio Caraffini</name>
    </author>
    <author>
      <name>Daniela Zaharie</name>
    </author>
    <link href="http://arxiv.org/abs/2305.12221v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.12221v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2305.13043v1</id>
    <updated>2023-05-22T13:48:46Z</updated>
    <published>2023-05-22T13:48:46Z</published>
    <title>Self-Replication, Spontaneous Mutations, and Exponential Genetic Drift
  in Neural Cellular Automata</title>
    <summary>  This paper reports on patterns exhibiting self-replication with spontaneous,
inheritable mutations and exponential genetic drift in Neural Cellular
Automata. Despite the models not being explicitly trained for mutation or
inheritability, the descendant patterns exponentially drift away from ancestral
patterns, even when the automaton is deterministic. While this is far from
being the first instance of evolutionary dynamics in a cellular automaton, it
is the first to do so by exploiting the power and convenience of Neural
Cellular Automata, arguably increasing the space of variations and the
opportunity for Open Ended Evolution.
</summary>
    <author>
      <name>Lana Sinapayen</name>
    </author>
    <link href="http://arxiv.org/abs/2305.13043v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2305.13043v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.06812v1</id>
    <updated>2023-06-12T01:06:22Z</updated>
    <published>2023-06-12T01:06:22Z</published>
    <title>Particularity</title>
    <summary>  We describe a design principle for adaptive systems under which adaptation is
driven by particular challenges that the environment poses, as opposed to
average or otherwise aggregated measures of performance over many challenges.
We trace the development of this "particularity" approach from the use of
lexicase selection in genetic programming to "particularist" approaches to
other forms of machine learning and to the design of adaptive systems more
generally.
</summary>
    <author>
      <name>Lee Spector</name>
    </author>
    <author>
      <name>Li Ding</name>
    </author>
    <author>
      <name>Ryan Boldi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Genetic Programming Theory and Practice XX</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.06812v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.06812v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.2; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.07416v1</id>
    <updated>2023-06-12T20:47:59Z</updated>
    <published>2023-06-12T20:47:59Z</published>
    <title>Synaptic Scaling and Optimal Bias Adjustments for Power Reduction in
  Neuromorphic Systems</title>
    <summary>  Recent animal studies have shown that biological brains can enter a low power
mode in times of food scarcity. This paper explores the possibility of applying
similar mechanisms to a broad class of neuromorphic systems where power
consumption is strongly dependent on the magnitude of synaptic weights. In
particular, we show through mathematical models and simulations that careful
scaling of synaptic weights can significantly reduce power consumption (by over
80\% in some of the cases tested) while having a relatively small impact on
accuracy. These results uncover an exciting opportunity to design neuromorphic
systems for edge AI applications, where power consumption can be dynamically
adjusted based on energy availability and performance requirements.
</summary>
    <author>
      <name>Cory Merkel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in MWSCAS</arxiv:comment>
    <link href="http://arxiv.org/abs/2306.07416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.07416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.09961v1</id>
    <updated>2023-06-16T16:44:14Z</updated>
    <published>2023-06-16T16:44:14Z</published>
    <title>The Evolution theory of Learning: From Natural Selection to
  Reinforcement Learning</title>
    <summary>  Evolution is a fundamental process that shapes the biological world we
inhabit, and reinforcement learning is a powerful tool used in artificial
intelligence to develop intelligent agents that learn from their environment.
In recent years, researchers have explored the connections between these two
seemingly distinct fields, and have found compelling evidence that they are
more closely related than previously thought. This paper examines these
connections and their implications, highlighting the potential for
reinforcement learning principles to enhance our understanding of evolution and
the role of feedback in evolutionary systems.
</summary>
    <author>
      <name>Taboubi Ahmed</name>
    </author>
    <link href="http://arxiv.org/abs/2306.09961v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.09961v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.bio-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2306.11761v1</id>
    <updated>2023-06-20T09:03:52Z</updated>
    <published>2023-06-20T09:03:52Z</published>
    <title>Learning and evolution: factors influencing an effective combination</title>
    <summary>  The mutual relationship between evolution and learning is a controversial
argument among the artificial intelligence and neuro-evolution communities.
After more than three decades, there is still no common agreement on the
matter. In this paper the author investigates whether combining learning and
evolution permits to find better solutions than those discovered by evolution
alone. More specifically, the author presents a series of empirical studies
that highlight some specific conditions determining the success of such a
combination, like the introduction of noise during the learning and selection
processes. Results are obtained in two qualitatively different domains, where
agent/environment interactions are minimal or absent.
</summary>
    <author>
      <name>Paolo Pagliuca</name>
    </author>
    <link href="http://arxiv.org/abs/2306.11761v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2306.11761v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.12713v2</id>
    <updated>2024-06-18T07:56:18Z</updated>
    <published>2023-07-24T11:49:10Z</published>
    <title>Formal description of ML models for unambiguous implementation</title>
    <summary>  Implementing deep neural networks in safety critical systems, in particular
in the aeronautical domain, will require to offer adequate specification
paradigms to preserve the semantics of the trained model on the final hardware
platform. We propose to extend the nnef language in order to allow traceable
distribution and parallelisation optimizations of a trained model. We show how
such a specification can be implemented in cuda on a Xavier platform.
</summary>
    <author>
      <name>Adrien Gauffriau</name>
    </author>
    <author>
      <name>Iryna De Albuquerque Silva</name>
    </author>
    <author>
      <name>Claire Pagetti</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">12th European Congress on Embedded Real Time Software and Systems
  (ERTS 2024), Jun 2024, Toulouse, France</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2307.12713v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.12713v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2307.15232v1</id>
    <updated>2023-07-27T23:43:32Z</updated>
    <published>2023-07-27T23:43:32Z</published>
    <title>Functional Specification of the RAVENS Neuroprocessor</title>
    <summary>  RAVENS is a neuroprocessor that has been developed by the TENNLab research
group at the University of Tennessee. Its main focus has been as a vehicle for
chip design with memristive elements; however it has also been the vehicle for
all-digital CMOS development, plus it has implementations on FPGA's,
microcontrollers and software simulation. The software simulation is supported
by the TENNLab neuromorphic software framework so that researchers may develop
RAVENS solutions for a variety of neuromorphic computing applications. This
document provides a functional specification of RAVENS that should apply to all
implementations of the RAVENS neuroprocessor.
</summary>
    <author>
      <name>Adam Z. Foshie</name>
    </author>
    <author>
      <name>James S. Plank</name>
    </author>
    <author>
      <name>Garrett S. Rose</name>
    </author>
    <author>
      <name>Catherine D. Schuman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 11 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2307.15232v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2307.15232v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.00677v2</id>
    <updated>2023-11-05T19:22:12Z</updated>
    <published>2023-07-29T16:47:03Z</published>
    <title>Discrete neural nets and polymorphic learning</title>
    <summary>  Theorems from universal algebra such as that of Murski\u{i} from the 1970s
have a striking similarity to universal approximation results for neural nets
along the lines of Cybenko's from the 1980s. We consider here a discrete
analogue of the classical notion of a neural net which places these results in
a unified setting. We introduce a learning algorithm based on polymorphisms of
relational structures and show how to use it for a classical learning task.
</summary>
    <author>
      <name>Charlotte Aten</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Version 2 includes the figures which were missing in the first
  version</arxiv:comment>
    <link href="http://arxiv.org/abs/2308.00677v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.00677v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.RA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T07, 08A70, 05C60" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2308.08053v1</id>
    <updated>2023-08-15T21:43:11Z</updated>
    <published>2023-08-15T21:43:11Z</published>
    <title>Natural Evolution Strategies as a Black Box Estimator for Stochastic
  Variational Inference</title>
    <summary>  Stochastic variational inference and its derivatives in the form of
variational autoencoders enjoy the ability to perform Bayesian inference on
large datasets in an efficient manner. However, performing inference with a VAE
requires a certain design choice (i.e. reparameterization trick) to allow
unbiased and low variance gradient estimation, restricting the types of models
that can be created. To overcome this challenge, an alternative estimator based
on natural evolution strategies is proposed. This estimator does not make
assumptions about the kind of distributions used, allowing for the creation of
models that would otherwise not have been possible under the VAE framework.
</summary>
    <author>
      <name>Ahmad Ayaz Amin</name>
    </author>
    <link href="http://arxiv.org/abs/2308.08053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2308.08053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.00149v1</id>
    <updated>2023-08-31T21:50:23Z</updated>
    <published>2023-08-31T21:50:23Z</published>
    <title>TurboGP: A flexible and advanced python based GP library</title>
    <summary>  We introduce TurboGP, a Genetic Programming (GP) library fully written in
Python and specifically designed for machine learning tasks. TurboGP implements
modern features not available in other GP implementations, such as island and
cellular population schemes, different types of genetic operations (migration,
protected crossovers), online learning, among other features. TurboGP's most
distinctive characteristic is its native support for different types of GP
nodes to allow different abstraction levels, this makes TurboGP particularly
useful for processing a wide variety of data sources.
</summary>
    <author>
      <name>Lino Rodriguez-Coayahuitl</name>
    </author>
    <author>
      <name>Alicia Morales-Reyes</name>
    </author>
    <author>
      <name>Hugo Jair Escalante</name>
    </author>
    <link href="http://arxiv.org/abs/2309.00149v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.00149v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.08449v1</id>
    <updated>2023-09-15T14:53:07Z</updated>
    <published>2023-09-15T14:53:07Z</published>
    <title>Do Random and Chaotic Sequences Really Cause Different PSO Performance?
  Further Results</title>
    <summary>  Empirical results show that PSO performance may be different if using either
chaotic or random sequences to drive the algorithm's search dynamics. We
analyze the phenomenon by evaluating the performance based on a benchmark of
test functions and comparing random and chaotic sequences according to equality
or difference in underlying distribution or density. Our results show that the
underlying distribution is the main influential factor in performance and thus
the assumption of general and systematic performance differences between chaos
and random appears not plausible.
</summary>
    <author>
      <name>{Paul Moritz Nörenberg</name>
    </author>
    <author>
      <name>Hendrik Richter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: text overlap with arXiv:2303.14099</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.08449v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.08449v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.07239v1</id>
    <updated>2023-10-11T07:07:53Z</updated>
    <published>2023-10-11T07:07:53Z</published>
    <title>Multidimensional Hopfield Networks for clustering</title>
    <summary>  We present the Multidimensional Hopfield Network (DHN), a natural
generalisation of the Hopfield Network. In our theoretical investigations we
focus on DHNs with a certain activation function and provide energy functions
for them. We conclude that these DHNs are convergent in finite time, and are
equivalent to greedy methods that aim to find graph clusterings of locally
minimal cuts. We also show that the general framework of DHNs encapsulates
several previously known algorithms used for generating graph embeddings and
clusterings. Namely, the Cleora graph embedding algorithm, the Louvain method,
and the Newmans method can be cast as DHNs with appropriate activation function
and update rule. Motivated by these findings we provide a generalisation of
Newmans method to the multidimensional case.
</summary>
    <author>
      <name>Gergely Stomfai</name>
    </author>
    <author>
      <name>Łukasz Sienkiewicz</name>
    </author>
    <author>
      <name>Barbara Rychalska</name>
    </author>
    <link href="http://arxiv.org/abs/2310.07239v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.07239v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.09267v1</id>
    <updated>2023-10-13T17:25:11Z</updated>
    <published>2023-10-13T17:25:11Z</published>
    <title>Genetic algorithms are strong baselines for molecule generation</title>
    <summary>  Generating molecules, both in a directed and undirected fashion, is a huge
part of the drug discovery pipeline. Genetic algorithms (GAs) generate
molecules by randomly modifying known molecules. In this paper we show that GAs
are very strong algorithms for such tasks, outperforming many complicated
machine learning methods: a result which many researchers may find surprising.
We therefore propose insisting during peer review that new algorithms must have
some clear advantage over GAs, which we call the GA criterion. Ultimately our
work suggests that a lot of research in molecule generation should be
re-assessed.
</summary>
    <author>
      <name>Austin Tripp</name>
    </author>
    <author>
      <name>José Miguel Hernández-Lobato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Currently under review. Code will be made available at a later date</arxiv:comment>
    <link href="http://arxiv.org/abs/2310.09267v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.09267v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.QM" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2310.19067v1</id>
    <updated>2023-10-29T16:46:26Z</updated>
    <published>2023-10-29T16:46:26Z</published>
    <title>Expanding memory in recurrent spiking networks</title>
    <summary>  Recurrent spiking neural networks (RSNNs) are notoriously difficult to train
because of the vanishing gradient problem that is enhanced by the binary nature
of the spikes. In this paper, we review the ability of the current
state-of-the-art RSNNs to solve long-term memory tasks, and show that they have
strong constraints both in performance, and for their implementation on
hardware analog neuromorphic processors. We present a novel spiking neural
network that circumvents these limitations. Our biologically inspired neural
network uses synaptic delays, branching factor regularization and a novel
surrogate derivative for the spiking function. The proposed network proves to
be more successful in using the recurrent connections on memory tasks.
</summary>
    <author>
      <name>Ismael Balafrej</name>
    </author>
    <author>
      <name>Fabien Alibart</name>
    </author>
    <author>
      <name>Jean Rouat</name>
    </author>
    <link href="http://arxiv.org/abs/2310.19067v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2310.19067v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.04204v1</id>
    <updated>2023-12-07T10:40:37Z</updated>
    <published>2023-12-07T10:40:37Z</published>
    <title>Wavelength-multiplexed Delayed Inputs for Memory Enhancement of
  Microring-based Reservoir Computing</title>
    <summary>  We numerically demonstrate a silicon add-drop microring-based reservoir
computing scheme that combines parallel delayed inputs and wavelength division
multiplexing. The scheme solves memory-demanding tasks like time-series
prediction with good performance without requiring external optical feedback.
</summary>
    <author>
      <name>Bernard J. Giron Castro</name>
    </author>
    <author>
      <name>Christophe Peucheret</name>
    </author>
    <author>
      <name>Francesco Da Ros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figures. Submitted to Conference on Lasers and
  Electro-Optics (CLEO) 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.04204v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.04204v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.07579v1</id>
    <updated>2023-12-10T19:12:03Z</updated>
    <published>2023-12-10T19:12:03Z</published>
    <title>Cross Fertilizing Empathy from Brain to Machine as a Value Alignment
  Strategy</title>
    <summary>  AI Alignment research seeks to align human and AI goals to ensure independent
actions by a machine are always ethical. This paper argues empathy is necessary
for this task, despite being often neglected in favor of more deductive
approaches. We offer an inside-out approach that grounds morality within the
context of the brain as a basis for algorithmically understanding ethics and
empathy. These arguments are justified via a survey of relevant literature. The
paper concludes with a suggested experimental approach to future research and
some initial experimental observations.
</summary>
    <author>
      <name>Devin Gonier</name>
    </author>
    <author>
      <name>Adrian Adduci</name>
    </author>
    <author>
      <name>Cassidy LoCascio</name>
    </author>
    <link href="http://arxiv.org/abs/2312.07579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.07579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.13597v2</id>
    <updated>2024-05-20T06:41:29Z</updated>
    <published>2023-12-21T06:02:27Z</published>
    <title>Trochoid Search Optimization</title>
    <summary>  This paper introduces the Trochoid Search Optimization Algorithm (TSO), a
novel metaheuristic leveraging the mathematical properties of trochoid curves.
The TSO algorithm employs a unique combination of simultaneous translational
and rotational motions inherent in trochoids, fostering a refined equilibrium
between explorative and exploitative search capabilities. Notably, TSO consists
of two pivotal phases global and local search that collectively contribute to
its efficiency and efficacy. Experimental validation demonstrates the TSO
algorithm's remarkable performance across various benchmark functions,
showcasing its competitive edge in balancing exploration and exploitation
within the search space. A distinguishing feature of TSO lies in its
simplicity, marked by a minimal requirement for user-defined parameters, making
it an accessible yet powerful optimization tool.
</summary>
    <author>
      <name>Abdesslem Layeb</name>
    </author>
    <link href="http://arxiv.org/abs/2312.13597v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.13597v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.14942v1</id>
    <updated>2023-12-05T17:09:21Z</updated>
    <published>2023-12-05T17:09:21Z</published>
    <title>Liquid State Genetic Programming</title>
    <summary>  A new Genetic Programming variant called Liquid State Genetic Programming
(LSGP) is proposed in this paper. LSGP is a hybrid method combining a dynamic
memory for storing the inputs (the liquid) and a Genetic Programming technique
used for the problem solving part. Several numerical experiments with LSGP are
performed by using several benchmarking problems. Numerical experiments show
that LSGP performs similarly and sometimes even better than standard Genetic
Programming for the considered test problems.
</summary>
    <author>
      <name>Mihai Oltean</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-3-540-71618-1_25</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-3-540-71618-1_25" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figure, ICANNGA 2007, Lecture Notes in Computer Science,
  pp 220-229, vol 4431. Springer. arXiv admin note: text overlap with
  arXiv:2110.02014, arXiv:2111.14790</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In: Beliczynski, B., Dzielinski, A., Iwanowski, M., Ribeiro, B.
  (eds) Adaptive and Natural Computing Algorithms. ICANNGA 2007. Lecture Notes
  in Computer Science, vol 4431. Springer</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2312.14942v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.14942v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.00976v1</id>
    <updated>2023-08-30T11:33:22Z</updated>
    <published>2023-08-30T11:33:22Z</published>
    <title>Nature-Inspired Algorithms in Optimization: Introduction, Hybridization
  and Insights</title>
    <summary>  Many problems in science and engineering are optimization problems, which may
require sophisticated optimization techniques to solve. Nature-inspired
algorithms are a class of metaheuristic algorithms for optimization, and some
algorithms or variants are often developed by hybridization. Benchmarking is
also important in evaluating the performance of optimization algorithms. This
chapter focuses on the overview of optimization, nature-inspired algorithms and
the role of hybridization. We will also highlight some issues with
hybridization of algorithms.
</summary>
    <author>
      <name>Xin-She Yang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-981-99-3970-1_1</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-981-99-3970-1_1" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.00976v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.00976v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="90C59, 90C26, 90C31, 68W20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.01265v1</id>
    <updated>2024-01-02T16:11:31Z</updated>
    <published>2024-01-02T16:11:31Z</published>
    <title>Optimal Synthesis of Finite State Machines with Universal Gates using
  Evolutionary Algorithm</title>
    <summary>  This work presents an optimization method for the synthesis of finite state
machines. The focus is on the reduction in the on-chip area and the cost of the
circuit. A list of finite state machines from MCNC91 benchmark circuits have
been evolved using Cartesian Genetic Programming. On the average, almost 30% of
reduction in the total number of gates has been achieved. The effects of some
parameters on the evolutionary process have also been discussed in the paper.
</summary>
    <author>
      <name>Noor Ullah</name>
    </author>
    <author>
      <name>Khawaja M. Yahya</name>
    </author>
    <author>
      <name>Irfan Ahmed</name>
    </author>
    <link href="http://arxiv.org/abs/2401.01265v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.01265v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.04134v1</id>
    <updated>2024-01-07T05:12:10Z</updated>
    <published>2024-01-07T05:12:10Z</published>
    <title>Web Neural Network with Complete DiGraphs</title>
    <summary>  This paper introduces a new neural network model that aims to mimic the
biological brain more closely by structuring the network as a complete directed
graph that processes continuous data for each timestep. Current neural networks
have structures that vaguely mimic the brain structure, such as neurons,
convolutions, and recurrence. The model proposed in this paper adds additional
structural properties by introducing cycles into the neuron connections and
removing the sequential nature commonly seen in other network layers.
Furthermore, the model has continuous input and output, inspired by spiking
neural networks, which allows the network to learn a process of classification,
rather than simply returning the final result.
</summary>
    <author>
      <name>Frank Li</name>
    </author>
    <link href="http://arxiv.org/abs/2401.04134v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.04134v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.11167v1</id>
    <updated>2024-01-20T08:32:03Z</updated>
    <published>2024-01-20T08:32:03Z</published>
    <title>Coevolving Artistic Images Using OMNIREP</title>
    <summary>  We have recently developed OMNIREP, a coevolutionary algorithm to discover
both a representation and an interpreter that solve a particular problem of
interest. Herein, we demonstrate that the OMNIREP framework can be successfully
applied within the field of evolutionary art. Specifically, we coevolve
representations that encode image position, alongside interpreters that
transform these positions into one of three pre-defined shapes (chunks,
polygons, or circles) of varying size, shape, and color. We showcase a sampling
of the unique image variations produced by this approach.
</summary>
    <author>
      <name>Moshe Sipper</name>
    </author>
    <author>
      <name>Jason H. Moore</name>
    </author>
    <author>
      <name>Ryan J. Urbanowicz</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">J. Romero et al. (Eds.), EvoMUSART 2020, LNCS 12103, pp. 165-178,
  2020</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2401.11167v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.11167v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.15212v1</id>
    <updated>2024-01-26T21:22:08Z</updated>
    <published>2024-01-26T21:22:08Z</published>
    <title>Speed-based Filtration and DBSCAN of Event-based Camera Data with
  Neuromorphic Computing</title>
    <summary>  Spiking neural networks are powerful computational elements that pair well
with event-based cameras (EBCs). In this work, we present two spiking neural
network architectures that process events from EBCs: one that isolates and
filters out events based on their speeds, and another that clusters events
based on the DBSCAN algorithm.
</summary>
    <author>
      <name>Charles P. Rizzo</name>
    </author>
    <author>
      <name>Catherine D. Schuman</name>
    </author>
    <author>
      <name>James S. Plank</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 5 figures, Submitted to Neuro Inspired Computational
  Elements Conference 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2401.15212v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.15212v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.00070v1</id>
    <updated>2024-01-30T19:37:21Z</updated>
    <published>2024-01-30T19:37:21Z</published>
    <title>EvoMerge: Neuroevolution for Large Language Models</title>
    <summary>  Extensive fine-tuning on Large Language Models does not always yield better
results. Oftentimes, models tend to get better at imitating one form of data
without gaining greater reasoning ability and may even end up losing some
intelligence. Here I introduce EvoMerge, a systematic approach to large
language model training and merging. Leveraging model merging for weight
crossover and fine-tuning for weight mutation, EvoMerge establishes an
evolutionary process aimed at pushing models beyond the limits of conventional
fine-tuning.
</summary>
    <author>
      <name>Yushu Jiang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The current submission is the first draft, published for the sole
  purpose of sharing an idea and encouraging community effort. A more
  consolidated version may come later</arxiv:comment>
    <link href="http://arxiv.org/abs/2402.00070v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.00070v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.00094v1</id>
    <updated>2024-01-31T14:49:44Z</updated>
    <published>2024-01-31T14:49:44Z</published>
    <title>Deep Neural Networks: A Formulation Via Non-Archimedean Analysis</title>
    <summary>  We introduce a new class of deep neural networks (DNNs) with multilayered
tree-like architectures. The architectures are codified using numbers from the
ring of integers of non-Archimdean local fields. These rings have a natural
hierarchical organization as infinite rooted trees. Natural morphisms on these
rings allow us to construct finite multilayered architectures. The new DNNs are
robust universal approximators of real-valued functions defined on the
mentioned rings. We also show that the DNNs are robust universal approximators
of real-valued square-integrable functions defined in the unit interval.
</summary>
    <author>
      <name>W. A. Zúñiga-Galindo</name>
    </author>
    <link href="http://arxiv.org/abs/2402.00094v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.00094v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="Primary 68T07, 65D15, Secondary 41A30, 11S85" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.00444v1</id>
    <updated>2024-02-01T09:18:34Z</updated>
    <published>2024-02-01T09:18:34Z</published>
    <title>Evaluating Genetic Algorithms through the Approximability Hierarchy</title>
    <summary>  Optimization problems frequently appear in any scientific domain. Most of the
times, the corresponding decision problem turns out to be NP-hard, and in these
cases genetic algorithms are often used to obtain approximated solutions.
However, the difficulty to approximate different NP-hard problems can vary a
lot. In this paper, we analyze the usefulness of using genetic algorithms
depending on the approximation class the problem belongs to. In particular, we
use the standard approximability hierarchy, showing that genetic algorithms are
especially useful for the most pessimistic classes of the hierarchy
</summary>
    <author>
      <name>Alba Muñoz</name>
    </author>
    <author>
      <name>Fernando Rubio</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/J.JOCS.2021.101388</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/J.JOCS.2021.101388" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 1 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Elsevier, Journal of Computational Science 2021,
  https://www.sciencedirect.com/science/article/pii/S1877750321000764</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2402.00444v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.00444v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2402.18472v1</id>
    <updated>2024-02-28T16:50:05Z</updated>
    <published>2024-02-28T16:50:05Z</published>
    <title>Implementing Online Reinforcement Learning with Clustering Neural
  Networks</title>
    <summary>  An agent employing reinforcement learning takes inputs (state variables) from
an environment and performs actions that affect the environment in order to
achieve some objective. Rewards (positive or negative) guide the agent toward
improved future actions. This paper builds on prior clustering neural network
research by constructing an agent with biologically plausible neo-Hebbian
three-factor synaptic learning rules, with a reward signal as the third factor
(in addition to pre- and post-synaptic spikes). The classic cart-pole problem
(balancing an inverted pendulum) is used as a running example throughout the
exposition. Simulation results demonstrate the efficacy of the approach, and
the proposed method may eventually serve as a low-level component of a more
general method.
</summary>
    <author>
      <name>James E. Smith</name>
    </author>
    <link href="http://arxiv.org/abs/2402.18472v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2402.18472v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T07" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.01900v2</id>
    <updated>2025-04-07T03:54:31Z</updated>
    <published>2024-03-04T09:59:11Z</published>
    <title>Universality of reservoir systems with recurrent neural networks</title>
    <summary>  Approximation capability of reservoir systems whose reservoir is a recurrent
neural network (RNN) is discussed. We show what we call uniform strong
universality of RNN reservoir systems for a certain class of dynamical systems.
This means that, given an approximation error to be achieved, one can construct
an RNN reservoir system that approximates each target dynamical system in the
class just via adjusting its linear readout. To show the universality, we
construct an RNN reservoir system via parallel concatenation that has an upper
bound of approximation error independent of each target in the class.
</summary>
    <author>
      <name>Hiroki Yasumoto</name>
    </author>
    <author>
      <name>Toshiyuki Tanaka</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neunet.2025.107413</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neunet.2025.107413" rel="related"/>
    <link href="http://arxiv.org/abs/2403.01900v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.01900v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.02331v4</id>
    <updated>2024-04-25T16:01:29Z</updated>
    <published>2024-03-04T18:58:09Z</published>
    <title>Toward Neuromic Computing: Neurons as Autoencoders</title>
    <summary>  This short paper presents the idea that neural backpropagation is using
dendritic processing to enable individual neurons to perform autoencoding.
Using a very simple connection weight search heuristic and artificial neural
network model, the effects of interleaving autoencoding for each neuron in a
hidden layer of a feedforward network are explored. This is contrasted to the
standard layered approach to autoencoding. It is shown that such individualised
processing is not detrimental and can improve network learning.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <link href="http://arxiv.org/abs/2403.02331v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.02331v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.09680v2</id>
    <updated>2024-04-08T17:51:31Z</updated>
    <published>2024-02-07T15:30:23Z</published>
    <title>Pre-Sorted Tsetlin Machine (The Genetic K-Medoid Method)</title>
    <summary>  This paper proposes a machine learning pre-sort stage to traditional
supervised learning using Tsetlin Machines. Initially, K data-points are
identified from the dataset using an expedited genetic algorithm to solve the
maximum dispersion problem. These are then used as the initial placement to run
the K-Medoid clustering algorithm. Finally, an expedited genetic algorithm is
used to align K independent Tsetlin Machines by maximising hamming distance.
For MNIST level classification problems, results demonstrate up to 10%
improvement in accuracy, approx. 383X reduction in training time and approx.
86X reduction in inference time.
</summary>
    <author>
      <name>Jordan Morris</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 12 figures, 3 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2403.09680v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.09680v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="B.6.0; B.7.0; C.1.0; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2403.14146v1</id>
    <updated>2024-03-21T05:42:17Z</updated>
    <published>2024-03-21T05:42:17Z</published>
    <title>Evolving Benchmark Functions to Compare Evolutionary Algorithms via
  Genetic Programming</title>
    <summary>  In this study, we use Genetic Programming (GP) to compose new optimization
benchmark functions. Optimization benchmarks have the important role of showing
the differences between evolutionary algorithms, making it possible for further
analysis and comparisons. We show that the benchmarks generated by GP are able
to differentiate algorithms better than human-made benchmark functions. The
fitness measure of the GP is the Wasserstein distance of the solutions found by
a pair of optimizers. Additionally, we use MAP-Elites to both enhance the
search power of the GP and also illustrate how the difference between
optimizers changes by various landscape features. Our approach provides a novel
way to automate the design of benchmark functions and to compare evolutionary
algorithms.
</summary>
    <author>
      <name>Yifan He</name>
    </author>
    <author>
      <name>Claus Aranha</name>
    </author>
    <link href="http://arxiv.org/abs/2403.14146v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2403.14146v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.06828v1</id>
    <updated>2024-04-10T08:32:29Z</updated>
    <published>2024-04-10T08:32:29Z</published>
    <title>Proposed modified computational model for the amoeba-inspired
  combinatorial optimization machine</title>
    <summary>  A single-celled amoeba can solve the traveling salesman problem through its
shape-changing dynamics. In this paper, we examine roles of several elements in
a previously proposed computational model of the solution-search process of
amoeba and three modifications towards enhancing the solution-search
preformance. We find that appropriate modifications can indeed significantly
improve the quality of solutions. It is also found that a condition associated
with the volume conservation can also be modified in contrast to the naive
belief that it is indispensable for the solution-search ability of amoeba. A
proposed modified model shows much better performance.
</summary>
    <author>
      <name>Yusuke Miyajima</name>
    </author>
    <author>
      <name>Masahito Mochizuki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.06828v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.06828v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.CO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.12750v1</id>
    <updated>2024-04-19T09:56:56Z</updated>
    <published>2024-04-19T09:56:56Z</published>
    <title>Leveraging Symbolic Regression for Heuristic Design in the Traveling
  Thief Problem</title>
    <summary>  The Traveling Thief Problem is an NP-hard combination of the well known
traveling salesman and knapsack packing problems. In this paper, we use
symbolic regression to learn useful features of near-optimal packing plans,
which we then use to design efficient metaheuristic genetic algorithms for the
traveling thief algorithm. By using symbolic regression again to initialize the
metaheuristic GA with near-optimal individuals, we are able to design a fast,
interpretable, and effective packing initialization scheme. Comparisons against
previous initialization schemes validates our algorithm design.
</summary>
    <author>
      <name>Andrew Ni</name>
    </author>
    <author>
      <name>Lee Spector</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2404.12750v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.12750v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2404.15837v1</id>
    <updated>2024-04-24T12:16:37Z</updated>
    <published>2024-04-24T12:16:37Z</published>
    <title>Empirical Analysis of the Dynamic Binary Value Problem with IOHprofiler</title>
    <summary>  Optimization problems in dynamic environments have recently been the source
of several theoretical studies. One of these problems is the monotonic Dynamic
Binary Value problem, which theoretically has high discriminatory power between
different Genetic Algorithms. Given this theoretical foundation, we integrate
several versions of this problem into the IOHprofiler benchmarking framework.
Using this integration, we perform several large-scale benchmarking experiments
to both recreate theoretical results on moderate dimensional problems and
investigate aspects of GA's performance which have not yet been studied
theoretically. Our results highlight some of the many synergies between theory
and benchmarking and offer a platform through which further research into
dynamic optimization problems can be performed.
</summary>
    <author>
      <name>Diederick Vermetten</name>
    </author>
    <author>
      <name>Johannes Lengler</name>
    </author>
    <author>
      <name>Dimitri Rusin</name>
    </author>
    <author>
      <name>Thomas Bäck</name>
    </author>
    <author>
      <name>Carola Doerr</name>
    </author>
    <link href="http://arxiv.org/abs/2404.15837v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2404.15837v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.00698v1</id>
    <updated>2024-04-12T19:24:06Z</updated>
    <published>2024-04-12T19:24:06Z</published>
    <title>CUDA-Accelerated Soft Robot Neural Evolution with Large Language Model
  Supervision</title>
    <summary>  This paper addresses the challenge of co-designing morphology and control in
soft robots via a novel neural network evolution approach. We propose an
innovative method to implicitly dual-encode soft robots, thus facilitating the
simultaneous design of morphology and control. Additionally, we introduce the
large language model to serve as the control center during the evolutionary
process. This advancement considerably optimizes the evolution speed compared
to traditional soft-bodied robot co-design methods. Further complementing our
work is the implementation of Gaussian positional encoding - an approach that
augments the neural network's comprehension of robot morphology. Our paper
offers a new perspective on soft robot design, illustrating substantial
improvements in efficiency and comprehension during the design and evolutionary
process.
</summary>
    <author>
      <name>Lechen Zhang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.00698v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.00698v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.01226v1</id>
    <updated>2024-05-02T12:10:52Z</updated>
    <published>2024-05-02T12:10:52Z</published>
    <title>Avoiding Redundant Restarts in Multimodal Global Optimization</title>
    <summary>  Na\"ive restarts of global optimization solvers when operating on multimodal
search landscapes may resemble the Coupon's Collector Problem, with a potential
to waste significant function evaluations budget on revisiting the same basins
of attractions. In this paper, we assess the degree to which such ``duplicate
restarts'' occur on standard multimodal benchmark functions, which defines the
\textit{redundancy potential} of each particular landscape. We then propose a
repelling mechanism to avoid such wasted restarts with the CMA-ES and
investigate its efficacy on test cases with high redundancy potential compared
to the standard restart mechanism.
</summary>
    <author>
      <name>Jacob de Nobel</name>
    </author>
    <author>
      <name>Diederick Vermetten</name>
    </author>
    <author>
      <name>Anna V. Kononova</name>
    </author>
    <author>
      <name>Ofer M. Shir</name>
    </author>
    <author>
      <name>Thomas Bäck</name>
    </author>
    <link href="http://arxiv.org/abs/2405.01226v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.01226v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.10098v1</id>
    <updated>2024-05-16T13:54:37Z</updated>
    <published>2024-05-16T13:54:37Z</published>
    <title>When Large Language Model Meets Optimization</title>
    <summary>  Optimization algorithms and large language models (LLMs) enhance
decision-making in dynamic environments by integrating artificial intelligence
with traditional techniques. LLMs, with extensive domain knowledge, facilitate
intelligent modeling and strategic decision-making in optimization, while
optimization algorithms refine LLM architectures and output quality. This
synergy offers novel approaches for advancing general AI, addressing both the
computational challenges of complex problems and the application of LLMs in
practical scenarios. This review outlines the progress and potential of
combining LLMs with optimization algorithms, providing insights for future
research directions.
</summary>
    <author>
      <name>Sen Huang</name>
    </author>
    <author>
      <name>Kaixiang Yang</name>
    </author>
    <author>
      <name>Sheng Qi</name>
    </author>
    <author>
      <name>Rui Wang</name>
    </author>
    <link href="http://arxiv.org/abs/2405.10098v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.10098v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.16551v1</id>
    <updated>2024-05-26T12:40:39Z</updated>
    <published>2024-05-26T12:40:39Z</published>
    <title>GPU Based Differential Evolution: New Insights and Comparative Study</title>
    <summary>  Differential Evolution (DE) is a highly successful population based global
optimisation algorithm, commonly used for solving numerical optimisation
problems. However, as the complexity of the objective function increases, the
wall-clock run-time of the algorithm suffers as many fitness function
evaluations must take place to effectively explore the search space. Due to the
inherently parallel nature of the DE algorithm, graphics processing units (GPU)
have been used to effectively accelerate both the fitness evaluation and DE
algorithm. This work reviews the main architectural choices made in the
literature for GPU based DE algorithms and introduces a new GPU based numerical
optimisation benchmark to evaluate and compare GPU based DE algorithms.
</summary>
    <author>
      <name>Dylan Janssen</name>
    </author>
    <author>
      <name>Wayne Pullan</name>
    </author>
    <author>
      <name>Alan Wee-Chung Liew</name>
    </author>
    <link href="http://arxiv.org/abs/2405.16551v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.16551v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.07790v1</id>
    <updated>2024-06-12T01:10:32Z</updated>
    <published>2024-06-12T01:10:32Z</published>
    <title>Hierarchical Neural Networks, p-Adic PDEs, and Applications to Image
  Processing</title>
    <summary>  The first goal of this article is to introduce a new type of p-adic
reaction-diffusion cellular neural network with delay. We study the stability
of these networks and provide numerical simulations of their responses. The
second goal is to provide a quick review of the state of the art of p-adic
cellular neural networks and their applications to image processing.
</summary>
    <author>
      <name>W. A. Zúñiga-Galindo</name>
    </author>
    <author>
      <name>B. A. Zambrano-Luna</name>
    </author>
    <author>
      <name>Baboucarr Dibba</name>
    </author>
    <link href="http://arxiv.org/abs/2406.07790v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.07790v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.AP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.13414v1</id>
    <updated>2024-06-19T10:08:12Z</updated>
    <published>2024-06-19T10:08:12Z</published>
    <title>Archive-based Single-Objective Evolutionary Algorithms for Submodular
  Optimization</title>
    <summary>  Constrained submodular optimization problems play a key role in the area of
combinatorial optimization as they capture many NP-hard optimization problems.
So far, Pareto optimization approaches using multi-objective formulations have
been shown to be successful to tackle these problems while single-objective
formulations lead to difficulties for algorithms such as the $(1+1)$-EA due to
the presence of local optima. We introduce for the first time single-objective
algorithms that are provably successful for different classes of constrained
submodular maximization problems. Our algorithms are variants of the
$(1+\lambda)$-EA and $(1+1)$-EA and increase the feasible region of the search
space incrementally in order to deal with the considered submodular problems.
</summary>
    <author>
      <name>Frank Neumann</name>
    </author>
    <author>
      <name>Günter Rudolph</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear at PPSN 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2406.13414v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.13414v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.14180v1</id>
    <updated>2024-06-20T10:38:50Z</updated>
    <published>2024-06-20T10:38:50Z</published>
    <title>RTFormer: Re-parameter TSBN Spiking Transformer</title>
    <summary>  The Spiking Neural Networks (SNNs), renowned for their bio-inspired
operational mechanism and energy efficiency, mirror the human brain's neural
activity. Yet, SNNs face challenges in balancing energy efficiency with the
computational demands of advanced tasks. Our research introduces the RTFormer,
a novel architecture that embeds Re-parameterized Temporal Sliding Batch
Normalization (TSBN) within the Spiking Transformer framework. This innovation
optimizes energy usage during inference while ensuring robust computational
performance. The crux of RTFormer lies in its integration of reparameterized
convolutions and TSBN, achieving an equilibrium between computational prowess
and energy conservation.
</summary>
    <author>
      <name>Hongzhi Wang</name>
    </author>
    <author>
      <name>Xiubo Liang</name>
    </author>
    <author>
      <name>Mengjian Li</name>
    </author>
    <author>
      <name>Tao Zhang</name>
    </author>
    <link href="http://arxiv.org/abs/2406.14180v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.14180v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.09545v1</id>
    <updated>2024-06-27T08:27:08Z</updated>
    <published>2024-06-27T08:27:08Z</published>
    <title>Designing Chaotic Attractors: A Semi-supervised Approach</title>
    <summary>  Chaotic dynamics are ubiquitous in nature and useful in engineering, but
their geometric design can be challenging. Here, we propose a method using
reservoir computing to generate chaos with a desired shape by providing a
periodic orbit as a template, called a skeleton. We exploit a bifurcation of
the reservoir to intentionally induce unsuccessful training of the skeleton,
revealing inherent chaos. The emergence of this untrained attractor, resulting
from the interaction between the skeleton and the reservoir's intrinsic
dynamics, offers a novel semi-supervised framework for designing chaos.
</summary>
    <author>
      <name>Tempei Kabayama</name>
    </author>
    <author>
      <name>Yasuo Kuniyoshi</name>
    </author>
    <author>
      <name>Kazuyuki Aihara</name>
    </author>
    <author>
      <name>Kohei Nakajima</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures (excluding supplementary material)</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.09545v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.09545v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.09585v1</id>
    <updated>2024-07-12T11:54:05Z</updated>
    <published>2024-07-12T11:54:05Z</published>
    <title>A Scale-Invariant Diagnostic Approach Towards Understanding Dynamics of
  Deep Neural Networks</title>
    <summary>  This paper introduces a scale-invariant methodology employing \textit{Fractal
Geometry} to analyze and explain the nonlinear dynamics of complex
connectionist systems. By leveraging architectural self-similarity in Deep
Neural Networks (DNNs), we quantify fractal dimensions and \textit{roughness}
to deeply understand their dynamics and enhance the quality of
\textit{intrinsic} explanations. Our approach integrates principles from Chaos
Theory to improve visualizations of fractal evolution and utilizes a
Graph-Based Neural Network for reconstructing network topology. This strategy
aims at advancing the \textit{intrinsic} explainability of connectionist
Artificial Intelligence (AI) systems.
</summary>
    <author>
      <name>Ambarish Moharil</name>
    </author>
    <author>
      <name>Damian Tamburri</name>
    </author>
    <author>
      <name>Indika Kumara</name>
    </author>
    <author>
      <name>Willem-Jan Van Den Heuvel</name>
    </author>
    <author>
      <name>Alireza Azarfar</name>
    </author>
    <link href="http://arxiv.org/abs/2407.09585v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.09585v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.10477v1</id>
    <updated>2024-07-15T07:05:34Z</updated>
    <published>2024-07-15T07:05:34Z</published>
    <title>Deep Learning-Based Operators for Evolutionary Algorithms</title>
    <summary>  We present two novel domain-independent genetic operators that harness the
capabilities of deep learning: a crossover operator for genetic algorithms and
a mutation operator for genetic programming. Deep Neural Crossover leverages
the capabilities of deep reinforcement learning and an encoder-decoder
architecture to select offspring genes. BERT mutation masks multiple gp-tree
nodes and then tries to replace these masks with nodes that will most likely
improve the individual's fitness. We show the efficacy of both operators
through experimentation.
</summary>
    <author>
      <name>Eliad Shem-Tov</name>
    </author>
    <author>
      <name>Moshe Sipper</name>
    </author>
    <author>
      <name>Achiya Elyasaf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 7 figures, 2 tables. Accepted to Genetic Programming Theory
  &amp; Practice XXI (GPTP 2024). arXiv admin note: text overlap with
  arXiv:2403.11159</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.10477v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.10477v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.11986v1</id>
    <updated>2024-06-04T15:54:57Z</updated>
    <published>2024-06-04T15:54:57Z</published>
    <title>Symbiotic Connectivity: Optimizing Rural Digital Infrastructure with
  Solar-Powered Mesh Networks Using Multi-Objective Evolutionary Algorithms</title>
    <summary>  I present an open-source, ecologically integrated model for rural
connectivity, merging the location of nodes mesh networks with renewable energy
systems. Employing evolutionary algorithms, this approach optimizes node
placement for internet access and symbiotic energy distribution. This model,
grounded in community collaboration, demonstrates a balance between
technological advancement and environmental stewardship, offering a blueprint
for sustainable infrastructure in similar rural settings.
</summary>
    <author>
      <name>Yadira Sanchez Benitez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To be published in Evo* 2024 -- Late-Breaking Abstracts Volume</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.11986v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.11986v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.18218v1</id>
    <updated>2024-07-25T17:35:11Z</updated>
    <published>2024-07-25T17:35:11Z</published>
    <title>An NKCS Model of Bookchins Communalism</title>
    <summary>  The NKCS model was introduced to explore coevolutionary systems, that is,
systems in which multiple species are closely interconnected. The fitness
landscapes of the species are coupled to a controllable amount, where the
underlying properties of the individual landscapes are also controllable. No
previous work has explored the use of hierarchical control within the model.
This paper explores the effects of using a confederation, based on Bookchins
communalism, and a single point of global control. Significant changes in
behaviour from the traditional model are seen across the parameter space.
</summary>
    <author>
      <name>Larry Bull</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages. arXiv admin note: text overlap with arXiv:2302.01694</arxiv:comment>
    <link href="http://arxiv.org/abs/2407.18218v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.18218v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.18257v1</id>
    <updated>2024-07-11T12:57:35Z</updated>
    <published>2024-07-11T12:57:35Z</published>
    <title>Estimation of Distribution Algorithms with Matrix Transpose in Bayesian
  Learning</title>
    <summary>  Estimation of distribution algorithms (EDAs) constitute a new branch of
evolutionary optimization algorithms, providing effective and efficient
optimization performance in a variety of research areas. Recent studies have
proposed new EDAs that employ mutation operators in standard EDAs to increase
the population diversity. We present a new mutation operator, a matrix
transpose, specifically designed for Bayesian structure learning, and we
evaluate its performance in Bayesian structure learning. The results indicate
that EDAs with transpose mutation give markedly better performance than
conventional EDAs.
</summary>
    <author>
      <name>Dae-Won Kim</name>
    </author>
    <author>
      <name>Song Ko</name>
    </author>
    <author>
      <name>Bo-Yeong Kang</name>
    </author>
    <link href="http://arxiv.org/abs/2407.18257v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.18257v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.00673v1</id>
    <updated>2024-08-01T16:15:07Z</updated>
    <published>2024-08-01T16:15:07Z</published>
    <title>Modeling stochastic eye tracking data: A comparison of quantum
  generative adversarial networks and Markov models</title>
    <summary>  We explore the use of quantum generative adversarial networks QGANs for
modeling eye movement velocity data. We assess whether the advanced
computational capabilities of QGANs can enhance the modeling of complex
stochastic distribution beyond the traditional mathematical models,
particularly the Markov model. The findings indicate that while QGANs
demonstrate potential in approximating complex distributions, the Markov model
consistently outperforms in accurately replicating the real data distribution.
This comparison underlines the challenges and avenues for refinement in time
series data generation using quantum computing techniques. It emphasizes the
need for further optimization of quantum models to better align with real-world
data characteristics.
</summary>
    <author>
      <name>Shailendra Bhandari</name>
    </author>
    <author>
      <name>Pedro Lincastre</name>
    </author>
    <author>
      <name>Pedro Lind</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3638530.3664134</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3638530.3664134" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.00673v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.00673v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2408.01166v3</id>
    <updated>2025-04-11T12:53:44Z</updated>
    <published>2024-08-02T10:33:52Z</published>
    <title>Continuous-Time Neural Networks Can Stably Memorize Random Spike Trains</title>
    <summary>  The paper explores the capability of continuous-time recurrent neural
networks to store and recall precisely timed scores of spike trains. We show
(by numerical experiments) that this is indeed possible: within some range of
parameters, any random score of spike trains (for all neurons in the network)
can be robustly memorized and autonomously reproduced with stable accurate
relative timing of all spikes, with probability close to one. We also
demonstrate associative recall under noisy conditions.
  In these experiments, the required synaptic weights are computed offline, to
satisfy a template that encourages temporal stability.
</summary>
    <author>
      <name>Hugo Aguettaz</name>
    </author>
    <author>
      <name>Hans-Andrea Loeliger</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 16 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2408.01166v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2408.01166v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.03187v1</id>
    <updated>2024-09-05T02:25:04Z</updated>
    <published>2024-09-05T02:25:04Z</published>
    <title>How noise affects memory in linear recurrent networks</title>
    <summary>  The effects of noise on memory in a linear recurrent network are
theoretically investigated. Memory is characterized by its ability to store
previous inputs in its instantaneous state of network, which receives a
correlated or uncorrelated noise. Two major properties are revealed: First, the
memory reduced by noise is uniquely determined by the noise's power spectral
density (PSD). Second, the memory will not decrease regardless of noise
intensity if the PSD is in a certain class of distribution (including power
law). The results are verified using the human brain signals, showing good
agreement.
</summary>
    <author>
      <name>JingChuan Guan</name>
    </author>
    <author>
      <name>Tomoyuki Kubota</name>
    </author>
    <author>
      <name>Yasuo Kuniyoshi</name>
    </author>
    <author>
      <name>Kohei Nakajima</name>
    </author>
    <link href="http://arxiv.org/abs/2409.03187v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.03187v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.07833v1</id>
    <updated>2024-09-12T08:29:55Z</updated>
    <published>2024-09-12T08:29:55Z</published>
    <title>Classifying Images with CoLaNET Spiking Neural Network -- the MNIST
  Example</title>
    <summary>  In the present paper, it is shown how the columnar/layered CoLaNET spiking
neural network (SNN) architecture can be used in supervised learning image
classification tasks. Image pixel brightness is coded by the spike count during
image presentation period. Image class label is indicated by activity of
special SNN input nodes (one node per class). The CoLaNET classification
accuracy is evaluated on the MNIST benchmark. It is demonstrated that CoLaNET
is almost as accurate as the most advanced machine learning algorithms (not
using convolutional approach).
</summary>
    <author>
      <name>Mikhail Kiselev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: substantial text overlap with arXiv:2409.01230</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.07833v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.07833v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.10821v1</id>
    <updated>2024-09-17T01:28:40Z</updated>
    <published>2024-09-17T01:28:40Z</published>
    <title>PReLU: Yet Another Single-Layer Solution to the XOR Problem</title>
    <summary>  This paper demonstrates that a single-layer neural network using Parametric
Rectified Linear Unit (PReLU) activation can solve the XOR problem, a simple
fact that has been overlooked so far. We compare this solution to the
multi-layer perceptron (MLP) and the Growing Cosine Unit (GCU) activation
function and explain why PReLU enables this capability. Our results show that
the single-layer PReLU network can achieve 100\% success rate in a wider range
of learning rates while using only three learnable parameters.
</summary>
    <author>
      <name>Rafael C. Pinto</name>
    </author>
    <author>
      <name>Anderson R. Tavares</name>
    </author>
    <link href="http://arxiv.org/abs/2409.10821v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.10821v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.10887v1</id>
    <updated>2024-09-17T04:48:45Z</updated>
    <published>2024-09-17T04:48:45Z</published>
    <title>Contrastive Learning in Memristor-based Neuromorphic Systems</title>
    <summary>  Spiking neural networks, the third generation of artificial neural networks,
have become an important family of neuron-based models that sidestep many of
the key limitations facing modern-day backpropagation-trained deep networks,
including their high energy inefficiency and long-criticized biological
implausibility. In this work, we design and investigate a proof-of-concept
instantiation of contrastive-signal-dependent plasticity (CSDP), a neuromorphic
form of forward-forward-based, backpropagation-free learning. Our experimental
simulations demonstrate that a hardware implementation of CSDP is capable of
learning simple logic functions without the need to resort to complex gradient
calculations.
</summary>
    <author>
      <name>Cory Merkel</name>
    </author>
    <author>
      <name>Alexander Ororbia</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted in SiPS 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.10887v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.10887v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.14298v1</id>
    <updated>2024-09-22T02:41:19Z</updated>
    <published>2024-09-22T02:41:19Z</published>
    <title>A Neuromorphic Implementation of the DBSCAN Algorithm</title>
    <summary>  DBSCAN is an algorithm that performs clustering in the presence of noise. In
this paper, we provide two constructions that allow DBSCAN to be implemented
neuromorphically, using spiking neural networks. The first construction is
termed "flat," resulting in large spiking neural networks that compute the
algorithm quickly, in five timesteps. Moreover, the networks allow pipelining,
so that a new DBSCAN calculation may be performed every timestep. The second
construction is termed "systolic", and generates much smaller networks, but
requires the inputs to be spiked in over several timesteps, column by column.
We provide precise specifications of the constructions and analyze them in
practical neuromorphic computing settings. We also provide an open-source
implementation.
</summary>
    <author>
      <name>Charles P. Rizzo</name>
    </author>
    <author>
      <name>James S. Plank</name>
    </author>
    <link href="http://arxiv.org/abs/2409.14298v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.14298v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.16958v1</id>
    <updated>2024-09-25T14:14:08Z</updated>
    <published>2024-09-25T14:14:08Z</published>
    <title>Metaheuristic Method for Solving Systems of Equations</title>
    <summary>  This study investigates the effectiveness of Genetic Algorithms (GAs) in
solving both linear and nonlinear systems of equations, comparing their
performance to traditional methods such as Gaussian Elimination, Newton's
Method, and Levenberg-Marquardt. The GA consistently delivered accurate
solutions across various test cases, demonstrating its robustness and
flexibility. A key advantage of the GA is its ability to explore the solution
space broadly, uncovering multiple sets of solutions -- a feat that traditional
methods, which typically converge to a single solution, cannot achieve. This
feature proved especially beneficial in complex nonlinear systems, where
multiple valid solutions exist, highlighting the GA's superiority in navigating
intricate solution landscapes.
</summary>
    <author>
      <name>Samson Odan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 Pages, 2 Figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.16958v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.16958v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.00584v1</id>
    <updated>2024-10-01T11:15:18Z</updated>
    <published>2024-10-01T11:15:18Z</published>
    <title>Asymmetrically connected reservoir networks learn better</title>
    <summary>  We show that connectivity within the high-dimensional recurrent layer of a
reservoir network is crucial for its performance. To this end, we
systematically investigate the impact of network connectivity on its
performance, i.e., we examine the symmetry and structure of the reservoir in
relation to its computational power. Reservoirs with random and asymmetric
connections are found to perform better for an exemplary Mackey-Glass time
series than all structured reservoirs, including biologically inspired
connectivities, such as small-world topologies. This result is quantified by
the information processing capacity of the different network topologies which
becomes highest for asymmetric and randomly connected networks.
</summary>
    <author>
      <name>Shailendra K. Rathor</name>
    </author>
    <author>
      <name>Martin Ziegler</name>
    </author>
    <author>
      <name>Jörg Schumacher</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 4 figures, supplementary material</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.00584v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.00584v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.14364v1</id>
    <updated>2024-10-18T10:51:10Z</updated>
    <published>2024-10-18T10:51:10Z</published>
    <title>Non-Invasive Qualitative Vibration Analysis using Event Camera</title>
    <summary>  This technical report investigates the application of event-based vision
sensors in non-invasive qualitative vibration analysis, with a particular focus
on frequency measurement and motion magnification. Event cameras, with their
high temporal resolution and dynamic range, offer promising capabilities for
real-time structural assessment and subtle motion analysis. Our study employs
cutting-edge event-based vision techniques to explore real-world scenarios in
frequency measurement in vibrational analysis and intensity reconstruction for
motion magnification. In the former, event-based sensors demonstrated
significant potential for real-time structural assessment. However, our work in
motion magnification revealed considerable challenges, particularly in
scenarios involving stationary cameras and isolated motion.
</summary>
    <author>
      <name>Dwijay Bane</name>
    </author>
    <author>
      <name>Anurag Gupta</name>
    </author>
    <author>
      <name>Manan Suri</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">13 pages, 11 figures, 2 table</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.14364v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.14364v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.19439v1</id>
    <updated>2024-10-25T09:58:15Z</updated>
    <published>2024-10-25T09:58:15Z</published>
    <title>Non-Dominated Sorting Bidirectional Differential Coevolution</title>
    <summary>  Constrained multiobjective optimization problems (CMOPs) are commonly found
in real-world applications. CMOP is a complex problem that needs to satisfy a
set of equality or inequality constraints. This paper proposes a variant of the
bidirectional coevolution algorithm (BiCo) with differential evolution (DE).
The novelties in the model include the DE differential mutation and crossover
operators as the main search engine and a non-dominated sorting selection
scheme. Experimental results on two benchmark test suites and eight real-world
CMOPs suggested that the proposed model reached better overall performance than
the original model.
</summary>
    <author>
      <name>Cicero S. R. Mendes</name>
    </author>
    <author>
      <name>Aluizio F. R. Araújo</name>
    </author>
    <author>
      <name>Lucas R. C. Farias</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.19439v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.19439v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2410.22730v1</id>
    <updated>2024-10-30T06:29:02Z</updated>
    <published>2024-10-30T06:29:02Z</published>
    <title>Extensional Properties of Recurrent Neural Networks</title>
    <summary>  A property of a recurrent neural network (RNN) is called \emph{extensional}
if, loosely speaking, it is a property of the function computed by the RNN
rather than a property of the RNN algorithm. Many properties of interest in
RNNs are extensional, for example, robustness against small changes of input or
good clustering of inputs. Given an RNN, it is natural to ask whether it has
such a property. We give a negative answer to the general question about
testing extensional properties of RNNs. Namely, we prove a version of Rice's
theorem for RNNs: any nontrivial extensional property of RNNs is undecidable.
</summary>
    <author>
      <name>Evgeny Dantsin</name>
    </author>
    <author>
      <name>Alexander Wolpert</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2410.22730v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2410.22730v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.00140v2</id>
    <updated>2025-04-13T01:39:54Z</updated>
    <published>2024-10-31T18:41:30Z</published>
    <title>ViT-LCA: A Neuromorphic Approach for Vision Transformers</title>
    <summary>  The recent success of Vision Transformers has generated significant interest
in attention mechanisms and transformer architectures. Although existing
methods have proposed spiking self-attention mechanisms compatible with spiking
neural networks, they often face challenges in effective deployment on current
neuromorphic platforms. This paper introduces a novel model that combines
vision transformers with the Locally Competitive Algorithm (LCA) to facilitate
efficient neuromorphic deployment. Our experiments show that ViT-LCA achieves
higher accuracy on ImageNet-1K dataset while consuming significantly less
energy than other spiking vision transformer counterparts. Furthermore,
ViT-LCA's neuromorphic-friendly design allows for more direct mapping onto
current neuromorphic architectures.
</summary>
    <author>
      <name>Sanaz Mahmoodi Takaghaj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted to Artificial Intelligence Circuits And Systems(AICAS), 2015</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.00140v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.00140v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.00802v1</id>
    <updated>2024-10-20T16:10:13Z</updated>
    <published>2024-10-20T16:10:13Z</published>
    <title>An Improved Chicken Swarm Optimization Algorithm for Handwritten
  Document Image Enhancement</title>
    <summary>  Chicken swarm optimization is a new meta-heuristic algorithm which mimics the
foraging hierarchical behavior of chicken. In this paper, we describe the
preprocessing of handwritten document by contrast enhancement while preserving
detail with an improved chicken swarm optimization algorithm.The results of the
algorithm are compared with other existing meta heuristic algorithms like
Cuckoo Search, Firefly Algorithm and the Artificial bee colony. The proposed
algorithm considerably outperforms all the above by giving good results.
</summary>
    <author>
      <name>Stanley Mugisha</name>
    </author>
    <author>
      <name>Lynn tar Gutu</name>
    </author>
    <author>
      <name>P Nagabhushan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 2 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.00802v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.00802v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.4.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.09004v1</id>
    <updated>2024-11-13T20:15:50Z</updated>
    <published>2024-11-13T20:15:50Z</published>
    <title>The geometry of the deep linear network</title>
    <summary>  This article provides an expository account of training dynamics in the Deep
Linear Network (DLN) from the perspective of the geometric theory of dynamical
systems. Rigorous results by several authors are unified into a thermodynamic
framework for deep learning.
  The analysis begins with a characterization of the invariant manifolds and
Riemannian geometry in the DLN. This is followed by exact formulas for a
Boltzmann entropy, as well as stochastic gradient descent of free energy using
a Riemannian Langevin Equation. Several links between the DLN and other areas
of mathematics are discussed, along with some open questions.
</summary>
    <author>
      <name>Govind Menon</name>
    </author>
    <link href="http://arxiv.org/abs/2411.09004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.09004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T07, 58D17, 37N40" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.11575v1</id>
    <updated>2024-11-18T13:53:10Z</updated>
    <published>2024-11-18T13:53:10Z</published>
    <title>Analysis of Generalized Hebbian Learning Algorithm for Neuromorphic
  Hardware Using Spinnaker</title>
    <summary>  Neuromorphic computing, inspired by biological neural networks, has emerged
as a promising approach for solving complex machine learning tasks with greater
efficiency and lower power consumption. The integration of biologically
plausible learning algorithms, such as the Generalized Hebbian Algorithm (GHA),
is key to enhancing the performance of neuromorphic systems. In this paper, we
explore the application of GHA in large-scale neuromorphic platforms,
specifically SpiNNaker, a hardware designed to simulate large neural networks.
Our results demonstrate significant improvements in classification accuracy,
showcasing the potential of biologically inspired learning algorithms in
advancing the field of neuromorphic computing.
</summary>
    <author>
      <name>Shivani Sharma</name>
    </author>
    <author>
      <name>Darshika G. Perera</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 1 figure, 7 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/2411.11575v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.11575v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.19422v1</id>
    <updated>2024-11-29T00:31:01Z</updated>
    <published>2024-11-29T00:31:01Z</published>
    <title>Wafer2Spike: Spiking Neural Network for Wafer Map Pattern Classification</title>
    <summary>  In integrated circuit design, the analysis of wafer map patterns is critical
to improve yield and detect manufacturing issues. We develop Wafer2Spike, an
architecture for wafer map pattern classification using a spiking neural
network (SNN), and demonstrate that a well-trained SNN achieves superior
performance compared to deep neural network-based solutions. Wafer2Spike
achieves an average classification accuracy of 98\% on the WM-811k wafer
benchmark dataset. It is also superior to existing approaches for classifying
defect patterns that are underrepresented in the original dataset. Wafer2Spike
achieves this improved precision with great computational efficiency.
</summary>
    <author>
      <name>Abhishek Mishra</name>
    </author>
    <author>
      <name>Suman Kumar</name>
    </author>
    <author>
      <name>Anush Lingamoorthy</name>
    </author>
    <author>
      <name>Anup Das</name>
    </author>
    <author>
      <name>Nagarajan Kandasamy</name>
    </author>
    <link href="http://arxiv.org/abs/2411.19422v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.19422v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.04487v1</id>
    <updated>2024-11-21T06:17:19Z</updated>
    <published>2024-11-21T06:17:19Z</published>
    <title>Coal Mine Safety Alert System: Refining BP Neural Network with Genetic
  Algorithm Optimization</title>
    <summary>  In response to the persistent safety challenges within coal mines, this study
proposes a novel approach integrating a three-layer feedforward backpropagation
artificial neural network with a genetic algorithm (GA-BP) for establishing a
safety early warning system. Focused on a coal mine in Shandong, China, the
model's effectiveness is evaluated using relevant data for training and
analysis. Results indicate the superiority of the GA-BP model over traditional
BP neural networks, offering enhanced capability for identifying potential
safety risks promptly. This advancement enables coal mine management to
implement timely interventions, ensuring the safety of miners. The findings
present valuable insights for engineering applications in similar contexts.
</summary>
    <author>
      <name>Jiabin Luo</name>
    </author>
    <author>
      <name>Hanzhe Pan</name>
    </author>
    <link href="http://arxiv.org/abs/2412.04487v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.04487v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.11047v1</id>
    <updated>2024-12-15T04:19:10Z</updated>
    <published>2024-12-15T04:19:10Z</published>
    <title>Deployment Pipeline from Rockpool to Xylo for Edge Computing</title>
    <summary>  Deploying Spiking Neural Networks (SNNs) on the Xylo neuromorphic chip via
the Rockpool framework represents a significant advancement in achieving
ultra-low-power consumption and high computational efficiency for edge
applications. This paper details a novel deployment pipeline, emphasizing the
integration of Rockpool's capabilities with Xylo's architecture, and evaluates
the system's performance in terms of energy efficiency and accuracy. The unique
advantages of the Xylo chip, including its digital spiking architecture and
event-driven processing model, are highlighted to demonstrate its suitability
for real-time, power-sensitive applications.
</summary>
    <author>
      <name>Peng Zhou</name>
    </author>
    <author>
      <name>Dylan R. Muir</name>
    </author>
    <link href="http://arxiv.org/abs/2412.11047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.11047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.15647v1</id>
    <updated>2024-12-20T08:05:42Z</updated>
    <published>2024-12-20T08:05:42Z</published>
    <title>Variable Metric Evolution Strategies for High-dimensional
  Multi-Objective Optimization</title>
    <summary>  We design a class of variable metric evolution strategies well suited for
high-dimensional problems. We target problems with many variables, not
(necessarily) with many objectives. The construction combines two independent
developments: efficient algorithms for scaling covariance matrix adaptation to
high dimensions, and evolution strategies for multi-objective optimization. In
order to design a specific instance of the class we first develop a (1+1)
version of the limited memory matrix adaptation evolution strategy and then use
an established standard construction to turn a population thereof into a
state-of-the-art multi-objective optimizer with indicator-based selection. The
method compares favorably to adaptation of the full covariance matrix.
</summary>
    <author>
      <name>Tobias Glasmachers</name>
    </author>
    <link href="http://arxiv.org/abs/2412.15647v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.15647v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.08416v1</id>
    <updated>2024-12-10T16:40:02Z</updated>
    <published>2024-12-10T16:40:02Z</published>
    <title>A Survey on Recent Advances in Self-Organizing Maps</title>
    <summary>  Self-organising maps are a powerful tool for cluster analysis in a wide range
of data contexts. From the pioneer work of Kohonen, many variants and
improvements have been proposed. This review focuses on the last decade, in
order to provide an overview of the main evolution of the seminal SOM algorithm
as well as of the methodological developments that have been achieved in order
to better fit to various application contexts and users' requirements. We also
highlight a specific and important application field that is related to
commercial use of SOM, which involves specific data management.
</summary>
    <author>
      <name>Axel Guérin</name>
    </author>
    <author>
      <name>Pierre Chauvet</name>
    </author>
    <author>
      <name>Frédéric Saubion</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">36 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2501.08416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.08416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.09725v1</id>
    <updated>2025-01-16T18:16:34Z</updated>
    <published>2025-01-16T18:16:34Z</published>
    <title>Parallel multi-objective metaheuristics for smart communications in
  vehicular networks</title>
    <summary>  This article analyzes the use of two parallel multi-objective soft computing
algorithms to automatically search for high-quality settings of the Ad hoc On
Demand Vector routing protocol for vehicular networks. These methods are based
on an evolutionary algorithm and on a swarm intelligence approach. The
experimental analysis demonstrates that the configurations computed by our
optimization algorithms outperform other state-of-the-art optimized ones. In
turn, the computational efficiency achieved by all the parallel versions is
greater than 87 %. Therefore, the line of work presented in this article
represents an efficient framework to improve vehicular communications.
</summary>
    <author>
      <name>Jamal Toutouh</name>
    </author>
    <author>
      <name>Enrique Alba</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s00500-015-1891-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s00500-015-1891-2" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">TSoft Computing, 21(8), 1949-1961 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2501.09725v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.09725v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.09996v1</id>
    <updated>2025-01-17T07:26:28Z</updated>
    <published>2025-01-17T07:26:28Z</published>
    <title>Fast energy-aware OLSR routing in VANETs by means of a parallel
  evolutionary algorithm</title>
    <summary>  This work tackles the problem of reducing the power consumption of the OLSR
routing protocol in vehicular networks. Nowadays, energy-aware and green
communication protocols are important research topics, specially when deploying
wireless mobile networks. This article introduces a fast automatic methodology
to search for energy-efficient OLSR configurations by using a parallel
evolutionary algorithm. The experimental analysis demonstrates that significant
improvements over the standard configuration can be attained in terms of power
consumption, with no noteworthy loss in the QoS.
</summary>
    <author>
      <name>Jamal Toutouh</name>
    </author>
    <author>
      <name>Sergio Nesmachnow</name>
    </author>
    <author>
      <name>Enrique Alba</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10586-012-0208-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10586-012-0208-9" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Cluster computing, 16, 435-450 (2013)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2501.09996v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.09996v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.10016v1</id>
    <updated>2025-01-17T07:51:26Z</updated>
    <published>2025-01-17T07:51:26Z</published>
    <title>Infrastructure Deployment in Vehicular Communication Networks Using a
  Parallel Multiobjective Evolutionary Algorithm</title>
    <summary>  This article describes the application of a multiobjective evolutionary
algorithm for locating roadside infrastructure for vehicular communication
networks over realistic urban areas. A multiobjective formulation of the
problem is introduced, considering quality-of-service and cost objectives. The
experimental analysis is performed over a real map of M\'alaga, using real
traffic information and antennas, and scenarios that model different
combinations of traffic patterns and applications (text/audio/video) in the
communications. The proposed multiobjective evolutionary algorithm computes
accurate trade-off solutions, significantly improving over state-of-the-art
algorithms previously applied to the problem.
</summary>
    <author>
      <name>Renzo Massobrio</name>
    </author>
    <author>
      <name>Jamal Toutouh</name>
    </author>
    <author>
      <name>Sergio Nesmachniw</name>
    </author>
    <author>
      <name>Enrique Alba</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/int.21890</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/int.21890" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IJIS, 32(8), 801-829 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2501.10016v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.10016v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.11477v4</id>
    <updated>2025-04-05T03:55:20Z</updated>
    <published>2025-01-20T13:18:13Z</published>
    <title>Image Classification Method using Dynamic Quantum Inspired Genetic
  Algorithm</title>
    <summary>  This study presents a dynamic Quantum-Inspired Genetic Algorithm (D-QIGA) for
feature selection, leveraging quantum principles like superposition and
rotation gates to enhance exploration and exploitation. D-QIGA introduces
adaptive mechanisms and a lengthening chromosome strategy to avoid local optima
and improve optimization. Tested on benchmark and real-world problems, it
significantly outperforms traditional Genetic Algorithms, achieving over 99.99%
classification accuracy compared to GA's 95%.
</summary>
    <author>
      <name>Akhilesh Kumar Singh</name>
    </author>
    <author>
      <name>Kirankumar R. Hiremath</name>
    </author>
    <link href="http://arxiv.org/abs/2501.11477v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.11477v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.10417v1</id>
    <updated>2025-02-01T14:29:31Z</updated>
    <published>2025-02-01T14:29:31Z</published>
    <title>Evolutionary Power-Aware Routing in VANETs using Monte-Carlo Simulation</title>
    <summary>  This work addresses the reduction of power consumption of the AODV routing
protocol in vehicular networks as an optimization problem. Nowadays, network
designers focus on energy-aware communication protocols, specially to deploy
wireless networks. Here, we introduce an automatic method to search for
energy-efficient AODV configurations by using an evolutionary algorithm and
parallel Monte-Carlo simulations to improve the accuracy of the evaluation of
tentative solutions. The experimental results demonstrate that significant
power consumption improvements over the standard configuration can be attained,
with no noteworthy loss in the quality of service.
</summary>
    <author>
      <name>J. Toutouh</name>
    </author>
    <author>
      <name>S. Nesmachnow</name>
    </author>
    <author>
      <name>E. Alba</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/HPCSim.2012.6266900</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/HPCSim.2012.6266900" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted and presented in 2012 International Conference on High
  Performance Computing &amp; Simulation (HPCS), Madrid, Spain, 2012</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.10417v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.10417v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.10433v1</id>
    <updated>2025-02-09T01:26:49Z</updated>
    <published>2025-02-09T01:26:49Z</published>
    <title>Neural Genetic Search in Discrete Spaces</title>
    <summary>  Effective search methods are crucial for improving the performance of deep
generative models at test time. In this paper, we introduce a novel test-time
search method, Neural Genetic Search (NGS), which incorporates the evolutionary
mechanism of genetic algorithms into the generation procedure of deep models.
The core idea behind NGS is its crossover, which is defined as
parent-conditioned generation using trained generative models. This approach
offers a versatile and easy-to-implement search algorithm for deep generative
models. We demonstrate the effectiveness and flexibility of NGS through
experiments across three distinct domains: routing problems, adversarial prompt
generation for language models, and molecular design.
</summary>
    <author>
      <name>Hyeonah Kim</name>
    </author>
    <author>
      <name>Sanghyeok Choi</name>
    </author>
    <author>
      <name>Jiwoo Son</name>
    </author>
    <author>
      <name>Jinkyoo Park</name>
    </author>
    <author>
      <name>Changhyun Kwon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.10433v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.10433v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.02013v1</id>
    <updated>2025-03-03T19:44:12Z</updated>
    <published>2025-03-03T19:44:12Z</published>
    <title>Sustainable AI: Mathematical Foundations of Spiking Neural Networks</title>
    <summary>  Deep learning's success comes with growing energy demands, raising concerns
about the long-term sustainability of the field. Spiking neural networks,
inspired by biological neurons, offer a promising alternative with potential
computational and energy-efficiency gains. This article examines the
computational properties of spiking networks through the lens of learning
theory, focusing on expressivity, training, and generalization, as well as
energy-efficient implementations while comparing them to artificial neural
networks. By categorizing spiking models based on time representation and
information encoding, we highlight their strengths, challenges, and potential
as an alternative computational paradigm.
</summary>
    <author>
      <name>Adalbert Fono</name>
    </author>
    <author>
      <name>Manjot Singh</name>
    </author>
    <author>
      <name>Ernesto Araya</name>
    </author>
    <author>
      <name>Philipp C. Petersen</name>
    </author>
    <author>
      <name>Holger Boche</name>
    </author>
    <author>
      <name>Gitta Kutyniok</name>
    </author>
    <link href="http://arxiv.org/abs/2503.02013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.02013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.07359v1</id>
    <updated>2025-04-10T00:59:54Z</updated>
    <published>2025-04-10T00:59:54Z</published>
    <title>A Balanced Approach of Rapid Genetic Exploration and Surrogate
  Exploitation for Hyperparameter Optimization</title>
    <summary>  This paper proposes a new method for hyperparameter optimization (HPO) that
balances exploration and exploitation. While evolutionary algorithms (EAs) show
promise in HPO, they often struggle with effective exploitation. To address
this, we integrate a linear surrogate model into a genetic algorithm (GA),
allowing for smooth integration of multiple strategies. This combination
improves exploitation performance, achieving an average improvement of 1.89
percent (max 6.55 percent, min -3.45 percent) over existing HPO methods.
</summary>
    <author>
      <name>Chul Kim</name>
    </author>
    <author>
      <name>Inwhee Joe</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/ACCESS.2024.3508269</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/ACCESS.2024.3508269" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in IEEE Access, 12 pages, 10 figures. DOI:
  10.1109/ACCESS.2024.3508269</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Access, vol. 12, pp. 29590-29601, 2024</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2504.07359v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.07359v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T20" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8; G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2504.17805v1</id>
    <updated>2025-04-21T16:44:17Z</updated>
    <published>2025-04-21T16:44:17Z</published>
    <title>Fuzzy Logic -- Based Scheduling System for Part-Time Workforce</title>
    <summary>  This paper explores the application of genetic fuzzy systems to efficiently
generate schedules for a team of part-time student workers at a university.
Given the preferred number of working hours and availability of employees, our
model generates feasible solutions considering various factors, such as maximum
weekly hours, required number of workers on duty, and the preferred number of
working hours. The algorithm is trained and tested with availability data
collected from students at the University of Cincinnati. The results
demonstrate the algorithm's efficiency in producing schedules that meet
operational criteria and its robustness in understaffed conditions.
</summary>
    <author>
      <name>Tri Nguyen</name>
    </author>
    <author>
      <name>Kelly Cohen</name>
    </author>
    <link href="http://arxiv.org/abs/2504.17805v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2504.17805v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.01313v1</id>
    <updated>2025-05-02T14:39:44Z</updated>
    <published>2025-05-02T14:39:44Z</published>
    <title>A Neural Architecture Search Method using Auxiliary Evaluation Metric
  based on ResNet Architecture</title>
    <summary>  This paper proposes a neural architecture search space using ResNet as a
framework, with search objectives including parameters for convolution,
pooling, fully connected layers, and connectivity of the residual network. In
addition to recognition accuracy, this paper uses the loss value on the
validation set as a secondary objective for optimization. The experimental
results demonstrate that the search space of this paper together with the
optimisation approach can find competitive network architectures on the MNIST,
Fashion-MNIST and CIFAR100 datasets.
</summary>
    <author>
      <name>Shang Wang</name>
    </author>
    <author>
      <name>Huanrong Tang</name>
    </author>
    <author>
      <name>Jianquan Ouyang</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">GECCO 2023</arxiv:comment>
    <link href="http://arxiv.org/abs/2505.01313v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.01313v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.01320v1</id>
    <updated>2025-05-02T14:48:14Z</updated>
    <published>2025-05-02T14:48:14Z</published>
    <title>ABCO: Adaptive Bacterial Colony Optimisation</title>
    <summary>  This paper introduces a new optimisation algorithm, called Adaptive Bacterial
Colony Optimisation (ABCO), modelled after the foraging behaviour of E. coli
bacteria. The algorithm follows three stages--explore, exploit and
reproduce--and is adaptable to meet the requirements of its applications. The
performance of the proposed ABCO algorithm is compared to that of established
optimisation algorithms--particle swarm optimisation (PSO) and ant colony
optimisation (ACO)--on a set of benchmark functions. Experimental results
demonstrate the benefits of the adaptive nature of the proposed algorithm: ABCO
runs much faster than PSO and ACO while producing competitive results and
outperforms PSO and ACO in a scenario where the running time is not crucial.
</summary>
    <author>
      <name>Barisi Kogam</name>
    </author>
    <author>
      <name>Yevgeniya Kovalchuk</name>
    </author>
    <author>
      <name>Mohamed Medhat Gaber</name>
    </author>
    <link href="http://arxiv.org/abs/2505.01320v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.01320v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.05661v1</id>
    <updated>2025-05-08T21:36:14Z</updated>
    <published>2025-05-08T21:36:14Z</published>
    <title>Smart Starts: Accelerating Convergence through Uncommon Region
  Exploration</title>
    <summary>  Initialization profoundly affects evolutionary algorithm (EA) efficacy by
dictating search trajectories and convergence. This study introduces a hybrid
initialization strategy combining empty-space search algorithm (ESA) and
opposition-based learning (OBL). OBL initially generates a diverse population,
subsequently augmented by ESA, which identifies under-explored regions. This
synergy enhances population diversity, accelerates convergence, and improves EA
performance on complex, high-dimensional optimization problems. Benchmark
results demonstrate the proposed method's superiority in solution quality and
convergence speed compared to conventional initialization techniques.
</summary>
    <author>
      <name>Xinyu Zhang</name>
    </author>
    <author>
      <name>Mário Antunes</name>
    </author>
    <author>
      <name>Tyler Estro</name>
    </author>
    <author>
      <name>Erez Zadok</name>
    </author>
    <author>
      <name>Klaus Mueller</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1145/3712255.3726720</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1145/3712255.3726720" rel="related"/>
    <link href="http://arxiv.org/abs/2505.05661v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.05661v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.08514v1</id>
    <updated>2025-05-13T12:47:13Z</updated>
    <published>2025-05-13T12:47:13Z</published>
    <title>Convolutional Spiking Neural Network for Image Classification</title>
    <summary>  We consider an implementation of convolutional architecture in a spiking
neural network (SNN) used to classify images. As in the traditional neural
network, the convolutional layers form informational "features" used as
predictors in the SNN-based classifier with CoLaNET architecture. Since weight
sharing contradicts the synaptic plasticity locality principle, the
convolutional weights are fixed in our approach. We describe a methodology for
their determination from a representative set of images from the same domain as
the classified ones. We illustrate and test our approach on a classification
task from the NEOVISION2 benchmark.
</summary>
    <author>
      <name>Mikhail Kiselev</name>
    </author>
    <author>
      <name>Andrey Lavrentyev</name>
    </author>
    <link href="http://arxiv.org/abs/2505.08514v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.08514v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2505.18969v1</id>
    <updated>2025-05-25T04:23:28Z</updated>
    <published>2025-05-25T04:23:28Z</published>
    <title>Machine Psychophysics: Cognitive Control in Vision-Language Models</title>
    <summary>  Cognitive control refers to the ability to flexibly coordinate thought and
action in pursuit of internal goals. A standard method for assessing cognitive
control involves conflict tasks that contrast congruent and incongruent trials,
measuring the ability to prioritize relevant information while suppressing
interference. We evaluate 108 vision-language models on three classic conflict
tasks and their more demanding "squared" variants across 2,220 trials. Model
performance corresponds closely to human behavior under resource constraints
and reveals individual differences. These results indicate that some form of
human-like executive function have emerged in current multi-modal foundational
models.
</summary>
    <author>
      <name>Dezhi Luo</name>
    </author>
    <author>
      <name>Maijunxian Wang</name>
    </author>
    <author>
      <name>Bingyang Wang</name>
    </author>
    <author>
      <name>Tianwei Zhao</name>
    </author>
    <author>
      <name>Yijiang Li</name>
    </author>
    <author>
      <name>Hokin Deng</name>
    </author>
    <link href="http://arxiv.org/abs/2505.18969v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2505.18969v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.1000v3</id>
    <updated>2010-03-07T12:56:29Z</updated>
    <published>2008-08-07T11:07:22Z</published>
    <title>Fitness Landscape Analysis for Dynamic Resource Allocation in Multiuser
  OFDM Based Cognitive Radio Systems</title>
    <summary>  This paper has been withdrawn.
</summary>
    <author>
      <name>Dong Huang</name>
    </author>
    <author>
      <name>Chunyan Miao</name>
    </author>
    <author>
      <name>Cyril Leung</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.1000v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.1000v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1205.4295v1</id>
    <updated>2012-05-19T04:25:04Z</updated>
    <published>2012-05-19T04:25:04Z</published>
    <title>Efficient Methods for Unsupervised Learning of Probabilistic Models</title>
    <summary>  In this thesis I develop a variety of techniques to train, evaluate, and
sample from intractable and high dimensional probabilistic models. Abstract
exceeds arXiv space limitations -- see PDF.
</summary>
    <author>
      <name>Jascha Sohl-Dickstein</name>
    </author>
    <link href="http://arxiv.org/abs/1205.4295v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1205.4295v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1510.00556v2</id>
    <updated>2016-02-08T07:06:58Z</updated>
    <published>2015-10-02T10:48:56Z</published>
    <title>Autonomous Perceptron Neural Network Inspired from Quantum computing</title>
    <summary>  This abstract will be modified after correcting the minor error in Eq.(2)
</summary>
    <author>
      <name>M. Zidan</name>
    </author>
    <author>
      <name>A. Sagheer</name>
    </author>
    <author>
      <name>N. Metwally</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn by the author due to a crucial sign
  error in equation 2</arxiv:comment>
    <link href="http://arxiv.org/abs/1510.00556v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1510.00556v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2104.05878v1</id>
    <updated>2021-04-13T00:57:39Z</updated>
    <published>2021-04-13T00:57:39Z</published>
    <title>On the validity of kernel approximations for orthogonally-initialized
  neural networks</title>
    <summary>  In this note we extend kernel function approximation results for neural
networks with Gaussian-distributed weights to single-layer networks initialized
using Haar-distributed random orthogonal matrices (with possible rescaling).
This is accomplished using recent results from random matrix theory.
</summary>
    <author>
      <name>James Martens</name>
    </author>
    <link href="http://arxiv.org/abs/2104.05878v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2104.05878v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1503.00036v2</id>
    <updated>2015-04-14T22:55:08Z</updated>
    <published>2015-02-27T23:50:22Z</published>
    <title>Norm-Based Capacity Control in Neural Networks</title>
    <summary>  We investigate the capacity, convexity and characterization of a general
family of norm-constrained feed-forward networks.
</summary>
    <author>
      <name>Behnam Neyshabur</name>
    </author>
    <author>
      <name>Ryota Tomioka</name>
    </author>
    <author>
      <name>Nathan Srebro</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">29 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1503.00036v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1503.00036v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09556v1</id>
    <updated>2017-06-29T02:43:37Z</updated>
    <published>2017-06-29T02:43:37Z</published>
    <title>Vision-based Detection of Acoustic Timed Events: a Case Study on
  Clarinet Note Onsets</title>
    <summary>  Acoustic events often have a visual counterpart. Knowledge of visual
information can aid the understanding of complex auditory scenes, even when
only a stereo mixdown is available in the audio domain, \eg identifying which
musicians are playing in large musical ensembles. In this paper, we consider a
vision-based approach to note onset detection. As a case study we focus on
challenging, real-world clarinetist videos and carry out preliminary
experiments on a 3D convolutional neural network based on multiple streams and
purposely avoiding temporal pooling. We release an audiovisual dataset with 4.5
hours of clarinetist videos together with cleaned annotations which include
about 36,000 onsets and the coordinates for a number of salient points and
regions of interest. By performing several training trials on our dataset, we
learned that the problem is challenging. We found that the CNN model is highly
sensitive to the optimization algorithm and hyper-parameters, and that treating
the problem as binary classification may prevent the joint optimization of
precision and recall. To encourage further research, we publicly share our
dataset, annotations and all models and detail which issues we came across
during our preliminary experiments.
</summary>
    <author>
      <name>A. Bazzica</name>
    </author>
    <author>
      <name>J. C. van Gemert</name>
    </author>
    <author>
      <name>C. C. S. Liem</name>
    </author>
    <author>
      <name>A. Hanjalic</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Conference on Deep Learning
  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc of the First Int Workshop on Deep Learning and Music.
  Anchorage, US. 1(1). pp 31-36 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.09556v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09556v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MM" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.10748v1</id>
    <updated>2020-06-17T22:03:31Z</updated>
    <published>2020-06-17T22:03:31Z</published>
    <title>Genetic Programming visitation scheduling solution can deliver a less
  austere COVID-19 pandemic population lockdown</title>
    <summary>  A computational methodology is introduced to minimize infection opportunities
for people suffering some degree of lockdown in response to a pandemic, as is
the 2020 COVID-19 pandemic. Persons use their mobile phone or computational
device to request trips to places of their need or interest indicating a rough
time of day: `morning', `afternoon', `night' or `any time' when they would like
to undertake these outings as well as the desired place to visit. An artificial
intelligence methodology which is a variant of Genetic Programming studies all
requests and responds with specific time allocations for such visits that
minimize the overall risks of infection, hospitalization and death of people. A
number of alternatives for this computation are presented and results of
numerical experiments involving over 230 people of various ages and background
health levels in over 1700 visits that take place over three consecutive days.
A novel partial infection model is introduced to discuss these proof of concept
solutions which are compared to round robin uninformed time scheduling for
visits to places. The computations indicate vast improvements with far fewer
dead and hospitalized. These auger well for a more realistic study using
accurate infection models with the view to test deployment in the real world.
The input that drives the infection model is the degree of infection by
taxonomic class, such as the information that may arise from population testing
for COVID-19 or, alternatively, any contamination model. The taxonomy class
assumed in the computations is the likely level of infection by age group.
</summary>
    <author>
      <name>Daniel Howard</name>
    </author>
    <link href="http://arxiv.org/abs/2006.10748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.10748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.soc-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0608073v1</id>
    <updated>2006-08-18T08:28:23Z</updated>
    <published>2006-08-18T08:28:23Z</published>
    <title>Parametrical Neural Networks and Some Other Similar Architectures</title>
    <summary>  A review of works on associative neural networks accomplished during last
four years in the Institute of Optical Neural Technologies RAS is given. The
presentation is based on description of parametrical neural networks (PNN). For
today PNN have record recognizing characteristics (storage capacity, noise
immunity and speed of operation). Presentation of basic ideas and principles is
accentuated.
</summary>
    <author>
      <name>Leonid B. Litinskii</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 2 figures, accepted for publication in "Optical Memory &amp;
  Neural Networks" (2006)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0608073v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0608073v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/nlin/0204038v1</id>
    <updated>2002-04-16T12:58:26Z</updated>
    <published>2002-04-16T12:58:26Z</published>
    <title>Neutrality: A Necessity for Self-Adaptation</title>
    <summary>  Self-adaptation is used in all main paradigms of evolutionary computation to
increase efficiency. We claim that the basis of self-adaptation is the use of
neutrality. In the absence of external control neutrality allows a variation of
the search distribution without the risk of fitness loss.
</summary>
    <author>
      <name>Marc Toussaint</name>
    </author>
    <author>
      <name>Christian Igel</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures, LaTeX</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Congress on Evolutionary Computation (CEC
  2002), 1354-1359.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/nlin/0204038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/nlin/0204038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.0883v5</id>
    <updated>2011-07-08T01:54:35Z</updated>
    <published>2007-09-06T16:04:42Z</published>
    <title>Liquid State Machines in Adbiatic Quantum Computers for General
  Computation</title>
    <summary>  Major mistakes do not read
</summary>
    <author>
      <name>Joshua Jay Herman</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Totally wrong</arxiv:comment>
    <link href="http://arxiv.org/abs/0709.0883v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.0883v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; F.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0805.0197v1</id>
    <updated>2008-05-02T09:20:11Z</updated>
    <published>2008-05-02T09:20:11Z</published>
    <title>Flatness of the Energy Landscape for Horn Clauses</title>
    <summary>  The Little-Hopfield neural network programmed with Horn clauses is studied.
We argue that the energy landscape of the system, corresponding to the
inconsistency function for logical interpretations of the sets of Horn clauses,
has minimal ruggedness. This is supported by computer simulations.
</summary>
    <author>
      <name>Saratha Sathasivam</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">USM</arxiv:affiliation>
    </author>
    <author>
      <name>Wan Ahmad Tajuddin Wan Abdullah</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Univ. Malaya</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Matematika 23 (2007) 147-156</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0805.0197v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0805.0197v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0808.0549v5</id>
    <updated>2010-02-26T07:41:18Z</updated>
    <published>2008-08-05T04:27:00Z</published>
    <title>Resource Allocation of MU-OFDM Based Cognitive Radio Systems Under
  Partial Channel State Information</title>
    <summary>  This paper has been withdrawn by the author due to some errors.
</summary>
    <author>
      <name>Dong Huang</name>
    </author>
    <author>
      <name>Chunyan Miao</name>
    </author>
    <author>
      <name>Cyril Leung</name>
    </author>
    <author>
      <name>Zhiqi Shen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This paper has been withdrawn</arxiv:comment>
    <link href="http://arxiv.org/abs/0808.0549v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0808.0549v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.CO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1108.5643v1</id>
    <updated>2011-08-29T16:36:05Z</updated>
    <published>2011-08-29T16:36:05Z</published>
    <title>Collective Adaptive Systems: Challenges Beyond Evolvability</title>
    <summary>  This position paper overviews several challenges of collective adaptive
systems, which are beyond the research objectives of current top-projects in
ICT, and especially in FET, initiatives. The attention is paid not only to
challenges and new research topics, but also to their impact and potential
breakthroughs in information and communication technologies.
</summary>
    <author>
      <name>Serge Kernbach</name>
    </author>
    <author>
      <name>Thomas Schmickl</name>
    </author>
    <author>
      <name>Jon Timmis</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Workshop "Fundamentals of Collective Adaptive Systems", European
  Commission, 3-4 November, 2009, Brussels</arxiv:comment>
    <link href="http://arxiv.org/abs/1108.5643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1108.5643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1208.5340v1</id>
    <updated>2012-08-27T08:58:46Z</updated>
    <published>2012-08-27T08:58:46Z</published>
    <title>New results of ant algorithms for the Linear Ordering Problem</title>
    <summary>  Ant-based algorithms are successful tools for solving complex problems. One
of these problems is the Linear Ordering Problem (LOP). The paper shows new
results on some LOP instances, using Ant Colony System (ACS) and the Step-Back
Sensitive Ant Model (SB-SAM).
</summary>
    <author>
      <name>Camelia-M. Pintea</name>
    </author>
    <author>
      <name>Camelia Chira</name>
    </author>
    <author>
      <name>D. Dumitrescu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 figures Zbl:06048718</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">An. Univ. Vest Timis., Ser. Mat.-Inform. 48, No. 3, 139-150 (2010)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1208.5340v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1208.5340v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1302.6031v1</id>
    <updated>2013-02-25T10:13:09Z</updated>
    <published>2013-02-25T10:13:09Z</published>
    <title>Phoneme discrimination using KS algebra I</title>
    <summary>  In our work we define a new algebra of operators as a substitute for fuzzy
logic. Its primary purpose is for construction of binary discriminators for
phonemes based on spectral content. It is optimized for design of
non-parametric computational circuits, and makes uses of 4 operations: $\min$,
$\max$, the difference and generalized additively homogenuous means.
</summary>
    <author>
      <name>Ondrej Such</name>
    </author>
    <link href="http://arxiv.org/abs/1302.6031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1302.6031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7; I.5.2; I.5.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1312.5853v4</id>
    <updated>2014-02-18T21:35:13Z</updated>
    <published>2013-12-20T08:45:07Z</published>
    <title>Multi-GPU Training of ConvNets</title>
    <summary>  In this work we evaluate different approaches to parallelize computation of
convolutional neural networks across several GPUs.
</summary>
    <author>
      <name>Omry Yadan</name>
    </author>
    <author>
      <name>Keith Adams</name>
    </author>
    <author>
      <name>Yaniv Taigman</name>
    </author>
    <author>
      <name>Marc'Aurelio Ranzato</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Machine Learning, Deep Learning, Convolutional Networks, Computer
  Vision, GPU, CUDA</arxiv:comment>
    <link href="http://arxiv.org/abs/1312.5853v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1312.5853v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1402.7351v1</id>
    <updated>2014-02-28T19:12:50Z</updated>
    <published>2014-02-28T19:12:50Z</published>
    <title>A Machine Learning Model for Stock Market Prediction</title>
    <summary>  Stock market prediction is the act of trying to determine the future value of
a company stock or other financial instrument traded on a financial exchange.
</summary>
    <author>
      <name>Osman Hegazy</name>
    </author>
    <author>
      <name>Omar S. Soliman</name>
    </author>
    <author>
      <name>Mustafa Abdul Salam</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 Pages. arXiv admin note: substantial text overlap with
  arXiv:1402.6366</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">International Journal of Computer Science and Telecommunications
  [Volume 4, Issue 12, December 2013]</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1402.7351v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1402.7351v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1406.0968v1</id>
    <updated>2014-06-04T08:25:56Z</updated>
    <published>2014-06-04T08:25:56Z</published>
    <title>Integration of a Predictive, Continuous Time Neural Network into
  Securities Market Trading Operations</title>
    <summary>  This paper describes recent development and test implementation of a
continuous time recurrent neural network that has been configured to predict
rates of change in securities. It presents outcomes in the context of popular
technical analysis indicators and highlights the potential impact of continuous
predictive capability on securities market trading operations.
</summary>
    <author>
      <name>Christopher S Kirk</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1406.0968v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1406.0968v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.3191v2</id>
    <updated>2014-12-14T03:18:33Z</updated>
    <published>2014-12-10T04:06:38Z</published>
    <title>Bach in 2014: Music Composition with Recurrent Neural Network</title>
    <summary>  We propose a framework for computer music composition that uses resilient
propagation (RProp) and long short term memory (LSTM) recurrent neural network.
In this paper, we show that LSTM network learns the structure and
characteristics of music pieces properly by demonstrating its ability to
recreate music. We also show that predicting existing music using RProp
outperforms Back propagation through time (BPTT).
</summary>
    <author>
      <name>I-Ting Liu</name>
    </author>
    <author>
      <name>Bhiksha Ramakrishnan</name>
    </author>
    <link href="http://arxiv.org/abs/1412.3191v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.3191v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1412.6061v1</id>
    <updated>2014-12-15T06:55:28Z</updated>
    <published>2014-12-15T06:55:28Z</published>
    <title>CITlab ARGUS for Arabic Handwriting</title>
    <summary>  In the recent years it turned out that multidimensional recurrent neural
networks (MDRNN) perform very well for offline handwriting recognition tasks
like the OpenHaRT 2013 evaluation DIR. With suitable writing preprocessing and
dictionary lookup, our ARGUS software completed this task with an error rate of
26.27% in its primary setup.
</summary>
    <author>
      <name>Gundram Leifert</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Rostock - CITlab</arxiv:affiliation>
    </author>
    <author>
      <name>Roger Labahn</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Rostock - CITlab</arxiv:affiliation>
    </author>
    <author>
      <name>Tobias Strauß</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Rostock - CITlab</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">http://www.nist.gov/itl/iad/mig/upload/OpenHaRT2013_SysDesc_CITLAB.pdf</arxiv:comment>
    <link href="http://arxiv.org/abs/1412.6061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1412.6061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T10, 68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1502.04163v1</id>
    <updated>2015-02-14T03:23:53Z</updated>
    <published>2015-02-14T03:23:53Z</published>
    <title>A Distributional Representation Model For Collaborative Filtering</title>
    <summary>  In this paper, we propose a very concise deep learning approach for
collaborative filtering that jointly models distributional representation for
users and items. The proposed framework obtains better performance when
compared against current state-of-art algorithms and that made the
distributional representation model a promising direction for further research
in the collaborative filtering.
</summary>
    <author>
      <name>Zhang Junlin</name>
    </author>
    <author>
      <name>Cai Heng</name>
    </author>
    <author>
      <name>Huang Tongwen</name>
    </author>
    <author>
      <name>Xue Huiping</name>
    </author>
    <link href="http://arxiv.org/abs/1502.04163v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1502.04163v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IR" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.07571v1</id>
    <updated>2015-04-28T17:08:18Z</updated>
    <published>2015-04-28T17:08:18Z</published>
    <title>Can Machines Truly Think</title>
    <summary>  Can machines truly think? This question and its answer have many implications
that depend, in large part, on any number of assumptions underlying how the
issue has been addressed or considered previously. A crucial question, and one
that is almost taken for granted, is the starting point for this discussion:
Can "thought" be achieved or emulated by algorithmic procedures?
</summary>
    <author>
      <name>Murat Okandan</name>
    </author>
    <link href="http://arxiv.org/abs/1504.07571v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.07571v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1504.08215v1</id>
    <updated>2015-04-30T13:26:46Z</updated>
    <published>2015-04-30T13:26:46Z</published>
    <title>Lateral Connections in Denoising Autoencoders Support Supervised
  Learning</title>
    <summary>  We show how a deep denoising autoencoder with lateral connections can be used
as an auxiliary unsupervised learning task to support supervised learning. The
proposed model is trained to minimize simultaneously the sum of supervised and
unsupervised cost functions by back-propagation, avoiding the need for
layer-wise pretraining. It improves the state of the art significantly in the
permutation-invariant MNIST classification task.
</summary>
    <author>
      <name>Antti Rasmus</name>
    </author>
    <author>
      <name>Harri Valpola</name>
    </author>
    <author>
      <name>Tapani Raiko</name>
    </author>
    <link href="http://arxiv.org/abs/1504.08215v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1504.08215v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.05053v1</id>
    <updated>2015-07-17T17:48:49Z</updated>
    <published>2015-07-17T17:48:49Z</published>
    <title>Massively Deep Artificial Neural Networks for Handwritten Digit
  Recognition</title>
    <summary>  Greedy Restrictive Boltzmann Machines yield an fairly low 0.72% error rate on
the famous MNIST database of handwritten digits. All that was required to
achieve this result was a high number of hidden layers consisting of many
neurons, and a graphics card to greatly speed up the rate of learning.
</summary>
    <author>
      <name>Keiron O'Shea</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 1 figure</arxiv:comment>
    <link href="http://arxiv.org/abs/1507.05053v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.05053v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1507.07374v1</id>
    <updated>2015-07-27T11:50:44Z</updated>
    <published>2015-07-27T11:50:44Z</published>
    <title>A genetic algorithm for autonomous navigation in partially observable
  domain</title>
    <summary>  The problem of autonomous navigation is one of the basic problems for
robotics. Although, in general, it may be challenging when an autonomous
vehicle is placed into partially observable domain. In this paper we consider
simplistic environment model and introduce a navigation algorithm based on
Learning Classifier System.
</summary>
    <author>
      <name>Maxim Borisyak</name>
    </author>
    <author>
      <name>Andrey Ustyuzhanin</name>
    </author>
    <link href="http://arxiv.org/abs/1507.07374v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1507.07374v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1603.08776v2</id>
    <updated>2016-05-19T11:58:22Z</updated>
    <published>2016-03-29T14:10:14Z</published>
    <title>COCO: The Experimental Procedure</title>
    <summary>  We present a budget-free experimental setup and procedure for benchmarking
numericaloptimization algorithms in a black-box scenario. This procedure can be
applied with the COCO benchmarking platform. We describe initialization of and
input to the algorithm and touch upon therelevance of termination and restarts.
</summary>
    <author>
      <name>Nikolaus Hansen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <author>
      <name>Tea Tusar</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <author>
      <name>Olaf Mersmann</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <author>
      <name>Anne Auger</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <author>
      <name>Dimo Brockhoff</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Inria</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">ArXiv e-prints, arXiv:1603.08776</arxiv:comment>
    <link href="http://arxiv.org/abs/1603.08776v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1603.08776v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.00404v1</id>
    <updated>2016-05-02T09:33:46Z</updated>
    <published>2016-05-02T09:33:46Z</published>
    <title>Simple2Complex: Global Optimization by Gradient Descent</title>
    <summary>  A method named simple2complex for modeling and training deep neural networks
is proposed. Simple2complex train deep neural networks by smoothly adding more
and more layers to the shallow networks, as the learning procedure going on,
the network is just like growing. Compared with learning by end2end,
simple2complex is with less possibility trapping into local minimal, namely,
owning ability for global optimization. Cifar10 is used for verifying the
superiority of simple2complex.
</summary>
    <author>
      <name>Ming Li</name>
    </author>
    <link href="http://arxiv.org/abs/1605.00404v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.00404v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1605.07009v2</id>
    <updated>2017-07-01T16:45:35Z</updated>
    <published>2016-05-23T13:31:59Z</published>
    <title>BMOBench: Black-Box Multi-Objective Optimization Benchmarking Platform</title>
    <summary>  This document briefly describes the Black-Box Multi-Objective Optimization
Benchmarking (BMOBench) platform. It presents the test problems, evaluation
procedure, and experimental setup. To this end, the BMOBench is demonstrated by
comparing recent multi-objective solvers from the literature, namely SMS-EMOA,
DMS, and MO-SOO.
</summary>
    <author>
      <name>Abdullah Al-Dujaili</name>
    </author>
    <author>
      <name>S. Suresh</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">supplement materials for Multi-Objective Simultaneous Optimistic
  Optimization (Information Sciences)</arxiv:comment>
    <link href="http://arxiv.org/abs/1605.07009v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1605.07009v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1705.01809v1</id>
    <updated>2017-05-04T12:20:56Z</updated>
    <published>2017-05-04T12:20:56Z</published>
    <title>Pixel Normalization from Numeric Data as Input to Neural Networks</title>
    <summary>  Text to image transformation for input to neural networks requires
intermediate steps. This paper attempts to present a new approach to pixel
normalization so as to convert textual data into image, suitable as input for
neural networks. This method can be further improved by its Graphics Processing
Unit (GPU) implementation to provide significant speedup in computational time.
</summary>
    <author>
      <name>Parth Sane</name>
    </author>
    <author>
      <name>Ravindra Agrawal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE WiSPNET 2017 conference in Chennai</arxiv:comment>
    <link href="http://arxiv.org/abs/1705.01809v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1705.01809v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1706.09557v1</id>
    <updated>2017-06-29T02:52:25Z</updated>
    <published>2017-06-29T02:52:25Z</published>
    <title>Machine listening intelligence</title>
    <summary>  This manifesto paper will introduce machine listening intelligence, an
integrated research framework for acoustic and musical signals modelling, based
on signal processing, deep learning and computational musicology.
</summary>
    <author>
      <name>C. E. Cella</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Conference on Deep Learning
  and Music, Anchorage, US, May, 2017 (arXiv:1706.08675v1 [cs.NE])</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the First International Workshop on Deep Learning
  and Music joint with IJCNN. Anchorage, US. 1(1). pp 50-55 (2017)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1706.09557v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1706.09557v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68Txx" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; H.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1707.06480v1</id>
    <updated>2017-07-20T12:46:09Z</updated>
    <published>2017-07-20T12:46:09Z</published>
    <title>Syllable-aware Neural Language Models: A Failure to Beat Character-aware
  Ones</title>
    <summary>  Syllabification does not seem to improve word-level RNN language modeling
quality when compared to character-based segmentation. However, our best
syllable-aware language model, achieving performance comparable to the
competitive character-aware model, has 18%-33% fewer parameters and is trained
1.2-2.2 times faster.
</summary>
    <author>
      <name>Zhenisbek Assylbekov</name>
    </author>
    <author>
      <name>Rustem Takhanov</name>
    </author>
    <author>
      <name>Bagdat Myrzakhmetov</name>
    </author>
    <author>
      <name>Jonathan N. Washington</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">EMNLP 2017</arxiv:comment>
    <link href="http://arxiv.org/abs/1707.06480v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1707.06480v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T50" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.7" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1801.03523v1</id>
    <updated>2018-01-09T03:35:20Z</updated>
    <published>2018-01-09T03:35:20Z</published>
    <title>Generative Models for Stochastic Processes Using Convolutional Neural
  Networks</title>
    <summary>  The present paper aims to demonstrate the usage of Convolutional Neural
Networks as a generative model for stochastic processes, enabling researchers
from a wide range of fields (such as quantitative finance and physics) to
develop a general tool for forecasts and simulations without the need to
identify/assume a specific system structure or estimate its parameters.
</summary>
    <author>
      <name>Fernando Fernandes Neto</name>
    </author>
    <link href="http://arxiv.org/abs/1801.03523v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1801.03523v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-fin.CP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04344v1</id>
    <updated>2018-06-12T05:57:14Z</updated>
    <published>2018-06-12T05:57:14Z</published>
    <title>Optimizing Variational Quantum Circuits using Evolution Strategies</title>
    <summary>  This version withdrawn by arXiv administrators because the submitter did not
have the right to agree to our license at the time of submission.
</summary>
    <author>
      <name>Johannes S. Otterbach</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This version withdrawn by arXiv administrators because the submitter
  did not have the right to agree to our license at the time of submission</arxiv:comment>
    <link href="http://arxiv.org/abs/1806.04344v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04344v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.04641v2</id>
    <updated>2018-06-18T06:08:45Z</updated>
    <published>2018-06-12T16:54:12Z</published>
    <title>Predicting Citation Counts with a Neural Network</title>
    <summary>  We here describe and present results of a simple neural network that predicts
individual researchers' future citation counts based on a variety of data from
the researchers' past. For publications available on the open access-server
arXiv.org we find a higher predictability than previous studies.
</summary>
    <author>
      <name>Tobias Mistele</name>
    </author>
    <author>
      <name>Tom Price</name>
    </author>
    <author>
      <name>Sabine Hossenfelder</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s11192-019-03110-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s11192-019-03110-2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 figure, 17 pages, typo fixed, reference updated</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Scientometrics (2019) 120: 87</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1806.04641v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.04641v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1806.08216v2</id>
    <updated>2018-06-26T11:16:36Z</updated>
    <published>2018-06-09T13:10:46Z</published>
    <title>Autoencoders for Multi-Label Prostate MR Segmentation</title>
    <summary>  Organ image segmentation can be improved by implementing prior knowledge
about the anatomy. One way of doing this is by training an autoencoder to learn
a lowdimensional representation of the segmentation. In this paper, this is
applied in multi-label prostate MR segmentation, with some positive results.
</summary>
    <author>
      <name>Ard de Gelder</name>
    </author>
    <author>
      <name>Henkjan Huisman</name>
    </author>
    <link href="http://arxiv.org/abs/1806.08216v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1806.08216v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.IV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1807.06540v1</id>
    <updated>2018-07-17T16:35:51Z</updated>
    <published>2018-07-17T16:35:51Z</published>
    <title>Icing on the Cake: An Easy and Quick Post-Learnig Method You Can Try
  After Deep Learning</title>
    <summary>  We found an easy and quick post-learning method named "Icing on the Cake" to
enhance a classification performance in deep learning. The method is that we
train only the final classifier again after an ordinary training is done.
</summary>
    <author>
      <name>Tomohiko Konno</name>
    </author>
    <author>
      <name>Michiaki Iwazume</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">3 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1807.06540v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1807.06540v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.02519v1</id>
    <updated>2019-01-12T14:27:19Z</updated>
    <published>2019-01-12T14:27:19Z</published>
    <title>RRAM based neuromorphic algorithms</title>
    <summary>  This submission is a report on RRAM based neuromorphic algorithms. This
report basically gives an overview of the algorithms implemented on
neuromorphic hardware with crossbar array of RRAM synapses. This report mainly
talks about the work on deep neural network to spiking neural network
conversion and its significance.
</summary>
    <author>
      <name>Roshan Gopalakrishnan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 6 figures, A version of this survey report is submitting as
  Springer book chapter</arxiv:comment>
    <link href="http://arxiv.org/abs/1903.02519v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.02519v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1903.11421v1</id>
    <updated>2019-03-27T13:41:17Z</updated>
    <published>2019-03-27T13:41:17Z</published>
    <title>Social Behavioral Phenotyping of Drosophila with a2D-3D Hybrid CNN
  Framework</title>
    <summary>  Behavioural phenotyping of Drosophila is an important means in biological and
medical research to identify genetic, pathologic or psychologic impact on
animal behaviour.
</summary>
    <author>
      <name>Ziping Jiang</name>
    </author>
    <author>
      <name>Paul L. Chazot</name>
    </author>
    <author>
      <name>M. Emre Celebi</name>
    </author>
    <author>
      <name>Danny Crookes</name>
    </author>
    <author>
      <name>Richard Jiang</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Access 2019</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/1903.11421v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1903.11421v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1907.07746v1</id>
    <updated>2019-07-17T20:26:21Z</updated>
    <published>2019-07-17T20:26:21Z</published>
    <title>Deep Invertible Networks for EEG-based brain-signal decoding</title>
    <summary>  In this manuscript, we investigate deep invertible networks for EEG-based
brain signal decoding and find them to generate realistic EEG signals as well
as classify novel signals above chance. Further ideas for their regularization
towards better decoding accuracies are discussed.
</summary>
    <author>
      <name>Robin Tibor Schirrmeister</name>
    </author>
    <author>
      <name>Tonio Ball</name>
    </author>
    <link href="http://arxiv.org/abs/1907.07746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1907.07746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2002.05202v1</id>
    <updated>2020-02-12T19:57:13Z</updated>
    <published>2020-02-12T19:57:13Z</published>
    <title>GLU Variants Improve Transformer</title>
    <summary>  Gated Linear Units (arXiv:1612.08083) consist of the component-wise product
of two linear projections, one of which is first passed through a sigmoid
function. Variations on GLU are possible, using different nonlinear (or even
linear) functions in place of sigmoid. We test these variants in the
feed-forward sublayers of the Transformer (arXiv:1706.03762)
sequence-to-sequence model, and find that some of them yield quality
improvements over the typically-used ReLU or GELU activations.
</summary>
    <author>
      <name>Noam Shazeer</name>
    </author>
    <link href="http://arxiv.org/abs/2002.05202v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2002.05202v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2004.08900v1</id>
    <updated>2020-04-19T16:28:35Z</updated>
    <published>2020-04-19T16:28:35Z</published>
    <title>The Cost of Training NLP Models: A Concise Overview</title>
    <summary>  We review the cost of training large-scale language models, and the drivers
of these costs. The intended audience includes engineers and scientists
budgeting their model-training experiments, as well as non-practitioners trying
to make sense of the economics of modern-day Natural Language Processing (NLP).
</summary>
    <author>
      <name>Or Sharir</name>
    </author>
    <author>
      <name>Barak Peleg</name>
    </author>
    <author>
      <name>Yoav Shoham</name>
    </author>
    <link href="http://arxiv.org/abs/2004.08900v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2004.08900v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2006.07237v1</id>
    <updated>2020-06-12T14:40:46Z</updated>
    <published>2020-06-12T14:40:46Z</published>
    <title>Power Consumption Variation over Activation Functions</title>
    <summary>  The power that machine learning models consume when making predictions can be
affected by a model's architecture. This paper presents various estimates of
power consumption for a range of different activation functions, a core factor
in neural network model architecture design. Substantial differences in
hardware performance exist between activation functions. This difference
informs how power consumption in machine learning models can be reduced.
</summary>
    <author>
      <name>Leon Derczynski</name>
    </author>
    <link href="http://arxiv.org/abs/2006.07237v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2006.07237v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2112.11687v1</id>
    <updated>2021-12-22T06:20:27Z</updated>
    <published>2021-12-22T06:20:27Z</published>
    <title>Squareplus: A Softplus-Like Algebraic Rectifier</title>
    <summary>  We present squareplus, an activation function that resembles softplus, but
which can be computed using only algebraic operations: addition,
multiplication, and square-root. Because squareplus is ~6x faster to evaluate
than softplus on a CPU and does not require access to transcendental functions,
it may have practical value in resource-limited deep learning applications.
</summary>
    <author>
      <name>Jonathan T. Barron</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">https://github.com/jonbarron/squareplus</arxiv:comment>
    <link href="http://arxiv.org/abs/2112.11687v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2112.11687v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1711.01416v1</id>
    <updated>2017-11-04T09:11:18Z</updated>
    <published>2017-11-04T09:11:18Z</published>
    <title>Language as a matrix product state</title>
    <summary>  We propose a statistical model for natural language that begins by
considering language as a monoid, then representing it in complex matrices with
a compatible translation invariant probability measure. We interpret the
probability measure as arising via the Born rule from a translation invariant
matrix product state.
</summary>
    <author>
      <name>Vasily Pestun</name>
    </author>
    <author>
      <name>John Terilla</name>
    </author>
    <author>
      <name>Yiannis Vlassopoulos</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/1711.01416v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1711.01416v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1805.09282v1</id>
    <updated>2018-05-23T16:54:50Z</updated>
    <published>2018-05-23T16:54:50Z</published>
    <title>On self-play computation of equilibrium in poker</title>
    <summary>  We compare performance of the genetic algorithm and the counterfactual regret
minimization algorithm in computing the near-equilibrium strategies in the
simplified poker games. We focus on the von Neumann poker and the simplified
version of the Texas Hold'Em poker, and test outputs of the considered
algorithms against analytical expressions defining the Nash equilibrium
strategies. We comment on the performance of the studied algorithms against
opponents deviating from equilibrium.
</summary>
    <author>
      <name>Mikhail Goykhman</name>
    </author>
    <link href="http://arxiv.org/abs/1805.09282v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1805.09282v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.GT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.00748v1</id>
    <updated>2019-05-21T07:09:12Z</updated>
    <published>2019-05-21T07:09:12Z</published>
    <title>Improving Minimal Gated Unit for Sequential Data</title>
    <summary>  In order to obtain a model which can process sequential data related to
machine translation and speech recognition faster and more accurately, we
propose adopting Chrono Initializer as the initialization method of Minimal
Gated Unit. We evaluated the method with two tasks: adding task and copy task.
As a result of the experiment, the effectiveness of the proposed method was
confirmed.
</summary>
    <author>
      <name>Kazuki Takamura</name>
    </author>
    <author>
      <name>Satoshi Yamane</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 5 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/1906.00748v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.00748v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/1906.02568v1</id>
    <updated>2019-06-06T13:18:03Z</updated>
    <published>2019-06-06T13:18:03Z</published>
    <title>Localizing Catastrophic Forgetting in Neural Networks</title>
    <summary>  Artificial neural networks (ANNs) suffer from catastrophic forgetting when
trained on a sequence of tasks. While this phenomenon was studied in the past,
there is only very limited recent research on this phenomenon. We propose a
method for determining the contribution of individual parameters in an ANN to
catastrophic forgetting. The method is used to analyze an ANNs response to
three different continual learning scenarios.
</summary>
    <author>
      <name>Felix Wiewel</name>
    </author>
    <author>
      <name>Bin Yang</name>
    </author>
    <link href="http://arxiv.org/abs/1906.02568v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/1906.02568v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2001.04505v1</id>
    <updated>2020-01-13T19:20:14Z</updated>
    <published>2020-01-13T19:20:14Z</published>
    <title>Fast Generation of Big Random Binary Trees</title>
    <summary>  random_tree() is a linear time and space C++ implementation able to create
trees of up to a billion nodes for genetic programming and genetic improvement
experiments. A 3.60GHz CPU can generate more than 18 million random nodes for
GP program trees per second.
</summary>
    <author>
      <name>William B. Langdon</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">C++ code:
  http://www.cs.ucl.ac.uk/staff/W.Langdon/ftp/gp-code/rand_tree.cc_r1.43</arxiv:comment>
    <link href="http://arxiv.org/abs/2001.04505v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2001.04505v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2009.09719v2</id>
    <updated>2020-09-28T13:27:14Z</updated>
    <published>2020-09-21T09:41:54Z</published>
    <title>A Survey on Machine Learning Applied to Dynamic Physical Systems</title>
    <summary>  This survey is on recent advancements in the intersection of physical
modeling and machine learning. We focus on the modeling of nonlinear systems
which are closer to electric motors. Survey on motor control and fault
detection in operation of electric motors has been done.
</summary>
    <author>
      <name>Sagar Verma</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">arXiv admin note: submission has been withdrawn by arXiv
  administrators due to inappropriate text overlap with external source</arxiv:comment>
    <link href="http://arxiv.org/abs/2009.09719v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2009.09719v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2102.12076v1</id>
    <updated>2021-02-24T05:43:44Z</updated>
    <published>2021-02-24T05:43:44Z</published>
    <title>Perspective: Purposeful Failure in Artificial Life and Artificial
  Intelligence</title>
    <summary>  Complex systems fail. I argue that failures can be a blueprint characterizing
living organisms and biological intelligence, a control mechanism to increase
complexity in evolutionary simulations, and an alternative to classical fitness
optimization. Imitating biological successes in Artificial Life and Artificial
Intelligence can be misleading; imitating failures offers a path towards
understanding and emulating life it in artificial systems.
</summary>
    <author>
      <name>Lana Sinapayen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2102.12076v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2102.12076v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2201.09746v1</id>
    <updated>2022-01-19T15:54:39Z</updated>
    <published>2022-01-19T15:54:39Z</published>
    <title>Reinforcement Learning Textbook</title>
    <summary>  This textbook covers principles behind main modern deep reinforcement
learning algorithms that achieved breakthrough results in many domains from
game AI to robotics. All required theory is explained with proofs using unified
notation and emphasize on the differences between different types of algorithms
and the reasons why they are constructed the way they are.
</summary>
    <author>
      <name>Sergey Ivanov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The text is in Russian</arxiv:comment>
    <link href="http://arxiv.org/abs/2201.09746v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2201.09746v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2205.04263v2</id>
    <updated>2022-06-01T13:29:46Z</updated>
    <published>2022-05-09T13:30:55Z</published>
    <title>Spiking Neural Network Equalization for IM/DD Optical Communication</title>
    <summary>  A spiking neural network (SNN) equalizer model suitable for electronic
neuromorphic hardware is designed for an IM/DD link. The SNN achieves the same
bit-error-rate as an artificial neural network, outperforming linear
equalization.
</summary>
    <author>
      <name>Elias Arnold</name>
    </author>
    <author>
      <name>Georg Böcherer</name>
    </author>
    <author>
      <name>Eric Müller</name>
    </author>
    <author>
      <name>Philipp Spilger</name>
    </author>
    <author>
      <name>Johannes Schemmel</name>
    </author>
    <author>
      <name>Stefano Calabrò</name>
    </author>
    <author>
      <name>Maxim Kuschnerov</name>
    </author>
    <link href="http://arxiv.org/abs/2205.04263v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2205.04263v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2208.11337v1</id>
    <updated>2022-08-24T07:36:05Z</updated>
    <published>2022-08-24T07:36:05Z</published>
    <title>A Bayesian Variational principle for dynamic Self Organizing Maps</title>
    <summary>  We propose organisation conditions that yield a method for training SOM with
adaptative neighborhood radius in a variational Bayesian framework. This method
is validated on a non-stationary setting and compared in an high-dimensional
setting with an other adaptative method.
</summary>
    <author>
      <name>Anthony Fillion</name>
    </author>
    <author>
      <name>Thibaut Kulak</name>
    </author>
    <author>
      <name>François Blayo</name>
    </author>
    <link href="http://arxiv.org/abs/2208.11337v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2208.11337v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2211.16038v1</id>
    <updated>2022-11-29T09:22:42Z</updated>
    <published>2022-11-29T09:22:42Z</published>
    <title>Data-efficient Modeling of Optical Matrix Multipliers Using Transfer
  Learning</title>
    <summary>  We demonstrate transfer learning-assisted neural network models for optical
matrix multipliers with scarce measurement data. Our approach uses &lt;10\% of
experimental data needed for best performance and outperforms analytical models
for a Mach-Zehnder interferometer mesh.
</summary>
    <author>
      <name>Ali Cem</name>
    </author>
    <author>
      <name>Ognjen Jovanovic</name>
    </author>
    <author>
      <name>Siqi Yan</name>
    </author>
    <author>
      <name>Yunhong Ding</name>
    </author>
    <author>
      <name>Darko Zibar</name>
    </author>
    <author>
      <name>Francesco Da Ros</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">2 pages, 2 figues, submitted to CLEO</arxiv:comment>
    <link href="http://arxiv.org/abs/2211.16038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2211.16038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.ET" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2301.08332v2</id>
    <updated>2023-02-07T22:01:41Z</updated>
    <published>2023-01-03T18:33:34Z</published>
    <title>Improving Reflexive Surfaces Efficiency with Genetic Algorithms</title>
    <summary>  We propose using a Genetic Algorithm to improve the efficiency of reflexive
surfaces in devices where the receiver's position is different from the classic
parabolic antenna. With this technique, we show that we can improve the
efficiency of the ARAPUCA photodetector.
</summary>
    <author>
      <name>A. Steklain</name>
    </author>
    <author>
      <name>M. Adames</name>
    </author>
    <author>
      <name>F. Ganacim</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1088/1748-0221/18/03/P03008</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1088/1748-0221/18/03/P03008" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 8 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2301.08332v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2301.08332v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.ins-det" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2309.00644v1</id>
    <updated>2023-08-30T10:02:52Z</updated>
    <published>2023-08-30T10:02:52Z</published>
    <title>Ten New Benchmarks for Optimization</title>
    <summary>  Benchmarks are used for testing new optimization algorithms and their
variants to evaluate their performance. Most existing benchmarks are smooth
functions. This chapter introduces ten new benchmarks with different
properties, including noise, discontinuity, parameter estimation and unknown
paths.
</summary>
    <author>
      <name>Xin-She Yang</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/978-981-99-3970-1_2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/978-981-99-3970-1_2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 6 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2309.00644v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2309.00644v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="68T20, 90C26, 65Y20" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2312.12909v1</id>
    <updated>2023-12-20T10:45:24Z</updated>
    <published>2023-12-20T10:45:24Z</published>
    <title>Energy-efficient Spiking Neural Network Equalization for IM/DD Systems
  with Optimized Neural Encoding</title>
    <summary>  We propose an energy-efficient equalizer for IM/DD systems based on spiking
neural networks. We optimize a neural spike encoding that boosts the
equalizer's performance while decreasing energy consumption.
</summary>
    <author>
      <name>Alexander von Bank</name>
    </author>
    <author>
      <name>Eike-Manuel Edelmann</name>
    </author>
    <author>
      <name>Laurent Schmalen</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Accepted for publication at OFC 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2312.12909v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2312.12909v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2401.01732v1</id>
    <updated>2024-01-03T13:11:59Z</updated>
    <published>2024-01-03T13:11:59Z</published>
    <title>Task and Explanation Network</title>
    <summary>  Explainability in deep networks has gained increased importance in recent
years. We argue herein that an AI must be tasked not just with a task but also
with an explanation of why said task was accomplished as such. We present a
basic framework -- Task and Explanation Network (TENet) -- which fully
integrates task completion and its explanation. We believe that the field of AI
as a whole should insist -- quite emphatically -- on explainability.
</summary>
    <author>
      <name>Moshe Sipper</name>
    </author>
    <link href="http://arxiv.org/abs/2401.01732v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2401.01732v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2407.00452v1</id>
    <updated>2024-06-29T14:36:37Z</updated>
    <published>2024-06-29T14:36:37Z</published>
    <title>KHNNs: hypercomplex neural networks computations via Keras using
  TensorFlow and PyTorch</title>
    <summary>  Neural networks used in computations with more advanced algebras than real
numbers perform better in some applications. However, there is no general
framework for constructing hypercomplex neural networks. We propose a library
integrated with Keras that can do computations within TensorFlow and PyTorch.
It provides Dense and Convolutional 1D, 2D, and 3D layers architectures.
</summary>
    <author>
      <name>Agnieszka Niemczynowicz</name>
    </author>
    <author>
      <name>Radosław Antoni Kycia</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.softx.2025.102163</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.softx.2025.102163" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">SoftwareX, 30 102163 (2025)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/2407.00452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2407.00452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.07468v1</id>
    <updated>2024-08-28T05:31:09Z</updated>
    <published>2024-08-28T05:31:09Z</published>
    <title>Machine Learning of Nonlinear Dynamical Systems with Control Parameters
  Using Feedforward Neural Networks</title>
    <summary>  Several authors have reported that the echo state network reproduces
bifurcation diagrams of some nonlinear differential equations using the data
for a few control parameters. We demonstrate that a simpler feedforward neural
network can also reproduce the bifurcation diagram of the logistics map and
synchronization transition in globally coupled Stuart-Landau equations.
</summary>
    <author>
      <name>Hidetsugu Sakaguchi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.07468v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.07468v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.CD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2409.12913v1</id>
    <updated>2024-09-19T17:10:14Z</updated>
    <published>2024-09-19T17:10:14Z</published>
    <title>Universal approximation theorem for neural networks with inputs from a
  topological vector space</title>
    <summary>  We study feedforward neural networks with inputs from a topological vector
space (TVS-FNNs). Unlike traditional feedforward neural networks, TVS-FNNs can
process a broader range of inputs, including sequences, matrices, functions and
more. We prove a universal approximation theorem for TVS-FNNs, which
demonstrates their capacity to approximate any continuous function defined on
this expanded input space.
</summary>
    <author>
      <name>Vugar Ismailov</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/2409.12913v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2409.12913v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
    <category term="41A30, 41A65, 68T05" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2412.18979v1</id>
    <updated>2024-12-25T20:21:24Z</updated>
    <published>2024-12-25T20:21:24Z</published>
    <title>Quantum memristors for neuromorphic quantum machine learning</title>
    <summary>  Quantum machine learning may permit to realize more efficient machine
learning calculations with near-term quantum devices. Among the diverse quantum
machine learning paradigms which are currently being considered, quantum
memristors are promising as a way of combining, in the same quantum hardware, a
unitary evolution with the nonlinearity provided by the measurement and
feedforward. Thus, an efficient way of deploying neuromorphic quantum computing
for quantum machine learning may be enabled.
</summary>
    <author>
      <name>Lucas Lamata</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Invited Perspective</arxiv:comment>
    <link href="http://arxiv.org/abs/2412.18979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.18979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="quant-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2502.17452v1</id>
    <updated>2025-02-08T04:20:28Z</updated>
    <published>2025-02-08T04:20:28Z</published>
    <title>Physics Informed Neural Network Estimated Circuit Parameter Adaptive
  Modulation of DAB</title>
    <summary>  This article presents the development, implementation, and validation of a
loss-optimized and circuit parameter-sensitive TPS modulation scheme for a
dual-active-bridge DC-DC converter. The proposed approach dynamically adjusts
control parameters based on circuit parameters estimated using a
physics-informed neural network.
</summary>
    <author>
      <name>Saikat Dey</name>
    </author>
    <author>
      <name>Ayan Mallik</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">21 pages, 24 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/2502.17452v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2502.17452v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SY" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2503.13001v1</id>
    <updated>2025-03-17T09:56:39Z</updated>
    <published>2025-03-17T09:56:39Z</published>
    <title>Linear-Size Neural Network Representation of Piecewise Affine Functions
  in $\mathbb{R}^2$</title>
    <summary>  It is shown that any continuous piecewise affine (CPA) function
$\mathbb{R}^2\to\mathbb{R}$ with $p$ pieces can be represented by a ReLU neural
network with two hidden layers and $O(p)$ neurons. Unlike prior work, which
focused on convex pieces, this analysis considers CPA functions with connected
but potentially non-convex pieces.
</summary>
    <author>
      <name>Leo Zanotti</name>
    </author>
    <link href="http://arxiv.org/abs/2503.13001v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2503.13001v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.ML" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809049v1</id>
    <updated>1998-09-23T11:01:55Z</updated>
    <published>1998-09-23T11:01:55Z</published>
    <title>Aspects of Evolutionary Design by Computers</title>
    <summary>  This paper examines the four main types of Evolutionary Design by computers:
Evolutionary Design Optimisation, Evolutionary Art, Evolutionary Artificial
Life Forms and Creative Evolutionary Design. Definitions for all four areas are
provided. A review of current work in each of these areas is given, with
examples of the types of applications that have been tackled. The different
properties and requirements of each are examined. Descriptions of typical
representations and evolutionary algorithms are provided and examples of
designs evolved using these techniques are shown. The paper then discusses how
the boundaries of these areas are beginning to merge, resulting in four new
'overlapping' types of Evolutionary Design: Integral Evolutionary Design,
Artificial Life Based Evolutionary Design, Aesthetic Evolutionary AL and
Aesthetic Evolutionary Design. Finally, the last part of the paper discusses
some common problems faced by creators of Evolutionary Design systems,
including: interdependent elements in designs, epistasis, and constraint
handling.
</summary>
    <author>
      <name>Peter J Bentley</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In Proceedings of the 3rd On-line World Conference on Soft Computing
  in Engineering Design and Manufacturing (WSC3)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9809049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A.1;E.2;F.4.1;I.2.0;I.2.6;I.2.8;I.2.9;I.2.11;I.3.5;I.6.0;J.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9809123v1</id>
    <updated>1998-09-30T04:06:36Z</updated>
    <published>1998-09-30T04:06:36Z</published>
    <title>A role of constraint in self-organization</title>
    <summary>  In this paper we introduce a neural network model of self-organization. This
model uses a variation of Hebb rule for updating its synaptic weights, and
surely converges to the equilibrium status. The key point of the convergence is
the update rule that constrains the total synaptic weight and this seems to
make the model stable. We investigate the role of the constraint and show that
it is the constraint that makes the model stable. For analyzing this setting,
we propose a simple probabilistic game that models the neural network and the
self-organization process. Then, we investigate the characteristics of this
game, namely, the probability that the game becomes stable and the number of
the steps it takes.
</summary>
    <author>
      <name>Carlos Domingo</name>
    </author>
    <author>
      <name>Osamu Watanabe</name>
    </author>
    <author>
      <name>Tadashi Yamazaki</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in the Proc. RANDOM'98, Oct. 1998</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9809123v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9809123v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6;J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9812003v1</id>
    <updated>1998-12-03T10:09:19Z</updated>
    <published>1998-12-03T10:09:19Z</published>
    <title>Neural Network Methods for Boundary Value Problems Defined in
  Arbitrarily Shaped Domains</title>
    <summary>  Partial differential equations (PDEs) with Dirichlet boundary conditions
defined on boundaries with simple geometry have been succesfuly treated using
sigmoidal multilayer perceptrons in previous works. This article deals with the
case of complex boundary geometry, where the boundary is determined by a number
of points that belong to it and are closely located, so as to offer a
reasonable representation. Two networks are employed: a multilayer perceptron
and a radial basis function network. The later is used to account for the
satisfaction of the boundary conditions. The method has been successfuly tested
on two-dimensional and three-dimensional PDEs and has yielded accurate
solutions.
</summary>
    <author>
      <name>I. E. Lagaris</name>
    </author>
    <author>
      <name>A. Likas</name>
    </author>
    <author>
      <name>D. G. Papageorgiou</name>
    </author>
    <link href="http://arxiv.org/abs/cs/9812003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9812003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cond-mat.dis-nn" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.MP" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.NA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.comp-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9812013v1</id>
    <updated>1998-12-11T20:51:38Z</updated>
    <published>1998-12-11T20:51:38Z</published>
    <title>The Self-Organizing Symbiotic Agent</title>
    <summary>  In [N. A. Baas, Emergence, Hierarchies, and Hyper-structures, in C.G. Langton
ed., Artificial Life III, Addison Wesley, 1994.] a general framework for the
study of Emergence and hyper-structure was presented. This approach is mostly
concerned with the description of such systems. In this paper we will try to
bring forth a different aspect of this model we feel will be useful in the
engineering of agent based solutions, namely the symbiotic approach. In this
approach a self-organizing method of dividing the more complex "main-problem"
to a hyper-structure of "sub-problems" with the aim of reducing complexity is
desired. A description of the general problem will be given along with some
instances of related work. This paper is intended to serve as an introductory
challenge for general solutions to the described problem.
</summary>
    <author>
      <name>Babak Hodjat</name>
    </author>
    <author>
      <name>Makoto Amamiya</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9812013v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9812013v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8; D.2.11;F.1.13; I.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9902006v1</id>
    <updated>1999-02-02T16:17:16Z</updated>
    <published>1999-02-02T16:17:16Z</published>
    <title>A Discipline of Evolutionary Programming</title>
    <summary>  Genetic fitness optimization using small populations or small population
updates across generations generally suffers from randomly diverging
evolutions. We propose a notion of highly probable fitness optimization through
feasible evolutionary computing runs on small size populations. Based on
rapidly mixing Markov chains, the approach pertains to most types of
evolutionary genetic algorithms, genetic programming and the like. We establish
that for systems having associated rapidly mixing Markov chains and appropriate
stationary distributions the new method finds optimal programs (individuals)
with probability almost 1. To make the method useful would require a structured
design methodology where the development of the program and the guarantee of
the rapidly mixing property go hand in hand. We analyze a simple example to
show that the method is implementable. More significant examples require
theoretical advances, for example with respect to the Metropolis filter.
</summary>
    <author>
      <name>Paul Vitanyi</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 pages, LaTeX source, Theoretical Computer Science, To appear</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Theoret. Comp. Sci., 241:1-2 (2000), 3--23.</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/9902006v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9902006v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2,E.1,F.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/9902025v1</id>
    <updated>1999-02-12T09:18:53Z</updated>
    <published>1999-02-12T09:18:53Z</published>
    <title>An Efficient Mean Field Approach to the Set Covering Problem</title>
    <summary>  A mean field feedback artificial neural network algorithm is developed and
explored for the set covering problem. A convenient encoding of the inequality
constraints is achieved by means of a multilinear penalty function. An
approximate energy minimum is obtained by iterating a set of mean field
equations, in combination with annealing. The approach is numerically tested
against a set of publicly available test problems with sizes ranging up to
5x10^3 rows and 10^6 columns. When comparing the performance with exact results
for sizes where these are available, the approach yields results within a few
percent from the optimal solutions. Comparisons with other approximate methods
also come out well, in particular given the very low CPU consumption required
-- typically a few seconds. Arbitrary problems can be processed using the
algorithm via a public domain server.
</summary>
    <author>
      <name>Mattias Ohlsson</name>
    </author>
    <author>
      <name>Carsten Peterson</name>
    </author>
    <author>
      <name>Bo Söderberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/9902025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/9902025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0006039v1</id>
    <updated>2000-06-28T10:17:43Z</updated>
    <published>2000-06-28T10:17:43Z</published>
    <title>Orthogonal Least Squares Algorithm for the Approximation of a Map and
  its Derivatives with a RBF Network</title>
    <summary>  Radial Basis Function Networks (RBFNs) are used primarily to solve
curve-fitting problems and for non-linear system modeling. Several algorithms
are known for the approximation of a non-linear curve from a sparse data set by
means of RBFNs. However, there are no procedures that permit to define
constrains on the derivatives of the curve. In this paper, the Orthogonal Least
Squares algorithm for the identification of RBFNs is modified to provide the
approximation of a non-linear 1-in 1-out map along with its derivatives, given
a set of training data. The interest on the derivatives of non-linear functions
concerns many identification and control tasks where the study of system
stability and robustness is addressed. The effectiveness of the proposed
algorithm is demonstrated by a study on the stability of a single loop feedback
system.
</summary>
    <author>
      <name>Carlo Drioli</name>
    </author>
    <author>
      <name>Davide Rocchesso</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 8 figures, submitted to IEEE Trans. on Systems, Man, and
  Cybernetics</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0006039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0006039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SD" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0010002v1</id>
    <updated>2000-09-30T14:37:23Z</updated>
    <published>2000-09-30T14:37:23Z</published>
    <title>Noise Effects in Fuzzy Modelling Systems</title>
    <summary>  Noise is source of ambiguity for fuzzy systems. Although being an important
aspect, the effects of noise in fuzzy modeling have been little investigated.
This paper presents a set of tests using three well-known fuzzy modeling
algorithms. These evaluate perturbations in the extracted rule-bases caused by
noise polluting the learning data, and the corresponding deformations in each
learned functional relation. We present results to show: 1) how these fuzzy
modeling systems deal with noise; 2) how the established fuzzy model structure
influences noise sensitivity of each algorithm; and 3) whose characteristics of
the learning algorithms are relevant to noise attenuation.
</summary>
    <author>
      <name>P. J. Costa Branco</name>
    </author>
    <author>
      <name>J. A. Dente</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In: Computational Intelligence and Applications, pp. 103-108,
  World Scientific and Engineering Society Press, Danvers, USA, 1999</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0010002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0010002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1; I.5.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0102014v1</id>
    <updated>2001-02-18T19:17:18Z</updated>
    <published>2001-02-18T19:17:18Z</published>
    <title>On the predictability of Rainfall in Kerala- An application of ABF
  Neural Network</title>
    <summary>  Rainfall in Kerala State, the southern part of Indian Peninsula in particular
is caused by the two monsoons and the two cyclones every year. In general,
climate and rainfall are highly nonlinear phenomena in nature giving rise to
what is known as the `butterfly effect'. We however attempt to train an ABF
neural network on the time series rainfall data and show for the first time
that in spite of the fluctuations resulting from the nonlinearity in the
system, the trends in the rainfall pattern in this corner of the globe have
remained unaffected over the past 87 years from 1893 to 1980. We also
successfully filter out the chaotic part of the system and illustrate that its
effects are marginal over long term predictions.
</summary>
    <author>
      <name>Ninan Sajeeth Philip</name>
    </author>
    <author>
      <name>K. Babu Joseph</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0102014v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0102014v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A0" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0104011v1</id>
    <updated>2001-04-06T21:15:31Z</updated>
    <published>2001-04-06T21:15:31Z</published>
    <title>Potholes on the Royal Road</title>
    <summary>  It is still unclear how an evolutionary algorithm (EA) searches a fitness
landscape, and on what fitness landscapes a particular EA will do well. The
validity of the building-block hypothesis, a major tenet of traditional genetic
algorithm theory, remains controversial despite its continued use to justify
claims about EAs. This paper outlines a research program to begin to answer
some of these open questions, by extending the work done in the royal road
project. The short-term goal is to find a simple class of functions which the
simple genetic algorithm optimizes better than other optimization methods, such
as hillclimbers. A dialectical heuristic for searching for such a class is
introduced. As an example of using the heuristic, the simple genetic algorithm
is compared with a set of hillclimbers on a simple subset of the
hyperplane-defined functions, the pothole functions.
</summary>
    <author>
      <name>Theodore C. Belding</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages; to appear in GECCO 2001</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0104011v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0104011v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0108009v1</id>
    <updated>2001-08-17T11:30:12Z</updated>
    <published>2001-08-17T11:30:12Z</published>
    <title>Artificial Neurons with Arbitrarily Complex Internal Structures</title>
    <summary>  Artificial neurons with arbitrarily complex internal structure are
introduced. The neurons can be described in terms of a set of internal
variables, a set activation functions which describe the time evolution of
these variables and a set of characteristic functions which control how the
neurons interact with one another. The information capacity of attractor
networks composed of these generalized neurons is shown to reach the maximum
allowed bound. A simple example taken from the domain of pattern recognition
demonstrates the increased computational power of these neurons. Furthermore, a
specific class of generalized neurons gives rise to a simple transformation
relating attractor networks of generalized neurons to standard three layer
feed-forward networks. Given this correspondence, we conjecture that the
maximum information capacity of a three layer feed-forward network is 2 bits
per weight.
</summary>
    <author>
      <name>G. A. Kohring</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 2 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neurocomputing, vol. 47, pp. 103-118 (2002).</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0108009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0108009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0110021v1</id>
    <updated>2001-10-08T11:34:34Z</updated>
    <published>2001-10-08T11:34:34Z</published>
    <title>Alife Model of Evolutionary Emergence of Purposeful Adaptive Behavior</title>
    <summary>  The process of evolutionary emergence of purposeful adaptive behavior is
investigated by means of computer simulations. The model proposed implies that
there is an evolving population of simple agents, which have two natural needs:
energy and reproduction. Any need is characterized quantitatively by a
corresponding motivation. Motivations determine goal-directed behavior of
agents. The model demonstrates that purposeful behavior does emerge in the
simulated evolutionary processes. Emergence of purposefulness is accompanied by
origin of a simple hierarchy in the control system of agents.
</summary>
    <author>
      <name>Mikhail S. Burtsev</name>
    </author>
    <author>
      <name>Vladimir G. Redko</name>
    </author>
    <author>
      <name>Roman V. Gusarev</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 5 figures. Full version of poster presentation on ECAL 2001
  (see "Advances in Artificial Life." J. Kelemen, P. Sosik (Eds.), 6th European
  Conference, ECAL 2001, Prague, Czech Republic, September 10-14, 2001,
  Proceedings, p. 413.)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0110021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0110021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.8; I.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0201024v2</id>
    <updated>2018-11-30T07:50:44Z</updated>
    <published>2002-01-27T21:01:45Z</published>
    <title>Design of statistical quality control procedures using genetic
  algorithms</title>
    <summary>  In general, we can not use algebraic or enumerative methods to optimize a
quality control (QC) procedure so as to detect the critical random and
systematic analytical errors with stated probabilities, while the probability
for false rejection is minimum. Genetic algorithms (GAs) offer an alternative,
as they do not require knowledge of the objective function to be optimized and
search through large parameter spaces quickly. To explore the application of
GAs in statistical QC, we have developed an interactive GAs based computer
program that designs a novel near optimal QC procedure, given an analytical
process. The program uses the deterministic crowding algorithm. An illustrative
application of the program suggests that it has the potential to design QC
procedures that are significantly better than 45 alternative ones that are used
in the clinical laboratories.
</summary>
    <author>
      <name>Aristides T. Hatjimihail</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Hellenic Complex Systems Laboratory, Drama, Greece</arxiv:affiliation>
    </author>
    <author>
      <name>Theophanes T. Hatjimihail</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Hellenic Complex Systems Laboratory, Drama, Greece</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">11 pages, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">LJ Eshelman (ed): Proceedings of the Sixth International
  Conference on Genetic Algorithms. San Francisco: Morgan Kauffman, 1995:551-7</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0201024v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0201024v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0205061v1</id>
    <updated>2002-05-23T15:34:57Z</updated>
    <published>2002-05-23T15:34:57Z</published>
    <title>Aging, double helix and small world property in genetic algorithms</title>
    <summary>  Over a quarter of century after the invention of genetic algorithms and
miriads of their modifications, as well as successful implementations, we are
still lacking many essential details of thorough analysis of it's inner
working. One of such fundamental questions is: how many generations do we need
to solve the optimization problem? This paper tries to answer this question,
albeit in a fuzzy way, making use of the double helix concept. As a byproduct
we gain better understanding of the ways, in which the genetic algorithm may be
fine tuned.
</summary>
    <author>
      <name>Marek W. Gutowski</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the workshop on evolutionary algorithms, Krakow
  (Cracow), Poland, Sept. 30, 2002, 6 pages, no figures, LaTeX 2.09 requires
  kaeog.sty (included)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0205061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0205061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DS" scheme="http://arxiv.org/schemas/atom"/>
    <category term="physics.data-an" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.1; G.1.6; I.1.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0210012v1</id>
    <updated>2002-10-14T09:00:23Z</updated>
    <published>2002-10-14T09:00:23Z</published>
    <title>Selection of future events from a time series in relation to estimations
  of forecasting uncertainty</title>
    <summary>  A new general procedure for a priori selection of more predictable events
from a time series of observed variable is proposed. The procedure is
applicable to time series which contains different types of events that feature
significantly different predictability, or, in other words, to heteroskedastic
time series. A priori selection of future events in accordance to expected
uncertainty of their forecasts may be helpful for making practical decisions.
The procedure first implies creation of two neural network based forecasting
models, one of which is aimed at prediction of conditional mean and other -
conditional dispersion, and then elaboration of the rule for future event
selection into groups of more and less predictable events. The method is
demonstrated and tested by the example of the computer generated time series,
and then applied to the real world time series, Dow Jones Industrial Average
index.
</summary>
    <author>
      <name>Igor B. Konovalov</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0210012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0210012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.2; J.4" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0212010v1</id>
    <updated>2002-12-08T00:26:49Z</updated>
    <published>2002-12-08T00:26:49Z</published>
    <title>JohnnyVon: Self-Replicating Automata in Continuous Two-Dimensional Space</title>
    <summary>  JohnnyVon is an implementation of self-replicating automata in continuous
two-dimensional space. Two types of particles drift about in a virtual liquid.
The particles are automata with discrete internal states but continuous
external relationships. Their internal states are governed by finite state
machines but their external relationships are governed by a simulated physics
that includes brownian motion, viscosity, and spring-like attractive and
repulsive forces. The particles can be assembled into patterns that can encode
arbitrary strings of bits. We demonstrate that, if an arbitrary "seed" pattern
is put in a "soup" of separate individual particles, the pattern will replicate
by assembling the individual particles into copies of itself. We also show
that, given sufficient time, a soup of separate individual particles will
eventually spontaneously form self-replicating patterns. We discuss the
implications of JohnnyVon for research in nanotechnology, theoretical biology,
and artificial life.
</summary>
    <author>
      <name>Arnold Smith</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National Research Council of Canada</arxiv:affiliation>
    </author>
    <author>
      <name>Peter Turney</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National Research Council of Canada</arxiv:affiliation>
    </author>
    <author>
      <name>Robert Ewaschuk</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Waterloo</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, issued 2002, Java code available at
  http://purl.org/net/johnnyvon/</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0212010v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0212010v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.3; I.6.8; J.2; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0212019v1</id>
    <updated>2002-12-10T15:18:33Z</updated>
    <published>2002-12-10T15:18:33Z</published>
    <title>Thinking, Learning, and Autonomous Problem Solving</title>
    <summary>  Ever increasing computational power will require methods for automatic
programming. We present an alternative to genetic programming, based on a
general model of thinking and learning. The advantage is that evolution takes
place in the space of constructs and can thus exploit the mathematical
structures of this space. The model is formalized, and a macro language is
presented which allows for a formal yet intuitive description of the problem
under consideration. A prototype has been developed to implement the scheme in
PERL. This method will lead to a concentration on the analysis of problems, to
a more rapid prototyping, to the treatment of new problem classes, and to the
investigation of philosophical problems. We see fields of application in
nonlinear differential equations, pattern recognition, robotics, model
building, and animated pictures.
</summary>
    <author>
      <name>Joerg D. Becker</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0212019v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0212019v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.1.1; I.2.0; I.2.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0212021v1</id>
    <updated>2002-12-10T15:52:43Z</updated>
    <published>2002-12-10T15:52:43Z</published>
    <title>A Simple Model of Unbounded Evolutionary Versatility as a Largest-Scale
  Trend in Organismal Evolution</title>
    <summary>  The idea that there are any large-scale trends in the evolution of biological
organisms is highly controversial. It is commonly believed, for example, that
there is a large-scale trend in evolution towards increasing complexity, but
empirical and theoretical arguments undermine this belief. Natural selection
results in organisms that are well adapted to their local environments, but it
is not clear how local adaptation can produce a global trend. In this paper, I
present a simple computational model, in which local adaptation to a randomly
changing environment results in a global trend towards increasing evolutionary
versatility. In this model, for evolutionary versatility to increase without
bound, the environment must be highly dynamic. The model also shows that
unbounded evolutionary versatility implies an accelerating evolutionary pace. I
believe that unbounded increase in evolutionary versatility is a large-scale
trend in evolution. I discuss some of the testable predictions about organismal
evolution that are suggested by the model.
</summary>
    <author>
      <name>Peter D. Turney</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National Research Council of Canada</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/106454600568357</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/106454600568357" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">32 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Life, (2000), 6, 109-128</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0212021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0212021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.3; I.6.8; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0302002v1</id>
    <updated>2003-02-02T03:30:52Z</updated>
    <published>2003-02-02T03:30:52Z</published>
    <title>Optimizing GoTools' Search Heuristics using Genetic Algorithms</title>
    <summary>  GoTools is a program which solves life &amp; death problems in the game of Go.
This paper describes experiments using a Genetic Algorithm to optimize
heuristic weights used by GoTools' tree-search. The complete set of heuristic
weights is composed of different subgroups, each of which can be optimized with
a suitable fitness function. As a useful side product, an MPI interface for
FreePascal was implemented to allow the use of a parallelized fitness function
running on a Beowulf cluster. The aim of this exercise is to optimize the
current version of GoTools, and to make tools available in preparation of an
extension of GoTools for solving open boundary life &amp; death problems, which
will introduce more heuristic parameters to be fine tuned.
</summary>
    <author>
      <name>Matthew Pratola</name>
    </author>
    <author>
      <name>Thomas Wolf</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">23 pages, to appear in Journal of ICGA</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0302002v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0302002v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0303032v1</id>
    <updated>2003-03-31T14:46:10Z</updated>
    <published>2003-03-31T14:46:10Z</published>
    <title>Recent Results on No-Free-Lunch Theorems for Optimization</title>
    <summary>  The sharpened No-Free-Lunch-theorem (NFL-theorem) states that the performance
of all optimization algorithms averaged over any finite set F of functions is
equal if and only if F is closed under permutation (c.u.p.) and each target
function in F is equally likely. In this paper, we first summarize some
consequences of this theorem, which have been proven recently: The average
number of evaluations needed to find a desirable (e.g., optimal) solution can
be calculated; the number of subsets c.u.p. can be neglected compared to the
overall number of possible subsets; and problem classes relevant in practice
are not likely to be c.u.p. Second, as the main result, the NFL-theorem is
extended. Necessary and sufficient conditions for NFL-results to hold are given
for arbitrary, non-uniform distributions of target functions. This yields the
most general NFL-theorem for optimization presented so far.
</summary>
    <author>
      <name>Christian Igel</name>
    </author>
    <author>
      <name>Marc Toussaint</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, LaTeX, see http://www.neuroinformatik.rub.de/PROJECTS/SONN/</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0303032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0303032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.OC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0304022v1</id>
    <updated>2003-04-15T19:33:45Z</updated>
    <published>2003-04-15T19:33:45Z</published>
    <title>Self-Replicating Machines in Continuous Space with Virtual Physics</title>
    <summary>  JohnnyVon is an implementation of self-replicating machines in continuous
two-dimensional space. Two types of particles drift about in a virtual liquid.
The particles are automata with discrete internal states but continuous
external relationships. Their internal states are governed by finite state
machines but their external relationships are governed by a simulated physics
that includes Brownian motion, viscosity, and spring-like attractive and
repulsive forces. The particles can be assembled into patterns that can encode
arbitrary strings of bits. We demonstrate that, if an arbitrary "seed" pattern
is put in a "soup" of separate individual particles, the pattern will replicate
by assembling the individual particles into copies of itself. We also show
that, given sufficient time, a soup of separate individual particles will
eventually spontaneously form self-replicating patterns. We discuss the
implications of JohnnyVon for research in nanotechnology, theoretical biology,
and artificial life.
</summary>
    <author>
      <name>Arnold Smith</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National Research Council of Canada</arxiv:affiliation>
    </author>
    <author>
      <name>Peter Turney</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">National Research Council of Canada</arxiv:affiliation>
    </author>
    <author>
      <name>Robert Ewaschuk</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University of Waterloo</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1162/106454603321489509</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1162/106454603321489509" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">39 pages, Java code available at http://purl.org/net/johnnyvon/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Artificial Life, (2003), 9, 21-40</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0304022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0304022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.PE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.6.3; I.6.8; J.2; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0306125v1</id>
    <updated>2003-06-24T06:28:12Z</updated>
    <published>2003-06-24T06:28:12Z</published>
    <title>Predicting Response-Function Results of Electrical/Mechanical Systems
  Through Artificial Neural Network</title>
    <summary>  In the present paper a newer application of Artificial Neural Network (ANN)
has been developed i.e., predicting response-function results of
electrical-mechanical system through ANN. This method is specially useful to
complex systems for which it is not possible to find the response-function
because of complexity of the system. The proposed approach suggests that how
even without knowing the response-function, the response-function results can
be predicted with the use of ANN to the system. The steps used are: (i)
Depending on the system, the ANN-architecture and the input &amp; output parameters
are decided, (ii) Training &amp; test data are generated from simplified circuits
and through tactic-superposition of it for complex circuits, (iii) Training the
ANN with training data through many cycles and (iv) Test-data are used for
predicting the response-function results. It is found that the proposed novel
method for response prediction works satisfactorily. Thus this method could be
used specially for complex systems where other methods are unable to tackle it.
In this paper the application of ANN is particularly demonstrated to
electrical-circuit system but can be applied to other systems too.
</summary>
    <author>
      <name>R. C. Gupta</name>
    </author>
    <author>
      <name>Ankur Agarwal</name>
    </author>
    <author>
      <name>Ruchi Gupta</name>
    </author>
    <author>
      <name>Sanjay Gupta</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages including 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0306125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0306125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F1.1;I2.6;I5,1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0307031v2</id>
    <updated>2003-07-16T13:10:30Z</updated>
    <published>2003-07-12T12:04:33Z</published>
    <title>Automatic Classification using Self-Organising Neural Networks in
  Astrophysical Experiments</title>
    <summary>  Self-Organising Maps (SOMs) are effective tools in classification problems,
and in recent years the even more powerful Dynamic Growing Neural Networks, a
variant of SOMs, have been developed. Automatic Classification (also called
clustering) is an important and difficult problem in many Astrophysical
experiments, for instance, Gamma Ray Burst classification, or gamma-hadron
separation. After a brief introduction to classification problem, we discuss
Self-Organising Maps in section 2. Section 3 discusses with various models of
growing neural networks and finally in section 4 we discuss the research
perspectives in growing neural networks for efficient classification in
astrophysical problems.
</summary>
    <author>
      <name>P. Boinee</name>
    </author>
    <author>
      <name>A. De Angelis</name>
    </author>
    <author>
      <name>E. Milotti</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 Pages, corrected authors name format</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">S. Ciprini, A. De Angelis, P. Lubrano and O. Mansutti (eds.):
  Proc. of ``Science with the New Generation of High Energy Gamma-ray
  Experiments'' (Perugia, Italy, May 2003). Forum, Udine 2003, p. 177</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0307031v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0307031v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="astro-ph" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0308025v1</id>
    <updated>2003-08-16T07:31:57Z</updated>
    <published>2003-08-16T07:31:57Z</published>
    <title>Controlled hierarchical filtering: Model of neocortical sensory
  processing</title>
    <summary>  A model of sensory information processing is presented. The model assumes
that learning of internal (hidden) generative models, which can predict the
future and evaluate the precision of that prediction, is of central importance
for information extraction. Furthermore, the model makes a bridge to
goal-oriented systems and builds upon the structural similarity between the
architecture of a robust controller and that of the hippocampal entorhinal
loop. This generative control architecture is mapped to the neocortex and to
the hippocampal entorhinal loop. Implicit memory phenomena; priming and
prototype learning are emerging features of the model. Mathematical theorems
ensure stability and attractive learning properties of the architecture.
Connections to reinforcement learning are also established: both the control
network, and the network with a hidden model converge to (near) optimal policy
under suitable conditions. Falsifying predictions, including the role of the
feedback connections between neocortical areas are made.
</summary>
    <author>
      <name>Andras Lorincz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Technical Report, 38 pages, 10 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0308025v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0308025v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.NC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; F.1.1.; I.2.0; I.2.6; I.2.10; I.4.3.; I.4.10; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0309012v1</id>
    <updated>2003-09-09T04:25:26Z</updated>
    <published>2003-09-09T04:25:26Z</published>
    <title>Exploration of RNA Editing and Design of Robust Genetic Algorithms</title>
    <summary>  This paper presents our computational methodology using Genetic Algorithms
(GA) for exploring the nature of RNA editing. These models are constructed
using several genetic editing characteristics that are gleaned from the RNA
editing system as observed in several organisms. We have expanded the
traditional Genetic Algorithm with artificial editing mechanisms as proposed by
(Rocha, 1997). The incorporation of editing mechanisms provides a means for
artificial agents with genetic descriptions to gain greater phenotypic
plasticity, which may be environmentally regulated. Our first implementations
of these ideas have shed some light into the evolutionary implications of RNA
editing. Based on these understandings, we demonstrate how to select proper RNA
editors for designing more robust GAs, and the results will show promising
applications to real-world problems. We expect that the framework proposed will
both facilitate determining the evolutionary role of RNA editing in biology,
and advance the current state of research in Genetic Algorithms.
</summary>
    <author>
      <name>C. Huang</name>
    </author>
    <author>
      <name>L. M. Rocha</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 2003 IEEE Congress on Evolutionary Computation.
  Camberra, Australia, December 2003</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0309012v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0309012v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.GN" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0309038v1</id>
    <updated>2003-09-22T13:05:51Z</updated>
    <published>2003-09-22T13:05:51Z</published>
    <title>A novel evolutionary formulation of the maximum independent set problem</title>
    <summary>  We introduce a novel evolutionary formulation of the problem of finding a
maximum independent set of a graph. The new formulation is based on the
relationship that exists between a graph's independence number and its acyclic
orientations. It views such orientations as individuals and evolves them with
the aid of evolutionary operators that are very heavily based on the structure
of the graph and its acyclic orientations. The resulting heuristic has been
tested on some of the Second DIMACS Implementation Challenge benchmark graphs,
and has been found to be competitive when compared to several of the other
heuristics that have also been tested on those graphs.
</summary>
    <author>
      <name>V. C. Barbosa</name>
    </author>
    <author>
      <name>L. C. D. Campos</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1007/s10878-004-4835-9</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1007/s10878-004-4835-9" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Combinatorial Optimization 8 (2004), 419-437</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0309038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0309038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0309039v1</id>
    <updated>2003-09-23T00:53:20Z</updated>
    <published>2003-09-23T00:53:20Z</published>
    <title>Two novel evolutionary formulations of the graph coloring problem</title>
    <summary>  We introduce two novel evolutionary formulations of the problem of coloring
the nodes of a graph. The first formulation is based on the relationship that
exists between a graph's chromatic number and its acyclic orientations. It
views such orientations as individuals and evolves them with the aid of
evolutionary operators that are very heavily based on the structure of the
graph and its acyclic orientations. The second formulation, unlike the first
one, does not tackle one graph at a time, but rather aims at evolving a
`program' to color all graphs belonging to a class whose members all have the
same number of nodes and other common attributes. The heuristics that result
from these formulations have been tested on some of the Second DIMACS
Implementation Challenge benchmark graphs, and have been found to be
competitive when compared to the several other heuristics that have also been
tested on those graphs.
</summary>
    <author>
      <name>V. C. Barbosa</name>
    </author>
    <author>
      <name>C. A. G. Assis</name>
    </author>
    <author>
      <name>J. O. do Nascimento</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1023/B:JOCO.0000021937.26468.b2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1023/B:JOCO.0000021937.26468.b2" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear in Journal of Combinatorial Optimization</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Combinatorial Optimization 8 (2004), 41-63</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0309039v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0309039v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.2.2; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0310009v3</id>
    <updated>2003-11-04T12:52:42Z</updated>
    <published>2003-10-06T15:40:44Z</published>
    <title>On Interference of Signals and Generalization in Feedforward Neural
  Networks</title>
    <summary>  This paper studies how the generalization ability of neurons can be affected
by mutual processing of different signals. This study is done on the basis of a
feedforward artificial neural network. The mutual processing of signals can
possibly be a good model of patterns in a set generalized by a neural network
and in effect may improve generalization. In this paper it is discussed that
the interference may also cause a highly random generalization. Adaptive
activation functions are discussed as a way of reducing that type of
generalization. A test of a feedforward neural network is performed that shows
the discussed random generalization.
</summary>
    <author>
      <name>Artur Rataj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures. Some changes in text to make it more concise</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0310009v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0310009v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0310050v4</id>
    <updated>2005-03-31T12:45:11Z</updated>
    <published>2003-10-27T14:27:14Z</published>
    <title>Feedforward Neural Networks with Diffused Nonlinear Weight Functions</title>
    <summary>  In this paper, feedforward neural networks are presented that have nonlinear
weight functions based on look--up tables, that are specially smoothed in a
regularization called the diffusion. The idea of such a type of networks is
based on the hypothesis that the greater number of adaptive parameters per a
weight function might reduce the total number of the weight functions needed to
solve a given problem. Then, if the computational complexity of a propagation
through a single such a weight function would be kept low, then the introduced
neural networks might possibly be relatively fast.
  A number of tests is performed, showing that the presented neural networks
may indeed perform better in some cases than the classic neural networks and a
number of other learning machines.
</summary>
    <author>
      <name>Artur Rataj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">17 pages, 7 figures. Corrected, some parts rewritten</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0310050v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0310050v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312003v1</id>
    <updated>2003-11-30T00:19:19Z</updated>
    <published>2003-11-30T00:19:19Z</published>
    <title>Hybrid LQG-Neural Controller for Inverted Pendulum System</title>
    <summary>  The paper presents a hybrid system controller, incorporating a neural and an
LQG controller. The neural controller has been optimized by genetic algorithms
directly on the inverted pendulum system. The failure free optimization process
stipulated a relatively small region of the asymptotic stability of the neural
controller, which is concentrated around the regulation point. The presented
hybrid controller combines benefits of a genetically optimized neural
controller and an LQG controller in a single system controller. High quality of
the regulation process is achieved through utilization of the neural
controller, while stability of the system during transient processes and a wide
range of operation are assured through application of the LQG controller. The
hybrid controller has been validated by applying it to a simulation model of an
inherently unstable system of inverted pendulum.
</summary>
    <author>
      <name>E. S. Sazonov</name>
    </author>
    <author>
      <name>P. Klinkhachorn</name>
    </author>
    <author>
      <name>R. L. Klein</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 35th Southeastern Symposium on System Theory
  (SSST), Morgantown, WV, March 2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0312003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6;C.1.3;I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312009v1</id>
    <updated>2003-12-03T22:29:01Z</updated>
    <published>2003-12-03T22:29:01Z</published>
    <title>Failure-Free Genetic Algorithm Optimization of a System Controller Using
  SAFE/LEARNING Controllers in Tandem</title>
    <summary>  The paper presents a method for failure free genetic algorithm optimization
of a system controller. Genetic algorithms present a powerful tool that
facilitates producing near-optimal system controllers. Applied to such methods
of computational intelligence as neural networks or fuzzy logic, these methods
are capable of combining the non-linear mapping capabilities of the latter with
learning the system behavior directly, that is, without a prior model. At the
same time, genetic algorithms routinely produce solutions that lead to the
failure of the controlled system. Such solutions are generally unacceptable for
applications where safe operation must be guaranteed. We present here a method
of design, which allows failure-free application of genetic algorithms through
utilization of SAFE and LEARNING controllers in tandem, where the SAFE
controller recovers the system from dangerous states while the LEARNING
controller learns its behavior. The method has been validated by applying it to
an inherently unstable system of inverted pendulum.
</summary>
    <author>
      <name>E. S. Sazonov</name>
    </author>
    <author>
      <name>D. Del Gobbo</name>
    </author>
    <author>
      <name>P. Klinkhachorn</name>
    </author>
    <author>
      <name>R. L. Klein</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of 34th Southeastern Symposium on System Theory
  (SSST), Huntsville, AL, March 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0312009v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312009v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6;C.1.3;I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0312047v1</id>
    <updated>2003-12-20T08:55:48Z</updated>
    <published>2003-12-20T08:55:48Z</published>
    <title>Mapping weblog communities</title>
    <summary>  Websites of a particular class form increasingly complex networks, and new
tools are needed to map and understand them. A way of visualizing this complex
network is by mapping it. A map highlights which members of the community have
similar interests, and reveals the underlying social network. In this paper, we
will map a network of websites using Kohonen's self-organizing map (SOM), a
neural-net like method generally used for clustering and visualization of
complex data sets. The set of websites considered has been the Blogalia weblog
hosting site (based at http://www.blogalia.com/), a thriving community of
around 200 members, created in January 2002. In this paper we show how SOM
discovers interesting community features, its relation with other
community-discovering algorithms, and the way it highlights the set of
communities formed over the network.
</summary>
    <author>
      <name>Juan-J. Merelo-Guervos</name>
    </author>
    <author>
      <name>Beatriz Prieto</name>
    </author>
    <author>
      <name>Fatima Rateb</name>
    </author>
    <author>
      <name>Fernando Tricas</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">22 pages, 8 figures, to be submitted to Computer Networks</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0312047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0312047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="J.4;H.3.5;I.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402031v1</id>
    <updated>2004-02-15T06:56:45Z</updated>
    <published>2004-02-15T06:56:45Z</published>
    <title>Parameter-less hierarchical BOA</title>
    <summary>  The parameter-less hierarchical Bayesian optimization algorithm (hBOA)
enables the use of hBOA without the need for tuning parameters for solving each
problem instance. There are three crucial parameters in hBOA: (1) the selection
pressure, (2) the window size for restricted tournaments, and (3) the
population size. Although both the selection pressure and the window size
influence hBOA performance, performance should remain low-order polynomial with
standard choices of these two parameters. However, there is no standard
population size that would work for all problems of interest and the population
size must thus be eliminated in a different way. To eliminate the population
size, the parameter-less hBOA adopts the population-sizing technique of the
parameter-less genetic algorithm. Based on the existing theory, the
parameter-less hBOA should be able to solve nearly decomposable and
hierarchical problems in quadratic or subquadratic number of function
evaluations without the need for setting any parameters whatsoever. A number of
experiments are presented to verify scalability of the parameter-less hBOA.
</summary>
    <author>
      <name>Martin Pelikan</name>
    </author>
    <author>
      <name>Tz-Kai Lin</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">about 12 pages, submitted to GECCO-2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402031v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402031v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; I.2.6; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0402032v1</id>
    <updated>2004-02-15T07:40:45Z</updated>
    <published>2004-02-15T07:40:45Z</published>
    <title>Fitness inheritance in the Bayesian optimization algorithm</title>
    <summary>  This paper describes how fitness inheritance can be used to estimate fitness
for a proportion of newly sampled candidate solutions in the Bayesian
optimization algorithm (BOA). The goal of estimating fitness for some candidate
solutions is to reduce the number of fitness evaluations for problems where
fitness evaluation is expensive. Bayesian networks used in BOA to model
promising solutions and generate the new ones are extended to allow not only
for modeling and sampling candidate solutions, but also for estimating their
fitness. The results indicate that fitness inheritance is a promising concept
in BOA, because population-sizing requirements for building appropriate models
of promising solutions lead to good fitness estimates even if only a small
proportion of candidate solutions is evaluated using the actual fitness
function. This can lead to a reduction of the number of actual fitness
evaluations by a factor of 30 or more.
</summary>
    <author>
      <name>Martin Pelikan</name>
    </author>
    <author>
      <name>Kumara Sastry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IlliGAL Report No. 2004009, Illinois Genetic Algorithms Laboratory,
  University of Illinois at Urbana-Champaign, Urbana, IL. Download also from
  http://www-illigal.ge.uiuc.edu/</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0402032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0402032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; G.3; I.2.6; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404017v1</id>
    <updated>2004-04-07T06:23:18Z</updated>
    <published>2004-04-07T06:23:18Z</published>
    <title>Exploring tradeoffs in pleiotropy and redundancy using evolutionary
  computing</title>
    <summary>  Evolutionary computation algorithms are increasingly being used to solve
optimization problems as they have many advantages over traditional
optimization algorithms. In this paper we use evolutionary computation to study
the trade-off between pleiotropy and redundancy in a client-server based
network. Pleiotropy is a term used to describe components that perform multiple
tasks, while redundancy refers to multiple components performing one same task.
Pleiotropy reduces cost but lacks robustness, while redundancy increases
network reliability but is more costly, as together, pleiotropy and redundancy
build flexibility and robustness into systems. Therefore it is desirable to
have a network that contains a balance between pleiotropy and redundancy. We
explore how factors such as link failure probability, repair rates, and the
size of the network influence the design choices that we explore using genetic
algorithms.
</summary>
    <author>
      <name>Matthew J. Berryman</name>
    </author>
    <author>
      <name>Wei-Li Khoo</name>
    </author>
    <author>
      <name>Hiep Nguyen</name>
    </author>
    <author>
      <name>Erin O'Neill</name>
    </author>
    <author>
      <name>Andrew Allison</name>
    </author>
    <author>
      <name>Derek Abbott</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1117/12.548001</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1117/12.548001" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. SPIE 5275, BioMEMS and Nanotechnology, Ed. Dan V. Nicolau,
  Perth, Australia, Dec. 2003, pp49-58</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0404017v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404017v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; C.2.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0404045v2</id>
    <updated>2004-04-24T16:56:38Z</updated>
    <published>2004-04-22T07:29:19Z</published>
    <title>Speculation on graph computation architectures and computing via
  synchronization</title>
    <summary>  A speculative overview of a future topic of research. The paper is a
collection of ideas concerning two related areas:
  1) Graph computation machines ("computing with graphs"). This is the class of
models of computation in which the state of the computation is represented as a
graph or network.
  2) Arc-based neural networks, which store information not as activation in
the nodes, but rather by adding and deleting arcs. Sometimes the arcs may be
interpreted as synchronization.
  Warnings to readers: this is not the sort of thing that one might submit to a
journal or conference. No proofs are presented. The presentation is informal,
and written at an introductory level. You'll probably want to wait for a more
concise presentation.
</summary>
    <author>
      <name>Bayle Shanks</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">61 pages. Informal, rambling. (replacment changed only abstract)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0404045v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0404045v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; J.3; I.2.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405062v1</id>
    <updated>2004-05-18T16:19:59Z</updated>
    <published>2004-05-18T16:19:59Z</published>
    <title>Efficiency Enhancement of Probabilistic Model Building Genetic
  Algorithms</title>
    <summary>  This paper presents two different efficiency-enhancement techniques for
probabilistic model building genetic algorithms. The first technique proposes
the use of a mutation operator which performs local search in the sub-solution
neighborhood identified through the probabilistic model. The second technique
proposes building and using an internal probabilistic model of the fitness
along with the probabilistic model of variable interactions. The fitness values
of some offspring are estimated using the probabilistic model, thereby avoiding
computationally expensive function evaluations. The scalability of the
aforementioned techniques are analyzed using facetwise models for convergence
time and population sizing. The speed-up obtained by each of the methods is
predicted and verified with empirical results. The results show that for
additively separable problems the competent mutation operator requires O(k 0.5
logm)--where k is the building-block size, and m is the number of building
blocks--less function evaluations than its selectorecombinative counterpart.
The results also show that the use of an internal probabilistic fitness model
reduces the required number of function evaluations to as low as 1-10% and
yields a speed-up of 2--50.
</summary>
    <author>
      <name>Kumara Sastry</name>
    </author>
    <author>
      <name>David E. Goldberg</name>
    </author>
    <author>
      <name>Martin Pelikan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Optimization by Building and Using Probabilistic Models. Workshop at
  the 2004 Genetic and Evolutionary Computation Conference</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0405062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; G.3; I.2.6; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405063v1</id>
    <updated>2004-05-18T16:31:56Z</updated>
    <published>2004-05-18T16:31:56Z</published>
    <title>Let's Get Ready to Rumble: Crossover Versus Mutation Head to Head</title>
    <summary>  This paper analyzes the relative advantages between crossover and mutation on
a class of deterministic and stochastic additively separable problems. This
study assumes that the recombination and mutation operators have the knowledge
of the building blocks (BBs) and effectively exchange or search among competing
BBs. Facetwise models of convergence time and population sizing have been used
to determine the scalability of each algorithm. The analysis shows that for
additively separable deterministic problems, the BB-wise mutation is more
efficient than crossover, while the crossover outperforms the mutation on
additively separable problems perturbed with additive Gaussian noise. The
results show that the speed-up of using BB-wise mutation on deterministic
problems is O(k^{0.5}logm), where k is the BB size, and m is the number of BBs.
Likewise, the speed-up of using crossover on stochastic problems with fixed
noise variance is O(mk^{0.5}log m).
</summary>
    <author>
      <name>Kumara Sastry</name>
    </author>
    <author>
      <name>David E. Goldberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Genetic and Evolutionary Computation Conference (GECCO-2004)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0405063v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405063v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; G.3; I.2.6; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405064v1</id>
    <updated>2004-05-18T16:41:34Z</updated>
    <published>2004-05-18T16:41:34Z</published>
    <title>Designing Competent Mutation Operators via Probabilistic Model Building
  of Neighborhoods</title>
    <summary>  This paper presents a competent selectomutative genetic algorithm (GA), that
adapts linkage and solves hard problems quickly, reliably, and accurately. A
probabilistic model building process is used to automatically identify key
building blocks (BBs) of the search problem. The mutation operator uses the
probabilistic model of linkage groups to find the best among competing building
blocks. The competent selectomutative GA successfully solves additively
separable problems of bounded difficulty, requiring only subquadratic number of
function evaluations. The results show that for additively separable problems
the probabilistic model building BB-wise mutation scales as O(2^km^{1.5}), and
requires O(k^{0.5}logm) less function evaluations than its selectorecombinative
counterpart, confirming theoretical results reported elsewhere (Sastry &amp;
Goldberg, 2004).
</summary>
    <author>
      <name>Kumara Sastry</name>
    </author>
    <author>
      <name>David E. Goldberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Genetic and Evolutionary Computation Conference (GECCO-2004)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0405064v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405064v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; G.3; I.2.6; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0405065v1</id>
    <updated>2004-05-18T16:55:00Z</updated>
    <published>2004-05-18T16:55:00Z</published>
    <title>Efficiency Enhancement of Genetic Algorithms via Building-Block-Wise
  Fitness Estimation</title>
    <summary>  This paper studies fitness inheritance as an efficiency enhancement technique
for a class of competent genetic algorithms called estimation distribution
algorithms. Probabilistic models of important sub-solutions are developed to
estimate the fitness of a proportion of individuals in the population, thereby
avoiding computationally expensive function evaluations. The effect of fitness
inheritance on the convergence time and population sizing are modeled and the
speed-up obtained through inheritance is predicted. The results show that a
fitness-inheritance mechanism which utilizes information on building-block
fitnesses provides significant efficiency enhancement. For additively separable
problems, fitness inheritance reduces the number of function evaluations to
about half and yields a speed-up of about 1.75--2.25.
</summary>
    <author>
      <name>Kumara Sastry</name>
    </author>
    <author>
      <name>Martin Pelikan</name>
    </author>
    <author>
      <name>David E. Goldberg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CEC.2004.1330930</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CEC.2004.1330930" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE International Conference on Evolutionary Computation (CEC-2004)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0405065v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0405065v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; G.3; I.2.6; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0406007v1</id>
    <updated>2004-06-03T15:43:46Z</updated>
    <published>2004-06-03T15:43:46Z</published>
    <title>Parallel Mixed Bayesian Optimization Algorithm: A Scaleup Analysis</title>
    <summary>  Estimation of Distribution Algorithms have been proposed as a new paradigm
for evolutionary optimization. This paper focuses on the parallelization of
Estimation of Distribution Algorithms. More specifically, the paper discusses
how to predict performance of parallel Mixed Bayesian Optimization Algorithm
(MBOA) that is based on parallel construction of Bayesian networks with
decision trees. We determine the time complexity of parallel Mixed Bayesian
Optimization Algorithm and compare this complexity with experimental results
obtained by solving the spin glass optimization problem. The empirical results
fit well the theoretical time complexity, so the scalability and efficiency of
parallel Mixed Bayesian Optimization Algorithm for unknown instances of spin
glass benchmarks can be predicted. Furthermore, we derive the guidelines that
can be used to design effective parallel Estimation of Distribution Algorithms
with the speedup proportional to the number of variables in the problem.
</summary>
    <author>
      <name>Jiri Ocenasek</name>
    </author>
    <author>
      <name>Martin Pelikan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Optimization by Building and Using Probabilistic Models OBUPM-2004</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0406007v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0406007v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; G.3; I.2.6; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0406017v2</id>
    <updated>2004-08-22T15:06:32Z</updated>
    <published>2004-06-08T14:45:45Z</published>
    <title>Using Self-Organising Mappings to Learn the Structure of Data Manifolds</title>
    <summary>  In this paper it is shown how to map a data manifold into a simpler form by
progressively discarding small degrees of freedom. This is the key to
self-organising data fusion, where the raw data is embedded in a very
high-dimensional space (e.g. the pixel values of one or more images), and the
requirement is to isolate the important degrees of freedom which lie on a
low-dimensional manifold. A useful advantage of the approach used in this paper
is that the computations are arranged as a feed-forward processing chain, where
all the details of the processing in each stage of the chain are learnt by
self-organisation. This approach is demonstrated using hierarchically
correlated data, which causes the processing chain to split the data into
separate processing channels, and then to progressively merge these channels
wherever they are correlated with each other. This is the key to
self-organising data fusion.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">26 pages, 19 figures, version 1 translated into LaTeX</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0406017v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0406017v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0408049v1</id>
    <updated>2004-08-21T19:40:24Z</updated>
    <published>2004-08-21T19:40:24Z</published>
    <title>Using Stochastic Encoders to Discover Structure in Data</title>
    <summary>  In this paper a stochastic generalisation of the standard Linde-Buzo-Gray
(LBG) approach to vector quantiser (VQ) design is presented, in which the
encoder is implemented as the sampling of a vector of code indices from a
probability distribution derived from the input vector, and the decoder is
implemented as a superposition of reconstruction vectors. This stochastic VQ
(SVQ) is optimised using a minimum mean Euclidean reconstruction distortion
criterion, as in the LBG case. Numerical simulations are used to demonstrate
how this leads to self-organisation of the SVQ, where different stochastically
sampled code indices become associated with different input subspaces.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">18 pages, 9 figures. Full version of a short paper that was published
  in the Digest of the 5th IMA International Conference on Mathematics in
  Signal Processing, 18-20 December 2000, Warwick University, UK</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0408049v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0408049v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0408050v1</id>
    <updated>2004-08-21T23:06:45Z</updated>
    <published>2004-08-21T23:06:45Z</published>
    <title>Invariant Stochastic Encoders</title>
    <summary>  The theory of stochastic vector quantisers (SVQ) has been extended to allow
the quantiser to develop invariances, so that only "large" degrees of freedom
in the input vector are represented in the code. This has been applied to the
problem of encoding data vectors which are a superposition of a "large" jammer
and a "small" signal, so that only the jammer is represented in the code. This
allows the jammer to be subtracted from the total input vector (i.e. the jammer
is nulled), leaving a residual that contains only the underlying signal. The
main advantage of this approach to jammer nulling is that little prior
knowledge of the jammer is assumed, because these properties are automatically
discovered by the SVQ as it is trained on examples of input vectors.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 12 figures. Full version of a short paper that was
  published in the Digest of the 5th IMA International Conference on
  Mathematics in Signal Processing, 18-20 December 2000, Warwick University, UK</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0408050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0408050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0410020v1</id>
    <updated>2004-10-10T18:30:03Z</updated>
    <published>2004-10-10T18:30:03Z</published>
    <title>Adaptive Cluster Expansion (ACE): A Hierarchical Bayesian Network</title>
    <summary>  Using the maximum entropy method, we derive the "adaptive cluster expansion"
(ACE), which can be trained to estimate probability density functions in high
dimensional spaces. The main advantage of ACE over other Bayesian networks is
its ability to capture high order statistics after short training times, which
it achieves by making use of a hierarchical vector quantisation of the input
data. We derive a scheme for representing the state of an ACE network as a
"probability image", which allows us to identify statistically anomalous
regions in an otherwise statistically homogeneous image, for instance. Finally,
we present some probability images that we obtained after training ACE on some
Brodatz texture images - these demonstrate the ability of ACE to detect subtle
textural anomalies.
</summary>
    <author>
      <name>Stephen Luttrell</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">35 pages, 20 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0410020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0410020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0411052v1</id>
    <updated>2004-11-17T10:04:55Z</updated>
    <published>2004-11-17T10:04:55Z</published>
    <title>Spontaneous Dynamics of Asymmetric Random Recurrent Spiking Neural
  Networks</title>
    <summary>  We study in this paper the effect of an unique initial stimulation on random
recurrent networks of leaky integrate and fire neurons. Indeed given a
stochastic connectivity this so-called spontaneous mode exhibits various non
trivial dynamics. This study brings forward a mathematical formalism that
allows us to examine the variability of the afterward dynamics according to the
parameters of the weight distribution. Provided independence hypothesis (e.g.
in the case of very large networks) we are able to compute the average number
of neurons that fire at a given time -- the spiking activity. In accordance
with numerical simulations, we prove that this spiking activity reaches a
steady-state, we characterize this steady-state and explore the transients.
</summary>
    <author>
      <name>H. Soula</name>
    </author>
    <author>
      <name>G. Beslon</name>
    </author>
    <author>
      <name>O. Mazet</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">28 pages, 7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0411052v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0411052v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.PR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0412059v1</id>
    <updated>2004-12-13T08:00:55Z</updated>
    <published>2004-12-13T08:00:55Z</published>
    <title>Vector Symbolic Architectures answer Jackendoff's challenges for
  cognitive neuroscience</title>
    <summary>  Jackendoff (2002) posed four challenges that linguistic combinatoriality and
rules of language present to theories of brain function. The essence of these
problems is the question of how to neurally instantiate the rapid construction
and transformation of the compositional structures that are typically taken to
be the domain of symbolic processing. He contended that typical connectionist
approaches fail to meet these challenges and that the dialogue between
linguistic theory and cognitive neuroscience will be relatively unproductive
until the importance of these problems is widely recognised and the challenges
answered by some technical innovation in connectionist modelling. This paper
claims that a little-known family of connectionist models (Vector Symbolic
Architectures) are able to meet Jackendoff's challenges.
</summary>
    <author>
      <name>Ross W. Gayler</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This is a slightly updated version of the paper presented at the
  Joint International Conference on Cognitive Science, 13-17 July 2003,
  University of New South Wales, Sydney, Australia. 6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0412059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0412059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; I.2.0, I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502021v1</id>
    <updated>2005-02-04T04:15:15Z</updated>
    <published>2005-02-04T04:15:15Z</published>
    <title>Oiling the Wheels of Change: The Role of Adaptive Automatic Problem
  Decomposition in Non--Stationary Environments</title>
    <summary>  Genetic algorithms (GAs) that solve hard problems quickly, reliably and
accurately are called competent GAs. When the fitness landscape of a problem
changes overtime, the problem is called non--stationary, dynamic or
time--variant problem. This paper investigates the use of competent GAs for
optimizing non--stationary optimization problems. More specifically, we use an
information theoretic approach based on the minimum description length
principle to adaptively identify regularities and substructures that can be
exploited to respond quickly to changes in the environment. We also develop a
special type of problems with bounded difficulties to test non--stationary
optimization problems. The results provide new insights into non-stationary
optimization problems and show that a search algorithm which automatically
identifies and exploits possible decompositions is more robust and responds
quickly to changes than a simple genetic algorithm.
</summary>
    <author>
      <name>H. A. Abbass</name>
    </author>
    <author>
      <name>K. Sastry</name>
    </author>
    <author>
      <name>D. E. Goldberg</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0502021v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502021v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502022v1</id>
    <updated>2005-02-04T04:29:15Z</updated>
    <published>2005-02-04T04:29:15Z</published>
    <title>Sub-Structural Niching in Non-Stationary Environments</title>
    <summary>  Niching enables a genetic algorithm (GA) to maintain diversity in a
population. It is particularly useful when the problem has multiple optima
where the aim is to find all or as many as possible of these optima. When the
fitness landscape of a problem changes overtime, the problem is called
non--stationary, dynamic or time--variant problem. In these problems, niching
can maintain useful solutions to respond quickly, reliably and accurately to a
change in the environment. In this paper, we present a niching method that
works on the problem substructures rather than the whole solution, therefore it
has less space complexity than previously known niching mechanisms. We show
that the method is responding accurately when environmental changes occur.
</summary>
    <author>
      <name>K. Sastry</name>
    </author>
    <author>
      <name>H. A. Abbass</name>
    </author>
    <author>
      <name>D. E. Goldberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Final version published in 2005 Australian Artificial Intelligence
  Conference, pp. 873--885</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0502022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502023v1</id>
    <updated>2005-02-04T04:46:04Z</updated>
    <published>2005-02-04T04:46:04Z</published>
    <title>Sub-structural Niching in Estimation of Distribution Algorithms</title>
    <summary>  We propose a sub-structural niching method that fully exploits the problem
decomposition capability of linkage-learning methods such as the estimation of
distribution algorithms and concentrate on maintaining diversity at the
sub-structural level. The proposed method consists of three key components: (1)
Problem decomposition and sub-structure identification, (2) sub-structure
fitness estimation, and (3) sub-structural niche preservation. The
sub-structural niching method is compared to restricted tournament selection
(RTS)--a niching method used in hierarchical Bayesian optimization
algorithm--with special emphasis on sustained preservation of multiple global
solutions of a class of boundedly-difficult, additively-separable multimodal
problems. The results show that sub-structural niching successfully maintains
multiple global optima over large number of generations and does so with
significantly less population than RTS. Additionally, the market share of each
of the niche is much closer to the expected level in sub-structural niching
when compared to RTS.
</summary>
    <author>
      <name>K. Sastry</name>
    </author>
    <author>
      <name>H. A. Abbass</name>
    </author>
    <author>
      <name>D. E. Goldberg</name>
    </author>
    <author>
      <name>D. D. Johnson</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0502023v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502023v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502029v1</id>
    <updated>2005-02-07T19:40:01Z</updated>
    <published>2005-02-07T19:40:01Z</published>
    <title>Scalability of Genetic Programming and Probabilistic Incremental Program
  Evolution</title>
    <summary>  This paper discusses scalability of standard genetic programming (GP) and the
probabilistic incremental program evolution (PIPE). To investigate the need for
both effective mixing and linkage learning, two test problems are considered:
ORDER problem, which is rather easy for any recombination-based GP, and TRAP or
the deceptive trap problem, which requires the algorithm to learn interactions
among subsets of terminals. The scalability results show that both GP and PIPE
scale up polynomially with problem size on the simple ORDER problem, but they
both scale up exponentially on the deceptive problem. This indicates that while
standard recombination is sufficient when no interactions need to be
considered, for some problems linkage learning is necessary. These results are
in agreement with the lessons learned in the domain of binary-string genetic
algorithms (GAs). Furthermore, the paper investigates the effects of
introducing utnnecessary and irrelevant primitives on the performance of GP and
PIPE.
</summary>
    <author>
      <name>Radovan Ondas</name>
    </author>
    <author>
      <name>Martin Pelikan</name>
    </author>
    <author>
      <name>Kumara Sastry</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to GECCO-2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0502029v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502029v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8; I.2.6; G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0502034v1</id>
    <updated>2005-02-07T05:26:13Z</updated>
    <published>2005-02-07T05:26:13Z</published>
    <title>Multiobjective hBOA, Clustering, and Scalability</title>
    <summary>  This paper describes a scalable algorithm for solving multiobjective
decomposable problems by combining the hierarchical Bayesian optimization
algorithm (hBOA) with the nondominated sorting genetic algorithm (NSGA-II) and
clustering in the objective space. It is first argued that for good
scalability, clustering or some other form of niching in the objective space is
necessary and the size of each niche should be approximately equal.
Multiobjective hBOA (mohBOA) is then described that combines hBOA, NSGA-II and
clustering in the objective space. The algorithm mohBOA differs from the
multiobjective variants of BOA and hBOA proposed in the past by including
clustering in the objective space and allocating an approximately equally sized
portion of the population to each cluster. The algorithm mohBOA is shown to
scale up well on a number of problems on which standard multiobjective
evolutionary algorithms perform poorly.
</summary>
    <author>
      <name>Martin Pelikan</name>
    </author>
    <author>
      <name>Kumara Sastry</name>
    </author>
    <author>
      <name>David E. Goldberg</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Also IlliGAL Report No. 2005005 (http://www-illigal.ge.uiuc.edu/).
  Submitted to GECCO-2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0502034v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0502034v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8; I.2.6; G.1.6; I.5.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0503078v1</id>
    <updated>2005-03-29T19:40:14Z</updated>
    <published>2005-03-29T19:40:14Z</published>
    <title>Obtaining Membership Functions from a Neuron Fuzzy System extended by
  Kohonen Network</title>
    <summary>  This article presents the Neo-Fuzzy-Neuron Modified by Kohonen Network
(NFN-MK), an hybrid computational model that combines fuzzy system technique
and artificial neural networks. Its main task consists in the automatic
generation of membership functions, in particular, triangle forms, aiming a
dynamic modeling of a system. The model is tested by simulating real systems,
here represented by a nonlinear mathematical function. Comparison with the
results obtained by traditional neural networks, and correlated studies of
neurofuzzy systems applied in system identification area, shows that the NFN-MK
model has a similar performance, despite its greater simplicity.
</summary>
    <author>
      <name>Angelo Luis Pagliosa</name>
    </author>
    <author>
      <name>Claudio Cesar de Sa</name>
    </author>
    <author>
      <name>Fernando D. Sasse</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 6 figures, 5th Congress of Logic Applied to Technology
  (LAPTEC 2005) Himeji, Japan, April 2-6, 2005</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0503078v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0503078v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504035v1</id>
    <updated>2005-04-11T10:42:41Z</updated>
    <published>2005-04-11T10:42:41Z</published>
    <title>Fitness Uniform Deletion: A Simple Way to Preserve Diversity</title>
    <summary>  A commonly experienced problem with population based optimisation methods is
the gradual decline in population diversity that tends to occur over time. This
can slow a system's progress or even halt it completely if the population
converges on a local optimum from which it cannot escape. In this paper we
present the Fitness Uniform Deletion Scheme (FUDS), a simple but somewhat
unconventional approach to this problem. Under FUDS the deletion operation is
modified to only delete those individuals which are "common" in the sense that
there exist many other individuals of similar fitness in the population. This
makes it impossible for the population to collapse to a collection of highly
related individuals with similar fitness. Our experimental results on a range
of optimisation problems confirm this, in particular for deceptive optimisation
problems the performance is significantly more robust to variation in the
selection intensity.
</summary>
    <author>
      <name>Shane Legg</name>
    </author>
    <author>
      <name>Marcus Hutter</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 two-column pages, 19 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proc. Genetic and Evolutionary Computation Conference (GECCO 2005)
  1271-1278</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0504035v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504035v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.M" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504054v1</id>
    <updated>2005-04-13T13:40:38Z</updated>
    <published>2005-04-13T13:40:38Z</published>
    <title>Learning from Web: Review of Approaches</title>
    <summary>  Knowledge discovery is defined as non-trivial extraction of implicit,
previously unknown and potentially useful information from given data.
Knowledge extraction from web documents deals with unstructured, free-format
documents whose number is enormous and rapidly growing. The artificial neural
networks are well suitable to solve a problem of knowledge discovery from web
documents because trained networks are able more accurately and easily to
classify the learning and testing examples those represent the text mining
domain. However, the neural networks that consist of large number of weighted
connections and activation units often generate the incomprehensible and
hard-to-understand models of text classification. This problem may be also
addressed to most powerful recurrent neural networks that employ the feedback
links from hidden or output units to their input units. Due to feedback links,
recurrent neural networks are able take into account of a context in document.
To be useful for data mining, self-organizing neural network techniques of
knowledge extraction have been explored and developed. Self-organization
principles were used to create an adequate neural-network structure and reduce
a dimensionality of features used to describe text documents. The use of these
principles seems interesting because ones are able to reduce a neural-network
redundancy and considerably facilitate the knowledge representation.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504054v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504054v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504057v1</id>
    <updated>2005-04-13T14:03:02Z</updated>
    <published>2005-04-13T14:03:02Z</published>
    <title>Diagnostic Rule Extraction Using Neural Networks</title>
    <summary>  The neural networks have trained on incomplete sets that a doctor could
collect. Trained neural networks have correctly classified all the presented
instances. The number of intervals entered for encoding the quantitative
variables is equal two. The number of features as well as the number of neurons
and layers in trained neural networks was minimal. Trained neural networks are
adequately represented as a set of logical formulas that more comprehensible
and easy-to-understand. These formulas are as the syndrome-complexes, which may
be easily tabulated and represented as a diagnostic table that the doctors
usually use. Decision rules provide the evaluations of their confidence in
which interested a doctor. Conducted clinical researches have shown that
iagnostic decisions produced by symbolic rules have coincided with the doctor's
conclusions.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <author>
      <name>Anatoly Brazhnikov</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504057v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504057v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504059v1</id>
    <updated>2005-04-13T14:28:48Z</updated>
    <published>2005-04-13T14:28:48Z</published>
    <title>A Neural Network Decision Tree for Learning Concepts from EEG Data</title>
    <summary>  To learn the multi-class conceptions from the electroencephalogram (EEG) data
we developed a neural network decision tree (DT), that performs the linear
tests, and a new training algorithm. We found that the known methods fail
inducting the classification models when the data are presented by the features
some of them are irrelevant, and the classes are heavily overlapped. To train
the DT, our algorithm exploits a bottom up search of the features that provide
the best classification accuracy of the linear tests. We applied the developed
algorithm to induce the DT from the large EEG dataset consisted of 65 patients
belonging to 16 age groups. In these recordings each EEG segment was
represented by 72 calculated features. The DT correctly classified 80.8% of the
training and 80.1% of the testing examples. Correspondingly it correctly
classified 89.2% and 87.7% of the EEG recordings.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504059v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504059v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0504069v1</id>
    <updated>2005-04-14T10:47:38Z</updated>
    <published>2005-04-14T10:47:38Z</published>
    <title>A Neural-Network Technique to Learn Concepts from Electroencephalograms</title>
    <summary>  A new technique is presented developed to learn multi-class concepts from
clinical electroencephalograms. A desired concept is represented as a neuronal
computational model consisting of the input, hidden, and output neurons. In
this model the hidden neurons learn independently to classify the
electroencephalogram segments presented by spectral and statistical features.
This technique has been applied to the electroencephalogram data recorded from
65 sleeping healthy newborns in order to learn a brain maturation concept of
newborns aged between 35 and 51 weeks. The 39399 and 19670 segments from these
data have been used for learning and testing the concept, respectively. As a
result, the concept has correctly classified 80.1% of the testing segments or
87.7% of the 65 records.
</summary>
    <author>
      <name>Vitaly Schetinin</name>
    </author>
    <author>
      <name>Joachim Schult</name>
    </author>
    <link href="http://arxiv.org/abs/cs/0504069v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0504069v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505021v3</id>
    <updated>2007-06-08T09:20:49Z</updated>
    <published>2005-05-10T11:36:35Z</published>
    <title>Distant generalization by feedforward neural networks</title>
    <summary>  This paper discusses the notion of generalization of training samples over
long distances in the input space of a feedforward neural network. Such a
generalization might occur in various ways, that differ in how great the
contribution of different training features should be.
  The structure of a neuron in a feedforward neural network is analyzed and it
is concluded, that the actual performance of the discussed generalization in
such neural networks may be problematic -- while such neural networks might be
capable for such a distant generalization, a random and spurious generalization
may occur as well.
  To illustrate the differences in generalizing of the same function by
different learning machines, results given by the support vector machines are
also presented.
</summary>
    <author>
      <name>Artur Rataj</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">7 pages, 3 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505021v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505021v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0505044v1</id>
    <updated>2005-05-16T19:31:36Z</updated>
    <published>2005-05-16T19:31:36Z</published>
    <title>Separating a Real-Life Nonlinear Image Mixture</title>
    <summary>  When acquiring an image of a paper document, the image printed on the back
page sometimes shows through. The mixture of the front- and back-page images
thus obtained is markedly nonlinear, and thus constitutes a good real-life test
case for nonlinear blind source separation.
  This paper addresses a difficult version of this problem, corresponding to
the use of "onion skin" paper, which results in a relatively strong
nonlinearity of the mixture, which becomes close to singular in the lighter
regions of the images. The separation is achieved through the MISEP technique,
which is an extension of the well known INFOMAX method. The separation results
are assessed with objective quality measures. They show an improvement over the
results obtained with linear separation, but have room for further improvement.
</summary>
    <author>
      <name>Luis B. Almeida</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to the Journal of Machine Learning Research, May 2005 The
  copy stored in Arxiv has low-resolution images. To get a copy with
  full-resolution images download from:
  http://www.lx.it.pt/~lbalmeida/papers/AlmeidaJMLR05.pdf (7MB, a few artifacts
  in images) http://www.lx.it.pt/~lbalmeida/papers/AlmeidaJMLR05.ps.zip (14MB,
  no artifacts in images)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0505044v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0505044v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0506032v1</id>
    <updated>2005-06-10T05:30:41Z</updated>
    <published>2005-06-10T05:30:41Z</published>
    <title>Framework for Hopfield Network based Adaptive routing - A design level
  approach for adaptive routing phenomena with Artificial Neural Network</title>
    <summary>  Routing, as a basic phenomena, by itself, has got umpteen scopes to analyse,
discuss and arrive at an optimal solution for the technocrats over years.
Routing is analysed based on many factors; few key constraints that decide the
factors are communication medium, time dependency, information source nature.
Parametric routing has become the requirement of the day, with some kind of
adaptation to the underlying network environment. Satellite constellations,
particularly LEO satellite constellations have become a reality in operational
to have a non-breaking voice/data communication around the world.Routing in
these constellations has to be treated in a non conventional way, taking their
network geometry into consideration. One of the efficient methods of
optimization is putting Neural Networks to use. Few Artificial Neural Network
models are very much suitable for the adaptive control mechanism, by their
nature of network arrangement. One such efficient model is Hopfield Network
model.
  This paper is an attempt to design a framework for the Hopfield Network based
adaptive routing phenomena in satellite constellations.
</summary>
    <author>
      <name>R. Shankar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">(13 pages, 7 figures, code)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0506032v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0506032v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0512018v2</id>
    <updated>2006-03-21T12:31:02Z</updated>
    <published>2005-12-05T06:57:39Z</published>
    <title>DAMNED: A Distributed and Multithreaded Neural Event-Driven simulation
  framework</title>
    <summary>  In a Spiking Neural Networks (SNN), spike emissions are sparsely and
irregularly distributed both in time and in the network architecture. Since a
current feature of SNNs is a low average activity, efficient implementations of
SNNs are usually based on an Event-Driven Simulation (EDS). On the other hand,
simulations of large scale neural networks can take advantage of distributing
the neurons on a set of processors (either workstation cluster or parallel
computer). This article presents DAMNED, a large scale SNN simulation framework
able to gather the benefits of EDS and parallel computing. Two levels of
parallelism are combined: Distributed mapping of the neural topology, at the
network level, and local multithreaded allocation of resources for simultaneous
processing of events, at the neuron level. Based on the causality of events, a
distributed solution is proposed for solving the complex problem of scheduling
without synchronization barrier.
</summary>
    <author>
      <name>Anthony Mouraud</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GRIMAAG, ISC</arxiv:affiliation>
    </author>
    <author>
      <name>Didier Puzenat</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">GRIMAAG</arxiv:affiliation>
    </author>
    <author>
      <name>Hélène Paugam-Moisy</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">ISC</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0512018v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0512018v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0512037v2</id>
    <updated>2005-12-13T09:52:53Z</updated>
    <published>2005-12-09T20:30:02Z</published>
    <title>Evolving Stochastic Learning Algorithm Based on Tsallis Entropic Index</title>
    <summary>  In this paper, inspired from our previous algorithm, which was based on the
theory of Tsallis statistical mechanics, we develop a new evolving stochastic
learning algorithm for neural networks. The new algorithm combines
deterministic and stochastic search steps by employing a different adaptive
stepsize for each network weight, and applies a form of noise that is
characterized by the nonextensive entropic index q, regulated by a weight decay
term. The behavior of the learning algorithm can be made more stochastic or
deterministic depending on the trade off between the temperature T and the q
values. This is achieved by introducing a formula that defines a
time--dependent relationship between these two important learning parameters.
Our experimental study verifies that there are indeed improvements in the
convergence speed of this new evolving stochastic learning algorithm, which
makes learning faster than using the original Hybrid Learning Scheme (HLS). In
addition, experiments are conducted to explore the influence of the entropic
index q and temperature T on the convergence speed and stability of the
proposed method.
</summary>
    <author>
      <name>Aristoklis D. Anastasiadis</name>
    </author>
    <author>
      <name>George D. Magoulas</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1140/epjb/e2006-00137-6</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1140/epjb/e2006-00137-6" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 11 figures, journal</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0512037v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0512037v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0512062v1</id>
    <updated>2005-12-15T15:05:22Z</updated>
    <published>2005-12-15T15:05:22Z</published>
    <title>Evolino for recurrent support vector machines</title>
    <summary>  Traditional Support Vector Machines (SVMs) need pre-wired finite time windows
to predict and classify time series. They do not have an internal state
necessary to deal with sequences involving arbitrary long-term dependencies.
Here we introduce a new class of recurrent, truly sequential SVM-like devices
with internal adaptive states, trained by a novel method called EVOlution of
systems with KErnel-based outputs (Evoke), an instance of the recent Evolino
class of methods. Evoke evolves recurrent neural networks to detect and
represent temporal dependencies while using quadratic programming/support
vector regression to produce precise outputs. Evoke is the first SVM-based
mechanism learning to classify a context-sensitive language. It also
outperforms recent state-of-the-art gradient-based recurrent neural networks
(RNNs) on various time series prediction tasks.
</summary>
    <author>
      <name>Juergen Schmidhuber</name>
    </author>
    <author>
      <name>Matteo Gagliolo</name>
    </author>
    <author>
      <name>Daan Wierstra</name>
    </author>
    <author>
      <name>Faustino Gomez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0512062v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0512062v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; I.2.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0602051v1</id>
    <updated>2006-02-14T10:30:36Z</updated>
    <published>2006-02-14T10:30:36Z</published>
    <title>On the utility of the multimodal problem generator for assessing the
  performance of Evolutionary Algorithms</title>
    <summary>  This paper looks in detail at how an evolutionary algorithm attempts to solve
instances from the multimodal problem generator. The paper shows that in order
to consistently reach the global optimum, an evolutionary algorithm requires a
population size that should grow at least linearly with the number of peaks. It
is also shown a close relationship between the supply and decision making
issues that have been identified previously in the context of population sizing
models for additively decomposable problems.
  The most important result of the paper, however, is that solving an instance
of the multimodal problem generator is like solving a peak-in-a-haystack, and
it is argued that evolutionary algorithms are not the best algorithms for such
a task. Finally, and as opposed to what several researchers have been doing, it
is our strong belief that the multimodal problem generator is not adequate for
assessing the performance of evolutionary algorithms.
</summary>
    <author>
      <name>Fernando G. Lobo</name>
    </author>
    <author>
      <name>Claudio F. Lima</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Also UALG-ILAB Report No. 200601</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0602051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0602051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0602055v1</id>
    <updated>2006-02-15T13:48:13Z</updated>
    <published>2006-02-15T13:48:13Z</published>
    <title>Revisiting Evolutionary Algorithms with On-the-Fly Population Size
  Adjustment</title>
    <summary>  In an evolutionary algorithm, the population has a very important role as its
size has direct implications regarding solution quality, speed, and
reliability. Theoretical studies have been done in the past to investigate the
role of population sizing in evolutionary algorithms. In addition to those
studies, several self-adjusting population sizing mechanisms have been proposed
in the literature. This paper revisits the latter topic and pays special
attention to the genetic algorithm with adaptive population size (APGA), for
which several researchers have claimed to be very effective at autonomously
(re)sizing the population.
  As opposed to those previous claims, this paper suggests a complete opposite
view. Specifically, it shows that APGA is not capable of adapting the
population size at all. This claim is supported on theoretical grounds and
confirmed by computer simulations.
</summary>
    <author>
      <name>Fernando G. Lobo</name>
    </author>
    <author>
      <name>Claudio F. Lima</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Also UALG-ILAB Report No. 200602</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0602055v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0602055v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0603004v1</id>
    <updated>2006-03-01T12:26:09Z</updated>
    <published>2006-03-01T12:26:09Z</published>
    <title>Lamarckian Evolution and the Baldwin Effect in Evolutionary Neural
  Networks</title>
    <summary>  Hybrid neuro-evolutionary algorithms may be inspired on Darwinian or
Lamarckian evolu- tion. In the case of Darwinian evolution, the Baldwin effect,
that is, the progressive incorporation of learned characteristics to the
genotypes, can be observed and leveraged to improve the search. The purpose of
this paper is to carry out an exper- imental study into how learning can
improve G-Prop genetic search. Two ways of combining learning and genetic
search are explored: one exploits the Baldwin effect, while the other uses a
Lamarckian strategy. Our experiments show that using a Lamarckian op- erator
makes the algorithm find networks with a low error rate, and the smallest size,
while using the Bald- win effect obtains MLPs with the smallest error rate, and
a larger size, taking longer to reach a solution. Both approaches obtain a
lower average error than other BP-based algorithms like RPROP, other evolu-
tionary methods and fuzzy logic based methods
</summary>
    <author>
      <name>P. A. Castillo</name>
    </author>
    <author>
      <name>M. G. Arenas</name>
    </author>
    <author>
      <name>J. G. Castellano</name>
    </author>
    <author>
      <name>J. J. Merelo</name>
    </author>
    <author>
      <name>A. Prieto</name>
    </author>
    <author>
      <name>V. Rivas</name>
    </author>
    <author>
      <name>G. Romero</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Presented in a Spanish conference, MAEB</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0603004v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0603004v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="C.1.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0603042v1</id>
    <updated>2006-03-09T17:35:31Z</updated>
    <published>2006-03-09T17:35:31Z</published>
    <title>The NoN Approach to Autonomic Face Recognition</title>
    <summary>  A method of autonomic face recognition based on the biologically plausible
network of networks (NoN) model of information processing is presented. The NoN
model is based on locally parallel and globally coordinated transformations in
which the neurons or computational units form distributed networks, which
themselves link to form larger networks. This models the structures in the
cerebral cortex described by Mountcastle and the architecture based on that
proposed for information processing by Sutton. In the proposed implementation,
face images are processed by a nested family of locally operating networks
along with a hierarchically superior network that classifies the information
from each of the local networks. The results of the experiments yielded a
maximum of 98.5% recognition accuracy and an average of 97.4% recognition
accuracy on a benchmark database.
</summary>
    <author>
      <name>Willie L. Scott II</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">15 pages, 6 figures, 6 tables</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0603042v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0603042v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0603090v2</id>
    <updated>2006-07-28T13:41:39Z</updated>
    <published>2006-03-22T22:52:23Z</published>
    <title>Topological Grammars for Data Approximation</title>
    <summary>  A method of {\it topological grammars} is proposed for multidimensional data
approximation. For data with complex topology we define a {\it principal cubic
complex} of low dimension and given complexity that gives the best
approximation for the dataset. This complex is a generalization of linear and
non-linear principal manifolds and includes them as particular cases. The
problem of optimal principal complex construction is transformed into a series
of minimization problems for quadratic functionals. These quadratic functionals
have a physically transparent interpretation in terms of elastic energy. For
the energy computation, the whole complex is represented as a system of nodes
and springs. Topologically, the principal complex is a product of
one-dimensional continuums (represented by graphs), and the grammars describe
how these continuums transform during the process of optimal complex
construction. This factorization of the whole process onto one-dimensional
transformations using minimization of quadratic energy functionals allow us to
construct efficient algorithms.
</summary>
    <author>
      <name>A. N. Gorban</name>
    </author>
    <author>
      <name>N. R. Sumner</name>
    </author>
    <author>
      <name>A. Y. Zinovyev</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.aml.2006.04.022</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.aml.2006.04.022" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Corrected Journal version, Appl. Math. Lett., in press. 7 pgs., 2
  figs</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Applied Mathematics Letters 20 (2007) 382--386</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0603090v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0603090v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0605065v4</id>
    <updated>2007-04-08T20:19:23Z</updated>
    <published>2006-05-15T17:56:55Z</published>
    <title>On the possible Computational Power of the Human Mind</title>
    <summary>  The aim of this paper is to address the question: Can an artificial neural
network (ANN) model be used as a possible characterization of the power of the
human mind? We will discuss what might be the relationship between such a model
and its natural counterpart. A possible characterization of the different power
capabilities of the mind is suggested in terms of the information contained (in
its computational complexity) or achievable by it. Such characterization takes
advantage of recent results based on natural neural networks (NNN) and the
computational power of arbitrary artificial neural networks (ANN). The possible
acceptance of neural networks as the model of the human mind's operation makes
the aforementioned quite relevant.
</summary>
    <author>
      <name>Hector Zenil</name>
    </author>
    <author>
      <name>Francisco Hernandez-Quiroz</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1142/9789812707420_0020</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1142/9789812707420_0020" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Complexity, Science and Society Conference, 2005, University of
  Liverpool, UK. 23 pages</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0605065v4" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0605065v4" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0606126v1</id>
    <updated>2006-06-29T22:33:31Z</updated>
    <published>2006-06-29T22:33:31Z</published>
    <title>May We Have Your Attention: Analysis of a Selective Attention Task</title>
    <summary>  In this paper we present a deeper analysis than has previously been carried
out of a selective attention problem, and the evolution of continuous-time
recurrent neural networks to solve it. We show that the task has a rich
structure, and agents must solve a variety of subproblems to perform well. We
consider the relationship between the complexity of an agent and the ease with
which it can evolve behavior that generalizes well across subproblems, and
demonstrate a shaping protocol that improves generalization.
</summary>
    <author>
      <name>Eldan Goldenberg</name>
    </author>
    <author>
      <name>Jacob R. Garcowski</name>
    </author>
    <author>
      <name>Randall D. Beer</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">In S. Schaal, A. Ijspeert, A. Billard, S. Vijayakumar, J. Hallam &amp;
  J-A. Meyer (Eds.), From Animals to Animats 8: Proceedings of the Eighth
  International Conference on the Simulation of Adaptive Behavior (pp 49-56).
  MIT Press</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0606126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0606126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0607048v1</id>
    <updated>2006-07-11T15:25:56Z</updated>
    <published>2006-07-11T15:25:56Z</published>
    <title>Evaluation de Techniques de Traitement des Refusés pour l'Octroi de
  Crédit</title>
    <summary>  We present the problem of "Reject Inference" for credit acceptance. Because
of the current legal framework (Basel II), credit institutions need to
industrialize their processes for credit acceptance, including Reject
Inference. We present here a methodology to compare various techniques of
Reject Inference and show that it is necessary, in the absence of real
theoretical results, to be able to produce and compare models adapted to
available data (selection of "best" model conditionnaly on data). We describe
some simulations run on a small data set to illustrate the approach and some
strategies for choosing the control group, which is the only valid approach to
Reject Inference.
</summary>
    <author>
      <name>Emmanuel Viennet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIPN</arxiv:affiliation>
    </author>
    <author>
      <name>Françoise Fogelman Soulié</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">KXEN</arxiv:affiliation>
    </author>
    <author>
      <name>Benoit Rognier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">KXEN</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">38i\`{e}mes Journ\'{e}es de Statistiques (2006) 105</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0607048v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0607048v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0609125v1</id>
    <updated>2006-09-22T12:40:07Z</updated>
    <published>2006-09-22T12:40:07Z</published>
    <title>Problem Evolution: A new approach to problem solving systems</title>
    <summary>  In this paper we present a novel tool to evaluate problem solving systems.
Instead of using a system to solve a problem, we suggest using the problem to
evaluate the system. By finding a numerical representation of a problem's
complexity, one can implement genetic algorithm to search for the most complex
problem the given system can solve. This allows a comparison between different
systems that solve the same set of problems. In this paper we implement this
approach on pattern recognition neural networks to try and find the most
complex pattern a given configuration can solve. The complexity of the pattern
is calculated using linguistic complexity. The results demonstrate the power of
the problem evolution approach in ranking different neural network
configurations according to their pattern recognition abilities. Future
research and implementations of this technique are also discussed.
</summary>
    <author>
      <name>Goren Gordon</name>
    </author>
    <author>
      <name>Uri Einziger-Lowicz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 2 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0609125v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0609125v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0610126v1</id>
    <updated>2006-10-20T16:37:11Z</updated>
    <published>2006-10-20T16:37:11Z</published>
    <title>Fitness Uniform Optimization</title>
    <summary>  In evolutionary algorithms, the fitness of a population increases with time
by mutating and recombining individuals and by a biased selection of more fit
individuals. The right selection pressure is critical in ensuring sufficient
optimization progress on the one hand and in preserving genetic diversity to be
able to escape from local optima on the other hand. Motivated by a universal
similarity relation on the individuals, we propose a new selection scheme,
which is uniform in the fitness values. It generates selection pressure toward
sparsely populated fitness regions, not necessarily toward higher fitness, as
is the case for all other selection schemes. We show analytically on a simple
example that the new selection scheme can be much more effective than standard
selection schemes. We also propose a new deletion scheme which achieves a
similar result via deletion and show how such a scheme preserves genetic
diversity more effectively than standard approaches. We compare the performance
of the new schemes to tournament selection and random deletion on an artificial
deceptive problem and a range of NP-hard problems: traveling salesman, set
covering and satisfiability.
</summary>
    <author>
      <name>Marcus Hutter</name>
    </author>
    <author>
      <name>Shane Legg</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/TEVC.2005.863127</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/TEVC.2005.863127" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">25 double-column pages, 12 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">IEEE Transactions on Evolutionary Computation, 10:5 (2006) 568-589</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0610126v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0610126v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611020v1</id>
    <updated>2006-11-05T01:15:01Z</updated>
    <published>2006-11-05T01:15:01Z</published>
    <title>An associative memory for the on-line recognition and prediction of
  temporal sequences</title>
    <summary>  This paper presents the design of an associative memory with feedback that is
capable of on-line temporal sequence learning. A framework for on-line sequence
learning has been proposed, and different sequence learning models have been
analysed according to this framework. The network model is an associative
memory with a separate store for the sequence context of a symbol. A sparse
distributed memory is used to gain scalability. The context store combines the
functionality of a neural layer with a shift register. The sensitivity of the
machine to the sequence context is controllable, resulting in different
characteristic behaviours. The model can store and predict on-line sequences of
various types and length. Numerical simulations on the model have been carried
out to determine its properties.
</summary>
    <author>
      <name>J. Bose</name>
    </author>
    <author>
      <name>S. B. Furber</name>
    </author>
    <author>
      <name>J. L. Shapiro</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/IJCNN.2005.1556028</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/IJCNN.2005.1556028" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Published in IJCNN 2005, Montreal, Canada</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0611020v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611020v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0611058v1</id>
    <updated>2006-11-14T13:19:46Z</updated>
    <published>2006-11-14T13:19:46Z</published>
    <title>Advances in Self Organising Maps</title>
    <summary>  The Self-Organizing Map (SOM) with its related extensions is the most popular
artificial neural algorithm for use in unsupervised learning, clustering,
classification and data visualization. Over 5,000 publications have been
reported in the open literature, and many commercial projects employ the SOM as
a tool for solving hard real-world problems. Each two years, the "Workshop on
Self-Organizing Maps" (WSOM) covers the new developments in the field. The WSOM
series of conferences was initiated in 1997 by Prof. Teuvo Kohonen, and has
been successfully organized in 1997 and 1999 by the Helsinki University of
Technology, in 2001 by the University of Lincolnshire and Humberside, and in
2003 by the Kyushu Institute of Technology. The Universit\'{e} Paris I
Panth\'{e}on Sorbonne (SAMOS-MATISSE research centre) organized WSOM 2005 in
Paris on September 5-8, 2005.
</summary>
    <author>
      <name>Marie Cottrell</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CES, SAMOS</arxiv:affiliation>
    </author>
    <author>
      <name>Michel Verleysen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DICE</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neunet.2006.05.011</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neunet.2006.05.011" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Special Issue of the Neural Networks Journal after WSOM 05 in Paris</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks Volume 19, Issues 6-7 (2006) 721-722</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0611058v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0611058v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.ST" scheme="http://arxiv.org/schemas/atom"/>
    <category term="nlin.AO" scheme="http://arxiv.org/schemas/atom"/>
    <category term="stat.TH" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0612104v2</id>
    <updated>2007-05-05T02:25:05Z</updated>
    <published>2006-12-21T02:18:17Z</published>
    <title>Sufficient Conditions for Coarse-Graining Evolutionary Dynamics</title>
    <summary>  It is commonly assumed that the ability to track the frequencies of a set of
schemata in the evolving population of an infinite population genetic algorithm
(IPGA) under different fitness functions will advance efforts to obtain a
theory of adaptation for the simple GA. Unfortunately, for IPGAs with long
genomes and non-trivial fitness functions there do not currently exist
theoretical results that allow such a study. We develop a simple framework for
analyzing the dynamics of an infinite population evolutionary algorithm (IPEA).
This framework derives its simplicity from its abstract nature. In particular
we make no commitment to the data-structure of the genomes, the kind of
variation performed, or the number of parents involved in a variation
operation. We use this framework to derive abstract conditions under which the
dynamics of an IPEA can be coarse-grained. We then use this result to derive
concrete conditions under which it becomes computationally feasible to closely
approximate the frequencies of a family of schemata of relatively low order
over multiple generations, even when the bitstsrings in the evolving population
of the IPGA are long.
</summary>
    <author>
      <name>Keki Burjorjee</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">19 pages, 1 figure. Accepted to the Foundations of Genetic Algorithms
  Conference 2007 (FOGA IX)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0612104v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0612104v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8; F.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0701003v1</id>
    <updated>2006-12-30T11:48:32Z</updated>
    <published>2006-12-30T11:48:32Z</published>
    <title>Magnification Laws of Winner-Relaxing and Winner-Enhancing Kohonen
  Feature Maps</title>
    <summary>  Self-Organizing Maps are models for unsupervised representation formation of
cortical receptor fields by stimuli-driven self-organization in laterally
coupled winner-take-all feedforward structures. This paper discusses
modifications of the original Kohonen model that were motivated by a potential
function, in their ability to set up a neural mapping of maximal mutual
information. Enhancing the winner update, instead of relaxing it, results in an
algorithm that generates an infomax map corresponding to magnification exponent
of one. Despite there may be more than one algorithm showing the same
magnification exponent, the magnification law is an experimentally accessible
quantity and therefore suitable for quantitative description of neural
optimization principles.
</summary>
    <author>
      <name>Jens Christian Claussen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">University Kiel</arxiv:affiliation>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, 3 figures. ESMTB 2002 Milano. For the extended journal
  version see cond-mat/0208414</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">pp. 17-22 in : V. Capasso (Ed.): Mathematical Modeling &amp; Computing
  in Biology and Medicine, Miriam Series, Progetto Leonardo, Bologna (2003)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0701003v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0701003v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.IT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="math.IT" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702038v1</id>
    <updated>2007-02-07T05:54:00Z</updated>
    <published>2007-02-07T05:54:00Z</published>
    <title>Genetic Representations for Evolutionary Minimization of Network Coding
  Resources</title>
    <summary>  We demonstrate how a genetic algorithm solves the problem of minimizing the
resources used for network coding, subject to a throughput constraint, in a
multicast scenario. A genetic algorithm avoids the computational complexity
that makes the problem NP-hard and, for our experiments, greatly improves on
sub-optimal solutions of established methods. We compare two different genotype
encodings, which tradeoff search space size with fitness landscape, as well as
the associated genetic operators. Our finding favors a smaller encoding despite
its fewer intermediate solutions and demonstrates the impact of the modularity
enforced by genetic operators on the performance of the algorithm.
</summary>
    <author>
      <name>Minkyu Kim</name>
    </author>
    <author>
      <name>Varun Aggarwal</name>
    </author>
    <author>
      <name>Una-May O'Reilly</name>
    </author>
    <author>
      <name>Muriel Medard</name>
    </author>
    <author>
      <name>Wonsik Kim</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 3 figures, accepted to the 4th European Workshop on the
  Application of Nature-Inspired Techniques to Telecommunication Networks and
  Other Connected Systems (EvoCOMNET 2007)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0702038v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702038v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0702096v1</id>
    <updated>2007-02-16T21:47:04Z</updated>
    <published>2007-02-16T21:47:04Z</published>
    <title>Overcoming Hierarchical Difficulty by Hill-Climbing the Building Block
  Structure</title>
    <summary>  The Building Block Hypothesis suggests that Genetic Algorithms (GAs) are
well-suited for hierarchical problems, where efficient solving requires proper
problem decomposition and assembly of solution from sub-solution with strong
non-linear interdependencies. The paper proposes a hill-climber operating over
the building block (BB) space that can efficiently address hierarchical
problems. The new Building Block Hill-Climber (BBHC) uses past hill-climb
experience to extract BB information and adapts its neighborhood structure
accordingly. The perpetual adaptation of the neighborhood structure allows the
method to climb the hierarchical structure solving successively the
hierarchical levels. It is expected that for fully non deceptive hierarchical
BB structures the BBHC can solve hierarchical problems in linearithmic time.
Empirical results confirm that the proposed method scales almost linearly with
the problem size thus clearly outperforms population based recombinative
methods.
</summary>
    <author>
      <name>David Iclanzan</name>
    </author>
    <author>
      <name>Dan Dumitrescu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">submited to GECCO 2007 (jan 31)</arxiv:comment>
    <link href="http://arxiv.org/abs/cs/0702096v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0702096v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6; I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/cs/0703002v9</id>
    <updated>2010-02-05T12:03:19Z</updated>
    <published>2007-02-28T22:52:43Z</published>
    <title>Integral Biomathics: A Post-Newtonian View into the Logos of Bios (On
  the New Meaning, Relations and Principles of Life in Science)</title>
    <summary>  This work is an attempt for a state-of-the-art survey of natural and life
sciences with the goal to define the scope and address the central questions of
an original research program. It is focused on the phenomena of emergence,
adaptive dynamics and evolution of self-assembling, self-organizing,
self-maintaining and self-replicating biosynthetic systems viewed from a
newly-arranged perspective and understanding of computation and communication
in the living nature.
</summary>
    <author>
      <name>Plamen L. Simeonov</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.pbiomolbio.2010.01.005</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.pbiomolbio.2010.01.005" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">85 pages, 6 figures</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Progress in Biophysics and Molecular Biology, Vol. 102, Issues
  2-3, 2010, pp. 85-121</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/cs/0703002v9" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/cs/0703002v9" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CC" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.1.1; F.4.0; H.1.1; I.2.0; J.3" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.0047v1</id>
    <updated>2007-04-01T13:06:50Z</updated>
    <published>2007-04-01T13:06:50Z</published>
    <title>Intelligent location of simultaneously active acoustic emission sources:
  Part I</title>
    <summary>  The intelligent acoustic emission locator is described in Part I, while Part
II discusses blind source separation, time delay estimation and location of two
simultaneously active continuous acoustic emission sources.
  The location of acoustic emission on complicated aircraft frame structures is
a difficult problem of non-destructive testing. This article describes an
intelligent acoustic emission source locator. The intelligent locator comprises
a sensor antenna and a general regression neural network, which solves the
location problem based on learning from examples. Locator performance was
tested on different test specimens. Tests have shown that the accuracy of
location depends on sound velocity and attenuation in the specimen, the
dimensions of the tested area, and the properties of stored data. The location
accuracy achieved by the intelligent locator is comparable to that obtained by
the conventional triangulation method, while the applicability of the
intelligent locator is more general since analysis of sonic ray paths is
avoided. This is a promising method for non-destructive testing of aircraft
frame structures by the acoustic emission method.
</summary>
    <author>
      <name>T. Kosel</name>
    </author>
    <author>
      <name>I. Grabec</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 5 eps figures, uses IEEEtran.cls</arxiv:comment>
    <link href="http://arxiv.org/abs/0704.0047v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.0047v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.0050v1</id>
    <updated>2007-04-01T18:53:13Z</updated>
    <published>2007-04-01T18:53:13Z</published>
    <title>Intelligent location of simultaneously active acoustic emission sources:
  Part II</title>
    <summary>  Part I describes an intelligent acoustic emission locator, while Part II
discusses blind source separation, time delay estimation and location of two
continuous acoustic emission sources.
  Acoustic emission (AE) analysis is used for characterization and location of
developing defects in materials. AE sources often generate a mixture of various
statistically independent signals. A difficult problem of AE analysis is
separation and characterization of signal components when the signals from
various sources and the mode of mixing are unknown. Recently, blind source
separation (BSS) by independent component analysis (ICA) has been used to solve
these problems. The purpose of this paper is to demonstrate the applicability
of ICA to locate two independent simultaneously active acoustic emission
sources on an aluminum band specimen. The method is promising for
non-destructive testing of aircraft frame structures by acoustic emission
analysis.
</summary>
    <author>
      <name>T. Kosel</name>
    </author>
    <author>
      <name>I. Grabec</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages, 7 eps figures, uses IEEEtran.cls</arxiv:comment>
    <link href="http://arxiv.org/abs/0704.0050v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.0050v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.0985v1</id>
    <updated>2007-04-07T13:40:49Z</updated>
    <published>2007-04-07T13:40:49Z</published>
    <title>Architecture for Pseudo Acausal Evolvable Embedded Systems</title>
    <summary>  Advances in semiconductor technology are contributing to the increasing
complexity in the design of embedded systems. Architectures with novel
techniques such as evolvable nature and autonomous behavior have engrossed lot
of attention. This paper demonstrates conceptually evolvable embedded systems
can be characterized basing on acausal nature. It is noted that in acausal
systems, future input needs to be known, here we make a mechanism such that the
system predicts the future inputs and exhibits pseudo acausal nature. An
embedded system that uses theoretical framework of acausality is proposed. Our
method aims at a novel architecture that features the hardware evolability and
autonomous behavior alongside pseudo acausality. Various aspects of this
architecture are discussed in detail along with the limitations.
</summary>
    <author>
      <name>Mohd Abubakr</name>
    </author>
    <author>
      <name>R. M. Vinay</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages, 2 figures. Submitted to SASO 2007</arxiv:comment>
    <link href="http://arxiv.org/abs/0704.0985v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.0985v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.1198v1</id>
    <updated>2007-04-10T13:36:44Z</updated>
    <published>2007-04-10T13:36:44Z</published>
    <title>A Doubly Distributed Genetic Algorithm for Network Coding</title>
    <summary>  We present a genetic algorithm which is distributed in two novel ways: along
genotype and temporal axes. Our algorithm first distributes, for every member
of the population, a subset of the genotype to each network node, rather than a
subset of the population to each. This genotype distribution is shown to offer
a significant gain in running time. Then, for efficient use of the
computational resources in the network, our algorithm divides the candidate
solutions into pipelined sets and thus the distribution is in the temporal
domain, rather that in the spatial domain. This temporal distribution may lead
to temporal inconsistency in selection and replacement, however our experiments
yield better efficiency in terms of the time to convergence without incurring
significant penalties.
</summary>
    <author>
      <name>Minkyu Kim</name>
    </author>
    <author>
      <name>Varun Aggarwal</name>
    </author>
    <author>
      <name>Una-May O'Reilly</name>
    </author>
    <author>
      <name>Muriel Medard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 7 figures, accepted to the Genetic and Evolutionary
  Computation Conference (GECCO 2007)</arxiv:comment>
    <link href="http://arxiv.org/abs/0704.1198v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.1198v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.2725v2</id>
    <updated>2007-12-07T03:06:49Z</updated>
    <published>2007-04-20T15:58:04Z</published>
    <title>Exploiting Heavy Tails in Training Times of Multilayer Perceptrons: A
  Case Study with the UCI Thyroid Disease Database</title>
    <summary>  The random initialization of weights of a multilayer perceptron makes it
possible to model its training process as a Las Vegas algorithm, i.e. a
randomized algorithm which stops when some required training error is obtained,
and whose execution time is a random variable. This modeling is used to perform
a case study on a well-known pattern recognition benchmark: the UCI Thyroid
Disease Database. Empirical evidence is presented of the training time
probability distribution exhibiting a heavy tail behavior, meaning a big
probability mass of long executions. This fact is exploited to reduce the
training time cost by applying two simple restart strategies. The first assumes
full knowledge of the distribution yielding a 40% cut down in expected time
with respect to the training without restarts. The second, assumes null
knowledge, yielding a reduction ranging from 9% to 23%.
</summary>
    <author>
      <name>Manuel Cebrian</name>
    </author>
    <author>
      <name>Ivan Cantador</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages, 4 figures, submitted for consideration to the "Statistics
  and Its Interface" journal</arxiv:comment>
    <link href="http://arxiv.org/abs/0704.2725v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.2725v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0704.3780v1</id>
    <updated>2007-04-28T06:52:19Z</updated>
    <published>2007-04-28T06:52:19Z</published>
    <title>Stochastic Optimization Algorithms</title>
    <summary>  When looking for a solution, deterministic methods have the enormous
advantage that they do find global optima. Unfortunately, they are very
CPU-intensive, and are useless on untractable NP-hard problems that would
require thousands of years for cutting-edge computers to explore. In order to
get a result, one needs to revert to stochastic algorithms, that sample the
search space without exploring it thoroughly. Such algorithms can find very
good results, without any guarantee that the global optimum has been reached;
but there is often no other choice than using them. This chapter is a short
introduction to the main methods used in stochastic optimization.
</summary>
    <author>
      <name>Pierre Collet</name>
    </author>
    <author>
      <name>Jean-Philippe Rennard</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, 4 figures, 2 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Rennard, J.-P., Handbook of Research on Nature Inspired Computing
  for Economics and Management, IGR, 2006</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0704.3780v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0704.3780v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.0457v1</id>
    <updated>2007-06-04T16:08:22Z</updated>
    <published>2007-06-04T16:08:22Z</published>
    <title>Challenges and Opportunities of Evolutionary Robotics</title>
    <summary>  Robotic hardware designs are becoming more complex as the variety and number
of on-board sensors increase and as greater computational power is provided in
ever-smaller packages on-board robots. These advances in hardware, however, do
not automatically translate into better software for controlling complex
robots. Evolutionary techniques hold the potential to solve many difficult
problems in robotics which defy simple conventional approaches, but present
many challenges as well. Numerous disciplines including artificial life,
cognitive science and neural networks, rule-based systems, behavior-based
control, genetic algorithms and other forms of evolutionary computation have
contributed to shaping the current state of evolutionary robotics. This paper
provides an overview of developments in the emerging field of evolutionary
robotics, and discusses some of the opportunities and challenges which
currently face practitioners in the field.
</summary>
    <author>
      <name>D. A. Sofge</name>
    </author>
    <author>
      <name>M. A. Potter</name>
    </author>
    <author>
      <name>M. D. Bugajska</name>
    </author>
    <author>
      <name>A. C. Schultz</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">D.A. Sofge, M.A. Potter, M.D. Bugajska, and A.C. Schultz,
  "Challenges and Opportunities of Evolutionary Robotics." In Proc. 2nd Int'l
  Conf. on Computational Intelligence, Robotics, and Autonomous Systems, 2003</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0706.0457v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.0457v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.1051v1</id>
    <updated>2007-06-07T18:13:59Z</updated>
    <published>2007-06-07T18:13:59Z</published>
    <title>Improved Neural Modeling of Real-World Systems Using Genetic Algorithm
  Based Variable Selection</title>
    <summary>  Neural network models of real-world systems, such as industrial processes,
made from sensor data must often rely on incomplete data. System states may not
all be known, sensor data may be biased or noisy, and it is not often known
which sensor data may be useful for predictive modelling. Genetic algorithms
may be used to help to address this problem by determining the near optimal
subset of sensor variables most appropriate to produce good models. This paper
describes the use of genetic search to optimize variable selection to determine
inputs into the neural network model. We discuss genetic algorithm
implementation issues including data representation types and genetic operators
such as crossover and mutation. We present the use of this technique for neural
network modelling of a typical industrial application, a liquid fed ceramic
melter, and detail the results of the genetic search to optimize the neural
network model for this application.
</summary>
    <author>
      <name>Donald A. Sofge</name>
    </author>
    <author>
      <name>David L. Elliott</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">4 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">D. Sofge and D. Elliott, "Improved Neural Modeling of Real-World
  Systems Using Genetic Algorithm Based Variable Selection," In Int'l Conf. on
  Neural Networks and Brain (ICNN&amp;B'98-Beijing), 1998</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0706.1051v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.1051v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0706.1061v1</id>
    <updated>2007-06-07T19:27:12Z</updated>
    <published>2007-06-07T19:27:12Z</published>
    <title>Design, Implementation, and Cooperative Coevolution of an Autonomous/
  Teleoperated Control System for a Serpentine Robotic Manipulator</title>
    <summary>  Design, implementation, and machine learning issues associated with
developing a control system for a serpentine robotic manipulator are explored.
The controller developed provides autonomous control of the serpentine robotic
manipulatorduring operation of the manipulator within an enclosed environment
such as an underground storage tank. The controller algorithms make use of both
low-level joint angle control employing force/position feedback constraints,
and high-level coordinated control of end-effector positioning. This approach
has resulted in both high-level full robotic control and low-level telerobotic
control modes, and provides a high level of dexterity for the operator.
</summary>
    <author>
      <name>Donald Sofge</name>
    </author>
    <author>
      <name>Gerald Chiang</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">D. Sofge and G. Chiang, "Design, ... a Serpentine Automated Waste
  Retrieval Manipulator," Amer. Nucl. Soc. 9th Top. Meeting on Robotics and
  Remote Systems, 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0706.1061v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0706.1061v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.RO" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.0548v1</id>
    <updated>2007-07-04T06:57:52Z</updated>
    <published>2007-07-04T06:57:52Z</published>
    <title>From Royal Road to Epistatic Road for Variable Length Evolution
  Algorithm</title>
    <summary>  Although there are some real world applications where the use of variable
length representation (VLR) in Evolutionary Algorithm is natural and suitable,
an academic framework is lacking for such representations. In this work we
propose a family of tunable fitness landscapes based on VLR of genotypes. The
fitness landscapes we propose possess a tunable degree of both neutrality and
epistasis; they are inspired, on the one hand by the Royal Road fitness
landscapes, and the other hand by the NK fitness landscapes. So these
landscapes offer a scale of continuity from Royal Road functions, with
neutrality and no epistasis, to landscapes with a large amount of epistasis and
no redundancy. To gain insight into these fitness landscapes, we first use
standard tools such as adaptive walks and correlation length. Second, we
evaluate the performances of evolutionary algorithms on these landscapes for
various values of the neutral and the epistatic parameters; the results allow
us to correlate the performances with the expected degrees of neutrality and
epistasis.
</summary>
    <author>
      <name>Michael Defoin Platel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Sebastien Verel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Manuel Clergue</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Collard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Lecture notes in computer science (Lect. notes comput. sci.) ISSN
  0302-9743 (27/10/2003) 3-14</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0707.0548v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.0548v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.0641v1</id>
    <updated>2007-07-04T15:30:54Z</updated>
    <published>2007-07-04T15:30:54Z</published>
    <title>Where are Bottlenecks in NK Fitness Landscapes?</title>
    <summary>  Usually the offspring-parent fitness correlation is used to visualize and
analyze some caracteristics of fitness landscapes such as evolvability. In this
paper, we introduce a more general representation of this correlation, the
Fitness Cloud (FC). We use the bottleneck metaphor to emphasise fitness levels
in landscape that cause local search process to slow down. For a local search
heuristic such as hill-climbing or simulated annealing, FC allows to visualize
bottleneck and neutrality of landscapes. To confirm the relevance of the FC
representation we show where the bottlenecks are in the well-know NK fitness
landscape and also how to use neutrality information from the FC to combine
some neutral operator with local search heuristic.
</summary>
    <author>
      <name>Sébastien Verel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Collard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Manuel Clergue</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CEC.2003.1299585</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CEC.2003.1299585" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Evolutionary Computation, 2003. CEC'03 (08/12/2003) 273--280</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0707.0641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.0641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.0643v1</id>
    <updated>2007-07-04T15:36:35Z</updated>
    <published>2007-07-04T15:36:35Z</published>
    <title>Scuba Search : when selection meets innovation</title>
    <summary>  We proposed a new search heuristic using the scuba diving metaphor. This
approach is based on the concept of evolvability and tends to exploit
neutrality in fitness landscape. Despite the fact that natural evolution does
not directly select for evolvability, the basic idea behind the scuba search
heuristic is to explicitly push the evolvability to increase. The search
process switches between two phases: Conquest-of-the-Waters and
Invasion-of-the-Land. A comparative study of the new algorithm and standard
local search heuristics on the NKq-landscapes has shown advantage and limit of
the scuba search. To enlighten qualitative differences between neutral search
processes, the space is changed into a connected graph to visualize the
pathways that the search is likely to follow.
</summary>
    <author>
      <name>Sébastien Verel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Philippe Collard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Manuel Clergue</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/CEC.2004.1330960</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/CEC.2004.1330960" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Evolutionary Computation, 2004. CEC2004 (23/06/2004) 924 - 931</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0707.0643v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.0643v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.0652v1</id>
    <updated>2007-07-04T16:12:17Z</updated>
    <published>2007-07-04T16:12:17Z</published>
    <title>How to use the Scuba Diving metaphor to solve problem with neutrality ?</title>
    <summary>  We proposed a new search heuristic using the scuba diving metaphor. This
approach is based on the concept of evolvability and tends to exploit
neutrality which exists in many real-world problems. Despite the fact that
natural evolution does not directly select for evolvability, the basic idea
behind the scuba search heuristic is to explicitly push evolvability to
increase. A comparative study of the scuba algorithm and standard local search
heuristics has shown the advantage and the limitation of the scuba search. In
order to tune neutrality, we use the NKq fitness landscapes and a family of
travelling salesman problems (TSP) where cities are randomly placed on a
lattice and where travel distance between cities is computed with the Manhattan
metric. In this last problem the amount of neutrality varies with the city
concentration on the grid ; assuming the concentration below one, this TSP
reasonably remains a NP-hard problem.
</summary>
    <author>
      <name>Philippe Collard</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Sébastien Verel</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <author>
      <name>Manuel Clergue</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">I3S</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">ECAI'2004 (27/08/2004) 166-170</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0707.0652v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.0652v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.3030v1</id>
    <updated>2007-07-20T10:07:27Z</updated>
    <published>2007-07-20T10:07:27Z</published>
    <title>Optimal Design of Ad Hoc Injection Networks by Using Genetic Algorithms</title>
    <summary>  This work aims at optimizing injection networks, which consist in adding a
set of long-range links (called bypass links) in mobile multi-hop ad hoc
networks so as to improve connectivity and overcome network partitioning. To
this end, we rely on small-world network properties, that comprise a high
clustering coefficient and a low characteristic path length. We investigate the
use of two genetic algorithms (generational and steady-state) to optimize three
instances of this topology control problem and present results that show
initial evidence of their capacity to solve it.
</summary>
    <author>
      <name>Gregoire Danoy</name>
    </author>
    <author>
      <name>Pascal Bouvry</name>
    </author>
    <author>
      <name>Matthias R. Brust</name>
    </author>
    <author>
      <name>Enrique Alba</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">1 page, 1 figure</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Genetic and Evolutionary Computation Conference (GECCO 2007), ISBN
  978-1-59593-697-4</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0707.3030v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.3030v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.8" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0707.3979v1</id>
    <updated>2007-07-26T18:03:23Z</updated>
    <published>2007-07-26T18:03:23Z</published>
    <title>Clifford Algebra of the Vector Space of Conics for decision boundary
  Hyperplanes in m-Euclidean Space</title>
    <summary>  In this paper we embed $m$-dimensional Euclidean space in the geometric
algebra $Cl_m $ to extend the operators of incidence in ${R^m}$ to operators of
incidence in the geometric algebra to generalize the notion of separator to a
decision boundary hyperconic in the Clifford algebra of hyperconic sections
denoted as ${Cl}({Co}_{2})$. This allows us to extend the concept of a linear
perceptron or the spherical perceptron in conformal geometry and introduce the
more general conic perceptron, namely the {elliptical perceptron}. Using
Clifford duality a vector orthogonal to the decision boundary hyperplane is
determined. Experimental results are shown in 2-dimensional Euclidean space
where we separate data that are naturally separated by some typical plane conic
separators by this procedure. This procedure is more general in the sense that
it is independent of the dimension of the input data and hence we can speak of
the hyperconic elliptic perceptron.
</summary>
    <author>
      <name>Isidro B. Nieto</name>
    </author>
    <author>
      <name>J. Refugio Vallejo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 2 figures, 1 table</arxiv:comment>
    <link href="http://arxiv.org/abs/0707.3979v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0707.3979v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1; I.5.2; I.2.6; I.3.5" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.3461v1</id>
    <updated>2007-09-21T15:20:07Z</updated>
    <published>2007-09-21T15:20:07Z</published>
    <title>Fast Algorithm and Implementation of Dissimilarity Self-Organizing Maps</title>
    <summary>  In many real world applications, data cannot be accurately represented by
vectors. In those situations, one possible solution is to rely on dissimilarity
measures that enable sensible comparison between observations. Kohonen's
Self-Organizing Map (SOM) has been adapted to data described only through their
dissimilarity matrix. This algorithm provides both non linear projection and
clustering of non vector data. Unfortunately, the algorithm suffers from a high
cost that makes it quite difficult to use with voluminous data sets. In this
paper, we propose a new algorithm that provides an important reduction of the
theoretical cost of the dissimilarity SOM without changing its outcome (the
results are exactly the same as the ones obtained with the original algorithm).
Moreover, we introduce implementation methods that result in very short running
times. Improvements deduced from the theoretical cost model are validated on
simulated and real world data (a word list clustering problem). We also
demonstrate that the proposed implementation methods reduce by a factor up to 3
the running time of the fast algorithm over a standard implementation.
</summary>
    <author>
      <name>Brieuc Conan-Guez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LITA</arxiv:affiliation>
    </author>
    <author>
      <name>Fabrice Rossi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <author>
      <name>Aïcha El Golli</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neunet.2006.05.002</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neunet.2006.05.002" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neural Networks 19, 6-7 (2006) 855-863</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0709.3461v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.3461v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0709.3641v1</id>
    <updated>2007-09-23T14:10:08Z</updated>
    <published>2007-09-23T14:10:08Z</published>
    <title>Representation of Functional Data in Neural Networks</title>
    <summary>  Functional Data Analysis (FDA) is an extension of traditional data analysis
to functional data, for example spectra, temporal series, spatio-temporal
images, gesture recognition data, etc. Functional data are rarely known in
practice; usually a regular or irregular sampling is known. For this reason,
some processing is needed in order to benefit from the smooth character of
functional data in the analysis methods. This paper shows how to extend the
Radial-Basis Function Networks (RBFN) and Multi-Layer Perceptron (MLP) models
to functional data inputs, in particular when the latter are known through
lists of input-output pairs. Various possibilities for functional processing
are discussed, including the projection on smooth bases, Functional Principal
Component Analysis, functional centering and reduction, and the use of
differential operators. It is shown how to incorporate these functional
processing into the RBFN and MLP models. The functional approach is illustrated
on a benchmark of spectrometric data analysis.
</summary>
    <author>
      <name>Fabrice Rossi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE</arxiv:affiliation>
    </author>
    <author>
      <name>Nicolas Delannay</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DICE - MLG</arxiv:affiliation>
    </author>
    <author>
      <name>Brieuc Conan-Guez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE</arxiv:affiliation>
    </author>
    <author>
      <name>Michel Verleysen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DICE - MLG</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.neucom.2004.11.012</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.neucom.2004.11.012" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Also available online from:
  http://www.sciencedirect.com/science/journal/09252312</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Neurocomputing 64 (2005) 183--210</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0709.3641v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0709.3641v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.2782v2</id>
    <updated>2007-10-16T13:27:03Z</updated>
    <published>2007-10-15T15:28:47Z</published>
    <title>Effective linkage learning using low-order statistics and clustering</title>
    <summary>  The adoption of probabilistic models for the best individuals found so far is
a powerful approach for evolutionary computation. Increasingly more complex
models have been used by estimation of distribution algorithms (EDAs), which
often result better effectiveness on finding the global optima for hard
optimization problems. Supervised and unsupervised learning of Bayesian
networks are very effective options, since those models are able to capture
interactions of high order among the variables of a problem. Diversity
preservation, through niching techniques, has also shown to be very important
to allow the identification of the problem structure as much as for keeping
several global optima. Recently, clustering was evaluated as an effective
niching technique for EDAs, but the performance of simpler low-order EDAs was
not shown to be much improved by clustering, except for some simple multimodal
problems. This work proposes and evaluates a combination operator guided by a
measure from information theory which allows a clustered low-order EDA to
effectively solve a comprehensive range of benchmark optimization problems.
</summary>
    <author>
      <name>Leonardo Emmendorfer</name>
    </author>
    <author>
      <name>Aurora Pozo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Submitted to IEEE Transactions on Evolutionary Computation</arxiv:comment>
    <link href="http://arxiv.org/abs/0710.2782v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.2782v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0710.4182v1</id>
    <updated>2007-10-23T03:43:57Z</updated>
    <published>2007-10-23T03:43:57Z</published>
    <title>Beyond Feedforward Models Trained by Backpropagation: a Practical
  Training Tool for a More Efficient Universal Approximator</title>
    <summary>  Cellular Simultaneous Recurrent Neural Network (SRN) has been shown to be a
function approximator more powerful than the MLP. This means that the
complexity of MLP would be prohibitively large for some problems while SRN
could realize the desired mapping with acceptable computational constraints.
The speed of training of complex recurrent networks is crucial to their
successful application. Present work improves the previous results by training
the network with extended Kalman filter (EKF). We implemented a generic
Cellular SRN and applied it for solving two challenging problems: 2D maze
navigation and a subset of the connectedness problem. The speed of convergence
has been improved by several orders of magnitude in comparison with the earlier
results in the case of maze navigation, and superior generalization has been
demonstrated in the case of connectedness. The implications of this
improvements are discussed.
</summary>
    <author>
      <name>Roman Ilin</name>
    </author>
    <author>
      <name>Robert Kozma</name>
    </author>
    <author>
      <name>Paul J. Werbos</name>
    </author>
    <link href="http://arxiv.org/abs/0710.4182v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0710.4182v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.2478v1</id>
    <updated>2007-11-15T18:19:39Z</updated>
    <published>2007-11-15T18:19:39Z</published>
    <title>A Compact Self-organizing Cellular Automata-based Genetic Algorithm</title>
    <summary>  A Genetic Algorithm (GA) is proposed in which each member of the population
can change schemata only with its neighbors according to a rule. The rule
methodology and the neighborhood structure employ elements from the Cellular
Automata (CA) strategies. Each member of the GA population is assigned to a
cell and crossover takes place only between adjacent cells, according to the
predefined rule. Although combinations of CA and GA approaches have appeared
previously, here we rely on the inherent self-organizing features of CA, rather
than on parallelism. This conceptual shift directs us toward the evolution of
compact populations containing only a handful of members. We find that the
resulting algorithm can search the design space more efficiently than
traditional GA strategies due to its ability to exploit mutations within this
compact self-organizing population. Consequently, premature convergence is
avoided and the final results often are more accurate. In order to reinforce
the superior mutation capability, a re-initialization strategy also is
implemented. Ten test functions and two benchmark structural engineering truss
design problems are examined in order to demonstrate the performance of the
method.
</summary>
    <author>
      <name>Vasileios Barmpoutis</name>
    </author>
    <author>
      <name>Gary F. Dargush</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">24 pages, 18 figures, Submitted to Evolutionary Computation</arxiv:comment>
    <link href="http://arxiv.org/abs/0711.2478v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.2478v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.2897v1</id>
    <updated>2007-11-19T11:24:47Z</updated>
    <published>2007-11-19T11:24:47Z</published>
    <title>Estimation of fuzzy anomalies in Water Distribution Systems</title>
    <summary>  State estimation is necessary in diagnosing anomalies in Water Demand Systems
(WDS). In this paper we present a neural network performing such a task. State
estimation is performed by using optimization, which tries to reconcile all the
available information. Quantification of the uncertainty of the input data
(telemetry measures and demand predictions) can be achieved by means of robust
estate estimation. Using a mathematical model of the network, fuzzy estimated
states for anomalous states of the network can be obtained. They are used to
train a neural network capable of assessing WDS anomalies associated with
particular sets of measurements.
</summary>
    <author>
      <name>J. Izquierdo</name>
    </author>
    <author>
      <name>M. M. Tung</name>
    </author>
    <author>
      <name>R. Perez</name>
    </author>
    <author>
      <name>F. J. Martinez</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">5 pages</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Progress in Industrial Mathematics at ECMI 2006 (edited by L. L.
  Bonilla, M. A. Moscoso, G. Platero, and J. M. Vega), vol. 12 of Mathematics
  in Industry, pp. 801-805 (Springer, Berlin, 2007), ISBN 978-3-540-71991-5</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0711.2897v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.2897v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.5.1" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0711.3591v2</id>
    <updated>2008-03-03T17:14:51Z</updated>
    <published>2007-11-22T15:16:21Z</published>
    <title>An Estimation of Distribution Algorithm with Intelligent Local Search
  for Rule-based Nurse Rostering</title>
    <summary>  This paper proposes a new memetic evolutionary algorithm to achieve explicit
learning in rule-based nurse rostering, which involves applying a set of
heuristic rules for each nurse's assignment. The main framework of the
algorithm is an estimation of distribution algorithm, in which an ant-miner
methodology improves the individual solutions produced in each generation.
Unlike our previous work (where learning is implicit), the learning in the
memetic estimation of distribution algorithm is explicit, i.e. we are able to
identify building blocks directly. The overall approach learns by building a
probabilistic model, i.e. an estimation of the probability distribution of
individual nurse-rule pairs that are used to construct schedules. The local
search processor (i.e. the ant-miner) reinforces nurse-rule pairs that receive
higher rewards. A challenging real world nurse rostering problem is used as the
test problem. Computational results show that the proposed approach outperforms
most existing approaches. It is suggested that the learning methodologies
suggested in this paper may be applied to other scheduling problems where
schedules are built systematically according to specific rules
</summary>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Edmund Burke</name>
    </author>
    <author>
      <name>Jingpeng Li</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1057/palgrave.jors.2602308</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1057/palgrave.jors.2602308" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of the Operational Research Society, 58 (12), pp
  1574-1585, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0711.3591v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0711.3591v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.1863v1</id>
    <updated>2007-12-12T03:58:01Z</updated>
    <published>2007-12-12T03:58:01Z</published>
    <title>Constructing Bio-molecular Databases on a DNA-based Computer</title>
    <summary>  Codd [Codd 1970] wrote the first paper in which the model of a relational
database was proposed. Adleman [Adleman 1994] wrote the first paper in which
DNA strands in a test tube were used to solve an instance of the Hamiltonian
path problem. From [Adleman 1994], it is obviously indicated that for storing
information in molecules of DNA allows for an information density of
approximately 1 bit per cubic nm (nanometer) and a dramatic improvement over
existing storage media such as video tape which store information at a density
of approximately 1 bit per 1012 cubic nanometers. This paper demonstrates that
biological operations can be applied to construct bio-molecular databases where
data records in relational tables are encoded as DNA strands. In order to
achieve the goal, DNA algorithms are proposed to perform eight operations of
relational algebra (calculus) on bio-molecular relational databases, which
include Cartesian product, union, set difference, selection, projection,
intersection, join and division. Furthermore, this work presents clear evidence
of the ability of molecular computing to perform data retrieval operations on
bio-molecular relational databases.
</summary>
    <author>
      <name>Weng-Long Chang</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shan-Hui</arxiv:affiliation>
    </author>
    <author>
      <name> Michael</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">Shan-Hui</arxiv:affiliation>
    </author>
    <author>
      <name> Ho</name>
    </author>
    <author>
      <name>Minyi Guo</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">The article includes 35 pages, several tables and figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0712.1863v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.1863v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="q-bio.OT" scheme="http://arxiv.org/schemas/atom"/>
    <category term="H.3.0; H.3.3; D.3.0; D.3.1; D.3.m" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.3654v1</id>
    <updated>2007-12-21T10:05:52Z</updated>
    <published>2007-12-21T10:05:52Z</published>
    <title>Improving the Performance of PieceWise Linear Separation Incremental
  Algorithms for Practical Hardware Implementations</title>
    <summary>  In this paper we shall review the common problems associated with Piecewise
Linear Separation incremental algorithms. This kind of neural models yield poor
performances when dealing with some classification problems, due to the
evolving schemes used to construct the resulting networks. So as to avoid this
undesirable behavior we shall propose a modification criterion. It is based
upon the definition of a function which will provide information about the
quality of the network growth process during the learning phase. This function
is evaluated periodically as the network structure evolves, and will permit, as
we shall show through exhaustive benchmarks, to considerably improve the
performance(measured in terms of network complexity and generalization
capabilities) offered by the networks generated by these incremental models.
</summary>
    <author>
      <name>Alejandro Chinea Manrique De Lara</name>
    </author>
    <author>
      <name>Juan Manuel Moreno</name>
    </author>
    <author>
      <name>Arostegui Jordi Madrenas</name>
    </author>
    <author>
      <name>Joan Cabestany</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">10 pages, 1 figure, 3 tables</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Biological and Artificial Computation: From Neuroscience to
  Technology, J.Mira, R.Moreno-Diaz, J.Cabestany (eds.), pp. 607-616,
  Springer-Verlag, 1997</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0712.3654v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.3654v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.3973v1</id>
    <updated>2007-12-24T07:31:58Z</updated>
    <published>2007-12-24T07:31:58Z</published>
    <title>GUIDE: Unifying Evolutionary Engines through a Graphical User Interface</title>
    <summary>  Many kinds of Evolutionary Algorithms (EAs) have been described in the
literature since the last 30 years. However, though most of them share a common
structure, no existing software package allows the user to actually shift from
one model to another by simply changing a few parameters, e.g. in a single
window of a Graphical User Interface. This paper presents GUIDE, a Graphical
User Interface for DREAM Experiments that, among other user-friendly features,
unifies all kinds of EAs into a single panel, as far as evolution parameters
are concerned. Such a window can be used either to ask for one of the well
known ready-to-use algorithms, or to very easily explore new combinations that
have not yet been studied. Another advantage of grouping all necessary elements
to describe virtually all kinds of EAs is that it creates a fantastic pedagogic
tool to teach EAs to students and newcomers to the field.
</summary>
    <author>
      <name>Pierre Collet</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">LIL</arxiv:affiliation>
    </author>
    <author>
      <name>Marc Schoenauer</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Dans Evolution Artificielle 2936 (2003) 203-215</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0712.3973v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.3973v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.4099v3</id>
    <updated>2009-09-21T00:39:27Z</updated>
    <published>2007-12-26T04:13:20Z</published>
    <title>Digital Ecosystems: Optimisation by a Distributed Intelligence</title>
    <summary>  Can intelligence optimise Digital Ecosystems? How could a distributed
intelligence interact with the ecosystem dynamics? Can the software components
that are part of genetic selection be intelligent in themselves, as in an
adaptive technology? We consider the effect of a distributed intelligence
mechanism on the evolutionary and ecological dynamics of our Digital Ecosystem,
which is the digital counterpart of a biological ecosystem for evolving
software services in a distributed network. We investigate Neural Networks and
Support Vector Machine for the learning based pattern recognition functionality
of our distributed intelligence. Simulation results imply that the Digital
Ecosystem performs better with the application of a distributed intelligence,
marginally more effectively when powered by Support Vector Machine than Neural
Networks, and suggest that it can contribute to optimising the operation of our
Digital Ecosystem.
</summary>
    <author>
      <name>G. Briscoe</name>
    </author>
    <author>
      <name>P. De Wilde</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">12 pages, 14 figures, conference</arxiv:comment>
    <link href="http://arxiv.org/abs/0712.4099v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.4099v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.4153v2</id>
    <updated>2008-10-28T09:19:00Z</updated>
    <published>2007-12-26T21:56:52Z</published>
    <title>Biology of Applied Digital Ecosystems</title>
    <summary>  A primary motivation for our research in Digital Ecosystems is the desire to
exploit the self-organising properties of biological ecosystems. Ecosystems are
thought to be robust, scalable architectures that can automatically solve
complex, dynamic problems. However, the biological processes that contribute to
these properties have not been made explicit in Digital Ecosystems research.
Here, we discuss how biological properties contribute to the self-organising
features of biological ecosystems, including population dynamics, evolution, a
complex dynamic environment, and spatial distributions for generating local
interactions. The potential for exploiting these properties in artificial
systems is then considered. We suggest that several key features of biological
ecosystems have not been fully explored in existing digital ecosystems, and
discuss how mimicking these features may assist in developing robust, scalable
self-organising architectures. An example architecture, the Digital Ecosystem,
is considered in detail. The Digital Ecosystem is then measured experimentally
through simulations, with measures originating from theoretical ecology, to
confirm its likeness to a biological ecosystem. Including the responsiveness to
requests for applications from the user base, as a measure of the 'ecological
succession' (development).
</summary>
    <author>
      <name>G. Briscoe</name>
    </author>
    <author>
      <name>S. Sadedin</name>
    </author>
    <author>
      <name>G. Paperin</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1109/DEST.2007.372015</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1109/DEST.2007.372015" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">9 pages, 4 figure, conference</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In IEEE First International Conference on Digital Ecosystems and
  Technologies, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0712.4153v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.4153v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.MA" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0712.4159v5</id>
    <updated>2012-11-27T15:39:35Z</updated>
    <published>2007-12-26T23:32:10Z</published>
    <title>Creating a Digital Ecosystem: Service-Oriented Architectures with
  Distributed Evolutionary Computing</title>
    <summary>  We start with a discussion of the relevant literature, including Nature
Inspired Computing as a framework in which to understand this work, and the
process of biomimicry to be used in mimicking the necessary biological
processes to create Digital Ecosystems. We then consider the relevant
theoretical ecology in creating the digital counterpart of a biological
ecosystem, including the topological structure of ecosystems, and evolutionary
processes within distributed environments. This leads to a discussion of the
relevant fields from computer science for the creation of Digital Ecosystems,
including evolutionary computing, Multi-Agent Systems, and Service-Oriented
Architectures. We then define Ecosystem-Oriented Architectures for the creation
of Digital Ecosystems, imbibed with the properties of self-organisation and
scalability from biological ecosystems, including a novel form of distributed
evolutionary computing.
</summary>
    <author>
      <name>G Briscoe</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">This has been withdrawn by the author due to an error in using
  presentation notes in submission</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">In JavaOne Conference, 2006, BOF-0759</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0712.4159v5" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0712.4159v5" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.3111v1</id>
    <updated>2008-01-21T00:20:50Z</updated>
    <published>2008-01-21T00:20:50Z</published>
    <title>Analysis of Estimation of Distribution Algorithms and Genetic Algorithms
  on NK Landscapes</title>
    <summary>  This study analyzes performance of several genetic and evolutionary
algorithms on randomly generated NK fitness landscapes with various values of n
and k. A large number of NK problem instances are first generated for each n
and k, and the global optimum of each instance is obtained using the
branch-and-bound algorithm. Next, the hierarchical Bayesian optimization
algorithm (hBOA), the univariate marginal distribution algorithm (UMDA), and
the simple genetic algorithm (GA) with uniform and two-point crossover
operators are applied to all generated instances. Performance of all algorithms
is then analyzed and compared, and the results are discussed.
</summary>
    <author>
      <name>Martin Pelikan</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">Also available at the MEDAL web site, http://medal.cs.umsl.edu/</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Genetic and Evolutionary Computation Conference
  (GECCO-2008), ACM Press, 1033-1040</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.3111v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.3111v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="I.2.6; I.2.8; G.1.6" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.3209v2</id>
    <updated>2008-03-03T17:10:36Z</updated>
    <published>2008-01-21T15:55:22Z</published>
    <title>A Pyramidal Evolutionary Algorithm with Different Inter-Agent Partnering
  Strategies for Scheduling Problems</title>
    <summary>  This paper combines the idea of a hierarchical distributed genetic algorithm
with different inter-agent partnering strategies. Cascading clusters of
sub-populations are built from bottom up, with higher-level sub-populations
optimising larger parts of the problem. Hence higher-level sub-populations
search a larger search space with a lower resolution whilst lower-level
sub-populations search a smaller search space with a higher resolution. The
effects of different partner selection schemes amongst the agents on solution
quality are examined for two multiple-choice optimisation problems. It is shown
that partnering strategies that exploit problem-specific knowledge are superior
and can counter inappropriate (sub-) fitness measurements.
</summary>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Genetic and Evolutionary Computation Conference
  (GECCO 2001), late-breaking papers volume, pp 1-8, San Francisco, USA</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.3209v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.3209v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.3539v3</id>
    <updated>2008-05-16T10:42:42Z</updated>
    <published>2008-01-23T09:59:06Z</published>
    <title>On the Effects of Idiotypic Interactions for Recommendation Communities
  in Artificial Immune Systems</title>
    <summary>  It has previously been shown that a recommender based on immune system
idiotypic principles can out perform one based on correlation alone. This paper
reports the results of work in progress, where we undertake some investigations
into the nature of this beneficial effect. The initial findings are that the
immune system recommender tends to produce different neighbourhoods, and that
the superior performance of this recommender is due partly to the different
neighbourhoods, and partly to the way that the idiotypic effect is used to
weight each neighbours recommendations.
</summary>
    <author>
      <name>Steve Cayzer</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 1st International Conference on Artificial
  Immune Systems (ICARIS 2002), pp 154-160, Canterbury, UK, 2001</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.3539v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.3539v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.3547v2</id>
    <updated>2008-03-03T17:09:24Z</updated>
    <published>2008-01-23T10:42:49Z</published>
    <title>A Recommender System based on the Immune Network</title>
    <summary>  The immune system is a complex biological system with a highly distributed,
adaptive and self-organising nature. This paper presents an artificial immune
system (AIS) that exploits some of these characteristics and is applied to the
task of film recommendation by collaborative filtering (CF). Natural evolution
and in particular the immune system have not been designed for classical
optimisation. However, for this problem, we are not interested in finding a
single optimum. Rather we intend to identify a sub-set of good matches on which
recommendations can be based. It is our hypothesis that an AIS built on two
central aspects of the biological immune system will be an ideal candidate to
achieve this: Antigen - antibody interaction for matching and antibody -
antibody interaction for diversity. Computational results are presented in
support of this conjecture and compared to those found by other CF techniques.
</summary>
    <author>
      <name>Steve Cazyer</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the IEEE Congress on Evolutionary Computation (CEC
  2002), pp 807-813, Honolulu, USA, 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.3547v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.3547v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.3549v3</id>
    <updated>2008-05-16T10:45:49Z</updated>
    <published>2008-01-23T11:01:31Z</published>
    <title>The Danger Theory and Its Application to Artificial Immune Systems</title>
    <summary>  Over the last decade, a new idea challenging the classical self-non-self
viewpoint has become popular amongst immunologists. It is called the Danger
Theory. In this conceptual paper, we look at this theory from the perspective
of Artificial Immune System practitioners. An overview of the Danger Theory is
presented with particular emphasis on analogies in the Artificial Immune
Systems world. A number of potential application areas are then used to provide
a framing for a critical assessment of the concept, and its relevance for
Artificial Immune Systems.
</summary>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Steve Cayzer</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 1st International Conference on Artificial
  Immune Systems (ICARIS 2002), pp 141-148, Canterbury, Uk, 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.3549v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.3549v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.3550v2</id>
    <updated>2008-03-03T17:08:00Z</updated>
    <published>2008-01-23T11:12:39Z</published>
    <title>Partnering Strategies for Fitness Evaluation in a Pyramidal Evolutionary
  Algorithm</title>
    <summary>  This paper combines the idea of a hierarchical distributed genetic algorithm
with different inter-agent partnering strategies. Cascading clusters of
sub-populations are built from bottom up, with higher-level sub-populations
optimising larger parts of the problem. Hence higher-level sub-populations
search a larger search space with a lower resolution whilst lower-level
sub-populations search a smaller search space with a higher resolution. The
effects of different partner selection schemes for (sub-)fitness evaluation
purposes are examined for two multiple-choice optimisation problems. It is
shown that random partnering strategies perform best by providing better
sampling and more diversity.
</summary>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Larry Bull</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the Genetic and Evolutionary Computation Conference
  (GECCO 2002), pp 263-270, New York, USA, 2002</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.3550v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.3550v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.4119v3</id>
    <updated>2008-05-16T10:43:30Z</updated>
    <published>2008-01-28T15:36:56Z</published>
    <title>Strategic Alert Throttling for Intrusion Detection Systems</title>
    <summary>  Network intrusion detection systems are themselves becoming targets of
attackers. Alert flood attacks may be used to conceal malicious activity by
hiding it among a deluge of false alerts sent by the attacker. Although these
types of attacks are very hard to stop completely, our aim is to present
techniques that improve alert throughput and capacity to such an extent that
the resources required to successfully mount the attack become prohibitive. The
key idea presented is to combine a token bucket filter with a realtime
correlation algorithm. The proposed algorithm throttles alert output from the
IDS when an attack is detected. The attack graph used in the correlation
algorithm is used to make sure that alerts crucial to forming strategies are
not discarded by throttling.
</summary>
    <author>
      <name>Gianni Tedesco</name>
    </author>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">4th WSEAS International Conference on Information Security (WSEAS
  2005), Tenerife, Spain, 2005</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.4119v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.4119v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.4307v3</id>
    <updated>2008-05-16T10:44:44Z</updated>
    <published>2008-01-28T15:14:45Z</published>
    <title>On Affinity Measures for Artificial Immune System Movie Recommenders</title>
    <summary>  We combine Artificial Immune Systems 'AIS', technology with Collaborative
Filtering 'CF' and use it to build a movie recommendation system. We already
know that Artificial Immune Systems work well as movie recommenders from
previous work by Cayzer and Aickelin 3, 4, 5. Here our aim is to investigate
the effect of different affinity measure algorithms for the AIS. Two different
affinity measures, Kendalls Tau and Weighted Kappa, are used to calculate the
correlation coefficients for the movie recommender. We compare the results with
those published previously and show that Weighted Kappa is more suitable than
others for movie problems. We also show that AIS are generally robust movie
recommenders and that, as long as a suitable affinity measure is chosen,
results are good.
</summary>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Qi Chen</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Proceedings of the 5th International Conference on Recent Advances
  in Soft Computing (RASC 2004), Nottingham, UK</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.4307v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.4307v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CY" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.4312v3</id>
    <updated>2008-05-16T10:43:07Z</updated>
    <published>2008-01-28T15:26:59Z</published>
    <title>Investigating Artificial Immune Systems For Job Shop Rescheduling In
  Changing Environments</title>
    <summary>  Artificial immune system can be used to generate schedules in changing
environments and it has been proven to be more robust than schedules developed
using a genetic algorithm. Good schedules can be produced especially when the
number of the antigens is increased. However, an increase in the range of the
antigens had somehow affected the fitness of the immune system. In this
research, we are trying to improve the result of the system by rescheduling the
same problem using the same method while at the same time maintaining the
robustness of the schedules.
</summary>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Edmund Burke</name>
    </author>
    <author>
      <name>Aniza Din</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">6th International Conference in Adaptive Computing in Design and
  Manufacture (ACDM 2004), Bristol, UK, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.4312v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.4312v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0801.4314v3</id>
    <updated>2008-05-16T10:46:24Z</updated>
    <published>2008-01-28T15:32:05Z</published>
    <title>Artificial Immune Systems (AIS) - A New Paradigm for Heuristic Decision
  Making</title>
    <summary>  Over the last few years, more and more heuristic decision making techniques
have been inspired by nature, e.g. evolutionary algorithms, ant colony
optimisation and simulated annealing. More recently, a novel computational
intelligence technique inspired by immunology has emerged, called Artificial
Immune Systems (AIS). This immune system inspired technique has already been
useful in solving some computational problems. In this keynote, we will very
briefly describe the immune system metaphors that are relevant to AIS. We will
then give some illustrative real-world problems suitable for AIS use and show a
step-by-step algorithm walkthrough. A comparison of AIS to other well-known
algorithms and areas for future work will round this keynote off. It should be
noted that as AIS is still a young and evolving field, there is not yet a fixed
algorithm template and hence actual implementations might differ somewhat from
the examples given here.
</summary>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Invited Keynote Talk, Annual Operational Research Conference 46,
  York, UK, 2004</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0801.4314v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0801.4314v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.0251v1</id>
    <updated>2008-02-02T15:09:42Z</updated>
    <published>2008-02-02T15:09:42Z</published>
    <title>Multi-Layer Perceptrons and Symbolic Data</title>
    <summary>  In some real world situations, linear models are not sufficient to represent
accurately complex relations between input variables and output variables of a
studied system. Multilayer Perceptrons are one of the most successful
non-linear regression tool but they are unfortunately restricted to inputs and
outputs that belong to a normed vector space. In this chapter, we propose a
general recoding method that allows to use symbolic data both as inputs and
outputs to Multilayer Perceptrons. The recoding is quite simple to implement
and yet provides a flexible framework that allows to deal with almost all
practical cases. The proposed method is illustrated on a real world data set.
</summary>
    <author>
      <name>Fabrice Rossi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis, CEREMADE</arxiv:affiliation>
    </author>
    <author>
      <name>Brieuc Conan-Guez</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis, LITA</arxiv:affiliation>
    </author>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Symbolic Data Analysis and the SODAS Software Wiley (Ed.) (2008)
  373-391</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.0251v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.0251v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.0287v1</id>
    <updated>2008-02-03T19:02:49Z</updated>
    <published>2008-02-03T19:02:49Z</published>
    <title>A data-driven functional projection approach for the selection of
  feature ranges in spectra with ICA or cluster analysis</title>
    <summary>  Prediction problems from spectra are largely encountered in chemometry. In
addition to accurate predictions, it is often needed to extract information
about which wavelengths in the spectra contribute in an effective way to the
quality of the prediction. This implies to select wavelengths (or wavelength
intervals), a problem associated to variable selection. In this paper, it is
shown how this problem may be tackled in the specific case of smooth (for
example infrared) spectra. The functional character of the spectra (their
smoothness) is taken into account through a functional variable projection
procedure. Contrarily to standard approaches, the projection is performed on a
basis that is driven by the spectra themselves, in order to best fit their
characteristics. The methodology is illustrated by two examples of functional
projection, using Independent Component Analysis and functional variable
clustering, respectively. The performances on two standard infrared spectra
benchmarks are illustrated.
</summary>
    <author>
      <name>Catherine Krier</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DICE</arxiv:affiliation>
    </author>
    <author>
      <name>Fabrice Rossi</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">INRIA Rocquencourt / INRIA Sophia Antipolis</arxiv:affiliation>
    </author>
    <author>
      <name>Damien François</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">CESAME</arxiv:affiliation>
    </author>
    <author>
      <name>Michel Verleysen</name>
      <arxiv:affiliation xmlns:arxiv="http://arxiv.org/schemas/atom">DICE - MLG</arxiv:affiliation>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.chemolab.2007.09.004</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.chemolab.2007.09.004" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">A paraitre</arxiv:comment>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Chemometrics and Intelligent Laboratory Systems (2008)</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.0287v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.0287v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.1412v1</id>
    <updated>2008-02-11T11:12:06Z</updated>
    <published>2008-02-11T11:12:06Z</published>
    <title>Extreme Learning Machine for land cover classification</title>
    <summary>  This paper explores the potential of extreme learning machine based
supervised classification algorithm for land cover classification. In
comparison to a backpropagation neural network, which requires setting of
several user-defined parameters and may produce local minima, extreme learning
machine require setting of one parameter and produce a unique solution. ETM+
multispectral data set (England) was used to judge the suitability of extreme
learning machine for remote sensing classifications. A back propagation neural
network was used to compare its performance in term of classification accuracy
and computational cost. Results suggest that the extreme learning machine
perform equally well to back propagation neural network in term of
classification accuracy with this data set. The computational cost using
extreme learning machine is very small in comparison to back propagation neural
network.
</summary>
    <author>
      <name>Mahesh Pal</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1080/01431160902788636</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1080/01431160902788636" rel="related"/>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">6 pages, mapindia 2008 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/0802.1412v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.1412v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.2001v3</id>
    <updated>2008-05-16T10:44:23Z</updated>
    <published>2008-02-14T11:25:37Z</published>
    <title>Exploiting problem structure in a genetic algorithm approach to a nurse
  rostering problem</title>
    <summary>  There is considerable interest in the use of genetic algorithms to solve
problems arising in the areas of scheduling and timetabling. However, the
classical genetic algorithm paradigm is not well equipped to handle the
conflict between objectives and constraints that typically occurs in such
problems. In order to overcome this, successful implementations frequently make
use of problem specific knowledge. This paper is concerned with the development
of a GA for a nurse rostering problem at a major UK hospital. The structure of
the constraints is used as the basis for a co-evolutionary strategy using
co-operating sub-populations. Problem specific knowledge is also used to define
a system of incentives and disincentives, and a complementary mutation
operator. Empirical results based on 52 weeks of live data show how these
features are able to improve an unsuccessful canonical GA to the point where it
is able to provide a practical solution to the problem
</summary>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Kathryn Dowsland</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1002/(SICI)1099-1425(200005/06)3:3&lt;139::AID-JOS41&gt;3.0.CO;2-2</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1002/(SICI)1099-1425(200005/06)3:3&lt;139::AID-JOS41&gt;3.0.CO;2-2" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Journal of Scheduling, 3(3), pp 139-153, 2000</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.2001v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.2001v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.2411v1</id>
    <updated>2008-02-18T03:47:45Z</updated>
    <published>2008-02-18T03:47:45Z</published>
    <title>Multiclass Approaches for Support Vector Machine Based Land Cover
  Classification</title>
    <summary>  SVMs were initially developed to perform binary classification; though,
applications of binary classification are very limited. Most of the practical
applications involve multiclass classification, especially in remote sensing
land cover classification. A number of methods have been proposed to implement
SVMs to produce multiclass classification. A number of methods to generate
multiclass SVMs from binary SVMs have been proposed by researchers and is still
a continuing research topic. This paper compares the performance of six
multi-class approaches to solve classification problem with remote sensing data
in term of classification accuracy and computational cost. One vs. one, one vs.
rest, Directed Acyclic Graph (DAG), and Error Corrected Output Coding (ECOC)
based multiclass approaches creates many binary classifiers and combines their
results to determine the class label of a test pixel. Another catogery of multi
class approach modify the binary class objective function and allows
simultaneous computation of multiclass classification by solving a single
optimisation problem. Results from this study conclude the usefulness of One
vs. One multi class approach in term of accuracy and computational cost over
other multi class approaches.
</summary>
    <author>
      <name>Mahesh Pal</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">16 pages, MapIndia 2005 conference</arxiv:comment>
    <link href="http://arxiv.org/abs/0802.2411v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.2411v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CV" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0802.4002v3</id>
    <updated>2008-05-16T10:45:07Z</updated>
    <published>2008-02-27T12:15:08Z</published>
    <title>Sensing Danger: Innate Immunology for Intrusion Detection</title>
    <summary>  The immune system provides an ideal metaphor for anomaly detection in general
and computer security in particular. Based on this idea, artificial immune
systems have been used for a number of years for intrusion detection,
unfortunately so far with little success. However, these previous systems were
largely based on immunological theory from the 1970s and 1980s and over the
last decade our understanding of immunological processes has vastly improved.
In this paper we present two new immune inspired algorithms based on the latest
immunological discoveries, such as the behaviour of Dendritic Cells. The
resultant algorithms are applied to real world intrusion problems and show
encouraging results. Overall, we believe there is a bright future for these
next generation artificial immune algorithms.
</summary>
    <author>
      <name>Uwe Aickelin</name>
    </author>
    <author>
      <name>Julie Greensmith</name>
    </author>
    <arxiv:doi xmlns:arxiv="http://arxiv.org/schemas/atom">10.1016/j.istr.2007.10.003</arxiv:doi>
    <link title="doi" href="http://dx.doi.org/10.1016/j.istr.2007.10.003" rel="related"/>
    <arxiv:journal_ref xmlns:arxiv="http://arxiv.org/schemas/atom">Information Security Technical Report, 12(4), pp 218-227, 2007</arxiv:journal_ref>
    <link href="http://arxiv.org/abs/0802.4002v3" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0802.4002v3" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CR" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/0804.0352v1</id>
    <updated>2008-04-02T13:45:51Z</updated>
    <published>2008-04-02T13:45:51Z</published>
    <title>Permeability Analysis based on information granulation theory</title>
    <summary>  This paper describes application of information granulation theory, on the
analysis of "lugeon data". In this manner, using a combining of Self Organizing
Map (SOM) and Neuro-Fuzzy Inference System (NFIS), crisp and fuzzy granules are
obtained. Balancing of crisp granules and sub- fuzzy granules, within non fuzzy
information (initial granulation), is rendered in open-close iteration. Using
two criteria, "simplicity of rules "and "suitable adaptive threshold error
level", stability of algorithm is guaranteed. In other part of paper, rough set
theory (RST), to approximate analysis, has been employed &gt;.Validation of the
proposed methods, on the large data set of in-situ permeability in rock masses,
in the Shivashan dam, Iran, has been highlighted. By the implementation of the
proposed algorithm on the lugeon data set, was proved the suggested method,
relating the approximate analysis on the permeability, could be applied.
</summary>
    <author>
      <name>M. Sharifzadeh</name>
    </author>
    <author>
      <name>H. Owladeghaffari</name>
    </author>
    <author>
      <name>K. Shahriar</name>
    </author>
    <author>
      <name>E. Bakhtavar</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">8 pages,7 figures</arxiv:comment>
    <link href="http://arxiv.org/abs/0804.0352v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/0804.0352v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.NE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
    <category term="F.4.1; K.3.2" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
